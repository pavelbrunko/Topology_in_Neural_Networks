{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract MLP's matrix of weights for news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download all needed dependencies\n",
    "# ! pip install git+https://github.com/aimclub/eXplain-NNs &> /dev/null\n",
    "# ! pip install torchmetrics &> /dev/null\n",
    "# ! pip install giotto-ph==0.2.2 &> /dev/null\n",
    "# ! pip install giotto-tda==0.6.0 &> /dev/null\n",
    "# ! pip install umap-learn==0.5.3 &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all necessary libraries\n",
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import eXNN.topology\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "pd.set_option('display.max_columns', 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>data_channel_is_lifestyle</th>\n",
       "      <th>data_channel_is_entertainment</th>\n",
       "      <th>data_channel_is_bus</th>\n",
       "      <th>data_channel_is_socmed</th>\n",
       "      <th>data_channel_is_tech</th>\n",
       "      <th>data_channel_is_world</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>kw_max_min</th>\n",
       "      <th>kw_avg_min</th>\n",
       "      <th>kw_min_max</th>\n",
       "      <th>kw_max_max</th>\n",
       "      <th>kw_avg_max</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>kw_max_avg</th>\n",
       "      <th>kw_avg_avg</th>\n",
       "      <th>self_reference_min_shares</th>\n",
       "      <th>self_reference_max_shares</th>\n",
       "      <th>self_reference_avg_sharess</th>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <th>weekday_is_saturday</th>\n",
       "      <th>weekday_is_sunday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>LDA_00</th>\n",
       "      <th>LDA_01</th>\n",
       "      <th>LDA_02</th>\n",
       "      <th>LDA_03</th>\n",
       "      <th>LDA_04</th>\n",
       "      <th>global_subjectivity</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <th>rate_positive_words</th>\n",
       "      <th>rate_negative_words</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.680365</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>496.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>496.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500331</td>\n",
       "      <td>0.378279</td>\n",
       "      <td>0.040005</td>\n",
       "      <td>0.041263</td>\n",
       "      <td>0.040123</td>\n",
       "      <td>0.521617</td>\n",
       "      <td>0.092562</td>\n",
       "      <td>0.045662</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.378636</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.913725</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.799756</td>\n",
       "      <td>0.050047</td>\n",
       "      <td>0.050096</td>\n",
       "      <td>0.050101</td>\n",
       "      <td>0.050001</td>\n",
       "      <td>0.341246</td>\n",
       "      <td>0.148948</td>\n",
       "      <td>0.043137</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.286915</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.393365</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>918.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>918.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.217792</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>0.033351</td>\n",
       "      <td>0.033334</td>\n",
       "      <td>0.682188</td>\n",
       "      <td>0.702222</td>\n",
       "      <td>0.323333</td>\n",
       "      <td>0.056872</td>\n",
       "      <td>0.009479</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.495833</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.404896</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028573</td>\n",
       "      <td>0.419300</td>\n",
       "      <td>0.494651</td>\n",
       "      <td>0.028905</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.429850</td>\n",
       "      <td>0.100705</td>\n",
       "      <td>0.041431</td>\n",
       "      <td>0.020716</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.385965</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>731.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.682836</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>545.0</td>\n",
       "      <td>16000.0</td>\n",
       "      <td>3151.157895</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028633</td>\n",
       "      <td>0.028794</td>\n",
       "      <td>0.028575</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.885427</td>\n",
       "      <td>0.513502</td>\n",
       "      <td>0.281003</td>\n",
       "      <td>0.074627</td>\n",
       "      <td>0.012127</td>\n",
       "      <td>0.860215</td>\n",
       "      <td>0.139785</td>\n",
       "      <td>0.411127</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39639</th>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>346.0</td>\n",
       "      <td>0.529052</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.684783</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.523121</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>671.0</td>\n",
       "      <td>173.125</td>\n",
       "      <td>26900.0</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>374962.500000</td>\n",
       "      <td>2514.742857</td>\n",
       "      <td>4004.342857</td>\n",
       "      <td>3031.115764</td>\n",
       "      <td>11400.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>37033.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025038</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.151701</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.773260</td>\n",
       "      <td>0.482679</td>\n",
       "      <td>0.141964</td>\n",
       "      <td>0.037572</td>\n",
       "      <td>0.014451</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.333791</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.260000</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39640</th>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>0.696296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.885057</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>4.405488</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>616.0</td>\n",
       "      <td>184.000</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>192985.714286</td>\n",
       "      <td>1664.267857</td>\n",
       "      <td>5470.168651</td>\n",
       "      <td>3411.660830</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029349</td>\n",
       "      <td>0.028575</td>\n",
       "      <td>0.231866</td>\n",
       "      <td>0.681635</td>\n",
       "      <td>0.028575</td>\n",
       "      <td>0.564374</td>\n",
       "      <td>0.194249</td>\n",
       "      <td>0.039634</td>\n",
       "      <td>0.009146</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.374825</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.211111</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39641</th>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>442.0</td>\n",
       "      <td>0.516355</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.644128</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.076923</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>691.0</td>\n",
       "      <td>168.250</td>\n",
       "      <td>6200.0</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>295850.000000</td>\n",
       "      <td>1753.882353</td>\n",
       "      <td>6880.687034</td>\n",
       "      <td>4206.439195</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.159004</td>\n",
       "      <td>0.025025</td>\n",
       "      <td>0.025207</td>\n",
       "      <td>0.643794</td>\n",
       "      <td>0.146970</td>\n",
       "      <td>0.510296</td>\n",
       "      <td>0.024609</td>\n",
       "      <td>0.033937</td>\n",
       "      <td>0.024887</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.423077</td>\n",
       "      <td>0.307273</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.356439</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39642</th>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>682.0</td>\n",
       "      <td>0.539493</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.692661</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.975073</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>254600.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3384.316871</td>\n",
       "      <td>1777.895883</td>\n",
       "      <td>452.0</td>\n",
       "      <td>452.0</td>\n",
       "      <td>452.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040004</td>\n",
       "      <td>0.040003</td>\n",
       "      <td>0.839987</td>\n",
       "      <td>0.040002</td>\n",
       "      <td>0.040004</td>\n",
       "      <td>0.358578</td>\n",
       "      <td>-0.008066</td>\n",
       "      <td>0.020528</td>\n",
       "      <td>0.023460</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.236851</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.205246</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.012500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39643</th>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>0.701987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.471338</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>23.500</td>\n",
       "      <td>205600.0</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>366200.000000</td>\n",
       "      <td>3035.080555</td>\n",
       "      <td>3613.512953</td>\n",
       "      <td>3296.909481</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>2100.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050001</td>\n",
       "      <td>0.799339</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050659</td>\n",
       "      <td>0.050001</td>\n",
       "      <td>0.517893</td>\n",
       "      <td>0.104892</td>\n",
       "      <td>0.063694</td>\n",
       "      <td>0.012739</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.247338</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39644 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       timedelta  n_tokens_title  n_tokens_content  n_unique_tokens  \\\n",
       "0          731.0            12.0             219.0         0.663594   \n",
       "1          731.0             9.0             255.0         0.604743   \n",
       "2          731.0             9.0             211.0         0.575130   \n",
       "3          731.0             9.0             531.0         0.503788   \n",
       "4          731.0            13.0            1072.0         0.415646   \n",
       "...          ...             ...               ...              ...   \n",
       "39639        8.0            11.0             346.0         0.529052   \n",
       "39640        8.0            12.0             328.0         0.696296   \n",
       "39641        8.0            10.0             442.0         0.516355   \n",
       "39642        8.0             6.0             682.0         0.539493   \n",
       "39643        8.0            10.0             157.0         0.701987   \n",
       "\n",
       "       n_non_stop_words  n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  \\\n",
       "0                   1.0                  0.815385        4.0             2.0   \n",
       "1                   1.0                  0.791946        3.0             1.0   \n",
       "2                   1.0                  0.663866        3.0             1.0   \n",
       "3                   1.0                  0.665635        9.0             0.0   \n",
       "4                   1.0                  0.540890       19.0            19.0   \n",
       "...                 ...                       ...        ...             ...   \n",
       "39639               1.0                  0.684783        9.0             7.0   \n",
       "39640               1.0                  0.885057        9.0             7.0   \n",
       "39641               1.0                  0.644128       24.0             1.0   \n",
       "39642               1.0                  0.692661       10.0             1.0   \n",
       "39643               1.0                  0.846154        1.0             1.0   \n",
       "\n",
       "       num_imgs  num_videos  average_token_length  num_keywords  \\\n",
       "0           1.0         0.0              4.680365           5.0   \n",
       "1           1.0         0.0              4.913725           4.0   \n",
       "2           1.0         0.0              4.393365           6.0   \n",
       "3           1.0         0.0              4.404896           7.0   \n",
       "4          20.0         0.0              4.682836           7.0   \n",
       "...         ...         ...                   ...           ...   \n",
       "39639       1.0         1.0              4.523121           8.0   \n",
       "39640       3.0        48.0              4.405488           7.0   \n",
       "39641      12.0         1.0              5.076923           8.0   \n",
       "39642       1.0         0.0              4.975073           5.0   \n",
       "39643       0.0         2.0              4.471338           4.0   \n",
       "\n",
       "       data_channel_is_lifestyle  data_channel_is_entertainment  \\\n",
       "0                            0.0                            1.0   \n",
       "1                            0.0                            0.0   \n",
       "2                            0.0                            0.0   \n",
       "3                            0.0                            1.0   \n",
       "4                            0.0                            0.0   \n",
       "...                          ...                            ...   \n",
       "39639                        0.0                            0.0   \n",
       "39640                        0.0                            0.0   \n",
       "39641                        0.0                            0.0   \n",
       "39642                        0.0                            0.0   \n",
       "39643                        0.0                            1.0   \n",
       "\n",
       "       data_channel_is_bus  data_channel_is_socmed  data_channel_is_tech  \\\n",
       "0                      0.0                     0.0                   0.0   \n",
       "1                      1.0                     0.0                   0.0   \n",
       "2                      1.0                     0.0                   0.0   \n",
       "3                      0.0                     0.0                   0.0   \n",
       "4                      0.0                     0.0                   1.0   \n",
       "...                    ...                     ...                   ...   \n",
       "39639                  0.0                     0.0                   1.0   \n",
       "39640                  0.0                     1.0                   0.0   \n",
       "39641                  0.0                     0.0                   0.0   \n",
       "39642                  0.0                     0.0                   0.0   \n",
       "39643                  0.0                     0.0                   0.0   \n",
       "\n",
       "       data_channel_is_world  kw_min_min  kw_max_min  kw_avg_min  kw_min_max  \\\n",
       "0                        0.0         0.0         0.0       0.000         0.0   \n",
       "1                        0.0         0.0         0.0       0.000         0.0   \n",
       "2                        0.0         0.0         0.0       0.000         0.0   \n",
       "3                        0.0         0.0         0.0       0.000         0.0   \n",
       "4                        0.0         0.0         0.0       0.000         0.0   \n",
       "...                      ...         ...         ...         ...         ...   \n",
       "39639                    0.0        -1.0       671.0     173.125     26900.0   \n",
       "39640                    0.0        -1.0       616.0     184.000      6500.0   \n",
       "39641                    0.0        -1.0       691.0     168.250      6200.0   \n",
       "39642                    1.0        -1.0         0.0      -1.000         0.0   \n",
       "39643                    0.0        -1.0        97.0      23.500    205600.0   \n",
       "\n",
       "       kw_max_max     kw_avg_max   kw_min_avg   kw_max_avg   kw_avg_avg  \\\n",
       "0             0.0       0.000000     0.000000     0.000000     0.000000   \n",
       "1             0.0       0.000000     0.000000     0.000000     0.000000   \n",
       "2             0.0       0.000000     0.000000     0.000000     0.000000   \n",
       "3             0.0       0.000000     0.000000     0.000000     0.000000   \n",
       "4             0.0       0.000000     0.000000     0.000000     0.000000   \n",
       "...           ...            ...          ...          ...          ...   \n",
       "39639    843300.0  374962.500000  2514.742857  4004.342857  3031.115764   \n",
       "39640    843300.0  192985.714286  1664.267857  5470.168651  3411.660830   \n",
       "39641    843300.0  295850.000000  1753.882353  6880.687034  4206.439195   \n",
       "39642    843300.0  254600.000000     0.000000  3384.316871  1777.895883   \n",
       "39643    843300.0  366200.000000  3035.080555  3613.512953  3296.909481   \n",
       "\n",
       "       self_reference_min_shares  self_reference_max_shares  \\\n",
       "0                          496.0                      496.0   \n",
       "1                            0.0                        0.0   \n",
       "2                          918.0                      918.0   \n",
       "3                            0.0                        0.0   \n",
       "4                          545.0                    16000.0   \n",
       "...                          ...                        ...   \n",
       "39639                    11400.0                    48000.0   \n",
       "39640                     2100.0                     2100.0   \n",
       "39641                     1400.0                     1400.0   \n",
       "39642                      452.0                      452.0   \n",
       "39643                     2100.0                     2100.0   \n",
       "\n",
       "       self_reference_avg_sharess  weekday_is_monday  weekday_is_tuesday  \\\n",
       "0                      496.000000                1.0                 0.0   \n",
       "1                        0.000000                1.0                 0.0   \n",
       "2                      918.000000                1.0                 0.0   \n",
       "3                        0.000000                1.0                 0.0   \n",
       "4                     3151.157895                1.0                 0.0   \n",
       "...                           ...                ...                 ...   \n",
       "39639                37033.333333                0.0                 0.0   \n",
       "39640                 2100.000000                0.0                 0.0   \n",
       "39641                 1400.000000                0.0                 0.0   \n",
       "39642                  452.000000                0.0                 0.0   \n",
       "39643                 2100.000000                0.0                 0.0   \n",
       "\n",
       "       weekday_is_wednesday  weekday_is_thursday  weekday_is_friday  \\\n",
       "0                       0.0                  0.0                0.0   \n",
       "1                       0.0                  0.0                0.0   \n",
       "2                       0.0                  0.0                0.0   \n",
       "3                       0.0                  0.0                0.0   \n",
       "4                       0.0                  0.0                0.0   \n",
       "...                     ...                  ...                ...   \n",
       "39639                   1.0                  0.0                0.0   \n",
       "39640                   1.0                  0.0                0.0   \n",
       "39641                   1.0                  0.0                0.0   \n",
       "39642                   1.0                  0.0                0.0   \n",
       "39643                   1.0                  0.0                0.0   \n",
       "\n",
       "       weekday_is_saturday  weekday_is_sunday  is_weekend    LDA_00    LDA_01  \\\n",
       "0                      0.0                0.0         0.0  0.500331  0.378279   \n",
       "1                      0.0                0.0         0.0  0.799756  0.050047   \n",
       "2                      0.0                0.0         0.0  0.217792  0.033334   \n",
       "3                      0.0                0.0         0.0  0.028573  0.419300   \n",
       "4                      0.0                0.0         0.0  0.028633  0.028794   \n",
       "...                    ...                ...         ...       ...       ...   \n",
       "39639                  0.0                0.0         0.0  0.025038  0.025001   \n",
       "39640                  0.0                0.0         0.0  0.029349  0.028575   \n",
       "39641                  0.0                0.0         0.0  0.159004  0.025025   \n",
       "39642                  0.0                0.0         0.0  0.040004  0.040003   \n",
       "39643                  0.0                0.0         0.0  0.050001  0.799339   \n",
       "\n",
       "         LDA_02    LDA_03    LDA_04  global_subjectivity  \\\n",
       "0      0.040005  0.041263  0.040123             0.521617   \n",
       "1      0.050096  0.050101  0.050001             0.341246   \n",
       "2      0.033351  0.033334  0.682188             0.702222   \n",
       "3      0.494651  0.028905  0.028572             0.429850   \n",
       "4      0.028575  0.028572  0.885427             0.513502   \n",
       "...         ...       ...       ...                  ...   \n",
       "39639  0.151701  0.025000  0.773260             0.482679   \n",
       "39640  0.231866  0.681635  0.028575             0.564374   \n",
       "39641  0.025207  0.643794  0.146970             0.510296   \n",
       "39642  0.839987  0.040002  0.040004             0.358578   \n",
       "39643  0.050000  0.050659  0.050001             0.517893   \n",
       "\n",
       "       global_sentiment_polarity  global_rate_positive_words  \\\n",
       "0                       0.092562                    0.045662   \n",
       "1                       0.148948                    0.043137   \n",
       "2                       0.323333                    0.056872   \n",
       "3                       0.100705                    0.041431   \n",
       "4                       0.281003                    0.074627   \n",
       "...                          ...                         ...   \n",
       "39639                   0.141964                    0.037572   \n",
       "39640                   0.194249                    0.039634   \n",
       "39641                   0.024609                    0.033937   \n",
       "39642                  -0.008066                    0.020528   \n",
       "39643                   0.104892                    0.063694   \n",
       "\n",
       "       global_rate_negative_words  rate_positive_words  rate_negative_words  \\\n",
       "0                        0.013699             0.769231             0.230769   \n",
       "1                        0.015686             0.733333             0.266667   \n",
       "2                        0.009479             0.857143             0.142857   \n",
       "3                        0.020716             0.666667             0.333333   \n",
       "4                        0.012127             0.860215             0.139785   \n",
       "...                           ...                  ...                  ...   \n",
       "39639                    0.014451             0.722222             0.277778   \n",
       "39640                    0.009146             0.812500             0.187500   \n",
       "39641                    0.024887             0.576923             0.423077   \n",
       "39642                    0.023460             0.466667             0.533333   \n",
       "39643                    0.012739             0.833333             0.166667   \n",
       "\n",
       "       avg_positive_polarity  min_positive_polarity  max_positive_polarity  \\\n",
       "0                   0.378636               0.100000                   0.70   \n",
       "1                   0.286915               0.033333                   0.70   \n",
       "2                   0.495833               0.100000                   1.00   \n",
       "3                   0.385965               0.136364                   0.80   \n",
       "4                   0.411127               0.033333                   1.00   \n",
       "...                      ...                    ...                    ...   \n",
       "39639               0.333791               0.100000                   0.75   \n",
       "39640               0.374825               0.136364                   0.70   \n",
       "39641               0.307273               0.136364                   0.50   \n",
       "39642               0.236851               0.062500                   0.50   \n",
       "39643               0.247338               0.100000                   0.50   \n",
       "\n",
       "       avg_negative_polarity  min_negative_polarity  max_negative_polarity  \\\n",
       "0                  -0.350000                 -0.600              -0.200000   \n",
       "1                  -0.118750                 -0.125              -0.100000   \n",
       "2                  -0.466667                 -0.800              -0.133333   \n",
       "3                  -0.369697                 -0.600              -0.166667   \n",
       "4                  -0.220192                 -0.500              -0.050000   \n",
       "...                      ...                    ...                    ...   \n",
       "39639              -0.260000                 -0.500              -0.125000   \n",
       "39640              -0.211111                 -0.400              -0.100000   \n",
       "39641              -0.356439                 -0.800              -0.166667   \n",
       "39642              -0.205246                 -0.500              -0.012500   \n",
       "39643              -0.200000                 -0.200              -0.200000   \n",
       "\n",
       "       title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0                0.500000                 -0.187500                0.000000   \n",
       "1                0.000000                  0.000000                0.500000   \n",
       "2                0.000000                  0.000000                0.500000   \n",
       "3                0.000000                  0.000000                0.500000   \n",
       "4                0.454545                  0.136364                0.045455   \n",
       "...                   ...                       ...                     ...   \n",
       "39639            0.100000                  0.000000                0.400000   \n",
       "39640            0.300000                  1.000000                0.200000   \n",
       "39641            0.454545                  0.136364                0.045455   \n",
       "39642            0.000000                  0.000000                0.500000   \n",
       "39643            0.333333                  0.250000                0.166667   \n",
       "\n",
       "       abs_title_sentiment_polarity  shares  \n",
       "0                          0.187500     593  \n",
       "1                          0.000000     711  \n",
       "2                          0.000000    1500  \n",
       "3                          0.000000    1200  \n",
       "4                          0.136364     505  \n",
       "...                             ...     ...  \n",
       "39639                      0.000000    1800  \n",
       "39640                      1.000000    1900  \n",
       "39641                      0.136364    1900  \n",
       "39642                      0.000000    1100  \n",
       "39643                      0.250000    1300  \n",
       "\n",
       "[39644 rows x 60 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = pd.read_csv('./OnlineNewsPopularity.csv')\n",
    "news_df.drop('url', axis=1, inplace=True)\n",
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank( timedelta)</th>\n",
       "      <th>rank( n_tokens_title)</th>\n",
       "      <th>rank( n_tokens_content)</th>\n",
       "      <th>rank( n_unique_tokens)</th>\n",
       "      <th>rank( n_non_stop_words)</th>\n",
       "      <th>rank( n_non_stop_unique_tokens)</th>\n",
       "      <th>rank( num_hrefs)</th>\n",
       "      <th>rank( num_self_hrefs)</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>rank( average_token_length)</th>\n",
       "      <th>rank( num_keywords)</th>\n",
       "      <th>data_channel_is_lifestyle</th>\n",
       "      <th>data_channel_is_entertainment</th>\n",
       "      <th>data_channel_is_bus</th>\n",
       "      <th>data_channel_is_socmed</th>\n",
       "      <th>data_channel_is_tech</th>\n",
       "      <th>data_channel_is_world</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>rank( kw_max_min)</th>\n",
       "      <th>rank( kw_avg_min)</th>\n",
       "      <th>kw_min_max</th>\n",
       "      <th>kw_max_max</th>\n",
       "      <th>rank( kw_avg_max)</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>rank( kw_max_avg)</th>\n",
       "      <th>rank( kw_avg_avg)</th>\n",
       "      <th>rank( self_reference_min_shares)</th>\n",
       "      <th>rank( self_reference_max_shares)</th>\n",
       "      <th>rank( self_reference_avg_sharess)</th>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <th>weekday_is_saturday</th>\n",
       "      <th>weekday_is_sunday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>rank( LDA_00)</th>\n",
       "      <th>rank( LDA_01)</th>\n",
       "      <th>rank( LDA_02)</th>\n",
       "      <th>rank( LDA_03)</th>\n",
       "      <th>rank( LDA_04)</th>\n",
       "      <th>rank( global_subjectivity)</th>\n",
       "      <th>rank( global_sentiment_polarity)</th>\n",
       "      <th>rank( global_rate_positive_words)</th>\n",
       "      <th>rank( global_rate_negative_words)</th>\n",
       "      <th>rank( rate_positive_words)</th>\n",
       "      <th>rank( rate_negative_words)</th>\n",
       "      <th>rank( avg_positive_polarity)</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>rank( avg_negative_polarity)</th>\n",
       "      <th>rank( min_negative_polarity)</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.091313</td>\n",
       "      <td>0.746312</td>\n",
       "      <td>-0.830751</td>\n",
       "      <td>1.171875</td>\n",
       "      <td>-0.859076</td>\n",
       "      <td>1.250329</td>\n",
       "      <td>-0.773700</td>\n",
       "      <td>-0.247319</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060832</td>\n",
       "      <td>-1.077791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.314162</td>\n",
       "      <td>-2.013451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-0.809729</td>\n",
       "      <td>-0.897243</td>\n",
       "      <td>-0.895353</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.011874</td>\n",
       "      <td>1.107189</td>\n",
       "      <td>0.003762</td>\n",
       "      <td>0.133906</td>\n",
       "      <td>-0.019697</td>\n",
       "      <td>0.834551</td>\n",
       "      <td>-0.299737</td>\n",
       "      <td>0.407371</td>\n",
       "      <td>-0.186943</td>\n",
       "      <td>0.425334</td>\n",
       "      <td>-0.344559</td>\n",
       "      <td>0.257404</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.840613</td>\n",
       "      <td>-0.309042</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.091313</td>\n",
       "      <td>-0.627749</td>\n",
       "      <td>-0.629945</td>\n",
       "      <td>0.636396</td>\n",
       "      <td>-0.682170</td>\n",
       "      <td>1.037852</td>\n",
       "      <td>-1.082209</td>\n",
       "      <td>-0.770293</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.874264</td>\n",
       "      <td>-1.654145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.314162</td>\n",
       "      <td>-2.013451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-1.337803</td>\n",
       "      <td>-1.337726</td>\n",
       "      <td>-1.337803</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.506066</td>\n",
       "      <td>0.481113</td>\n",
       "      <td>0.164789</td>\n",
       "      <td>0.215657</td>\n",
       "      <td>0.071541</td>\n",
       "      <td>-1.206170</td>\n",
       "      <td>0.345029</td>\n",
       "      <td>0.255150</td>\n",
       "      <td>0.038674</td>\n",
       "      <td>0.167738</td>\n",
       "      <td>-0.092096</td>\n",
       "      <td>-0.900984</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.303623</td>\n",
       "      <td>1.355812</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.091313</td>\n",
       "      <td>-0.627749</td>\n",
       "      <td>-0.877419</td>\n",
       "      <td>0.346405</td>\n",
       "      <td>-0.977397</td>\n",
       "      <td>-0.284000</td>\n",
       "      <td>-1.082209</td>\n",
       "      <td>-0.770293</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.983430</td>\n",
       "      <td>-0.561454</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.314162</td>\n",
       "      <td>-2.013451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-0.314418</td>\n",
       "      <td>-0.753138</td>\n",
       "      <td>-0.722589</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.628018</td>\n",
       "      <td>-0.102896</td>\n",
       "      <td>-0.194219</td>\n",
       "      <td>-0.319837</td>\n",
       "      <td>1.102533</td>\n",
       "      <td>2.439542</td>\n",
       "      <td>2.017790</td>\n",
       "      <td>1.034661</td>\n",
       "      <td>-0.690612</td>\n",
       "      <td>1.106431</td>\n",
       "      <td>-0.977397</td>\n",
       "      <td>1.564511</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-1.637609</td>\n",
       "      <td>-0.868165</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.091313</td>\n",
       "      <td>-0.627749</td>\n",
       "      <td>0.309539</td>\n",
       "      <td>-0.344123</td>\n",
       "      <td>0.321434</td>\n",
       "      <td>-0.264207</td>\n",
       "      <td>0.207090</td>\n",
       "      <td>-1.494860</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.947209</td>\n",
       "      <td>-0.086922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.314162</td>\n",
       "      <td>-2.013451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-1.337803</td>\n",
       "      <td>-1.337726</td>\n",
       "      <td>-1.337803</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.437130</td>\n",
       "      <td>1.172315</td>\n",
       "      <td>0.875191</td>\n",
       "      <td>-0.392816</td>\n",
       "      <td>-0.811837</td>\n",
       "      <td>-0.291810</td>\n",
       "      <td>-0.208737</td>\n",
       "      <td>0.150161</td>\n",
       "      <td>0.574307</td>\n",
       "      <td>-0.284132</td>\n",
       "      <td>0.363172</td>\n",
       "      <td>0.351546</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-0.984764</td>\n",
       "      <td>-0.309042</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.091313</td>\n",
       "      <td>1.191898</td>\n",
       "      <td>1.253929</td>\n",
       "      <td>-1.200952</td>\n",
       "      <td>1.406604</td>\n",
       "      <td>-1.358432</td>\n",
       "      <td>1.046996</td>\n",
       "      <td>2.268357</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.069322</td>\n",
       "      <td>-0.086922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.314162</td>\n",
       "      <td>-2.013451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-0.768720</td>\n",
       "      <td>1.144183</td>\n",
       "      <td>0.294747</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.314651</td>\n",
       "      <td>-0.220448</td>\n",
       "      <td>-0.531720</td>\n",
       "      <td>-0.626094</td>\n",
       "      <td>1.680026</td>\n",
       "      <td>0.738234</td>\n",
       "      <td>1.686806</td>\n",
       "      <td>1.934743</td>\n",
       "      <td>-0.367261</td>\n",
       "      <td>1.143818</td>\n",
       "      <td>-1.009924</td>\n",
       "      <td>0.671991</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.327633</td>\n",
       "      <td>0.032505</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39639</th>\n",
       "      <td>-3.716757</td>\n",
       "      <td>0.299307</td>\n",
       "      <td>-0.208059</td>\n",
       "      <td>-0.100354</td>\n",
       "      <td>-0.394045</td>\n",
       "      <td>-0.060357</td>\n",
       "      <td>0.207090</td>\n",
       "      <td>1.378593</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.507837</td>\n",
       "      <td>0.343989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.024409</td>\n",
       "      <td>-0.424676</td>\n",
       "      <td>26900.0</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>0.911184</td>\n",
       "      <td>2514.742857</td>\n",
       "      <td>-0.200666</td>\n",
       "      <td>0.179677</td>\n",
       "      <td>1.547643</td>\n",
       "      <td>1.792045</td>\n",
       "      <td>1.960439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.686325</td>\n",
       "      <td>-0.822431</td>\n",
       "      <td>0.344962</td>\n",
       "      <td>-0.966295</td>\n",
       "      <td>1.300303</td>\n",
       "      <td>0.369968</td>\n",
       "      <td>0.265549</td>\n",
       "      <td>-0.088382</td>\n",
       "      <td>-0.099273</td>\n",
       "      <td>0.086256</td>\n",
       "      <td>-0.011160</td>\n",
       "      <td>-0.331371</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.056493</td>\n",
       "      <td>0.032505</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39640</th>\n",
       "      <td>-3.716757</td>\n",
       "      <td>0.746312</td>\n",
       "      <td>-0.278442</td>\n",
       "      <td>1.442174</td>\n",
       "      <td>-0.472542</td>\n",
       "      <td>1.915979</td>\n",
       "      <td>0.207090</td>\n",
       "      <td>1.378593</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>-0.944538</td>\n",
       "      <td>-0.086922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.111863</td>\n",
       "      <td>-0.343553</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>-0.463864</td>\n",
       "      <td>1664.267857</td>\n",
       "      <td>0.457255</td>\n",
       "      <td>0.522204</td>\n",
       "      <td>0.512085</td>\n",
       "      <td>-0.200537</td>\n",
       "      <td>-0.045952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.229267</td>\n",
       "      <td>-0.398422</td>\n",
       "      <td>0.508340</td>\n",
       "      <td>1.064995</td>\n",
       "      <td>-0.650512</td>\n",
       "      <td>1.306810</td>\n",
       "      <td>0.854330</td>\n",
       "      <td>0.039433</td>\n",
       "      <td>-0.730368</td>\n",
       "      <td>0.757048</td>\n",
       "      <td>-0.660623</td>\n",
       "      <td>0.203796</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.416663</td>\n",
       "      <td>0.394831</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39641</th>\n",
       "      <td>-3.716757</td>\n",
       "      <td>-0.158415</td>\n",
       "      <td>0.091302</td>\n",
       "      <td>-0.222587</td>\n",
       "      <td>0.147508</td>\n",
       "      <td>-0.491646</td>\n",
       "      <td>1.335872</td>\n",
       "      <td>-0.770293</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.415071</td>\n",
       "      <td>0.343989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.072048</td>\n",
       "      <td>-0.460873</td>\n",
       "      <td>6200.0</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>0.404934</td>\n",
       "      <td>1753.882353</td>\n",
       "      <td>0.855287</td>\n",
       "      <td>1.068122</td>\n",
       "      <td>0.159376</td>\n",
       "      <td>-0.483671</td>\n",
       "      <td>-0.385722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.505177</td>\n",
       "      <td>-0.640777</td>\n",
       "      <td>-0.697652</td>\n",
       "      <td>1.015044</td>\n",
       "      <td>0.250808</td>\n",
       "      <td>0.697813</td>\n",
       "      <td>-0.997733</td>\n",
       "      <td>-0.324699</td>\n",
       "      <td>0.953765</td>\n",
       "      <td>-0.792776</td>\n",
       "      <td>0.900083</td>\n",
       "      <td>-0.660859</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.892713</td>\n",
       "      <td>-0.868165</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39642</th>\n",
       "      <td>-3.716757</td>\n",
       "      <td>-2.153192</td>\n",
       "      <td>0.617082</td>\n",
       "      <td>0.002434</td>\n",
       "      <td>0.705942</td>\n",
       "      <td>0.023808</td>\n",
       "      <td>0.329067</td>\n",
       "      <td>-0.770293</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.082436</td>\n",
       "      <td>-1.077791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.314162</td>\n",
       "      <td>-2.375909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>0.081465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.198484</td>\n",
       "      <td>-1.659634</td>\n",
       "      <td>-0.842009</td>\n",
       "      <td>-0.902979</td>\n",
       "      <td>-0.902124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.193188</td>\n",
       "      <td>0.241684</td>\n",
       "      <td>1.515967</td>\n",
       "      <td>0.021404</td>\n",
       "      <td>-0.064950</td>\n",
       "      <td>-1.048091</td>\n",
       "      <td>-1.469914</td>\n",
       "      <td>-1.181859</td>\n",
       "      <td>0.827942</td>\n",
       "      <td>-1.331563</td>\n",
       "      <td>1.541598</td>\n",
       "      <td>-1.397816</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.475867</td>\n",
       "      <td>0.032505</td>\n",
       "      <td>-0.012500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39643</th>\n",
       "      <td>-3.716757</td>\n",
       "      <td>-0.158415</td>\n",
       "      <td>-1.245233</td>\n",
       "      <td>1.489854</td>\n",
       "      <td>-1.299567</td>\n",
       "      <td>1.540148</td>\n",
       "      <td>-1.696325</td>\n",
       "      <td>-0.770293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.700679</td>\n",
       "      <td>-1.654145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.622922</td>\n",
       "      <td>-1.749738</td>\n",
       "      <td>205600.0</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>0.859487</td>\n",
       "      <td>3035.080555</td>\n",
       "      <td>-0.473319</td>\n",
       "      <td>0.424711</td>\n",
       "      <td>0.512085</td>\n",
       "      <td>-0.200537</td>\n",
       "      <td>-0.045952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.309937</td>\n",
       "      <td>1.816184</td>\n",
       "      <td>0.115936</td>\n",
       "      <td>0.239210</td>\n",
       "      <td>0.067294</td>\n",
       "      <td>0.792819</td>\n",
       "      <td>-0.159920</td>\n",
       "      <td>1.407029</td>\n",
       "      <td>-0.296564</td>\n",
       "      <td>0.920030</td>\n",
       "      <td>-0.810958</td>\n",
       "      <td>-1.304289</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.532339</td>\n",
       "      <td>0.988570</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39644 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rank( timedelta)  rank( n_tokens_title)  rank( n_tokens_content)  \\\n",
       "0              3.091313               0.746312                -0.830751   \n",
       "1              3.091313              -0.627749                -0.629945   \n",
       "2              3.091313              -0.627749                -0.877419   \n",
       "3              3.091313              -0.627749                 0.309539   \n",
       "4              3.091313               1.191898                 1.253929   \n",
       "...                 ...                    ...                      ...   \n",
       "39639         -3.716757               0.299307                -0.208059   \n",
       "39640         -3.716757               0.746312                -0.278442   \n",
       "39641         -3.716757              -0.158415                 0.091302   \n",
       "39642         -3.716757              -2.153192                 0.617082   \n",
       "39643         -3.716757              -0.158415                -1.245233   \n",
       "\n",
       "       rank( n_unique_tokens)  rank( n_non_stop_words)  \\\n",
       "0                    1.171875                -0.859076   \n",
       "1                    0.636396                -0.682170   \n",
       "2                    0.346405                -0.977397   \n",
       "3                   -0.344123                 0.321434   \n",
       "4                   -1.200952                 1.406604   \n",
       "...                       ...                      ...   \n",
       "39639               -0.100354                -0.394045   \n",
       "39640                1.442174                -0.472542   \n",
       "39641               -0.222587                 0.147508   \n",
       "39642                0.002434                 0.705942   \n",
       "39643                1.489854                -1.299567   \n",
       "\n",
       "       rank( n_non_stop_unique_tokens)  rank( num_hrefs)  \\\n",
       "0                             1.250329         -0.773700   \n",
       "1                             1.037852         -1.082209   \n",
       "2                            -0.284000         -1.082209   \n",
       "3                            -0.264207          0.207090   \n",
       "4                            -1.358432          1.046996   \n",
       "...                                ...               ...   \n",
       "39639                        -0.060357          0.207090   \n",
       "39640                         1.915979          0.207090   \n",
       "39641                        -0.491646          1.335872   \n",
       "39642                         0.023808          0.329067   \n",
       "39643                         1.540148         -1.696325   \n",
       "\n",
       "       rank( num_self_hrefs)   num_imgs   num_videos  \\\n",
       "0                  -0.247319        1.0          0.0   \n",
       "1                  -0.770293        1.0          0.0   \n",
       "2                  -0.770293        1.0          0.0   \n",
       "3                  -1.494860        1.0          0.0   \n",
       "4                   2.268357       20.0          0.0   \n",
       "...                      ...        ...          ...   \n",
       "39639               1.378593        1.0          1.0   \n",
       "39640               1.378593        3.0         48.0   \n",
       "39641              -0.770293       12.0          1.0   \n",
       "39642              -0.770293        1.0          0.0   \n",
       "39643              -0.770293        0.0          2.0   \n",
       "\n",
       "       rank( average_token_length)  rank( num_keywords)  \\\n",
       "0                         0.060832            -1.077791   \n",
       "1                         0.874264            -1.654145   \n",
       "2                        -0.983430            -0.561454   \n",
       "3                        -0.947209            -0.086922   \n",
       "4                         0.069322            -0.086922   \n",
       "...                            ...                  ...   \n",
       "39639                    -0.507837             0.343989   \n",
       "39640                    -0.944538            -0.086922   \n",
       "39641                     1.415071             0.343989   \n",
       "39642                     1.082436            -1.077791   \n",
       "39643                    -0.700679            -1.654145   \n",
       "\n",
       "        data_channel_is_lifestyle   data_channel_is_entertainment  \\\n",
       "0                             0.0                             1.0   \n",
       "1                             0.0                             0.0   \n",
       "2                             0.0                             0.0   \n",
       "3                             0.0                             1.0   \n",
       "4                             0.0                             0.0   \n",
       "...                           ...                             ...   \n",
       "39639                         0.0                             0.0   \n",
       "39640                         0.0                             0.0   \n",
       "39641                         0.0                             0.0   \n",
       "39642                         0.0                             0.0   \n",
       "39643                         0.0                             1.0   \n",
       "\n",
       "        data_channel_is_bus   data_channel_is_socmed   data_channel_is_tech  \\\n",
       "0                       0.0                      0.0                    0.0   \n",
       "1                       1.0                      0.0                    0.0   \n",
       "2                       1.0                      0.0                    0.0   \n",
       "3                       0.0                      0.0                    0.0   \n",
       "4                       0.0                      0.0                    1.0   \n",
       "...                     ...                      ...                    ...   \n",
       "39639                   0.0                      0.0                    1.0   \n",
       "39640                   0.0                      1.0                    0.0   \n",
       "39641                   0.0                      0.0                    0.0   \n",
       "39642                   0.0                      0.0                    0.0   \n",
       "39643                   0.0                      0.0                    0.0   \n",
       "\n",
       "        data_channel_is_world   kw_min_min  rank( kw_max_min)  \\\n",
       "0                         0.0          0.0          -2.314162   \n",
       "1                         0.0          0.0          -2.314162   \n",
       "2                         0.0          0.0          -2.314162   \n",
       "3                         0.0          0.0          -2.314162   \n",
       "4                         0.0          0.0          -2.314162   \n",
       "...                       ...          ...                ...   \n",
       "39639                     0.0         -1.0           0.024409   \n",
       "39640                     0.0         -1.0          -0.111863   \n",
       "39641                     0.0         -1.0           0.072048   \n",
       "39642                     1.0         -1.0          -2.314162   \n",
       "39643                     0.0         -1.0          -1.622922   \n",
       "\n",
       "       rank( kw_avg_min)   kw_min_max   kw_max_max  rank( kw_avg_max)  \\\n",
       "0              -2.013451          0.0          0.0          -3.091313   \n",
       "1              -2.013451          0.0          0.0          -3.091313   \n",
       "2              -2.013451          0.0          0.0          -3.091313   \n",
       "3              -2.013451          0.0          0.0          -3.091313   \n",
       "4              -2.013451          0.0          0.0          -3.091313   \n",
       "...                  ...          ...          ...                ...   \n",
       "39639          -0.424676      26900.0     843300.0           0.911184   \n",
       "39640          -0.343553       6500.0     843300.0          -0.463864   \n",
       "39641          -0.460873       6200.0     843300.0           0.404934   \n",
       "39642          -2.375909          0.0     843300.0           0.081465   \n",
       "39643          -1.749738     205600.0     843300.0           0.859487   \n",
       "\n",
       "        kw_min_avg  rank( kw_max_avg)  rank( kw_avg_avg)  \\\n",
       "0         0.000000          -3.091313          -3.091313   \n",
       "1         0.000000          -3.091313          -3.091313   \n",
       "2         0.000000          -3.091313          -3.091313   \n",
       "3         0.000000          -3.091313          -3.091313   \n",
       "4         0.000000          -3.091313          -3.091313   \n",
       "...            ...                ...                ...   \n",
       "39639  2514.742857          -0.200666           0.179677   \n",
       "39640  1664.267857           0.457255           0.522204   \n",
       "39641  1753.882353           0.855287           1.068122   \n",
       "39642     0.000000          -1.198484          -1.659634   \n",
       "39643  3035.080555          -0.473319           0.424711   \n",
       "\n",
       "       rank( self_reference_min_shares)  rank( self_reference_max_shares)  \\\n",
       "0                             -0.809729                         -0.897243   \n",
       "1                             -1.337803                         -1.337726   \n",
       "2                             -0.314418                         -0.753138   \n",
       "3                             -1.337803                         -1.337726   \n",
       "4                             -0.768720                          1.144183   \n",
       "...                                 ...                               ...   \n",
       "39639                          1.547643                          1.792045   \n",
       "39640                          0.512085                         -0.200537   \n",
       "39641                          0.159376                         -0.483671   \n",
       "39642                         -0.842009                         -0.902979   \n",
       "39643                          0.512085                         -0.200537   \n",
       "\n",
       "       rank( self_reference_avg_sharess)   weekday_is_monday  \\\n",
       "0                              -0.895353                 1.0   \n",
       "1                              -1.337803                 1.0   \n",
       "2                              -0.722589                 1.0   \n",
       "3                              -1.337803                 1.0   \n",
       "4                               0.294747                 1.0   \n",
       "...                                  ...                 ...   \n",
       "39639                           1.960439                 0.0   \n",
       "39640                          -0.045952                 0.0   \n",
       "39641                          -0.385722                 0.0   \n",
       "39642                          -0.902124                 0.0   \n",
       "39643                          -0.045952                 0.0   \n",
       "\n",
       "        weekday_is_tuesday   weekday_is_wednesday   weekday_is_thursday  \\\n",
       "0                      0.0                    0.0                   0.0   \n",
       "1                      0.0                    0.0                   0.0   \n",
       "2                      0.0                    0.0                   0.0   \n",
       "3                      0.0                    0.0                   0.0   \n",
       "4                      0.0                    0.0                   0.0   \n",
       "...                    ...                    ...                   ...   \n",
       "39639                  0.0                    1.0                   0.0   \n",
       "39640                  0.0                    1.0                   0.0   \n",
       "39641                  0.0                    1.0                   0.0   \n",
       "39642                  0.0                    1.0                   0.0   \n",
       "39643                  0.0                    1.0                   0.0   \n",
       "\n",
       "        weekday_is_friday   weekday_is_saturday   weekday_is_sunday  \\\n",
       "0                     0.0                   0.0                 0.0   \n",
       "1                     0.0                   0.0                 0.0   \n",
       "2                     0.0                   0.0                 0.0   \n",
       "3                     0.0                   0.0                 0.0   \n",
       "4                     0.0                   0.0                 0.0   \n",
       "...                   ...                   ...                 ...   \n",
       "39639                 0.0                   0.0                 0.0   \n",
       "39640                 0.0                   0.0                 0.0   \n",
       "39641                 0.0                   0.0                 0.0   \n",
       "39642                 0.0                   0.0                 0.0   \n",
       "39643                 0.0                   0.0                 0.0   \n",
       "\n",
       "        is_weekend  rank( LDA_00)  rank( LDA_01)  rank( LDA_02)  \\\n",
       "0              0.0       1.011874       1.107189       0.003762   \n",
       "1              0.0       1.506066       0.481113       0.164789   \n",
       "2              0.0       0.628018      -0.102896      -0.194219   \n",
       "3              0.0      -0.437130       1.172315       0.875191   \n",
       "4              0.0      -0.314651      -0.220448      -0.531720   \n",
       "...            ...            ...            ...            ...   \n",
       "39639          0.0      -0.686325      -0.822431       0.344962   \n",
       "39640          0.0      -0.229267      -0.398422       0.508340   \n",
       "39641          0.0       0.505177      -0.640777      -0.697652   \n",
       "39642          0.0       0.193188       0.241684       1.515967   \n",
       "39643          0.0       0.309937       1.816184       0.115936   \n",
       "\n",
       "       rank( LDA_03)  rank( LDA_04)  rank( global_subjectivity)  \\\n",
       "0           0.133906      -0.019697                    0.834551   \n",
       "1           0.215657       0.071541                   -1.206170   \n",
       "2          -0.319837       1.102533                    2.439542   \n",
       "3          -0.392816      -0.811837                   -0.291810   \n",
       "4          -0.626094       1.680026                    0.738234   \n",
       "...              ...            ...                         ...   \n",
       "39639      -0.966295       1.300303                    0.369968   \n",
       "39640       1.064995      -0.650512                    1.306810   \n",
       "39641       1.015044       0.250808                    0.697813   \n",
       "39642       0.021404      -0.064950                   -1.048091   \n",
       "39643       0.239210       0.067294                    0.792819   \n",
       "\n",
       "       rank( global_sentiment_polarity)  rank( global_rate_positive_words)  \\\n",
       "0                             -0.299737                           0.407371   \n",
       "1                              0.345029                           0.255150   \n",
       "2                              2.017790                           1.034661   \n",
       "3                             -0.208737                           0.150161   \n",
       "4                              1.686806                           1.934743   \n",
       "...                                 ...                                ...   \n",
       "39639                          0.265549                          -0.088382   \n",
       "39640                          0.854330                           0.039433   \n",
       "39641                         -0.997733                          -0.324699   \n",
       "39642                         -1.469914                          -1.181859   \n",
       "39643                         -0.159920                           1.407029   \n",
       "\n",
       "       rank( global_rate_negative_words)  rank( rate_positive_words)  \\\n",
       "0                              -0.186943                    0.425334   \n",
       "1                               0.038674                    0.167738   \n",
       "2                              -0.690612                    1.106431   \n",
       "3                               0.574307                   -0.284132   \n",
       "4                              -0.367261                    1.143818   \n",
       "...                                  ...                         ...   \n",
       "39639                          -0.099273                    0.086256   \n",
       "39640                          -0.730368                    0.757048   \n",
       "39641                           0.953765                   -0.792776   \n",
       "39642                           0.827942                   -1.331563   \n",
       "39643                          -0.296564                    0.920030   \n",
       "\n",
       "       rank( rate_negative_words)  rank( avg_positive_polarity)  \\\n",
       "0                       -0.344559                      0.257404   \n",
       "1                       -0.092096                     -0.900984   \n",
       "2                       -0.977397                      1.564511   \n",
       "3                        0.363172                      0.351546   \n",
       "4                       -1.009924                      0.671991   \n",
       "...                           ...                           ...   \n",
       "39639                   -0.011160                     -0.331371   \n",
       "39640                   -0.660623                      0.203796   \n",
       "39641                    0.900083                     -0.660859   \n",
       "39642                    1.541598                     -1.397816   \n",
       "39643                   -0.810958                     -1.304289   \n",
       "\n",
       "        min_positive_polarity   max_positive_polarity  \\\n",
       "0                    0.100000                    0.70   \n",
       "1                    0.033333                    0.70   \n",
       "2                    0.100000                    1.00   \n",
       "3                    0.136364                    0.80   \n",
       "4                    0.033333                    1.00   \n",
       "...                       ...                     ...   \n",
       "39639                0.100000                    0.75   \n",
       "39640                0.136364                    0.70   \n",
       "39641                0.136364                    0.50   \n",
       "39642                0.062500                    0.50   \n",
       "39643                0.100000                    0.50   \n",
       "\n",
       "       rank( avg_negative_polarity)  rank( min_negative_polarity)  \\\n",
       "0                         -0.840613                     -0.309042   \n",
       "1                          1.303623                      1.355812   \n",
       "2                         -1.637609                     -0.868165   \n",
       "3                         -0.984764                     -0.309042   \n",
       "4                          0.327633                      0.032505   \n",
       "...                             ...                           ...   \n",
       "39639                     -0.056493                      0.032505   \n",
       "39640                      0.416663                      0.394831   \n",
       "39641                     -0.892713                     -0.868165   \n",
       "39642                      0.475867                      0.032505   \n",
       "39643                      0.532339                      0.988570   \n",
       "\n",
       "        max_negative_polarity   title_subjectivity   title_sentiment_polarity  \\\n",
       "0                   -0.200000             0.500000                  -0.187500   \n",
       "1                   -0.100000             0.000000                   0.000000   \n",
       "2                   -0.133333             0.000000                   0.000000   \n",
       "3                   -0.166667             0.000000                   0.000000   \n",
       "4                   -0.050000             0.454545                   0.136364   \n",
       "...                       ...                  ...                        ...   \n",
       "39639               -0.125000             0.100000                   0.000000   \n",
       "39640               -0.100000             0.300000                   1.000000   \n",
       "39641               -0.166667             0.454545                   0.136364   \n",
       "39642               -0.012500             0.000000                   0.000000   \n",
       "39643               -0.200000             0.333333                   0.250000   \n",
       "\n",
       "        abs_title_subjectivity   abs_title_sentiment_polarity   shares  \n",
       "0                     0.000000                       0.187500      593  \n",
       "1                     0.500000                       0.000000      711  \n",
       "2                     0.500000                       0.000000     1500  \n",
       "3                     0.500000                       0.000000     1200  \n",
       "4                     0.045455                       0.136364      505  \n",
       "...                        ...                            ...      ...  \n",
       "39639                 0.400000                       0.000000     1800  \n",
       "39640                 0.200000                       1.000000     1900  \n",
       "39641                 0.045455                       0.136364     1900  \n",
       "39642                 0.500000                       0.000000     1100  \n",
       "39643                 0.166667                       0.250000     1300  \n",
       "\n",
       "[39644 rows x 60 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read preprocessed news dataset\n",
    "prep_news_df = pd.read_csv('./prep_news_data.csv')\n",
    "prep_news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Full-Connected Neural Network class\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_dim: int, output_dim: int, dropout: float) -> None:\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input_dim, 164),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(164, 191),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(191, 224),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Linear(224, 114),\n",
    "            nn.ReLU(),   \n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Linear(114, output_dim),\n",
    "            # nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    # Define forward loop for NN\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval_model(model, criterion, optimizer, train_loader, test_loader, epochs: int, batch: int, out_dir: str, test_data) -> tuple:\n",
    "    ''' \n",
    "    Method performs training, evaluation and getting barcodes from MLP model with CIFAR-10 dataset\n",
    "    Params:\n",
    "    model - model to train;\n",
    "    criterion - way to measure loss function;\n",
    "    optimizer - method to perform gradient descent;\n",
    "    train_loader - DataLoader with train data;\n",
    "    test_loader - DataLoader with test data;\n",
    "    epochs - number of epochs for train;\n",
    "    batch - batch size;\n",
    "    out_dir - folder for storing output data;\n",
    "    test_data - Dataset with test data.\n",
    "    Output:\n",
    "    fig - figure with train and test losses;\n",
    "    model_weights - dict with model weights.\n",
    "    '''\n",
    "    home_dir = f'./news_output/{out_dir}/weights_graphs_mlp_DataAmount{batch}/barcodes/'\n",
    "    os.makedirs(home_dir)\n",
    "    bar_datum = dict()\n",
    "    bar_evaluation = dict()\n",
    "\n",
    "    # Initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf  # Set initial \"min\" to infinity\n",
    "\n",
    "    train_losses = list()\n",
    "    test_losses = list()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        layer_list = ['layer1', 'layer2', 'layer3', 'layer4', 'layer5']\n",
    "        barcodes_data = dict()\n",
    "        barcodes_eval = dict()\n",
    "        if epoch == 0 and batch == 1000:\n",
    "            barc_data = dict()\n",
    "            barc_eval = dict()\n",
    "            model_start_weights = model.state_dict()\n",
    "            # Saving model's start weights\n",
    "            epoch_dir = f'./news_output/{out_dir}/weights_graphs_mlp_DataAmount0/'\n",
    "            os.makedirs(epoch_dir, exist_ok=True)\n",
    "            torch.save(model_start_weights, f'./news_output/{out_dir}/weights_graphs_mlp_DataAmount0/weights_mlp_DataAmount0.pth')\n",
    "\n",
    "            # Obtain barcodes from untrained network\n",
    "            barcodes = eXNN.topology.get_nn_barcodes(model, data=torch.stack([test_data[i][0] for i in range(100)]), layers=layer_list, hom_type=\"standard\", coefficient_type=\"2\") #, coefs_type=\"2\")\n",
    "            epoch_dir = f'./news_output/{out_dir}/weights_graphs_mlp_DataAmount0/barcodes/'\n",
    "            os.makedirs(epoch_dir, exist_ok=True)\n",
    "            zero_barc_data = dict()\n",
    "            zero_barc_eval = dict()\n",
    "            for i in range(1, 5):\n",
    "                barcode = barcodes[f'layer{i}']\n",
    "                # Save barcodes' info into the JSON\n",
    "                zero_barc_data.setdefault(f'layer{i}', barcode)\n",
    "\n",
    "                # Obtain the barcode after epoch and saving it\n",
    "                bplot = eXNN.topology.plot_barcode(barcode)\n",
    "                save_fig_path = epoch_dir + f'layer{i}_barcode.png'\n",
    "                bplot.savefig(save_fig_path)\n",
    "\n",
    "                # Get and save evaluation for barcode into the JSON\n",
    "                bar_eval = eXNN.topology.evaluate_barcode(barcode)\n",
    "                zero_barc_eval.setdefault(f'layer{i}', bar_eval)\n",
    "\n",
    "            barc_data.setdefault(f'epoch{epoch}', zero_barc_data)\n",
    "            barc_eval.setdefault(f'epoch{epoch}', zero_barc_eval)\n",
    "\n",
    "            with open(f'{epoch_dir}/barcode_data.json', 'w') as zero_data:\n",
    "                json.dump(barc_data, zero_data, indent=4)\n",
    "\n",
    "            with open(f'{epoch_dir}/barcode_eval.json', 'w') as zero_data:\n",
    "                json.dump(barc_eval, zero_data, indent=4)\n",
    "\n",
    "        # Monitor training loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        # Train the model\n",
    "        model.train()  # Prep model for training\n",
    "        for data, target in train_loader:\n",
    "            # Clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # Calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # Perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # Update running training loss\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        # Validate the model\n",
    "        model.eval()  # Prep model for evaluation\n",
    "        for data, target in test_loader:\n",
    "            # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data)\n",
    "            # Calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            # Update running validation loss\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "        # Applying some TDA methods to study the obtained network\n",
    "        model = model.to(torch.device('cpu'))\n",
    "        data = torch.stack([test_data[i][0] for i in range(100)])\n",
    "        barcodes = eXNN.topology.get_nn_barcodes(model, data, layers=layer_list, hom_type=\"standard\", coefficient_type=\"2\") #, coefs_type=\"2\")\n",
    "        epoch_dir = home_dir + f'epoch{epoch+1}/'\n",
    "        os.makedirs(epoch_dir, exist_ok=True)\n",
    "        for i in range(1, 4):\n",
    "            barcode = barcodes[f'layer{i}']\n",
    "            # Save barcodes' info into the JSON\n",
    "            barcodes_data.setdefault(f'layer{i}', barcode)\n",
    "\n",
    "            # Obtain the barcode after epoch and saving it\n",
    "            bplot = eXNN.topology.plot_barcode(barcode)\n",
    "            save_fig_path = epoch_dir + f'layer{i}_barcode.png'\n",
    "            bplot.savefig(save_fig_path)\n",
    "\n",
    "            # Get and save evaluation for barcode into the JSON\n",
    "            bar_eval = eXNN.topology.evaluate_barcode(barcode)\n",
    "            barcodes_eval.setdefault(f'layer{i}', bar_eval)\n",
    "\n",
    "        bar_datum.setdefault(f'epoch{epoch+1}', barcodes_data)\n",
    "        bar_evaluation.setdefault(f'epoch{epoch+1}', barcodes_eval)\n",
    "\n",
    "        # Print training/validation statistics\n",
    "        # Calculate average loss over an epoch\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        valid_loss = valid_loss / len(test_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(valid_loss)\n",
    "\n",
    "        # Print training and validation loss for the current epoch\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch + 1,  # Epoch number (starting from 1)\n",
    "            train_loss,  # Average training loss for the epoch\n",
    "            valid_loss  # Average validation loss for the epoch\n",
    "        ))\n",
    "\n",
    "        # Save the model if the validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "                valid_loss_min,  # Previous minimum validation loss\n",
    "                valid_loss  # Current validation loss\n",
    "            ))\n",
    "            torch.save(model.state_dict(), './best_mlp_news.pt')  # Save the model's state dictionary\n",
    "            valid_loss_min = valid_loss  # Update the minimum validation loss\n",
    "\n",
    "    # Save dictionaries with barcodes' data and evaluation as JSON files\n",
    "    with open(f'{home_dir}barcode_data.json', 'w', encoding='utf-8') as file:\n",
    "        json.dump(bar_datum, file, indent=4)\n",
    "    \n",
    "    with open(f'{home_dir}barcode_evaluation.json', 'w', encoding='utf-8') as json_eval:\n",
    "        json.dump(bar_evaluation, json_eval, indent=4)\n",
    "\n",
    "    # Get model's weights\n",
    "    model_weights = model.state_dict()\n",
    "\n",
    "    # Loss and MAPE visualisation\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.title('Losses over epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Metric')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return fig, model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for training network on non-full data\n",
    "def batch_fit(batch_df, epochs: int, batch: int, out_dir: str, dropout: float):\n",
    "    # Separate data into features and targets\n",
    "    X = batch_df.iloc[:, :-1].values\n",
    "    y = batch_df.iloc[:, -1].values\n",
    "    y = y.reshape(-1, 1)\n",
    "\n",
    "    # Data normalization\n",
    "    scaler_X = MinMaxScaler()\n",
    "    X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "    scaler_y = MinMaxScaler()\n",
    "    y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "    # Splitting data into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.1, random_state=42)\n",
    "\n",
    "    # Convertation data to PyTorch tensors\n",
    "    train_features = torch.tensor(X_train, dtype=torch.float32)\n",
    "    test_features = torch.tensor(X_test, dtype=torch.float32)\n",
    "    train_targets = torch.tensor(y_train, dtype=torch.float32)\n",
    "    test_targets = torch.tensor(y_test, dtype=torch.float32)    \n",
    "\n",
    "    # Creating DataLoader\n",
    "    train_data = TensorDataset(train_features, train_targets)\n",
    "    test_data = TensorDataset(test_features, test_targets)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "    # Model initialization\n",
    "    model = MultiLayerPerceptron(59, 1, dropout)\n",
    "\n",
    "    # Define optimizator and loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    # Perform training\n",
    "    fig, model_weights = train_and_eval_model(model=model, criterion=criterion, optimizer=optimizer, train_loader=train_loader, test_loader=test_loader, epochs=epochs, batch=batch, out_dir=out_dir, test_data=test_data)\n",
    "\n",
    "    return fig, model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/eXNN/topology/metrics.py:255: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in scalar divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000452 \tValidation Loss: 0.000268\n",
      "Validation loss decreased (inf --> 0.000268).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/eXNN/topology/metrics.py:255: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in scalar divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tTraining Loss: 0.000257 \tValidation Loss: 0.000267\n",
      "Validation loss decreased (0.000268 --> 0.000267).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/eXNN/topology/metrics.py:255: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in scalar divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tTraining Loss: 0.000226 \tValidation Loss: 0.000270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/eXNN/topology/metrics.py:255: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in scalar divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tTraining Loss: 0.000212 \tValidation Loss: 0.000273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/eXNN/topology/metrics.py:255: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in scalar divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \tTraining Loss: 0.000206 \tValidation Loss: 0.000275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/eXNN/topology/metrics.py:255: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in scalar divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 \tTraining Loss: 0.000202 \tValidation Loss: 0.000273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/eXNN/topology/metrics.py:255: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in scalar divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 \tTraining Loss: 0.000198 \tValidation Loss: 0.000275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/eXNN/topology/metrics.py:255: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in scalar divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 \tTraining Loss: 0.000196 \tValidation Loss: 0.000275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/eXNN/topology/metrics.py:255: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in scalar divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 \tTraining Loss: 0.000194 \tValidation Loss: 0.000273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/eXNN/topology/metrics.py:255: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in scalar divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \tTraining Loss: 0.000193 \tValidation Loss: 0.000271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/eXNN/topology/metrics.py:255: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in scalar divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 \tTraining Loss: 0.000192 \tValidation Loss: 0.000271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/eXNN/topology/metrics.py:255: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in scalar divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 \tTraining Loss: 0.000190 \tValidation Loss: 0.000270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/eXNN/topology/metrics.py:255: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in scalar divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 \tTraining Loss: 0.000189 \tValidation Loss: 0.000269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/eXNN/topology/metrics.py:255: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in scalar divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 \tTraining Loss: 0.000189 \tValidation Loss: 0.000269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/eXNN/topology/metrics.py:255: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in scalar divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 \tTraining Loss: 0.000188 \tValidation Loss: 0.000268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/eXNN/topology/metrics.py:255: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in scalar divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 \tTraining Loss: 0.000187 \tValidation Loss: 0.000270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/eXNN/topology/metrics.py:255: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in scalar divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 \tTraining Loss: 0.000186 \tValidation Loss: 0.000266\n",
      "Validation loss decreased (0.000267 --> 0.000266).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/eXNN/topology/metrics.py:255: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in scalar divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 \tTraining Loss: 0.000186 \tValidation Loss: 0.000267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/eXNN/topology/metrics.py:255: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in scalar divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 \tTraining Loss: 0.000185 \tValidation Loss: 0.000266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/eXNN/topology/metrics.py:255: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in scalar divide\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 \tTraining Loss: 0.000185 \tValidation Loss: 0.000265\n",
      "Validation loss decreased (0.000266 --> 0.000265).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a name for output folder\n",
    "out_dir = 'exp_1'\n",
    "\n",
    "# # Create cycle for training network with different amounts of data\n",
    "# for batch in range(5000, len(prep_news_df), 5000):\n",
    "#     base_dir = f'./news_output/{out_dir}/weights_graphs_mlp_DataAmount{batch}/'\n",
    "#     # Getting part of the dataset\n",
    "#     df_batch = prep_news_df.head(batch)\n",
    "\n",
    "#     # Training model and getting it's weights\n",
    "#     fig, model_weights = batch_fit(df_batch, 60, batch, out_dir, 0.25)\n",
    "\n",
    "#     # Saving weights, loss function and RMSE\n",
    "#     torch.save(model_weights, f'{base_dir}weights_mlp_DataAmount{batch}.pth')\n",
    "#     fig.savefig(f'{base_dir}loss_and_rmse_mlp_DataAmount{batch}.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "\n",
    "# Getting part of the dataset\n",
    "df_batch = news_df.head(len(news_df))\n",
    "last_dir = f'./news_output/{out_dir}/weights_graphs_mlp_DataAmount{len(news_df)}/'\n",
    "\n",
    "# Training model and getting it's weights\n",
    "fig, model_weights = batch_fit(news_df, 20, len(news_df), out_dir, 0.25)\n",
    "\n",
    "# Saving weights and losses\n",
    "torch.save(model_weights, f'{last_dir}weights_mlp_DataAmount{len(news_df)}.pth')\n",
    "fig.savefig(f'{last_dir}loss_and_rmse_mlp_DataAmount{len(news_df)}.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nd/fsmv2lz93k3_cjwz_1lx75j80000gn/T/ipykernel_3992/440895906.py:5: FutureWarning:\n",
      "\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.0129\n",
      "MAPE: 188.6787%\n"
     ]
    }
   ],
   "source": [
    "# Model inicialization\n",
    "model = MultiLayerPerceptron(input_dim=59, output_dim=1, dropout=0.4)\n",
    "\n",
    "# Load models weights from .pt file\n",
    "model.load_state_dict(torch.load('/Users/pavelvavilov/Desktop/Topology_in_Neural_Networks/news_dataset/news_output/exp_0/weights_graphs_mlp_DataAmount39644/weights_mlp_DataAmount39644.pth'))\n",
    "model.eval()  # Turn on models evaluation mode\n",
    "\n",
    "X = prep_news_df.iloc[:, :-1].values\n",
    "y = prep_news_df.iloc[:, -1].values\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Data normalization\n",
    "scaler_X = MinMaxScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "scaler_y = MinMaxScaler()\n",
    "y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "# Splitting data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convertation data to PyTorch tensors\n",
    "train_features = torch.tensor(X_train, dtype=torch.float32)\n",
    "test_features = torch.tensor(X_test, dtype=torch.float32)\n",
    "train_targets = torch.tensor(y_train, dtype=torch.float32)\n",
    "test_targets = torch.tensor(y_test, dtype=torch.float32)    \n",
    "\n",
    "# Creating DataLoader\n",
    "train_data = TensorDataset(train_features, train_targets)\n",
    "test_data = TensorDataset(test_features, test_targets)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# Прогон тестовых данных через модель\n",
    "predictions = []\n",
    "true_values = []\n",
    "\n",
    "with torch.no_grad():  # Отключаем вычисление градиентов\n",
    "    for batch in test_loader:\n",
    "        inputs, targets = batch\n",
    "        outputs = model(inputs)\n",
    "        predictions.append(outputs)\n",
    "        true_values.append(targets)\n",
    "\n",
    "# Объединение всех батчей в один тензор\n",
    "predictions = torch.cat(predictions, dim=0)\n",
    "true_values = torch.cat(true_values, dim=0)\n",
    "\n",
    "# Вычисление RMSE\n",
    "rmse = np.sqrt(mean_squared_error(true_values.numpy(), predictions.numpy()))\n",
    "print(f'RMSE: {rmse:.4f}')\n",
    "\n",
    "# Вычисление MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    # Избегаем деления на ноль\n",
    "    non_zero_indices = y_true != 0\n",
    "    y_true = y_true[non_zero_indices]\n",
    "    y_pred = y_pred[non_zero_indices]\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "mape = mean_absolute_percentage_error(true_values.numpy(), predictions.numpy())\n",
    "print(f'MAPE: {mape:.4f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
