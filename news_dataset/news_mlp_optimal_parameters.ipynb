{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting optimal MLP's hiperparameters for news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import optuna\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank( timedelta)</th>\n",
       "      <th>rank( n_tokens_title)</th>\n",
       "      <th>rank( n_tokens_content)</th>\n",
       "      <th>rank( n_unique_tokens)</th>\n",
       "      <th>rank( n_non_stop_words)</th>\n",
       "      <th>rank( n_non_stop_unique_tokens)</th>\n",
       "      <th>rank( num_hrefs)</th>\n",
       "      <th>rank( num_self_hrefs)</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>rank( average_token_length)</th>\n",
       "      <th>rank( num_keywords)</th>\n",
       "      <th>data_channel_is_lifestyle</th>\n",
       "      <th>data_channel_is_entertainment</th>\n",
       "      <th>data_channel_is_bus</th>\n",
       "      <th>data_channel_is_socmed</th>\n",
       "      <th>data_channel_is_tech</th>\n",
       "      <th>data_channel_is_world</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>rank( kw_max_min)</th>\n",
       "      <th>rank( kw_avg_min)</th>\n",
       "      <th>kw_min_max</th>\n",
       "      <th>kw_max_max</th>\n",
       "      <th>rank( kw_avg_max)</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>rank( kw_max_avg)</th>\n",
       "      <th>rank( kw_avg_avg)</th>\n",
       "      <th>rank( self_reference_min_shares)</th>\n",
       "      <th>rank( self_reference_max_shares)</th>\n",
       "      <th>rank( self_reference_avg_sharess)</th>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <th>weekday_is_saturday</th>\n",
       "      <th>weekday_is_sunday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>rank( LDA_00)</th>\n",
       "      <th>rank( LDA_01)</th>\n",
       "      <th>rank( LDA_02)</th>\n",
       "      <th>rank( LDA_03)</th>\n",
       "      <th>rank( LDA_04)</th>\n",
       "      <th>rank( global_subjectivity)</th>\n",
       "      <th>rank( global_sentiment_polarity)</th>\n",
       "      <th>rank( global_rate_positive_words)</th>\n",
       "      <th>rank( global_rate_negative_words)</th>\n",
       "      <th>rank( rate_positive_words)</th>\n",
       "      <th>rank( rate_negative_words)</th>\n",
       "      <th>rank( avg_positive_polarity)</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>rank( avg_negative_polarity)</th>\n",
       "      <th>rank( min_negative_polarity)</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.091313</td>\n",
       "      <td>0.746312</td>\n",
       "      <td>-0.830751</td>\n",
       "      <td>1.171875</td>\n",
       "      <td>-0.859076</td>\n",
       "      <td>1.250329</td>\n",
       "      <td>-0.773700</td>\n",
       "      <td>-0.247319</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060832</td>\n",
       "      <td>-1.077791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.314162</td>\n",
       "      <td>-2.013451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-0.809729</td>\n",
       "      <td>-0.897243</td>\n",
       "      <td>-0.895353</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.011874</td>\n",
       "      <td>1.107189</td>\n",
       "      <td>0.003762</td>\n",
       "      <td>0.133906</td>\n",
       "      <td>-0.019697</td>\n",
       "      <td>0.834551</td>\n",
       "      <td>-0.299737</td>\n",
       "      <td>0.407371</td>\n",
       "      <td>-0.186943</td>\n",
       "      <td>0.425334</td>\n",
       "      <td>-0.344559</td>\n",
       "      <td>0.257404</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.840613</td>\n",
       "      <td>-0.309042</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.091313</td>\n",
       "      <td>-0.627749</td>\n",
       "      <td>-0.629945</td>\n",
       "      <td>0.636396</td>\n",
       "      <td>-0.682170</td>\n",
       "      <td>1.037852</td>\n",
       "      <td>-1.082209</td>\n",
       "      <td>-0.770293</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.874264</td>\n",
       "      <td>-1.654145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.314162</td>\n",
       "      <td>-2.013451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-1.337803</td>\n",
       "      <td>-1.337726</td>\n",
       "      <td>-1.337803</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.506066</td>\n",
       "      <td>0.481113</td>\n",
       "      <td>0.164789</td>\n",
       "      <td>0.215657</td>\n",
       "      <td>0.071541</td>\n",
       "      <td>-1.206170</td>\n",
       "      <td>0.345029</td>\n",
       "      <td>0.255150</td>\n",
       "      <td>0.038674</td>\n",
       "      <td>0.167738</td>\n",
       "      <td>-0.092096</td>\n",
       "      <td>-0.900984</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.303623</td>\n",
       "      <td>1.355812</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.091313</td>\n",
       "      <td>-0.627749</td>\n",
       "      <td>-0.877419</td>\n",
       "      <td>0.346405</td>\n",
       "      <td>-0.977397</td>\n",
       "      <td>-0.284000</td>\n",
       "      <td>-1.082209</td>\n",
       "      <td>-0.770293</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.983430</td>\n",
       "      <td>-0.561454</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.314162</td>\n",
       "      <td>-2.013451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-0.314418</td>\n",
       "      <td>-0.753138</td>\n",
       "      <td>-0.722589</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.628018</td>\n",
       "      <td>-0.102896</td>\n",
       "      <td>-0.194219</td>\n",
       "      <td>-0.319837</td>\n",
       "      <td>1.102533</td>\n",
       "      <td>2.439542</td>\n",
       "      <td>2.017790</td>\n",
       "      <td>1.034661</td>\n",
       "      <td>-0.690612</td>\n",
       "      <td>1.106431</td>\n",
       "      <td>-0.977397</td>\n",
       "      <td>1.564511</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-1.637609</td>\n",
       "      <td>-0.868165</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.091313</td>\n",
       "      <td>-0.627749</td>\n",
       "      <td>0.309539</td>\n",
       "      <td>-0.344123</td>\n",
       "      <td>0.321434</td>\n",
       "      <td>-0.264207</td>\n",
       "      <td>0.207090</td>\n",
       "      <td>-1.494860</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.947209</td>\n",
       "      <td>-0.086922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.314162</td>\n",
       "      <td>-2.013451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-1.337803</td>\n",
       "      <td>-1.337726</td>\n",
       "      <td>-1.337803</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.437130</td>\n",
       "      <td>1.172315</td>\n",
       "      <td>0.875191</td>\n",
       "      <td>-0.392816</td>\n",
       "      <td>-0.811837</td>\n",
       "      <td>-0.291810</td>\n",
       "      <td>-0.208737</td>\n",
       "      <td>0.150161</td>\n",
       "      <td>0.574307</td>\n",
       "      <td>-0.284132</td>\n",
       "      <td>0.363172</td>\n",
       "      <td>0.351546</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-0.984764</td>\n",
       "      <td>-0.309042</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.091313</td>\n",
       "      <td>1.191898</td>\n",
       "      <td>1.253929</td>\n",
       "      <td>-1.200952</td>\n",
       "      <td>1.406604</td>\n",
       "      <td>-1.358432</td>\n",
       "      <td>1.046996</td>\n",
       "      <td>2.268357</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.069322</td>\n",
       "      <td>-0.086922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.314162</td>\n",
       "      <td>-2.013451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-0.768720</td>\n",
       "      <td>1.144183</td>\n",
       "      <td>0.294747</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.314651</td>\n",
       "      <td>-0.220448</td>\n",
       "      <td>-0.531720</td>\n",
       "      <td>-0.626094</td>\n",
       "      <td>1.680026</td>\n",
       "      <td>0.738234</td>\n",
       "      <td>1.686806</td>\n",
       "      <td>1.934743</td>\n",
       "      <td>-0.367261</td>\n",
       "      <td>1.143818</td>\n",
       "      <td>-1.009924</td>\n",
       "      <td>0.671991</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.327633</td>\n",
       "      <td>0.032505</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39639</th>\n",
       "      <td>-3.716757</td>\n",
       "      <td>0.299307</td>\n",
       "      <td>-0.208059</td>\n",
       "      <td>-0.100354</td>\n",
       "      <td>-0.394045</td>\n",
       "      <td>-0.060357</td>\n",
       "      <td>0.207090</td>\n",
       "      <td>1.378593</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.507837</td>\n",
       "      <td>0.343989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.024409</td>\n",
       "      <td>-0.424676</td>\n",
       "      <td>26900.0</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>0.911184</td>\n",
       "      <td>2514.742857</td>\n",
       "      <td>-0.200666</td>\n",
       "      <td>0.179677</td>\n",
       "      <td>1.547643</td>\n",
       "      <td>1.792045</td>\n",
       "      <td>1.960439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.686325</td>\n",
       "      <td>-0.822431</td>\n",
       "      <td>0.344962</td>\n",
       "      <td>-0.966295</td>\n",
       "      <td>1.300303</td>\n",
       "      <td>0.369968</td>\n",
       "      <td>0.265549</td>\n",
       "      <td>-0.088382</td>\n",
       "      <td>-0.099273</td>\n",
       "      <td>0.086256</td>\n",
       "      <td>-0.011160</td>\n",
       "      <td>-0.331371</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.056493</td>\n",
       "      <td>0.032505</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39640</th>\n",
       "      <td>-3.716757</td>\n",
       "      <td>0.746312</td>\n",
       "      <td>-0.278442</td>\n",
       "      <td>1.442174</td>\n",
       "      <td>-0.472542</td>\n",
       "      <td>1.915979</td>\n",
       "      <td>0.207090</td>\n",
       "      <td>1.378593</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>-0.944538</td>\n",
       "      <td>-0.086922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.111863</td>\n",
       "      <td>-0.343553</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>-0.463864</td>\n",
       "      <td>1664.267857</td>\n",
       "      <td>0.457255</td>\n",
       "      <td>0.522204</td>\n",
       "      <td>0.512085</td>\n",
       "      <td>-0.200537</td>\n",
       "      <td>-0.045952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.229267</td>\n",
       "      <td>-0.398422</td>\n",
       "      <td>0.508340</td>\n",
       "      <td>1.064995</td>\n",
       "      <td>-0.650512</td>\n",
       "      <td>1.306810</td>\n",
       "      <td>0.854330</td>\n",
       "      <td>0.039433</td>\n",
       "      <td>-0.730368</td>\n",
       "      <td>0.757048</td>\n",
       "      <td>-0.660623</td>\n",
       "      <td>0.203796</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.416663</td>\n",
       "      <td>0.394831</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39641</th>\n",
       "      <td>-3.716757</td>\n",
       "      <td>-0.158415</td>\n",
       "      <td>0.091302</td>\n",
       "      <td>-0.222587</td>\n",
       "      <td>0.147508</td>\n",
       "      <td>-0.491646</td>\n",
       "      <td>1.335872</td>\n",
       "      <td>-0.770293</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.415071</td>\n",
       "      <td>0.343989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.072048</td>\n",
       "      <td>-0.460873</td>\n",
       "      <td>6200.0</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>0.404934</td>\n",
       "      <td>1753.882353</td>\n",
       "      <td>0.855287</td>\n",
       "      <td>1.068122</td>\n",
       "      <td>0.159376</td>\n",
       "      <td>-0.483671</td>\n",
       "      <td>-0.385722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.505177</td>\n",
       "      <td>-0.640777</td>\n",
       "      <td>-0.697652</td>\n",
       "      <td>1.015044</td>\n",
       "      <td>0.250808</td>\n",
       "      <td>0.697813</td>\n",
       "      <td>-0.997733</td>\n",
       "      <td>-0.324699</td>\n",
       "      <td>0.953765</td>\n",
       "      <td>-0.792776</td>\n",
       "      <td>0.900083</td>\n",
       "      <td>-0.660859</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.892713</td>\n",
       "      <td>-0.868165</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39642</th>\n",
       "      <td>-3.716757</td>\n",
       "      <td>-2.153192</td>\n",
       "      <td>0.617082</td>\n",
       "      <td>0.002434</td>\n",
       "      <td>0.705942</td>\n",
       "      <td>0.023808</td>\n",
       "      <td>0.329067</td>\n",
       "      <td>-0.770293</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.082436</td>\n",
       "      <td>-1.077791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.314162</td>\n",
       "      <td>-2.375909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>0.081465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.198484</td>\n",
       "      <td>-1.659634</td>\n",
       "      <td>-0.842009</td>\n",
       "      <td>-0.902979</td>\n",
       "      <td>-0.902124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.193188</td>\n",
       "      <td>0.241684</td>\n",
       "      <td>1.515967</td>\n",
       "      <td>0.021404</td>\n",
       "      <td>-0.064950</td>\n",
       "      <td>-1.048091</td>\n",
       "      <td>-1.469914</td>\n",
       "      <td>-1.181859</td>\n",
       "      <td>0.827942</td>\n",
       "      <td>-1.331563</td>\n",
       "      <td>1.541598</td>\n",
       "      <td>-1.397816</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.475867</td>\n",
       "      <td>0.032505</td>\n",
       "      <td>-0.012500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39643</th>\n",
       "      <td>-3.716757</td>\n",
       "      <td>-0.158415</td>\n",
       "      <td>-1.245233</td>\n",
       "      <td>1.489854</td>\n",
       "      <td>-1.299567</td>\n",
       "      <td>1.540148</td>\n",
       "      <td>-1.696325</td>\n",
       "      <td>-0.770293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.700679</td>\n",
       "      <td>-1.654145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.622922</td>\n",
       "      <td>-1.749738</td>\n",
       "      <td>205600.0</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>0.859487</td>\n",
       "      <td>3035.080555</td>\n",
       "      <td>-0.473319</td>\n",
       "      <td>0.424711</td>\n",
       "      <td>0.512085</td>\n",
       "      <td>-0.200537</td>\n",
       "      <td>-0.045952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.309937</td>\n",
       "      <td>1.816184</td>\n",
       "      <td>0.115936</td>\n",
       "      <td>0.239210</td>\n",
       "      <td>0.067294</td>\n",
       "      <td>0.792819</td>\n",
       "      <td>-0.159920</td>\n",
       "      <td>1.407029</td>\n",
       "      <td>-0.296564</td>\n",
       "      <td>0.920030</td>\n",
       "      <td>-0.810958</td>\n",
       "      <td>-1.304289</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.532339</td>\n",
       "      <td>0.988570</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39644 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rank( timedelta)  rank( n_tokens_title)  rank( n_tokens_content)  \\\n",
       "0              3.091313               0.746312                -0.830751   \n",
       "1              3.091313              -0.627749                -0.629945   \n",
       "2              3.091313              -0.627749                -0.877419   \n",
       "3              3.091313              -0.627749                 0.309539   \n",
       "4              3.091313               1.191898                 1.253929   \n",
       "...                 ...                    ...                      ...   \n",
       "39639         -3.716757               0.299307                -0.208059   \n",
       "39640         -3.716757               0.746312                -0.278442   \n",
       "39641         -3.716757              -0.158415                 0.091302   \n",
       "39642         -3.716757              -2.153192                 0.617082   \n",
       "39643         -3.716757              -0.158415                -1.245233   \n",
       "\n",
       "       rank( n_unique_tokens)  rank( n_non_stop_words)  \\\n",
       "0                    1.171875                -0.859076   \n",
       "1                    0.636396                -0.682170   \n",
       "2                    0.346405                -0.977397   \n",
       "3                   -0.344123                 0.321434   \n",
       "4                   -1.200952                 1.406604   \n",
       "...                       ...                      ...   \n",
       "39639               -0.100354                -0.394045   \n",
       "39640                1.442174                -0.472542   \n",
       "39641               -0.222587                 0.147508   \n",
       "39642                0.002434                 0.705942   \n",
       "39643                1.489854                -1.299567   \n",
       "\n",
       "       rank( n_non_stop_unique_tokens)  rank( num_hrefs)  \\\n",
       "0                             1.250329         -0.773700   \n",
       "1                             1.037852         -1.082209   \n",
       "2                            -0.284000         -1.082209   \n",
       "3                            -0.264207          0.207090   \n",
       "4                            -1.358432          1.046996   \n",
       "...                                ...               ...   \n",
       "39639                        -0.060357          0.207090   \n",
       "39640                         1.915979          0.207090   \n",
       "39641                        -0.491646          1.335872   \n",
       "39642                         0.023808          0.329067   \n",
       "39643                         1.540148         -1.696325   \n",
       "\n",
       "       rank( num_self_hrefs)   num_imgs   num_videos  \\\n",
       "0                  -0.247319        1.0          0.0   \n",
       "1                  -0.770293        1.0          0.0   \n",
       "2                  -0.770293        1.0          0.0   \n",
       "3                  -1.494860        1.0          0.0   \n",
       "4                   2.268357       20.0          0.0   \n",
       "...                      ...        ...          ...   \n",
       "39639               1.378593        1.0          1.0   \n",
       "39640               1.378593        3.0         48.0   \n",
       "39641              -0.770293       12.0          1.0   \n",
       "39642              -0.770293        1.0          0.0   \n",
       "39643              -0.770293        0.0          2.0   \n",
       "\n",
       "       rank( average_token_length)  rank( num_keywords)  \\\n",
       "0                         0.060832            -1.077791   \n",
       "1                         0.874264            -1.654145   \n",
       "2                        -0.983430            -0.561454   \n",
       "3                        -0.947209            -0.086922   \n",
       "4                         0.069322            -0.086922   \n",
       "...                            ...                  ...   \n",
       "39639                    -0.507837             0.343989   \n",
       "39640                    -0.944538            -0.086922   \n",
       "39641                     1.415071             0.343989   \n",
       "39642                     1.082436            -1.077791   \n",
       "39643                    -0.700679            -1.654145   \n",
       "\n",
       "        data_channel_is_lifestyle   data_channel_is_entertainment  \\\n",
       "0                             0.0                             1.0   \n",
       "1                             0.0                             0.0   \n",
       "2                             0.0                             0.0   \n",
       "3                             0.0                             1.0   \n",
       "4                             0.0                             0.0   \n",
       "...                           ...                             ...   \n",
       "39639                         0.0                             0.0   \n",
       "39640                         0.0                             0.0   \n",
       "39641                         0.0                             0.0   \n",
       "39642                         0.0                             0.0   \n",
       "39643                         0.0                             1.0   \n",
       "\n",
       "        data_channel_is_bus   data_channel_is_socmed   data_channel_is_tech  \\\n",
       "0                       0.0                      0.0                    0.0   \n",
       "1                       1.0                      0.0                    0.0   \n",
       "2                       1.0                      0.0                    0.0   \n",
       "3                       0.0                      0.0                    0.0   \n",
       "4                       0.0                      0.0                    1.0   \n",
       "...                     ...                      ...                    ...   \n",
       "39639                   0.0                      0.0                    1.0   \n",
       "39640                   0.0                      1.0                    0.0   \n",
       "39641                   0.0                      0.0                    0.0   \n",
       "39642                   0.0                      0.0                    0.0   \n",
       "39643                   0.0                      0.0                    0.0   \n",
       "\n",
       "        data_channel_is_world   kw_min_min  rank( kw_max_min)  \\\n",
       "0                         0.0          0.0          -2.314162   \n",
       "1                         0.0          0.0          -2.314162   \n",
       "2                         0.0          0.0          -2.314162   \n",
       "3                         0.0          0.0          -2.314162   \n",
       "4                         0.0          0.0          -2.314162   \n",
       "...                       ...          ...                ...   \n",
       "39639                     0.0         -1.0           0.024409   \n",
       "39640                     0.0         -1.0          -0.111863   \n",
       "39641                     0.0         -1.0           0.072048   \n",
       "39642                     1.0         -1.0          -2.314162   \n",
       "39643                     0.0         -1.0          -1.622922   \n",
       "\n",
       "       rank( kw_avg_min)   kw_min_max   kw_max_max  rank( kw_avg_max)  \\\n",
       "0              -2.013451          0.0          0.0          -3.091313   \n",
       "1              -2.013451          0.0          0.0          -3.091313   \n",
       "2              -2.013451          0.0          0.0          -3.091313   \n",
       "3              -2.013451          0.0          0.0          -3.091313   \n",
       "4              -2.013451          0.0          0.0          -3.091313   \n",
       "...                  ...          ...          ...                ...   \n",
       "39639          -0.424676      26900.0     843300.0           0.911184   \n",
       "39640          -0.343553       6500.0     843300.0          -0.463864   \n",
       "39641          -0.460873       6200.0     843300.0           0.404934   \n",
       "39642          -2.375909          0.0     843300.0           0.081465   \n",
       "39643          -1.749738     205600.0     843300.0           0.859487   \n",
       "\n",
       "        kw_min_avg  rank( kw_max_avg)  rank( kw_avg_avg)  \\\n",
       "0         0.000000          -3.091313          -3.091313   \n",
       "1         0.000000          -3.091313          -3.091313   \n",
       "2         0.000000          -3.091313          -3.091313   \n",
       "3         0.000000          -3.091313          -3.091313   \n",
       "4         0.000000          -3.091313          -3.091313   \n",
       "...            ...                ...                ...   \n",
       "39639  2514.742857          -0.200666           0.179677   \n",
       "39640  1664.267857           0.457255           0.522204   \n",
       "39641  1753.882353           0.855287           1.068122   \n",
       "39642     0.000000          -1.198484          -1.659634   \n",
       "39643  3035.080555          -0.473319           0.424711   \n",
       "\n",
       "       rank( self_reference_min_shares)  rank( self_reference_max_shares)  \\\n",
       "0                             -0.809729                         -0.897243   \n",
       "1                             -1.337803                         -1.337726   \n",
       "2                             -0.314418                         -0.753138   \n",
       "3                             -1.337803                         -1.337726   \n",
       "4                             -0.768720                          1.144183   \n",
       "...                                 ...                               ...   \n",
       "39639                          1.547643                          1.792045   \n",
       "39640                          0.512085                         -0.200537   \n",
       "39641                          0.159376                         -0.483671   \n",
       "39642                         -0.842009                         -0.902979   \n",
       "39643                          0.512085                         -0.200537   \n",
       "\n",
       "       rank( self_reference_avg_sharess)   weekday_is_monday  \\\n",
       "0                              -0.895353                 1.0   \n",
       "1                              -1.337803                 1.0   \n",
       "2                              -0.722589                 1.0   \n",
       "3                              -1.337803                 1.0   \n",
       "4                               0.294747                 1.0   \n",
       "...                                  ...                 ...   \n",
       "39639                           1.960439                 0.0   \n",
       "39640                          -0.045952                 0.0   \n",
       "39641                          -0.385722                 0.0   \n",
       "39642                          -0.902124                 0.0   \n",
       "39643                          -0.045952                 0.0   \n",
       "\n",
       "        weekday_is_tuesday   weekday_is_wednesday   weekday_is_thursday  \\\n",
       "0                      0.0                    0.0                   0.0   \n",
       "1                      0.0                    0.0                   0.0   \n",
       "2                      0.0                    0.0                   0.0   \n",
       "3                      0.0                    0.0                   0.0   \n",
       "4                      0.0                    0.0                   0.0   \n",
       "...                    ...                    ...                   ...   \n",
       "39639                  0.0                    1.0                   0.0   \n",
       "39640                  0.0                    1.0                   0.0   \n",
       "39641                  0.0                    1.0                   0.0   \n",
       "39642                  0.0                    1.0                   0.0   \n",
       "39643                  0.0                    1.0                   0.0   \n",
       "\n",
       "        weekday_is_friday   weekday_is_saturday   weekday_is_sunday  \\\n",
       "0                     0.0                   0.0                 0.0   \n",
       "1                     0.0                   0.0                 0.0   \n",
       "2                     0.0                   0.0                 0.0   \n",
       "3                     0.0                   0.0                 0.0   \n",
       "4                     0.0                   0.0                 0.0   \n",
       "...                   ...                   ...                 ...   \n",
       "39639                 0.0                   0.0                 0.0   \n",
       "39640                 0.0                   0.0                 0.0   \n",
       "39641                 0.0                   0.0                 0.0   \n",
       "39642                 0.0                   0.0                 0.0   \n",
       "39643                 0.0                   0.0                 0.0   \n",
       "\n",
       "        is_weekend  rank( LDA_00)  rank( LDA_01)  rank( LDA_02)  \\\n",
       "0              0.0       1.011874       1.107189       0.003762   \n",
       "1              0.0       1.506066       0.481113       0.164789   \n",
       "2              0.0       0.628018      -0.102896      -0.194219   \n",
       "3              0.0      -0.437130       1.172315       0.875191   \n",
       "4              0.0      -0.314651      -0.220448      -0.531720   \n",
       "...            ...            ...            ...            ...   \n",
       "39639          0.0      -0.686325      -0.822431       0.344962   \n",
       "39640          0.0      -0.229267      -0.398422       0.508340   \n",
       "39641          0.0       0.505177      -0.640777      -0.697652   \n",
       "39642          0.0       0.193188       0.241684       1.515967   \n",
       "39643          0.0       0.309937       1.816184       0.115936   \n",
       "\n",
       "       rank( LDA_03)  rank( LDA_04)  rank( global_subjectivity)  \\\n",
       "0           0.133906      -0.019697                    0.834551   \n",
       "1           0.215657       0.071541                   -1.206170   \n",
       "2          -0.319837       1.102533                    2.439542   \n",
       "3          -0.392816      -0.811837                   -0.291810   \n",
       "4          -0.626094       1.680026                    0.738234   \n",
       "...              ...            ...                         ...   \n",
       "39639      -0.966295       1.300303                    0.369968   \n",
       "39640       1.064995      -0.650512                    1.306810   \n",
       "39641       1.015044       0.250808                    0.697813   \n",
       "39642       0.021404      -0.064950                   -1.048091   \n",
       "39643       0.239210       0.067294                    0.792819   \n",
       "\n",
       "       rank( global_sentiment_polarity)  rank( global_rate_positive_words)  \\\n",
       "0                             -0.299737                           0.407371   \n",
       "1                              0.345029                           0.255150   \n",
       "2                              2.017790                           1.034661   \n",
       "3                             -0.208737                           0.150161   \n",
       "4                              1.686806                           1.934743   \n",
       "...                                 ...                                ...   \n",
       "39639                          0.265549                          -0.088382   \n",
       "39640                          0.854330                           0.039433   \n",
       "39641                         -0.997733                          -0.324699   \n",
       "39642                         -1.469914                          -1.181859   \n",
       "39643                         -0.159920                           1.407029   \n",
       "\n",
       "       rank( global_rate_negative_words)  rank( rate_positive_words)  \\\n",
       "0                              -0.186943                    0.425334   \n",
       "1                               0.038674                    0.167738   \n",
       "2                              -0.690612                    1.106431   \n",
       "3                               0.574307                   -0.284132   \n",
       "4                              -0.367261                    1.143818   \n",
       "...                                  ...                         ...   \n",
       "39639                          -0.099273                    0.086256   \n",
       "39640                          -0.730368                    0.757048   \n",
       "39641                           0.953765                   -0.792776   \n",
       "39642                           0.827942                   -1.331563   \n",
       "39643                          -0.296564                    0.920030   \n",
       "\n",
       "       rank( rate_negative_words)  rank( avg_positive_polarity)  \\\n",
       "0                       -0.344559                      0.257404   \n",
       "1                       -0.092096                     -0.900984   \n",
       "2                       -0.977397                      1.564511   \n",
       "3                        0.363172                      0.351546   \n",
       "4                       -1.009924                      0.671991   \n",
       "...                           ...                           ...   \n",
       "39639                   -0.011160                     -0.331371   \n",
       "39640                   -0.660623                      0.203796   \n",
       "39641                    0.900083                     -0.660859   \n",
       "39642                    1.541598                     -1.397816   \n",
       "39643                   -0.810958                     -1.304289   \n",
       "\n",
       "        min_positive_polarity   max_positive_polarity  \\\n",
       "0                    0.100000                    0.70   \n",
       "1                    0.033333                    0.70   \n",
       "2                    0.100000                    1.00   \n",
       "3                    0.136364                    0.80   \n",
       "4                    0.033333                    1.00   \n",
       "...                       ...                     ...   \n",
       "39639                0.100000                    0.75   \n",
       "39640                0.136364                    0.70   \n",
       "39641                0.136364                    0.50   \n",
       "39642                0.062500                    0.50   \n",
       "39643                0.100000                    0.50   \n",
       "\n",
       "       rank( avg_negative_polarity)  rank( min_negative_polarity)  \\\n",
       "0                         -0.840613                     -0.309042   \n",
       "1                          1.303623                      1.355812   \n",
       "2                         -1.637609                     -0.868165   \n",
       "3                         -0.984764                     -0.309042   \n",
       "4                          0.327633                      0.032505   \n",
       "...                             ...                           ...   \n",
       "39639                     -0.056493                      0.032505   \n",
       "39640                      0.416663                      0.394831   \n",
       "39641                     -0.892713                     -0.868165   \n",
       "39642                      0.475867                      0.032505   \n",
       "39643                      0.532339                      0.988570   \n",
       "\n",
       "        max_negative_polarity   title_subjectivity   title_sentiment_polarity  \\\n",
       "0                   -0.200000             0.500000                  -0.187500   \n",
       "1                   -0.100000             0.000000                   0.000000   \n",
       "2                   -0.133333             0.000000                   0.000000   \n",
       "3                   -0.166667             0.000000                   0.000000   \n",
       "4                   -0.050000             0.454545                   0.136364   \n",
       "...                       ...                  ...                        ...   \n",
       "39639               -0.125000             0.100000                   0.000000   \n",
       "39640               -0.100000             0.300000                   1.000000   \n",
       "39641               -0.166667             0.454545                   0.136364   \n",
       "39642               -0.012500             0.000000                   0.000000   \n",
       "39643               -0.200000             0.333333                   0.250000   \n",
       "\n",
       "        abs_title_subjectivity   abs_title_sentiment_polarity   shares  \n",
       "0                     0.000000                       0.187500      593  \n",
       "1                     0.500000                       0.000000      711  \n",
       "2                     0.500000                       0.000000     1500  \n",
       "3                     0.500000                       0.000000     1200  \n",
       "4                     0.045455                       0.136364      505  \n",
       "...                        ...                            ...      ...  \n",
       "39639                 0.400000                       0.000000     1800  \n",
       "39640                 0.200000                       1.000000     1900  \n",
       "39641                 0.045455                       0.136364     1900  \n",
       "39642                 0.500000                       0.000000     1100  \n",
       "39643                 0.166667                       0.250000     1300  \n",
       "\n",
       "[39644 rows x 60 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read preprocessed news dataset\n",
    "news_df = pd.read_csv('./prep_news_data.csv')\n",
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data into features and targets\n",
    "X = news_df.iloc[:, :-1].values\n",
    "y = news_df.iloc[:, -1].values\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Data normalization\n",
    "scaler_X = MinMaxScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "scaler_y = MinMaxScaler()\n",
    "y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "# Splitting data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.1, random_state=42)\n",
    "\n",
    "# Convertation data to PyTorch tensors\n",
    "train_features = torch.tensor(X_train, dtype=torch.float32)\n",
    "test_features = torch.tensor(X_test, dtype=torch.float32)\n",
    "train_targets = torch.tensor(y_train, dtype=torch.float32)\n",
    "test_targets = torch.tensor(y_test, dtype=torch.float32)    \n",
    "\n",
    "# Creating DataLoader\n",
    "train_data = TensorDataset(train_features, train_targets)\n",
    "test_data = TensorDataset(test_features, test_targets)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Neural Network class with flexible settings\n",
    "class FlexibleRegressionNN(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_layers_sizes: list, dropout_rate: float) -> None:\n",
    "        super(FlexibleRegressionNN, self).__init__()\n",
    "        layers = list()\n",
    "\n",
    "        # Define first and n-1 hidden layers\n",
    "        neuron_num = input_size\n",
    "        for size in hidden_layers_sizes:\n",
    "            layers.append(nn.Linear(neuron_num, size))\n",
    "            layers.append(nn.BatchNorm1d(size))\n",
    "            layers.append(nn.LeakyReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            neuron_num = size\n",
    "\n",
    "        layers.append(nn.Linear(neuron_num, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    # Define forward loop for NN\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for calculation RMSE\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    mse = torch.mean((y_true - y_pred) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "    return rmse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:43:42,946] A new study created in memory with name: no-name-c3a5546e-68a2-4888-b5ac-e084b0d1c851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qq/gt96f0sx7nn6wsr9d8n13r2w0000gn/T/ipykernel_83885/4155516338.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
      "/var/folders/qq/gt96f0sx7nn6wsr9d8n13r2w0000gn/T/ipykernel_83885/4155516338.py:37: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/88, Train Loss: 1.2208, Test Loss: 0.9766\n",
      "Epoch 2/88, Train Loss: 1.1397, Test Loss: 0.9927\n",
      "Epoch 3/88, Train Loss: 1.1520, Test Loss: 0.9977\n",
      "Epoch 4/88, Train Loss: 1.1443, Test Loss: 1.0054\n",
      "Epoch 5/88, Train Loss: 1.1566, Test Loss: 1.0008\n",
      "Epoch 6/88, Train Loss: 1.1566, Test Loss: 0.9944\n",
      "Epoch 7/88, Train Loss: 1.1496, Test Loss: 0.9934\n",
      "Epoch 8/88, Train Loss: 1.1322, Test Loss: 0.9888\n",
      "Epoch 9/88, Train Loss: 1.1313, Test Loss: 0.9815\n",
      "Epoch 10/88, Train Loss: 1.1417, Test Loss: 0.9734\n",
      "Epoch 11/88, Train Loss: 1.1015, Test Loss: 0.9768\n",
      "Epoch 12/88, Train Loss: 1.1528, Test Loss: 0.9725\n",
      "Epoch 13/88, Train Loss: 1.0913, Test Loss: 0.9687\n",
      "Epoch 14/88, Train Loss: 1.0934, Test Loss: 0.9627\n",
      "Epoch 15/88, Train Loss: 1.0836, Test Loss: 0.9561\n",
      "Epoch 16/88, Train Loss: 1.1194, Test Loss: 0.9563\n",
      "Epoch 17/88, Train Loss: 1.1267, Test Loss: 0.9598\n",
      "Epoch 18/88, Train Loss: 1.1254, Test Loss: 0.9616\n",
      "Epoch 19/88, Train Loss: 1.1204, Test Loss: 0.9602\n",
      "Epoch 20/88, Train Loss: 1.1000, Test Loss: 0.9594\n",
      "Epoch 21/88, Train Loss: 1.0798, Test Loss: 0.9551\n",
      "Epoch 22/88, Train Loss: 1.0887, Test Loss: 0.9532\n",
      "Epoch 23/88, Train Loss: 1.1221, Test Loss: 0.9529\n",
      "Epoch 24/88, Train Loss: 1.0854, Test Loss: 0.9553\n",
      "Epoch 25/88, Train Loss: 1.1026, Test Loss: 0.9557\n",
      "Epoch 26/88, Train Loss: 1.0778, Test Loss: 0.9564\n",
      "Epoch 27/88, Train Loss: 1.1118, Test Loss: 0.9573\n",
      "Epoch 28/88, Train Loss: 1.0515, Test Loss: 0.9584\n",
      "Epoch 29/88, Train Loss: 1.0547, Test Loss: 0.9610\n",
      "Epoch 30/88, Train Loss: 1.1351, Test Loss: 0.9635\n",
      "Epoch 31/88, Train Loss: 1.0675, Test Loss: 0.9600\n",
      "Epoch 32/88, Train Loss: 1.0905, Test Loss: 0.9606\n",
      "Epoch 33/88, Train Loss: 1.0505, Test Loss: 0.9619\n",
      "Epoch 34/88, Train Loss: 1.0640, Test Loss: 0.9640\n",
      "Epoch 35/88, Train Loss: 1.0747, Test Loss: 0.9648\n",
      "Epoch 36/88, Train Loss: 1.0646, Test Loss: 0.9680\n",
      "Epoch 37/88, Train Loss: 1.1167, Test Loss: 0.9660\n",
      "Epoch 38/88, Train Loss: 1.0533, Test Loss: 0.9646\n",
      "Epoch 39/88, Train Loss: 1.0844, Test Loss: 0.9618\n",
      "Epoch 40/88, Train Loss: 1.0331, Test Loss: 0.9638\n",
      "Epoch 41/88, Train Loss: 1.0440, Test Loss: 0.9627\n",
      "Epoch 42/88, Train Loss: 1.0710, Test Loss: 0.9608\n",
      "Epoch 43/88, Train Loss: 1.0915, Test Loss: 0.9593\n",
      "Epoch 44/88, Train Loss: 1.0563, Test Loss: 0.9577\n",
      "Epoch 45/88, Train Loss: 1.0654, Test Loss: 0.9553\n",
      "Epoch 46/88, Train Loss: 1.0118, Test Loss: 0.9550\n",
      "Epoch 47/88, Train Loss: 1.0913, Test Loss: 0.9552\n",
      "Epoch 48/88, Train Loss: 1.0547, Test Loss: 0.9542\n",
      "Epoch 49/88, Train Loss: 1.0996, Test Loss: 0.9564\n",
      "Epoch 50/88, Train Loss: 1.0600, Test Loss: 0.9554\n",
      "Epoch 51/88, Train Loss: 1.0592, Test Loss: 0.9581\n",
      "Epoch 52/88, Train Loss: 1.0644, Test Loss: 0.9578\n",
      "Epoch 53/88, Train Loss: 1.0720, Test Loss: 0.9603\n",
      "Epoch 54/88, Train Loss: 1.0784, Test Loss: 0.9574\n",
      "Epoch 55/88, Train Loss: 1.0647, Test Loss: 0.9613\n",
      "Epoch 56/88, Train Loss: 1.0372, Test Loss: 0.9668\n",
      "Epoch 57/88, Train Loss: 1.0558, Test Loss: 0.9725\n",
      "Epoch 58/88, Train Loss: 1.0309, Test Loss: 0.9735\n",
      "Epoch 59/88, Train Loss: 1.0269, Test Loss: 0.9729\n",
      "Epoch 60/88, Train Loss: 1.0682, Test Loss: 0.9728\n",
      "Epoch 61/88, Train Loss: 1.0489, Test Loss: 0.9710\n",
      "Epoch 62/88, Train Loss: 1.0567, Test Loss: 0.9677\n",
      "Epoch 63/88, Train Loss: 1.0842, Test Loss: 0.9667\n",
      "Epoch 64/88, Train Loss: 1.0514, Test Loss: 0.9635\n",
      "Epoch 65/88, Train Loss: 1.0550, Test Loss: 0.9640\n",
      "Epoch 66/88, Train Loss: 1.0267, Test Loss: 0.9604\n",
      "Epoch 67/88, Train Loss: 1.0222, Test Loss: 0.9605\n",
      "Epoch 68/88, Train Loss: 1.0075, Test Loss: 0.9611\n",
      "Epoch 69/88, Train Loss: 1.0720, Test Loss: 0.9605\n",
      "Epoch 70/88, Train Loss: 1.0428, Test Loss: 0.9633\n",
      "Epoch 71/88, Train Loss: 1.0531, Test Loss: 0.9629\n",
      "Epoch 72/88, Train Loss: 1.0353, Test Loss: 0.9653\n",
      "Epoch 73/88, Train Loss: 1.0340, Test Loss: 0.9631\n",
      "Epoch 74/88, Train Loss: 1.0550, Test Loss: 0.9598\n",
      "Epoch 75/88, Train Loss: 1.0375, Test Loss: 0.9617\n",
      "Epoch 76/88, Train Loss: 1.0565, Test Loss: 0.9617\n",
      "Epoch 77/88, Train Loss: 1.0101, Test Loss: 0.9636\n",
      "Epoch 78/88, Train Loss: 1.0393, Test Loss: 0.9633\n",
      "Epoch 79/88, Train Loss: 1.0230, Test Loss: 0.9609\n",
      "Epoch 80/88, Train Loss: 1.0585, Test Loss: 0.9639\n",
      "Epoch 81/88, Train Loss: 1.0306, Test Loss: 0.9640\n",
      "Epoch 82/88, Train Loss: 1.0353, Test Loss: 0.9609\n",
      "Epoch 83/88, Train Loss: 1.0491, Test Loss: 0.9629\n",
      "Epoch 84/88, Train Loss: 1.0420, Test Loss: 0.9612\n",
      "Epoch 85/88, Train Loss: 1.0067, Test Loss: 0.9619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:43:48,492] Trial 0 finished with value: 0.9587828516960144 and parameters: {'num_hidden_layers': 9, 'layer_0_size': 104, 'layer_1_size': 130, 'layer_2_size': 40, 'layer_3_size': 199, 'layer_4_size': 110, 'layer_5_size': 59, 'layer_6_size': 63, 'layer_7_size': 236, 'layer_8_size': 206, 'dropout_rate': 0.38039803353944734, 'learning_rate': 9.78130281307678e-05, 'batch_size': 128, 'epochs': 88}. Best is trial 0 with value: 0.9587828516960144.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/88, Train Loss: 1.0598, Test Loss: 0.9599\n",
      "Epoch 87/88, Train Loss: 1.0577, Test Loss: 0.9610\n",
      "Epoch 88/88, Train Loss: 1.0261, Test Loss: 0.9588\n",
      "Epoch 1/33, Train Loss: 1.1686, Test Loss: 1.0060\n",
      "Epoch 2/33, Train Loss: 1.0783, Test Loss: 1.0033\n",
      "Epoch 3/33, Train Loss: 1.0418, Test Loss: 0.9984\n",
      "Epoch 4/33, Train Loss: 1.0223, Test Loss: 0.9929\n",
      "Epoch 5/33, Train Loss: 1.0099, Test Loss: 0.9920\n",
      "Epoch 6/33, Train Loss: 0.9672, Test Loss: 1.0000\n",
      "Epoch 7/33, Train Loss: 0.9477, Test Loss: 1.0141\n",
      "Epoch 8/33, Train Loss: 0.9076, Test Loss: 1.0313\n",
      "Epoch 9/33, Train Loss: 0.8814, Test Loss: 1.0475\n",
      "Epoch 10/33, Train Loss: 0.8393, Test Loss: 1.0632\n",
      "Epoch 11/33, Train Loss: 0.8089, Test Loss: 1.0809\n",
      "Epoch 12/33, Train Loss: 0.8202, Test Loss: 1.0901\n",
      "Epoch 13/33, Train Loss: 0.7747, Test Loss: 1.0997\n",
      "Epoch 14/33, Train Loss: 0.7111, Test Loss: 1.1091\n",
      "Epoch 15/33, Train Loss: 0.7346, Test Loss: 1.1266\n",
      "Epoch 16/33, Train Loss: 0.6523, Test Loss: 1.1674\n",
      "Epoch 17/33, Train Loss: 0.6162, Test Loss: 1.1976\n",
      "Epoch 18/33, Train Loss: 0.6243, Test Loss: 1.2193\n",
      "Epoch 19/33, Train Loss: 0.6124, Test Loss: 1.2091\n",
      "Epoch 20/33, Train Loss: 0.6160, Test Loss: 1.1990\n",
      "Epoch 21/33, Train Loss: 0.5816, Test Loss: 1.2237\n",
      "Epoch 22/33, Train Loss: 0.5537, Test Loss: 1.2419\n",
      "Epoch 23/33, Train Loss: 0.5268, Test Loss: 1.2576\n",
      "Epoch 24/33, Train Loss: 0.4922, Test Loss: 1.2491\n",
      "Epoch 25/33, Train Loss: 0.4598, Test Loss: 1.2400\n",
      "Epoch 26/33, Train Loss: 0.4564, Test Loss: 1.2400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:43:49,727] Trial 1 finished with value: 1.2910964488983154 and parameters: {'num_hidden_layers': 5, 'layer_0_size': 184, 'layer_1_size': 222, 'layer_2_size': 125, 'layer_3_size': 117, 'layer_4_size': 256, 'dropout_rate': 0.19053589558160855, 'learning_rate': 0.0012132199059513551, 'batch_size': 256, 'epochs': 33}. Best is trial 0 with value: 0.9587828516960144.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/33, Train Loss: 0.4478, Test Loss: 1.2593\n",
      "Epoch 28/33, Train Loss: 0.4424, Test Loss: 1.2792\n",
      "Epoch 29/33, Train Loss: 0.4633, Test Loss: 1.2581\n",
      "Epoch 30/33, Train Loss: 0.4075, Test Loss: 1.2431\n",
      "Epoch 31/33, Train Loss: 0.3879, Test Loss: 1.2537\n",
      "Epoch 32/33, Train Loss: 0.3966, Test Loss: 1.2745\n",
      "Epoch 33/33, Train Loss: 0.3874, Test Loss: 1.2911\n",
      "Epoch 1/60, Train Loss: 1.1662, Test Loss: 1.1537\n",
      "Epoch 2/60, Train Loss: 1.1779, Test Loss: 1.1552\n",
      "Epoch 3/60, Train Loss: 1.1740, Test Loss: 1.1549\n",
      "Epoch 4/60, Train Loss: 1.1182, Test Loss: 1.1540\n",
      "Epoch 5/60, Train Loss: 1.1688, Test Loss: 1.1537\n",
      "Epoch 6/60, Train Loss: 1.1281, Test Loss: 1.1551\n",
      "Epoch 7/60, Train Loss: 1.1466, Test Loss: 1.1597\n",
      "Epoch 8/60, Train Loss: 1.1295, Test Loss: 1.1603\n",
      "Epoch 9/60, Train Loss: 1.1445, Test Loss: 1.1586\n",
      "Epoch 10/60, Train Loss: 1.0918, Test Loss: 1.1618\n",
      "Epoch 11/60, Train Loss: 1.1185, Test Loss: 1.1590\n",
      "Epoch 12/60, Train Loss: 1.1308, Test Loss: 1.1500\n",
      "Epoch 13/60, Train Loss: 1.1302, Test Loss: 1.1449\n",
      "Epoch 14/60, Train Loss: 1.1173, Test Loss: 1.1411\n",
      "Epoch 15/60, Train Loss: 1.1328, Test Loss: 1.1485\n",
      "Epoch 16/60, Train Loss: 1.1092, Test Loss: 1.1547\n",
      "Epoch 17/60, Train Loss: 1.1090, Test Loss: 1.1601\n",
      "Epoch 18/60, Train Loss: 1.0915, Test Loss: 1.1687\n",
      "Epoch 19/60, Train Loss: 1.1064, Test Loss: 1.1759\n",
      "Epoch 20/60, Train Loss: 1.0967, Test Loss: 1.1741\n",
      "Epoch 21/60, Train Loss: 1.1467, Test Loss: 1.1710\n",
      "Epoch 22/60, Train Loss: 1.1059, Test Loss: 1.1663\n",
      "Epoch 23/60, Train Loss: 1.1232, Test Loss: 1.1655\n",
      "Epoch 24/60, Train Loss: 1.1177, Test Loss: 1.1622\n",
      "Epoch 25/60, Train Loss: 1.0976, Test Loss: 1.1628\n",
      "Epoch 26/60, Train Loss: 1.1059, Test Loss: 1.1686\n",
      "Epoch 27/60, Train Loss: 1.1383, Test Loss: 1.1694\n",
      "Epoch 28/60, Train Loss: 1.1215, Test Loss: 1.1749\n",
      "Epoch 29/60, Train Loss: 1.1100, Test Loss: 1.1725\n",
      "Epoch 30/60, Train Loss: 1.0966, Test Loss: 1.1703\n",
      "Epoch 31/60, Train Loss: 1.1163, Test Loss: 1.1626\n",
      "Epoch 32/60, Train Loss: 1.1045, Test Loss: 1.1606\n",
      "Epoch 33/60, Train Loss: 1.1106, Test Loss: 1.1562\n",
      "Epoch 34/60, Train Loss: 1.0951, Test Loss: 1.1599\n",
      "Epoch 35/60, Train Loss: 1.1124, Test Loss: 1.1600\n",
      "Epoch 36/60, Train Loss: 1.1087, Test Loss: 1.1580\n",
      "Epoch 37/60, Train Loss: 1.0783, Test Loss: 1.1577\n",
      "Epoch 38/60, Train Loss: 1.1099, Test Loss: 1.1579\n",
      "Epoch 39/60, Train Loss: 1.1185, Test Loss: 1.1614\n",
      "Epoch 40/60, Train Loss: 1.1224, Test Loss: 1.1620\n",
      "Epoch 41/60, Train Loss: 1.1246, Test Loss: 1.1645\n",
      "Epoch 42/60, Train Loss: 1.1007, Test Loss: 1.1678\n",
      "Epoch 43/60, Train Loss: 1.0865, Test Loss: 1.1688\n",
      "Epoch 44/60, Train Loss: 1.1065, Test Loss: 1.1680\n",
      "Epoch 45/60, Train Loss: 1.1137, Test Loss: 1.1652\n",
      "Epoch 46/60, Train Loss: 1.1218, Test Loss: 1.1627\n",
      "Epoch 47/60, Train Loss: 1.1175, Test Loss: 1.1623\n",
      "Epoch 48/60, Train Loss: 1.0931, Test Loss: 1.1623\n",
      "Epoch 49/60, Train Loss: 1.1243, Test Loss: 1.1631\n",
      "Epoch 50/60, Train Loss: 1.0983, Test Loss: 1.1653\n",
      "Epoch 51/60, Train Loss: 1.0961, Test Loss: 1.1647\n",
      "Epoch 52/60, Train Loss: 1.0881, Test Loss: 1.1704\n",
      "Epoch 53/60, Train Loss: 1.1153, Test Loss: 1.1701\n",
      "Epoch 54/60, Train Loss: 1.0991, Test Loss: 1.1659\n",
      "Epoch 55/60, Train Loss: 1.0932, Test Loss: 1.1670\n",
      "Epoch 56/60, Train Loss: 1.1010, Test Loss: 1.1697\n",
      "Epoch 57/60, Train Loss: 1.0955, Test Loss: 1.1776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:43:54,092] Trial 2 finished with value: 1.1781203746795654 and parameters: {'num_hidden_layers': 16, 'layer_0_size': 38, 'layer_1_size': 163, 'layer_2_size': 194, 'layer_3_size': 141, 'layer_4_size': 149, 'layer_5_size': 52, 'layer_6_size': 231, 'layer_7_size': 159, 'layer_8_size': 230, 'layer_9_size': 135, 'layer_10_size': 106, 'layer_11_size': 95, 'layer_12_size': 68, 'layer_13_size': 40, 'layer_14_size': 88, 'layer_15_size': 95, 'dropout_rate': 0.11299549924589446, 'learning_rate': 0.00037914430240358724, 'batch_size': 256, 'epochs': 60}. Best is trial 0 with value: 0.9587828516960144.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/60, Train Loss: 1.0944, Test Loss: 1.1797\n",
      "Epoch 59/60, Train Loss: 1.0947, Test Loss: 1.1817\n",
      "Epoch 60/60, Train Loss: 1.1103, Test Loss: 1.1781\n",
      "Epoch 1/79, Train Loss: 1.0325, Test Loss: 0.8557\n",
      "Epoch 2/79, Train Loss: 1.0652, Test Loss: 0.8611\n",
      "Epoch 3/79, Train Loss: 1.0646, Test Loss: 0.8486\n",
      "Epoch 4/79, Train Loss: 1.0371, Test Loss: 0.9296\n",
      "Epoch 5/79, Train Loss: 0.9940, Test Loss: 0.8998\n",
      "Epoch 6/79, Train Loss: 1.0193, Test Loss: 0.8575\n",
      "Epoch 7/79, Train Loss: 0.9948, Test Loss: 0.8574\n",
      "Epoch 8/79, Train Loss: 1.0053, Test Loss: 0.8434\n",
      "Epoch 9/79, Train Loss: 0.9867, Test Loss: 0.8380\n",
      "Epoch 10/79, Train Loss: 0.9642, Test Loss: 0.8374\n",
      "Epoch 11/79, Train Loss: 0.9752, Test Loss: 0.8826\n",
      "Epoch 12/79, Train Loss: 1.0018, Test Loss: 0.8557\n",
      "Epoch 13/79, Train Loss: 0.9901, Test Loss: 0.8556\n",
      "Epoch 14/79, Train Loss: 0.9681, Test Loss: 0.8902\n",
      "Epoch 15/79, Train Loss: 0.9624, Test Loss: 0.8518\n",
      "Epoch 16/79, Train Loss: 0.9739, Test Loss: 0.8595\n",
      "Epoch 17/79, Train Loss: 0.9675, Test Loss: 0.8623\n",
      "Epoch 18/79, Train Loss: 0.9553, Test Loss: 0.8581\n",
      "Epoch 19/79, Train Loss: 0.9779, Test Loss: 0.8358\n",
      "Epoch 20/79, Train Loss: 0.9672, Test Loss: 0.8451\n",
      "Epoch 21/79, Train Loss: 0.9580, Test Loss: 0.8397\n",
      "Epoch 22/79, Train Loss: 0.9800, Test Loss: 0.8370\n",
      "Epoch 23/79, Train Loss: 0.9613, Test Loss: 0.8404\n",
      "Epoch 24/79, Train Loss: 0.9448, Test Loss: 0.8432\n",
      "Epoch 25/79, Train Loss: 0.9492, Test Loss: 0.8450\n",
      "Epoch 26/79, Train Loss: 0.9615, Test Loss: 0.8418\n",
      "Epoch 27/79, Train Loss: 0.9552, Test Loss: 0.8444\n",
      "Epoch 28/79, Train Loss: 0.9635, Test Loss: 0.8576\n",
      "Epoch 29/79, Train Loss: 0.9415, Test Loss: 0.8758\n",
      "Epoch 30/79, Train Loss: 0.9563, Test Loss: 0.8821\n",
      "Epoch 31/79, Train Loss: 0.9526, Test Loss: 0.8546\n",
      "Epoch 32/79, Train Loss: 0.9557, Test Loss: 0.8395\n",
      "Epoch 33/79, Train Loss: 0.9499, Test Loss: 0.8400\n",
      "Epoch 34/79, Train Loss: 0.9537, Test Loss: 0.8327\n",
      "Epoch 35/79, Train Loss: 0.9466, Test Loss: 0.8326\n",
      "Epoch 36/79, Train Loss: 0.9573, Test Loss: 0.8461\n",
      "Epoch 37/79, Train Loss: 0.9415, Test Loss: 0.8691\n",
      "Epoch 38/79, Train Loss: 0.9340, Test Loss: 0.8498\n",
      "Epoch 39/79, Train Loss: 0.9339, Test Loss: 0.8554\n",
      "Epoch 40/79, Train Loss: 0.9389, Test Loss: 0.8506\n",
      "Epoch 41/79, Train Loss: 0.9427, Test Loss: 0.8466\n",
      "Epoch 42/79, Train Loss: 0.9439, Test Loss: 0.8498\n",
      "Epoch 43/79, Train Loss: 0.9512, Test Loss: 0.8497\n",
      "Epoch 44/79, Train Loss: 0.9418, Test Loss: 0.8486\n",
      "Epoch 45/79, Train Loss: 0.9580, Test Loss: 0.8324\n",
      "Epoch 46/79, Train Loss: 0.9465, Test Loss: 0.8375\n",
      "Epoch 47/79, Train Loss: 0.9514, Test Loss: 0.8397\n",
      "Epoch 48/79, Train Loss: 0.9499, Test Loss: 0.8408\n",
      "Epoch 49/79, Train Loss: 0.9441, Test Loss: 0.8454\n",
      "Epoch 50/79, Train Loss: 0.9419, Test Loss: 0.8350\n",
      "Epoch 51/79, Train Loss: 0.9335, Test Loss: 0.8308\n",
      "Epoch 52/79, Train Loss: 0.9554, Test Loss: 0.8311\n",
      "Epoch 53/79, Train Loss: 0.9253, Test Loss: 0.8233\n",
      "Epoch 54/79, Train Loss: 0.9323, Test Loss: 0.8300\n",
      "Epoch 55/79, Train Loss: 0.9405, Test Loss: 0.8295\n",
      "Epoch 56/79, Train Loss: 0.9522, Test Loss: 0.8324\n",
      "Epoch 57/79, Train Loss: 0.9306, Test Loss: 0.8313\n",
      "Epoch 58/79, Train Loss: 0.9433, Test Loss: 0.8226\n",
      "Epoch 59/79, Train Loss: 0.9299, Test Loss: 0.8246\n",
      "Epoch 60/79, Train Loss: 0.9442, Test Loss: 0.8236\n",
      "Epoch 61/79, Train Loss: 0.9263, Test Loss: 0.8218\n",
      "Epoch 62/79, Train Loss: 0.9254, Test Loss: 0.8215\n",
      "Epoch 63/79, Train Loss: 0.9083, Test Loss: 0.8179\n",
      "Epoch 64/79, Train Loss: 0.9457, Test Loss: 0.8239\n",
      "Epoch 65/79, Train Loss: 0.9373, Test Loss: 0.8215\n",
      "Epoch 66/79, Train Loss: 0.9447, Test Loss: 0.8260\n",
      "Epoch 67/79, Train Loss: 0.9222, Test Loss: 0.8216\n",
      "Epoch 68/79, Train Loss: 0.9170, Test Loss: 0.8179\n",
      "Epoch 69/79, Train Loss: 0.9179, Test Loss: 0.8211\n",
      "Epoch 70/79, Train Loss: 0.8946, Test Loss: 0.8170\n",
      "Epoch 71/79, Train Loss: 0.9159, Test Loss: 0.8161\n",
      "Epoch 72/79, Train Loss: 0.9345, Test Loss: 0.8255\n",
      "Epoch 73/79, Train Loss: 0.9056, Test Loss: 0.8221\n",
      "Epoch 74/79, Train Loss: 0.9249, Test Loss: 0.8202\n",
      "Epoch 75/79, Train Loss: 0.9591, Test Loss: 0.8138\n",
      "Epoch 76/79, Train Loss: 0.9118, Test Loss: 0.8159\n",
      "Epoch 77/79, Train Loss: 0.9110, Test Loss: 0.8140\n",
      "Epoch 78/79, Train Loss: 0.9009, Test Loss: 0.8132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:44:20,280] Trial 3 finished with value: 0.8122723911489759 and parameters: {'num_hidden_layers': 16, 'layer_0_size': 161, 'layer_1_size': 208, 'layer_2_size': 139, 'layer_3_size': 125, 'layer_4_size': 224, 'layer_5_size': 242, 'layer_6_size': 36, 'layer_7_size': 135, 'layer_8_size': 107, 'layer_9_size': 253, 'layer_10_size': 132, 'layer_11_size': 164, 'layer_12_size': 248, 'layer_13_size': 137, 'layer_14_size': 112, 'layer_15_size': 88, 'dropout_rate': 0.24460844960558054, 'learning_rate': 0.0005978178713041178, 'batch_size': 32, 'epochs': 79}. Best is trial 3 with value: 0.8122723911489759.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/79, Train Loss: 0.9210, Test Loss: 0.8123\n",
      "Epoch 1/68, Train Loss: 1.3270, Test Loss: 0.8565\n",
      "Epoch 2/68, Train Loss: 1.2590, Test Loss: 0.8573\n",
      "Epoch 3/68, Train Loss: 1.2540, Test Loss: 0.8577\n",
      "Epoch 4/68, Train Loss: 1.2453, Test Loss: 0.8594\n",
      "Epoch 5/68, Train Loss: 1.2428, Test Loss: 0.8623\n",
      "Epoch 6/68, Train Loss: 1.2870, Test Loss: 0.8664\n",
      "Epoch 7/68, Train Loss: 1.2478, Test Loss: 0.8703\n",
      "Epoch 8/68, Train Loss: 1.1927, Test Loss: 0.8720\n",
      "Epoch 9/68, Train Loss: 1.2417, Test Loss: 0.8745\n",
      "Epoch 10/68, Train Loss: 1.1948, Test Loss: 0.8754\n",
      "Epoch 11/68, Train Loss: 1.1885, Test Loss: 0.8748\n",
      "Epoch 12/68, Train Loss: 1.1524, Test Loss: 0.8758\n",
      "Epoch 13/68, Train Loss: 1.1570, Test Loss: 0.8773\n",
      "Epoch 14/68, Train Loss: 1.1980, Test Loss: 0.8767\n",
      "Epoch 15/68, Train Loss: 1.1444, Test Loss: 0.8760\n",
      "Epoch 16/68, Train Loss: 1.1520, Test Loss: 0.8760\n",
      "Epoch 17/68, Train Loss: 1.1468, Test Loss: 0.8765\n",
      "Epoch 18/68, Train Loss: 1.1318, Test Loss: 0.8741\n",
      "Epoch 19/68, Train Loss: 1.1225, Test Loss: 0.8741\n",
      "Epoch 20/68, Train Loss: 1.1388, Test Loss: 0.8727\n",
      "Epoch 21/68, Train Loss: 1.1476, Test Loss: 0.8683\n",
      "Epoch 22/68, Train Loss: 1.1511, Test Loss: 0.8648\n",
      "Epoch 23/68, Train Loss: 1.0524, Test Loss: 0.8641\n",
      "Epoch 24/68, Train Loss: 1.1151, Test Loss: 0.8619\n",
      "Epoch 25/68, Train Loss: 1.1094, Test Loss: 0.8603\n",
      "Epoch 26/68, Train Loss: 1.1107, Test Loss: 0.8598\n",
      "Epoch 27/68, Train Loss: 1.0907, Test Loss: 0.8606\n",
      "Epoch 28/68, Train Loss: 1.1102, Test Loss: 0.8608\n",
      "Epoch 29/68, Train Loss: 1.1050, Test Loss: 0.8612\n",
      "Epoch 30/68, Train Loss: 1.1156, Test Loss: 0.8593\n",
      "Epoch 31/68, Train Loss: 1.1463, Test Loss: 0.8593\n",
      "Epoch 32/68, Train Loss: 1.0829, Test Loss: 0.8611\n",
      "Epoch 33/68, Train Loss: 1.1289, Test Loss: 0.8607\n",
      "Epoch 34/68, Train Loss: 1.0708, Test Loss: 0.8599\n",
      "Epoch 35/68, Train Loss: 1.0993, Test Loss: 0.8603\n",
      "Epoch 36/68, Train Loss: 1.1221, Test Loss: 0.8574\n",
      "Epoch 37/68, Train Loss: 1.1092, Test Loss: 0.8565\n",
      "Epoch 38/68, Train Loss: 1.1213, Test Loss: 0.8548\n",
      "Epoch 39/68, Train Loss: 1.1264, Test Loss: 0.8545\n",
      "Epoch 40/68, Train Loss: 1.0873, Test Loss: 0.8537\n",
      "Epoch 41/68, Train Loss: 1.0762, Test Loss: 0.8529\n",
      "Epoch 42/68, Train Loss: 1.0965, Test Loss: 0.8519\n",
      "Epoch 43/68, Train Loss: 1.0947, Test Loss: 0.8513\n",
      "Epoch 44/68, Train Loss: 1.0935, Test Loss: 0.8527\n",
      "Epoch 45/68, Train Loss: 1.0756, Test Loss: 0.8532\n",
      "Epoch 46/68, Train Loss: 1.1137, Test Loss: 0.8514\n",
      "Epoch 47/68, Train Loss: 1.0731, Test Loss: 0.8517\n",
      "Epoch 48/68, Train Loss: 1.0896, Test Loss: 0.8518\n",
      "Epoch 49/68, Train Loss: 1.0446, Test Loss: 0.8513\n",
      "Epoch 50/68, Train Loss: 1.0773, Test Loss: 0.8514\n",
      "Epoch 51/68, Train Loss: 1.0798, Test Loss: 0.8500\n",
      "Epoch 52/68, Train Loss: 1.0807, Test Loss: 0.8510\n",
      "Epoch 53/68, Train Loss: 1.0610, Test Loss: 0.8509\n",
      "Epoch 54/68, Train Loss: 1.1239, Test Loss: 0.8512\n",
      "Epoch 55/68, Train Loss: 1.0756, Test Loss: 0.8514\n",
      "Epoch 56/68, Train Loss: 1.0901, Test Loss: 0.8505\n",
      "Epoch 57/68, Train Loss: 1.0715, Test Loss: 0.8520\n",
      "Epoch 58/68, Train Loss: 1.1442, Test Loss: 0.8511\n",
      "Epoch 59/68, Train Loss: 1.1087, Test Loss: 0.8508\n",
      "Epoch 60/68, Train Loss: 1.0814, Test Loss: 0.8515\n",
      "Epoch 61/68, Train Loss: 1.0555, Test Loss: 0.8516\n",
      "Epoch 62/68, Train Loss: 1.0930, Test Loss: 0.8518\n",
      "Epoch 63/68, Train Loss: 1.0741, Test Loss: 0.8522\n",
      "Epoch 64/68, Train Loss: 1.0818, Test Loss: 0.8531\n",
      "Epoch 65/68, Train Loss: 1.0606, Test Loss: 0.8526\n",
      "Epoch 66/68, Train Loss: 1.0939, Test Loss: 0.8540\n",
      "Epoch 67/68, Train Loss: 1.1301, Test Loss: 0.8534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:44:23,496] Trial 4 finished with value: 0.8545936346054077 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 150, 'layer_1_size': 53, 'layer_2_size': 128, 'layer_3_size': 235, 'layer_4_size': 144, 'layer_5_size': 164, 'layer_6_size': 241, 'layer_7_size': 252, 'dropout_rate': 0.25042662470300386, 'learning_rate': 4.305198687009446e-05, 'batch_size': 256, 'epochs': 68}. Best is trial 3 with value: 0.8122723911489759.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/68, Train Loss: 1.1011, Test Loss: 0.8546\n",
      "Epoch 1/76, Train Loss: 1.4570, Test Loss: 1.0388\n",
      "Epoch 2/76, Train Loss: 1.3379, Test Loss: 1.0394\n",
      "Epoch 3/76, Train Loss: 1.2191, Test Loss: 1.0394\n",
      "Epoch 4/76, Train Loss: 1.1132, Test Loss: 1.0500\n",
      "Epoch 5/76, Train Loss: 1.0493, Test Loss: 1.0670\n",
      "Epoch 6/76, Train Loss: 1.0512, Test Loss: 1.0568\n",
      "Epoch 7/76, Train Loss: 1.0221, Test Loss: 1.0535\n",
      "Epoch 8/76, Train Loss: 1.0575, Test Loss: 1.0734\n",
      "Epoch 9/76, Train Loss: 1.0495, Test Loss: 1.0447\n",
      "Epoch 10/76, Train Loss: 1.0471, Test Loss: 1.0415\n",
      "Epoch 11/76, Train Loss: 1.0252, Test Loss: 1.0404\n",
      "Epoch 12/76, Train Loss: 1.0299, Test Loss: 1.0406\n",
      "Epoch 13/76, Train Loss: 1.0157, Test Loss: 1.0506\n",
      "Epoch 14/76, Train Loss: 1.0288, Test Loss: 1.0598\n",
      "Epoch 15/76, Train Loss: 1.0294, Test Loss: 1.0594\n",
      "Epoch 16/76, Train Loss: 1.0184, Test Loss: 1.0584\n",
      "Epoch 17/76, Train Loss: 1.0395, Test Loss: 1.0571\n",
      "Epoch 18/76, Train Loss: 1.0107, Test Loss: 1.0580\n",
      "Epoch 19/76, Train Loss: 0.9944, Test Loss: 1.0481\n",
      "Epoch 20/76, Train Loss: 0.9940, Test Loss: 1.0647\n",
      "Epoch 21/76, Train Loss: 1.0173, Test Loss: 1.0664\n",
      "Epoch 22/76, Train Loss: 1.0180, Test Loss: 1.0522\n",
      "Epoch 23/76, Train Loss: 1.0207, Test Loss: 1.0686\n",
      "Epoch 24/76, Train Loss: 1.0241, Test Loss: 1.0406\n",
      "Epoch 25/76, Train Loss: 1.0149, Test Loss: 1.0462\n",
      "Epoch 26/76, Train Loss: 0.9926, Test Loss: 1.0568\n",
      "Epoch 27/76, Train Loss: 0.9912, Test Loss: 1.0419\n",
      "Epoch 28/76, Train Loss: 0.9888, Test Loss: 1.0480\n",
      "Epoch 29/76, Train Loss: 0.9822, Test Loss: 1.0497\n",
      "Epoch 30/76, Train Loss: 0.9519, Test Loss: 1.0462\n",
      "Epoch 31/76, Train Loss: 0.9697, Test Loss: 1.0452\n",
      "Epoch 32/76, Train Loss: 0.9662, Test Loss: 1.0529\n",
      "Epoch 33/76, Train Loss: 0.9534, Test Loss: 1.0561\n",
      "Epoch 34/76, Train Loss: 0.9641, Test Loss: 1.0496\n",
      "Epoch 35/76, Train Loss: 0.9426, Test Loss: 1.0593\n",
      "Epoch 36/76, Train Loss: 0.9481, Test Loss: 1.0483\n",
      "Epoch 37/76, Train Loss: 0.9251, Test Loss: 1.0507\n",
      "Epoch 38/76, Train Loss: 0.9304, Test Loss: 1.0518\n",
      "Epoch 39/76, Train Loss: 0.9363, Test Loss: 1.0480\n",
      "Epoch 40/76, Train Loss: 0.9337, Test Loss: 1.0489\n",
      "Epoch 41/76, Train Loss: 0.9145, Test Loss: 1.0455\n",
      "Epoch 42/76, Train Loss: 0.9029, Test Loss: 1.0474\n",
      "Epoch 43/76, Train Loss: 0.9162, Test Loss: 1.0473\n",
      "Epoch 44/76, Train Loss: 0.9032, Test Loss: 1.0535\n",
      "Epoch 45/76, Train Loss: 0.8861, Test Loss: 1.0561\n",
      "Epoch 46/76, Train Loss: 0.9038, Test Loss: 1.0552\n",
      "Epoch 47/76, Train Loss: 0.8722, Test Loss: 1.0670\n",
      "Epoch 48/76, Train Loss: 0.8743, Test Loss: 1.1039\n",
      "Epoch 49/76, Train Loss: 0.8551, Test Loss: 1.0943\n",
      "Epoch 50/76, Train Loss: 0.8258, Test Loss: 1.1085\n",
      "Epoch 51/76, Train Loss: 0.8706, Test Loss: 1.1170\n",
      "Epoch 52/76, Train Loss: 0.7858, Test Loss: 1.1103\n",
      "Epoch 53/76, Train Loss: 0.8411, Test Loss: 1.1276\n",
      "Epoch 54/76, Train Loss: 0.8295, Test Loss: 1.1082\n",
      "Epoch 55/76, Train Loss: 0.7913, Test Loss: 1.1976\n",
      "Epoch 56/76, Train Loss: 0.7760, Test Loss: 1.1283\n",
      "Epoch 57/76, Train Loss: 0.7778, Test Loss: 1.0851\n",
      "Epoch 58/76, Train Loss: 0.7530, Test Loss: 1.1338\n",
      "Epoch 59/76, Train Loss: 0.7465, Test Loss: 1.1219\n",
      "Epoch 60/76, Train Loss: 0.7579, Test Loss: 1.1012\n",
      "Epoch 61/76, Train Loss: 0.7148, Test Loss: 1.2267\n",
      "Epoch 62/76, Train Loss: 0.7045, Test Loss: 1.2750\n",
      "Epoch 63/76, Train Loss: 0.7210, Test Loss: 1.1589\n",
      "Epoch 64/76, Train Loss: 0.6976, Test Loss: 1.1248\n",
      "Epoch 65/76, Train Loss: 0.6870, Test Loss: 1.2286\n",
      "Epoch 66/76, Train Loss: 0.6842, Test Loss: 1.1951\n",
      "Epoch 67/76, Train Loss: 0.6379, Test Loss: 1.2261\n",
      "Epoch 68/76, Train Loss: 0.6229, Test Loss: 1.3256\n",
      "Epoch 69/76, Train Loss: 0.6019, Test Loss: 1.2693\n",
      "Epoch 70/76, Train Loss: 0.6308, Test Loss: 1.2347\n",
      "Epoch 71/76, Train Loss: 0.6118, Test Loss: 1.3455\n",
      "Epoch 72/76, Train Loss: 0.5866, Test Loss: 1.3642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:44:28,551] Trial 5 finished with value: 1.3592239618301392 and parameters: {'num_hidden_layers': 12, 'layer_0_size': 129, 'layer_1_size': 229, 'layer_2_size': 105, 'layer_3_size': 241, 'layer_4_size': 112, 'layer_5_size': 172, 'layer_6_size': 225, 'layer_7_size': 134, 'layer_8_size': 81, 'layer_9_size': 164, 'layer_10_size': 178, 'layer_11_size': 255, 'dropout_rate': 0.3308235094877362, 'learning_rate': 0.005487427832120202, 'batch_size': 256, 'epochs': 76}. Best is trial 3 with value: 0.8122723911489759.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/76, Train Loss: 0.5733, Test Loss: 1.2328\n",
      "Epoch 74/76, Train Loss: 0.5776, Test Loss: 1.4035\n",
      "Epoch 75/76, Train Loss: 0.5777, Test Loss: 1.3374\n",
      "Epoch 76/76, Train Loss: 0.5457, Test Loss: 1.3592\n",
      "Epoch 1/81, Train Loss: 1.2033, Test Loss: 1.0014\n",
      "Epoch 2/81, Train Loss: 1.1525, Test Loss: 1.0354\n",
      "Epoch 3/81, Train Loss: 1.0503, Test Loss: 1.0417\n",
      "Epoch 4/81, Train Loss: 1.0776, Test Loss: 1.0311\n",
      "Epoch 5/81, Train Loss: 1.0724, Test Loss: 1.0285\n",
      "Epoch 6/81, Train Loss: 1.0265, Test Loss: 1.0159\n",
      "Epoch 7/81, Train Loss: 1.0432, Test Loss: 1.0030\n",
      "Epoch 8/81, Train Loss: 1.0459, Test Loss: 0.9915\n",
      "Epoch 9/81, Train Loss: 1.0546, Test Loss: 0.9944\n",
      "Epoch 10/81, Train Loss: 1.0206, Test Loss: 1.0109\n",
      "Epoch 11/81, Train Loss: 1.0107, Test Loss: 1.0199\n",
      "Epoch 12/81, Train Loss: 1.0352, Test Loss: 1.0210\n",
      "Epoch 13/81, Train Loss: 1.0468, Test Loss: 1.0168\n",
      "Epoch 14/81, Train Loss: 1.0255, Test Loss: 1.0229\n",
      "Epoch 15/81, Train Loss: 1.0118, Test Loss: 1.0350\n",
      "Epoch 16/81, Train Loss: 1.0199, Test Loss: 1.0214\n",
      "Epoch 17/81, Train Loss: 1.0183, Test Loss: 1.0335\n",
      "Epoch 18/81, Train Loss: 0.9961, Test Loss: 1.0084\n",
      "Epoch 19/81, Train Loss: 1.0143, Test Loss: 1.0024\n",
      "Epoch 20/81, Train Loss: 0.9902, Test Loss: 0.9965\n",
      "Epoch 21/81, Train Loss: 1.0193, Test Loss: 0.9980\n",
      "Epoch 22/81, Train Loss: 1.0016, Test Loss: 1.0100\n",
      "Epoch 23/81, Train Loss: 0.9945, Test Loss: 0.9995\n",
      "Epoch 24/81, Train Loss: 1.0401, Test Loss: 0.9975\n",
      "Epoch 25/81, Train Loss: 1.0059, Test Loss: 1.0001\n",
      "Epoch 26/81, Train Loss: 1.0026, Test Loss: 1.0042\n",
      "Epoch 27/81, Train Loss: 1.0004, Test Loss: 0.9997\n",
      "Epoch 28/81, Train Loss: 0.9899, Test Loss: 1.0004\n",
      "Epoch 29/81, Train Loss: 0.9879, Test Loss: 1.0149\n",
      "Epoch 30/81, Train Loss: 0.9738, Test Loss: 1.0094\n",
      "Epoch 31/81, Train Loss: 0.9999, Test Loss: 1.0121\n",
      "Epoch 32/81, Train Loss: 0.9968, Test Loss: 1.0223\n",
      "Epoch 33/81, Train Loss: 1.0008, Test Loss: 1.0340\n",
      "Epoch 34/81, Train Loss: 0.9770, Test Loss: 1.0362\n",
      "Epoch 35/81, Train Loss: 1.0022, Test Loss: 1.0256\n",
      "Epoch 36/81, Train Loss: 1.0118, Test Loss: 1.0230\n",
      "Epoch 37/81, Train Loss: 0.9829, Test Loss: 1.0062\n",
      "Epoch 38/81, Train Loss: 1.0074, Test Loss: 1.0103\n",
      "Epoch 39/81, Train Loss: 0.9713, Test Loss: 1.0088\n",
      "Epoch 40/81, Train Loss: 0.9926, Test Loss: 1.0014\n",
      "Epoch 41/81, Train Loss: 0.9752, Test Loss: 0.9978\n",
      "Epoch 42/81, Train Loss: 0.9932, Test Loss: 0.9928\n",
      "Epoch 43/81, Train Loss: 0.9834, Test Loss: 1.0019\n",
      "Epoch 44/81, Train Loss: 0.9602, Test Loss: 0.9994\n",
      "Epoch 45/81, Train Loss: 1.0091, Test Loss: 1.0030\n",
      "Epoch 46/81, Train Loss: 1.0077, Test Loss: 1.0112\n",
      "Epoch 47/81, Train Loss: 0.9820, Test Loss: 0.9979\n",
      "Epoch 48/81, Train Loss: 0.9801, Test Loss: 1.0003\n",
      "Epoch 49/81, Train Loss: 0.9983, Test Loss: 1.0019\n",
      "Epoch 50/81, Train Loss: 1.0012, Test Loss: 0.9989\n",
      "Epoch 51/81, Train Loss: 0.9749, Test Loss: 0.9996\n",
      "Epoch 52/81, Train Loss: 1.0002, Test Loss: 1.0065\n",
      "Epoch 53/81, Train Loss: 0.9796, Test Loss: 0.9958\n",
      "Epoch 54/81, Train Loss: 0.9864, Test Loss: 0.9970\n",
      "Epoch 55/81, Train Loss: 0.9852, Test Loss: 1.0014\n",
      "Epoch 56/81, Train Loss: 0.9821, Test Loss: 0.9958\n",
      "Epoch 57/81, Train Loss: 0.9898, Test Loss: 0.9917\n",
      "Epoch 58/81, Train Loss: 1.0053, Test Loss: 1.0008\n",
      "Epoch 59/81, Train Loss: 0.9691, Test Loss: 1.0006\n",
      "Epoch 60/81, Train Loss: 0.9822, Test Loss: 1.0195\n",
      "Epoch 61/81, Train Loss: 0.9753, Test Loss: 1.0134\n",
      "Epoch 62/81, Train Loss: 0.9620, Test Loss: 1.0074\n",
      "Epoch 63/81, Train Loss: 0.9854, Test Loss: 0.9950\n",
      "Epoch 64/81, Train Loss: 0.9885, Test Loss: 1.0026\n",
      "Epoch 65/81, Train Loss: 0.9625, Test Loss: 1.0038\n",
      "Epoch 66/81, Train Loss: 0.9834, Test Loss: 1.0015\n",
      "Epoch 67/81, Train Loss: 0.9763, Test Loss: 0.9952\n",
      "Epoch 68/81, Train Loss: 0.9848, Test Loss: 0.9994\n",
      "Epoch 69/81, Train Loss: 0.9971, Test Loss: 1.0042\n",
      "Epoch 70/81, Train Loss: 0.9865, Test Loss: 1.0007\n",
      "Epoch 71/81, Train Loss: 1.0015, Test Loss: 1.0055\n",
      "Epoch 72/81, Train Loss: 0.9573, Test Loss: 0.9963\n",
      "Epoch 73/81, Train Loss: 0.9764, Test Loss: 1.0064\n",
      "Epoch 74/81, Train Loss: 0.9663, Test Loss: 1.0013\n",
      "Epoch 75/81, Train Loss: 0.9660, Test Loss: 1.0001\n",
      "Epoch 76/81, Train Loss: 0.9742, Test Loss: 0.9967\n",
      "Epoch 77/81, Train Loss: 0.9869, Test Loss: 1.0036\n",
      "Epoch 78/81, Train Loss: 0.9900, Test Loss: 1.0094\n",
      "Epoch 79/81, Train Loss: 0.9802, Test Loss: 0.9965\n",
      "Epoch 80/81, Train Loss: 0.9940, Test Loss: 1.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:44:56,252] Trial 6 finished with value: 0.9956796850476947 and parameters: {'num_hidden_layers': 17, 'layer_0_size': 201, 'layer_1_size': 44, 'layer_2_size': 109, 'layer_3_size': 168, 'layer_4_size': 214, 'layer_5_size': 157, 'layer_6_size': 237, 'layer_7_size': 52, 'layer_8_size': 238, 'layer_9_size': 119, 'layer_10_size': 252, 'layer_11_size': 204, 'layer_12_size': 88, 'layer_13_size': 176, 'layer_14_size': 245, 'layer_15_size': 68, 'layer_16_size': 116, 'dropout_rate': 0.254341726180068, 'learning_rate': 0.00035690930552310235, 'batch_size': 32, 'epochs': 81}. Best is trial 3 with value: 0.8122723911489759.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/81, Train Loss: 0.9926, Test Loss: 0.9957\n",
      "Epoch 1/36, Train Loss: 1.1962, Test Loss: 0.9629\n",
      "Epoch 2/36, Train Loss: 1.1573, Test Loss: 0.9664\n",
      "Epoch 3/36, Train Loss: 1.0509, Test Loss: 0.9711\n",
      "Epoch 4/36, Train Loss: 1.0221, Test Loss: 0.9679\n",
      "Epoch 5/36, Train Loss: 1.0713, Test Loss: 0.9680\n",
      "Epoch 6/36, Train Loss: 0.9964, Test Loss: 0.9651\n",
      "Epoch 7/36, Train Loss: 1.0370, Test Loss: 0.9681\n",
      "Epoch 8/36, Train Loss: 1.0055, Test Loss: 0.9679\n",
      "Epoch 9/36, Train Loss: 1.0153, Test Loss: 0.9666\n",
      "Epoch 10/36, Train Loss: 1.0051, Test Loss: 0.9624\n",
      "Epoch 11/36, Train Loss: 0.9759, Test Loss: 0.9640\n",
      "Epoch 12/36, Train Loss: 0.9889, Test Loss: 0.9672\n",
      "Epoch 13/36, Train Loss: 0.9933, Test Loss: 0.9691\n",
      "Epoch 14/36, Train Loss: 0.9541, Test Loss: 0.9711\n",
      "Epoch 15/36, Train Loss: 0.9602, Test Loss: 0.9709\n",
      "Epoch 16/36, Train Loss: 0.9357, Test Loss: 0.9727\n",
      "Epoch 17/36, Train Loss: 0.9463, Test Loss: 0.9780\n",
      "Epoch 18/36, Train Loss: 0.9104, Test Loss: 0.9840\n",
      "Epoch 19/36, Train Loss: 0.9157, Test Loss: 0.9952\n",
      "Epoch 20/36, Train Loss: 0.9142, Test Loss: 0.9915\n",
      "Epoch 21/36, Train Loss: 0.8696, Test Loss: 0.9882\n",
      "Epoch 22/36, Train Loss: 0.9040, Test Loss: 0.9902\n",
      "Epoch 23/36, Train Loss: 0.8707, Test Loss: 0.9891\n",
      "Epoch 24/36, Train Loss: 0.8535, Test Loss: 0.9873\n",
      "Epoch 25/36, Train Loss: 0.8432, Test Loss: 0.9935\n",
      "Epoch 26/36, Train Loss: 0.8544, Test Loss: 0.9974\n",
      "Epoch 27/36, Train Loss: 0.8311, Test Loss: 0.9995\n",
      "Epoch 28/36, Train Loss: 0.8111, Test Loss: 1.0093\n",
      "Epoch 29/36, Train Loss: 0.7946, Test Loss: 1.0112\n",
      "Epoch 30/36, Train Loss: 0.8105, Test Loss: 1.0130\n",
      "Epoch 31/36, Train Loss: 0.7848, Test Loss: 1.0186\n",
      "Epoch 32/36, Train Loss: 0.7984, Test Loss: 1.0285\n",
      "Epoch 33/36, Train Loss: 0.7430, Test Loss: 1.0356\n",
      "Epoch 34/36, Train Loss: 0.7305, Test Loss: 1.0532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:44:57,385] Trial 7 finished with value: 1.051755964756012 and parameters: {'num_hidden_layers': 4, 'layer_0_size': 217, 'layer_1_size': 50, 'layer_2_size': 180, 'layer_3_size': 179, 'dropout_rate': 0.3652937887391313, 'learning_rate': 0.0010300310564229467, 'batch_size': 128, 'epochs': 36}. Best is trial 3 with value: 0.8122723911489759.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/36, Train Loss: 0.7520, Test Loss: 1.0590\n",
      "Epoch 36/36, Train Loss: 0.7106, Test Loss: 1.0518\n",
      "Epoch 1/9, Train Loss: 1.1906, Test Loss: 1.0787\n",
      "Epoch 2/9, Train Loss: 1.2060, Test Loss: 1.0886\n",
      "Epoch 3/9, Train Loss: 1.1730, Test Loss: 1.0876\n",
      "Epoch 4/9, Train Loss: 1.1646, Test Loss: 1.0971\n",
      "Epoch 5/9, Train Loss: 1.1639, Test Loss: 1.0865\n",
      "Epoch 6/9, Train Loss: 1.1459, Test Loss: 1.0988\n",
      "Epoch 7/9, Train Loss: 1.1145, Test Loss: 1.0927\n",
      "Epoch 8/9, Train Loss: 1.1600, Test Loss: 1.0793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:45:00,063] Trial 8 finished with value: 1.0809285129819597 and parameters: {'num_hidden_layers': 16, 'layer_0_size': 102, 'layer_1_size': 230, 'layer_2_size': 250, 'layer_3_size': 98, 'layer_4_size': 44, 'layer_5_size': 129, 'layer_6_size': 102, 'layer_7_size': 54, 'layer_8_size': 148, 'layer_9_size': 138, 'layer_10_size': 100, 'layer_11_size': 163, 'layer_12_size': 244, 'layer_13_size': 126, 'layer_14_size': 65, 'layer_15_size': 200, 'dropout_rate': 0.1838478652107614, 'learning_rate': 0.0004043744890751282, 'batch_size': 32, 'epochs': 9}. Best is trial 3 with value: 0.8122723911489759.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/9, Train Loss: 1.1307, Test Loss: 1.0809\n",
      "Epoch 1/38, Train Loss: 1.0575, Test Loss: 1.0827\n",
      "Epoch 2/38, Train Loss: 0.8988, Test Loss: 1.1014\n",
      "Epoch 3/38, Train Loss: 0.8243, Test Loss: 1.1193\n",
      "Epoch 4/38, Train Loss: 0.7916, Test Loss: 1.1501\n",
      "Epoch 5/38, Train Loss: 0.7400, Test Loss: 1.1382\n",
      "Epoch 6/38, Train Loss: 0.6862, Test Loss: 1.1411\n",
      "Epoch 7/38, Train Loss: 0.6361, Test Loss: 1.1560\n",
      "Epoch 8/38, Train Loss: 0.6039, Test Loss: 1.1816\n",
      "Epoch 9/38, Train Loss: 0.5653, Test Loss: 1.2036\n",
      "Epoch 10/38, Train Loss: 0.5171, Test Loss: 1.2129\n",
      "Epoch 11/38, Train Loss: 0.4711, Test Loss: 1.2243\n",
      "Epoch 12/38, Train Loss: 0.4283, Test Loss: 1.2306\n",
      "Epoch 13/38, Train Loss: 0.4098, Test Loss: 1.2625\n",
      "Epoch 14/38, Train Loss: 0.3590, Test Loss: 1.2753\n",
      "Epoch 15/38, Train Loss: 0.3501, Test Loss: 1.3021\n",
      "Epoch 16/38, Train Loss: 0.3376, Test Loss: 1.3197\n",
      "Epoch 17/38, Train Loss: 0.3177, Test Loss: 1.3408\n",
      "Epoch 18/38, Train Loss: 0.2804, Test Loss: 1.3342\n",
      "Epoch 19/38, Train Loss: 0.2761, Test Loss: 1.3679\n",
      "Epoch 20/38, Train Loss: 0.2697, Test Loss: 1.3501\n",
      "Epoch 21/38, Train Loss: 0.2514, Test Loss: 1.3844\n",
      "Epoch 22/38, Train Loss: 0.2376, Test Loss: 1.4092\n",
      "Epoch 23/38, Train Loss: 0.2435, Test Loss: 1.3898\n",
      "Epoch 24/38, Train Loss: 0.2494, Test Loss: 1.3673\n",
      "Epoch 25/38, Train Loss: 0.2431, Test Loss: 1.3724\n",
      "Epoch 26/38, Train Loss: 0.2291, Test Loss: 1.4001\n",
      "Epoch 27/38, Train Loss: 0.2162, Test Loss: 1.4123\n",
      "Epoch 28/38, Train Loss: 0.2291, Test Loss: 1.4097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:45:00,344] Trial 9 finished with value: 1.3814185857772827 and parameters: {'num_hidden_layers': 1, 'layer_0_size': 113, 'dropout_rate': 0.14154306776418257, 'learning_rate': 0.0038352443164003528, 'batch_size': 256, 'epochs': 38}. Best is trial 3 with value: 0.8122723911489759.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/38, Train Loss: 0.2010, Test Loss: 1.3978\n",
      "Epoch 30/38, Train Loss: 0.2101, Test Loss: 1.3924\n",
      "Epoch 31/38, Train Loss: 0.1965, Test Loss: 1.3717\n",
      "Epoch 32/38, Train Loss: 0.2190, Test Loss: 1.3640\n",
      "Epoch 33/38, Train Loss: 0.1743, Test Loss: 1.3898\n",
      "Epoch 34/38, Train Loss: 0.1887, Test Loss: 1.4181\n",
      "Epoch 35/38, Train Loss: 0.1826, Test Loss: 1.3903\n",
      "Epoch 36/38, Train Loss: 0.1893, Test Loss: 1.3919\n",
      "Epoch 37/38, Train Loss: 0.1852, Test Loss: 1.3914\n",
      "Epoch 38/38, Train Loss: 0.1722, Test Loss: 1.3814\n",
      "Epoch 1/100, Train Loss: 1.2820, Test Loss: 1.0635\n",
      "Epoch 2/100, Train Loss: 1.3082, Test Loss: 1.0861\n",
      "Epoch 3/100, Train Loss: 1.2325, Test Loss: 1.0822\n",
      "Epoch 4/100, Train Loss: 1.2914, Test Loss: 1.0868\n",
      "Epoch 5/100, Train Loss: 1.2568, Test Loss: 1.0726\n",
      "Epoch 6/100, Train Loss: 1.2340, Test Loss: 1.0772\n",
      "Epoch 7/100, Train Loss: 1.1962, Test Loss: 1.0853\n",
      "Epoch 8/100, Train Loss: 1.2262, Test Loss: 1.0736\n",
      "Epoch 9/100, Train Loss: 1.2596, Test Loss: 1.0800\n",
      "Epoch 10/100, Train Loss: 1.1891, Test Loss: 1.0658\n",
      "Epoch 11/100, Train Loss: 1.2134, Test Loss: 1.0701\n",
      "Epoch 12/100, Train Loss: 1.2436, Test Loss: 1.0700\n",
      "Epoch 13/100, Train Loss: 1.2561, Test Loss: 1.0735\n",
      "Epoch 14/100, Train Loss: 1.2459, Test Loss: 1.0761\n",
      "Epoch 15/100, Train Loss: 1.2175, Test Loss: 1.0807\n",
      "Epoch 16/100, Train Loss: 1.1913, Test Loss: 1.0664\n",
      "Epoch 17/100, Train Loss: 1.2079, Test Loss: 1.0694\n",
      "Epoch 18/100, Train Loss: 1.1723, Test Loss: 1.0659\n",
      "Epoch 19/100, Train Loss: 1.2364, Test Loss: 1.0619\n",
      "Epoch 20/100, Train Loss: 1.2758, Test Loss: 1.0731\n",
      "Epoch 21/100, Train Loss: 1.1992, Test Loss: 1.0672\n",
      "Epoch 22/100, Train Loss: 1.2036, Test Loss: 1.0490\n",
      "Epoch 23/100, Train Loss: 1.2224, Test Loss: 1.0610\n",
      "Epoch 24/100, Train Loss: 1.2196, Test Loss: 1.0559\n",
      "Epoch 25/100, Train Loss: 1.2380, Test Loss: 1.0581\n",
      "Epoch 26/100, Train Loss: 1.2156, Test Loss: 1.0599\n",
      "Epoch 27/100, Train Loss: 1.2620, Test Loss: 1.0636\n",
      "Epoch 28/100, Train Loss: 1.1919, Test Loss: 1.0591\n",
      "Epoch 29/100, Train Loss: 1.1947, Test Loss: 1.0612\n",
      "Epoch 30/100, Train Loss: 1.2323, Test Loss: 1.0607\n",
      "Epoch 31/100, Train Loss: 1.1914, Test Loss: 1.0548\n",
      "Epoch 32/100, Train Loss: 1.1932, Test Loss: 1.0607\n",
      "Epoch 33/100, Train Loss: 1.2068, Test Loss: 1.0500\n",
      "Epoch 34/100, Train Loss: 1.1990, Test Loss: 1.0534\n",
      "Epoch 35/100, Train Loss: 1.1608, Test Loss: 1.0544\n",
      "Epoch 36/100, Train Loss: 1.1660, Test Loss: 1.0558\n",
      "Epoch 37/100, Train Loss: 1.2542, Test Loss: 1.0545\n",
      "Epoch 38/100, Train Loss: 1.1637, Test Loss: 1.0540\n",
      "Epoch 39/100, Train Loss: 1.1943, Test Loss: 1.0579\n",
      "Epoch 40/100, Train Loss: 1.2615, Test Loss: 1.0499\n",
      "Epoch 41/100, Train Loss: 1.1923, Test Loss: 1.0516\n",
      "Epoch 42/100, Train Loss: 1.2263, Test Loss: 1.0511\n",
      "Epoch 43/100, Train Loss: 1.2185, Test Loss: 1.0574\n",
      "Epoch 44/100, Train Loss: 1.1339, Test Loss: 1.0554\n",
      "Epoch 45/100, Train Loss: 1.1962, Test Loss: 1.0542\n",
      "Epoch 46/100, Train Loss: 1.2454, Test Loss: 1.0543\n",
      "Epoch 47/100, Train Loss: 1.2020, Test Loss: 1.0635\n",
      "Epoch 48/100, Train Loss: 1.1885, Test Loss: 1.0582\n",
      "Epoch 49/100, Train Loss: 1.2322, Test Loss: 1.0429\n",
      "Epoch 50/100, Train Loss: 1.2271, Test Loss: 1.0511\n",
      "Epoch 51/100, Train Loss: 1.1915, Test Loss: 1.0490\n",
      "Epoch 52/100, Train Loss: 1.1817, Test Loss: 1.0631\n",
      "Epoch 53/100, Train Loss: 1.2134, Test Loss: 1.0566\n",
      "Epoch 54/100, Train Loss: 1.1764, Test Loss: 1.0443\n",
      "Epoch 55/100, Train Loss: 1.2025, Test Loss: 1.0667\n",
      "Epoch 56/100, Train Loss: 1.1390, Test Loss: 1.0481\n",
      "Epoch 57/100, Train Loss: 1.2398, Test Loss: 1.0523\n",
      "Epoch 58/100, Train Loss: 1.2054, Test Loss: 1.0572\n",
      "Epoch 59/100, Train Loss: 1.2082, Test Loss: 1.0547\n",
      "Epoch 60/100, Train Loss: 1.1697, Test Loss: 1.0511\n",
      "Epoch 61/100, Train Loss: 1.2058, Test Loss: 1.0526\n",
      "Epoch 62/100, Train Loss: 1.2047, Test Loss: 1.0453\n",
      "Epoch 63/100, Train Loss: 1.1946, Test Loss: 1.0460\n",
      "Epoch 64/100, Train Loss: 1.1276, Test Loss: 1.0445\n",
      "Epoch 65/100, Train Loss: 1.2178, Test Loss: 1.0556\n",
      "Epoch 66/100, Train Loss: 1.1712, Test Loss: 1.0487\n",
      "Epoch 67/100, Train Loss: 1.1476, Test Loss: 1.0479\n",
      "Epoch 68/100, Train Loss: 1.2044, Test Loss: 1.0387\n",
      "Epoch 69/100, Train Loss: 1.1408, Test Loss: 1.0495\n",
      "Epoch 70/100, Train Loss: 1.2399, Test Loss: 1.0477\n",
      "Epoch 71/100, Train Loss: 1.1792, Test Loss: 1.0431\n",
      "Epoch 72/100, Train Loss: 1.1544, Test Loss: 1.0418\n",
      "Epoch 73/100, Train Loss: 1.1563, Test Loss: 1.0389\n",
      "Epoch 74/100, Train Loss: 1.2478, Test Loss: 1.0496\n",
      "Epoch 75/100, Train Loss: 1.1799, Test Loss: 1.0533\n",
      "Epoch 76/100, Train Loss: 1.1985, Test Loss: 1.0429\n",
      "Epoch 77/100, Train Loss: 1.1578, Test Loss: 1.0381\n",
      "Epoch 78/100, Train Loss: 1.1324, Test Loss: 1.0489\n",
      "Epoch 79/100, Train Loss: 1.1124, Test Loss: 1.0437\n",
      "Epoch 80/100, Train Loss: 1.1560, Test Loss: 1.0469\n",
      "Epoch 81/100, Train Loss: 1.1682, Test Loss: 1.0479\n",
      "Epoch 82/100, Train Loss: 1.1383, Test Loss: 1.0551\n",
      "Epoch 83/100, Train Loss: 1.1330, Test Loss: 1.0519\n",
      "Epoch 84/100, Train Loss: 1.1576, Test Loss: 1.0479\n",
      "Epoch 85/100, Train Loss: 1.2039, Test Loss: 1.0540\n",
      "Epoch 86/100, Train Loss: 1.1662, Test Loss: 1.0405\n",
      "Epoch 87/100, Train Loss: 1.1680, Test Loss: 1.0416\n",
      "Epoch 88/100, Train Loss: 1.1236, Test Loss: 1.0476\n",
      "Epoch 89/100, Train Loss: 1.1522, Test Loss: 1.0454\n",
      "Epoch 90/100, Train Loss: 1.1864, Test Loss: 1.0461\n",
      "Epoch 91/100, Train Loss: 1.1726, Test Loss: 1.0517\n",
      "Epoch 92/100, Train Loss: 1.1725, Test Loss: 1.0484\n",
      "Epoch 93/100, Train Loss: 1.1628, Test Loss: 1.0510\n",
      "Epoch 94/100, Train Loss: 1.1859, Test Loss: 1.0491\n",
      "Epoch 95/100, Train Loss: 1.1939, Test Loss: 1.0486\n",
      "Epoch 96/100, Train Loss: 1.1716, Test Loss: 1.0461\n",
      "Epoch 97/100, Train Loss: 1.1731, Test Loss: 1.0473\n",
      "Epoch 98/100, Train Loss: 1.1309, Test Loss: 1.0415\n",
      "Epoch 99/100, Train Loss: 1.1441, Test Loss: 1.0398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:45:23,516] Trial 10 finished with value: 1.0345676094293594 and parameters: {'num_hidden_layers': 20, 'layer_0_size': 253, 'layer_1_size': 166, 'layer_2_size': 55, 'layer_3_size': 37, 'layer_4_size': 208, 'layer_5_size': 245, 'layer_6_size': 39, 'layer_7_size': 134, 'layer_8_size': 59, 'layer_9_size': 249, 'layer_10_size': 51, 'layer_11_size': 58, 'layer_12_size': 255, 'layer_13_size': 254, 'layer_14_size': 170, 'layer_15_size': 153, 'layer_16_size': 234, 'layer_17_size': 137, 'layer_18_size': 132, 'layer_19_size': 209, 'dropout_rate': 0.4952709374338117, 'learning_rate': 1.1118610492907333e-05, 'batch_size': 64, 'epochs': 100}. Best is trial 3 with value: 0.8122723911489759.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100, Train Loss: 1.1222, Test Loss: 1.0346\n",
      "Epoch 1/61, Train Loss: 1.2625, Test Loss: 1.1010\n",
      "Epoch 2/61, Train Loss: 1.1657, Test Loss: 1.1294\n",
      "Epoch 3/61, Train Loss: 1.2201, Test Loss: 1.1163\n",
      "Epoch 4/61, Train Loss: 1.1436, Test Loss: 1.1134\n",
      "Epoch 5/61, Train Loss: 1.2121, Test Loss: 1.1212\n",
      "Epoch 6/61, Train Loss: 1.1679, Test Loss: 1.1187\n",
      "Epoch 7/61, Train Loss: 1.1564, Test Loss: 1.1159\n",
      "Epoch 8/61, Train Loss: 1.1407, Test Loss: 1.1137\n",
      "Epoch 9/61, Train Loss: 1.1377, Test Loss: 1.1074\n",
      "Epoch 10/61, Train Loss: 1.1470, Test Loss: 1.1018\n",
      "Epoch 11/61, Train Loss: 1.1815, Test Loss: 1.1185\n",
      "Epoch 12/61, Train Loss: 1.1487, Test Loss: 1.1302\n",
      "Epoch 13/61, Train Loss: 1.1187, Test Loss: 1.1323\n",
      "Epoch 14/61, Train Loss: 1.1401, Test Loss: 1.1241\n",
      "Epoch 15/61, Train Loss: 1.0758, Test Loss: 1.1401\n",
      "Epoch 16/61, Train Loss: 1.1493, Test Loss: 1.1367\n",
      "Epoch 17/61, Train Loss: 1.1270, Test Loss: 1.1342\n",
      "Epoch 18/61, Train Loss: 1.1243, Test Loss: 1.1181\n",
      "Epoch 19/61, Train Loss: 1.1264, Test Loss: 1.1275\n",
      "Epoch 20/61, Train Loss: 1.1369, Test Loss: 1.1325\n",
      "Epoch 21/61, Train Loss: 1.1009, Test Loss: 1.1134\n",
      "Epoch 22/61, Train Loss: 1.0895, Test Loss: 1.1132\n",
      "Epoch 23/61, Train Loss: 1.0758, Test Loss: 1.1326\n",
      "Epoch 24/61, Train Loss: 1.0973, Test Loss: 1.1350\n",
      "Epoch 25/61, Train Loss: 1.1057, Test Loss: 1.1382\n",
      "Epoch 26/61, Train Loss: 1.1396, Test Loss: 1.1479\n",
      "Epoch 27/61, Train Loss: 1.1068, Test Loss: 1.1396\n",
      "Epoch 28/61, Train Loss: 1.1070, Test Loss: 1.1342\n",
      "Epoch 29/61, Train Loss: 1.0877, Test Loss: 1.1225\n",
      "Epoch 30/61, Train Loss: 1.1045, Test Loss: 1.1344\n",
      "Epoch 31/61, Train Loss: 1.0760, Test Loss: 1.1217\n",
      "Epoch 32/61, Train Loss: 1.1083, Test Loss: 1.1256\n",
      "Epoch 33/61, Train Loss: 1.1253, Test Loss: 1.1142\n",
      "Epoch 34/61, Train Loss: 1.0989, Test Loss: 1.1098\n",
      "Epoch 35/61, Train Loss: 1.1156, Test Loss: 1.1248\n",
      "Epoch 36/61, Train Loss: 1.0997, Test Loss: 1.1083\n",
      "Epoch 37/61, Train Loss: 1.1236, Test Loss: 1.1131\n",
      "Epoch 38/61, Train Loss: 1.0990, Test Loss: 1.1068\n",
      "Epoch 39/61, Train Loss: 1.0953, Test Loss: 1.1001\n",
      "Epoch 40/61, Train Loss: 1.0719, Test Loss: 1.1049\n",
      "Epoch 41/61, Train Loss: 1.1232, Test Loss: 1.1035\n",
      "Epoch 42/61, Train Loss: 1.0757, Test Loss: 1.1038\n",
      "Epoch 43/61, Train Loss: 1.0825, Test Loss: 1.0995\n",
      "Epoch 44/61, Train Loss: 1.1436, Test Loss: 1.1210\n",
      "Epoch 45/61, Train Loss: 1.1061, Test Loss: 1.1141\n",
      "Epoch 46/61, Train Loss: 1.1206, Test Loss: 1.1281\n",
      "Epoch 47/61, Train Loss: 1.0721, Test Loss: 1.1127\n",
      "Epoch 48/61, Train Loss: 1.1129, Test Loss: 1.1044\n",
      "Epoch 49/61, Train Loss: 1.0836, Test Loss: 1.0995\n",
      "Epoch 50/61, Train Loss: 1.1203, Test Loss: 1.1004\n",
      "Epoch 51/61, Train Loss: 1.0727, Test Loss: 1.1135\n",
      "Epoch 52/61, Train Loss: 1.0712, Test Loss: 1.1033\n",
      "Epoch 53/61, Train Loss: 1.1088, Test Loss: 1.1190\n",
      "Epoch 54/61, Train Loss: 1.0891, Test Loss: 1.1152\n",
      "Epoch 55/61, Train Loss: 1.0760, Test Loss: 1.1123\n",
      "Epoch 56/61, Train Loss: 1.0676, Test Loss: 1.1058\n",
      "Epoch 57/61, Train Loss: 1.0500, Test Loss: 1.1110\n",
      "Epoch 58/61, Train Loss: 1.0988, Test Loss: 1.1113\n",
      "Epoch 59/61, Train Loss: 1.0869, Test Loss: 1.1149\n",
      "Epoch 60/61, Train Loss: 1.0658, Test Loss: 1.1099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:45:39,110] Trial 11 finished with value: 1.1094866394996643 and parameters: {'num_hidden_layers': 11, 'layer_0_size': 159, 'layer_1_size': 92, 'layer_2_size': 164, 'layer_3_size': 247, 'layer_4_size': 162, 'layer_5_size': 252, 'layer_6_size': 167, 'layer_7_size': 242, 'layer_8_size': 118, 'layer_9_size': 254, 'layer_10_size': 173, 'dropout_rate': 0.2583739550120179, 'learning_rate': 6.779868549925353e-05, 'batch_size': 32, 'epochs': 61}. Best is trial 3 with value: 0.8122723911489759.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/61, Train Loss: 1.1043, Test Loss: 1.1095\n",
      "Epoch 1/71, Train Loss: 1.3561, Test Loss: 1.1016\n",
      "Epoch 2/71, Train Loss: 1.3093, Test Loss: 1.0829\n",
      "Epoch 3/71, Train Loss: 1.2913, Test Loss: 1.0680\n",
      "Epoch 4/71, Train Loss: 1.2753, Test Loss: 1.0559\n",
      "Epoch 5/71, Train Loss: 1.2477, Test Loss: 1.0505\n",
      "Epoch 6/71, Train Loss: 1.2315, Test Loss: 1.0455\n",
      "Epoch 7/71, Train Loss: 1.2230, Test Loss: 1.0425\n",
      "Epoch 8/71, Train Loss: 1.1691, Test Loss: 1.0349\n",
      "Epoch 9/71, Train Loss: 1.1843, Test Loss: 1.0295\n",
      "Epoch 10/71, Train Loss: 1.1893, Test Loss: 1.0307\n",
      "Epoch 11/71, Train Loss: 1.1851, Test Loss: 1.0280\n",
      "Epoch 12/71, Train Loss: 1.1495, Test Loss: 1.0296\n",
      "Epoch 13/71, Train Loss: 1.1321, Test Loss: 1.0373\n",
      "Epoch 14/71, Train Loss: 1.1329, Test Loss: 1.0309\n",
      "Epoch 15/71, Train Loss: 1.1088, Test Loss: 1.0359\n",
      "Epoch 16/71, Train Loss: 1.1187, Test Loss: 1.0294\n",
      "Epoch 17/71, Train Loss: 1.1298, Test Loss: 1.0287\n",
      "Epoch 18/71, Train Loss: 1.1511, Test Loss: 1.0375\n",
      "Epoch 19/71, Train Loss: 1.1110, Test Loss: 1.0407\n",
      "Epoch 20/71, Train Loss: 1.1707, Test Loss: 1.0356\n",
      "Epoch 21/71, Train Loss: 1.1198, Test Loss: 1.0392\n",
      "Epoch 22/71, Train Loss: 1.1413, Test Loss: 1.0436\n",
      "Epoch 23/71, Train Loss: 1.1098, Test Loss: 1.0397\n",
      "Epoch 24/71, Train Loss: 1.1282, Test Loss: 1.0330\n",
      "Epoch 25/71, Train Loss: 1.1115, Test Loss: 1.0349\n",
      "Epoch 26/71, Train Loss: 1.1406, Test Loss: 1.0372\n",
      "Epoch 27/71, Train Loss: 1.1060, Test Loss: 1.0467\n",
      "Epoch 28/71, Train Loss: 1.1073, Test Loss: 1.0423\n",
      "Epoch 29/71, Train Loss: 1.0856, Test Loss: 1.0395\n",
      "Epoch 30/71, Train Loss: 1.1035, Test Loss: 1.0389\n",
      "Epoch 31/71, Train Loss: 1.1282, Test Loss: 1.0433\n",
      "Epoch 32/71, Train Loss: 1.0969, Test Loss: 1.0423\n",
      "Epoch 33/71, Train Loss: 1.0973, Test Loss: 1.0481\n",
      "Epoch 34/71, Train Loss: 1.1427, Test Loss: 1.0480\n",
      "Epoch 35/71, Train Loss: 1.0803, Test Loss: 1.0483\n",
      "Epoch 36/71, Train Loss: 1.1307, Test Loss: 1.0487\n",
      "Epoch 37/71, Train Loss: 1.0631, Test Loss: 1.0598\n",
      "Epoch 38/71, Train Loss: 1.0901, Test Loss: 1.0585\n",
      "Epoch 39/71, Train Loss: 1.0613, Test Loss: 1.0577\n",
      "Epoch 40/71, Train Loss: 1.0983, Test Loss: 1.0580\n",
      "Epoch 41/71, Train Loss: 1.0764, Test Loss: 1.0556\n",
      "Epoch 42/71, Train Loss: 1.0706, Test Loss: 1.0565\n",
      "Epoch 43/71, Train Loss: 1.0783, Test Loss: 1.0612\n",
      "Epoch 44/71, Train Loss: 1.1209, Test Loss: 1.0669\n",
      "Epoch 45/71, Train Loss: 1.1227, Test Loss: 1.0587\n",
      "Epoch 46/71, Train Loss: 1.1264, Test Loss: 1.0587\n",
      "Epoch 47/71, Train Loss: 1.0998, Test Loss: 1.0527\n",
      "Epoch 48/71, Train Loss: 1.1015, Test Loss: 1.0543\n",
      "Epoch 49/71, Train Loss: 1.1372, Test Loss: 1.0596\n",
      "Epoch 50/71, Train Loss: 1.0834, Test Loss: 1.0595\n",
      "Epoch 51/71, Train Loss: 1.0999, Test Loss: 1.0608\n",
      "Epoch 52/71, Train Loss: 1.1091, Test Loss: 1.0548\n",
      "Epoch 53/71, Train Loss: 1.0702, Test Loss: 1.0588\n",
      "Epoch 54/71, Train Loss: 1.0850, Test Loss: 1.0556\n",
      "Epoch 55/71, Train Loss: 1.0987, Test Loss: 1.0558\n",
      "Epoch 56/71, Train Loss: 1.1317, Test Loss: 1.0544\n",
      "Epoch 57/71, Train Loss: 1.0885, Test Loss: 1.0569\n",
      "Epoch 58/71, Train Loss: 1.1212, Test Loss: 1.0622\n",
      "Epoch 59/71, Train Loss: 1.1198, Test Loss: 1.0602\n",
      "Epoch 60/71, Train Loss: 1.1035, Test Loss: 1.0578\n",
      "Epoch 61/71, Train Loss: 1.0588, Test Loss: 1.0554\n",
      "Epoch 62/71, Train Loss: 1.0730, Test Loss: 1.0543\n",
      "Epoch 63/71, Train Loss: 1.0852, Test Loss: 1.0545\n",
      "Epoch 64/71, Train Loss: 1.1089, Test Loss: 1.0596\n",
      "Epoch 65/71, Train Loss: 1.0648, Test Loss: 1.0632\n",
      "Epoch 66/71, Train Loss: 1.0820, Test Loss: 1.0609\n",
      "Epoch 67/71, Train Loss: 1.1207, Test Loss: 1.0547\n",
      "Epoch 68/71, Train Loss: 1.0914, Test Loss: 1.0607\n",
      "Epoch 69/71, Train Loss: 1.0858, Test Loss: 1.0633\n",
      "Epoch 70/71, Train Loss: 1.1002, Test Loss: 1.0627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:45:45,433] Trial 12 finished with value: 1.0641068667173386 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 158, 'layer_1_size': 108, 'layer_2_size': 90, 'layer_3_size': 75, 'layer_4_size': 190, 'layer_5_size': 203, 'layer_6_size': 160, 'dropout_rate': 0.24590836839120314, 'learning_rate': 4.2776594730364394e-05, 'batch_size': 64, 'epochs': 71}. Best is trial 3 with value: 0.8122723911489759.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/71, Train Loss: 1.0829, Test Loss: 1.0641\n",
      "Epoch 1/51, Train Loss: 1.3067, Test Loss: 0.6737\n",
      "Epoch 2/51, Train Loss: 1.2535, Test Loss: 0.6772\n",
      "Epoch 3/51, Train Loss: 1.2570, Test Loss: 0.6827\n",
      "Epoch 4/51, Train Loss: 1.2599, Test Loss: 0.6763\n",
      "Epoch 5/51, Train Loss: 1.2351, Test Loss: 0.6742\n",
      "Epoch 6/51, Train Loss: 1.2424, Test Loss: 0.6737\n",
      "Epoch 7/51, Train Loss: 1.2619, Test Loss: 0.6765\n",
      "Epoch 8/51, Train Loss: 1.2211, Test Loss: 0.6689\n",
      "Epoch 9/51, Train Loss: 1.1938, Test Loss: 0.6752\n",
      "Epoch 10/51, Train Loss: 1.2599, Test Loss: 0.6762\n",
      "Epoch 11/51, Train Loss: 1.1950, Test Loss: 0.6738\n",
      "Epoch 12/51, Train Loss: 1.2361, Test Loss: 0.6730\n",
      "Epoch 13/51, Train Loss: 1.1587, Test Loss: 0.6756\n",
      "Epoch 14/51, Train Loss: 1.2002, Test Loss: 0.6695\n",
      "Epoch 15/51, Train Loss: 1.1673, Test Loss: 0.6813\n",
      "Epoch 16/51, Train Loss: 1.1315, Test Loss: 0.6758\n",
      "Epoch 17/51, Train Loss: 1.1522, Test Loss: 0.6763\n",
      "Epoch 18/51, Train Loss: 1.2124, Test Loss: 0.6731\n",
      "Epoch 19/51, Train Loss: 1.1553, Test Loss: 0.6736\n",
      "Epoch 20/51, Train Loss: 1.1818, Test Loss: 0.6761\n",
      "Epoch 21/51, Train Loss: 1.1374, Test Loss: 0.6810\n",
      "Epoch 22/51, Train Loss: 1.1671, Test Loss: 0.6746\n",
      "Epoch 23/51, Train Loss: 1.1210, Test Loss: 0.6702\n",
      "Epoch 24/51, Train Loss: 1.1743, Test Loss: 0.6780\n",
      "Epoch 25/51, Train Loss: 1.1460, Test Loss: 0.6831\n",
      "Epoch 26/51, Train Loss: 1.1721, Test Loss: 0.6751\n",
      "Epoch 27/51, Train Loss: 1.1255, Test Loss: 0.6735\n",
      "Epoch 28/51, Train Loss: 1.1274, Test Loss: 0.6763\n",
      "Epoch 29/51, Train Loss: 1.1528, Test Loss: 0.6705\n",
      "Epoch 30/51, Train Loss: 1.1421, Test Loss: 0.6741\n",
      "Epoch 31/51, Train Loss: 1.1514, Test Loss: 0.6815\n",
      "Epoch 32/51, Train Loss: 1.1417, Test Loss: 0.6745\n",
      "Epoch 33/51, Train Loss: 1.1613, Test Loss: 0.6728\n",
      "Epoch 34/51, Train Loss: 1.1011, Test Loss: 0.6708\n",
      "Epoch 35/51, Train Loss: 1.0879, Test Loss: 0.6743\n",
      "Epoch 36/51, Train Loss: 1.1671, Test Loss: 0.6763\n",
      "Epoch 37/51, Train Loss: 1.1165, Test Loss: 0.6754\n",
      "Epoch 38/51, Train Loss: 1.1132, Test Loss: 0.6740\n",
      "Epoch 39/51, Train Loss: 1.1370, Test Loss: 0.6756\n",
      "Epoch 40/51, Train Loss: 1.1173, Test Loss: 0.6744\n",
      "Epoch 41/51, Train Loss: 1.1223, Test Loss: 0.6728\n",
      "Epoch 42/51, Train Loss: 1.0990, Test Loss: 0.6754\n",
      "Epoch 43/51, Train Loss: 1.1051, Test Loss: 0.6775\n",
      "Epoch 44/51, Train Loss: 1.1490, Test Loss: 0.6739\n",
      "Epoch 45/51, Train Loss: 1.1021, Test Loss: 0.6758\n",
      "Epoch 46/51, Train Loss: 1.0722, Test Loss: 0.6737\n",
      "Epoch 47/51, Train Loss: 1.0846, Test Loss: 0.6697\n",
      "Epoch 48/51, Train Loss: 1.0952, Test Loss: 0.6715\n",
      "Epoch 49/51, Train Loss: 1.1208, Test Loss: 0.6774\n",
      "Epoch 50/51, Train Loss: 1.1053, Test Loss: 0.6743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:45:57,875] Trial 13 finished with value: 0.6764646470546722 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 71, 'layer_1_size': 190, 'layer_2_size': 148, 'layer_3_size': 142, 'layer_4_size': 100, 'layer_5_size': 111, 'layer_6_size': 112, 'layer_7_size': 185, 'layer_8_size': 154, 'layer_9_size': 68, 'layer_10_size': 233, 'layer_11_size': 139, 'layer_12_size': 171, 'dropout_rate': 0.310961851403328, 'learning_rate': 1.0054904762060992e-05, 'batch_size': 32, 'epochs': 51}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/51, Train Loss: 1.0343, Test Loss: 0.6765\n",
      "Epoch 1/48, Train Loss: 1.3107, Test Loss: 1.0652\n",
      "Epoch 2/48, Train Loss: 1.3096, Test Loss: 1.0699\n",
      "Epoch 3/48, Train Loss: 1.2988, Test Loss: 1.0444\n",
      "Epoch 4/48, Train Loss: 1.2758, Test Loss: 1.0521\n",
      "Epoch 5/48, Train Loss: 1.2549, Test Loss: 1.0482\n",
      "Epoch 6/48, Train Loss: 1.2011, Test Loss: 1.0596\n",
      "Epoch 7/48, Train Loss: 1.3009, Test Loss: 1.0538\n",
      "Epoch 8/48, Train Loss: 1.2644, Test Loss: 1.0563\n",
      "Epoch 9/48, Train Loss: 1.1910, Test Loss: 1.0436\n",
      "Epoch 10/48, Train Loss: 1.1762, Test Loss: 1.0482\n",
      "Epoch 11/48, Train Loss: 1.2333, Test Loss: 1.0423\n",
      "Epoch 12/48, Train Loss: 1.1983, Test Loss: 1.0382\n",
      "Epoch 13/48, Train Loss: 1.2131, Test Loss: 1.0414\n",
      "Epoch 14/48, Train Loss: 1.1690, Test Loss: 1.0389\n",
      "Epoch 15/48, Train Loss: 1.1854, Test Loss: 1.0370\n",
      "Epoch 16/48, Train Loss: 1.2255, Test Loss: 1.0475\n",
      "Epoch 17/48, Train Loss: 1.2144, Test Loss: 1.0338\n",
      "Epoch 18/48, Train Loss: 1.2241, Test Loss: 1.0391\n",
      "Epoch 19/48, Train Loss: 1.1700, Test Loss: 1.0340\n",
      "Epoch 20/48, Train Loss: 1.1825, Test Loss: 1.0377\n",
      "Epoch 21/48, Train Loss: 1.1681, Test Loss: 1.0313\n",
      "Epoch 22/48, Train Loss: 1.1892, Test Loss: 1.0299\n",
      "Epoch 23/48, Train Loss: 1.2144, Test Loss: 1.0268\n",
      "Epoch 24/48, Train Loss: 1.2346, Test Loss: 1.0322\n",
      "Epoch 25/48, Train Loss: 1.1475, Test Loss: 1.0295\n",
      "Epoch 26/48, Train Loss: 1.1559, Test Loss: 1.0291\n",
      "Epoch 27/48, Train Loss: 1.1710, Test Loss: 1.0318\n",
      "Epoch 28/48, Train Loss: 1.1592, Test Loss: 1.0297\n",
      "Epoch 29/48, Train Loss: 1.1747, Test Loss: 1.0303\n",
      "Epoch 30/48, Train Loss: 1.1713, Test Loss: 1.0249\n",
      "Epoch 31/48, Train Loss: 1.1134, Test Loss: 1.0268\n",
      "Epoch 32/48, Train Loss: 1.2127, Test Loss: 1.0266\n",
      "Epoch 33/48, Train Loss: 1.1310, Test Loss: 1.0285\n",
      "Epoch 34/48, Train Loss: 1.1117, Test Loss: 1.0272\n",
      "Epoch 35/48, Train Loss: 1.1772, Test Loss: 1.0272\n",
      "Epoch 36/48, Train Loss: 1.1305, Test Loss: 1.0258\n",
      "Epoch 37/48, Train Loss: 1.1279, Test Loss: 1.0276\n",
      "Epoch 38/48, Train Loss: 1.1426, Test Loss: 1.0296\n",
      "Epoch 39/48, Train Loss: 1.1949, Test Loss: 1.0254\n",
      "Epoch 40/48, Train Loss: 1.1489, Test Loss: 1.0269\n",
      "Epoch 41/48, Train Loss: 1.1696, Test Loss: 1.0246\n",
      "Epoch 42/48, Train Loss: 1.1298, Test Loss: 1.0253\n",
      "Epoch 43/48, Train Loss: 1.1407, Test Loss: 1.0252\n",
      "Epoch 44/48, Train Loss: 1.1235, Test Loss: 1.0273\n",
      "Epoch 45/48, Train Loss: 1.1364, Test Loss: 1.0245\n",
      "Epoch 46/48, Train Loss: 1.1181, Test Loss: 1.0250\n",
      "Epoch 47/48, Train Loss: 1.1866, Test Loss: 1.0248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:46:09,626] Trial 14 finished with value: 1.0242289219583784 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 58, 'layer_1_size': 193, 'layer_2_size': 216, 'layer_3_size': 129, 'layer_4_size': 57, 'layer_5_size': 113, 'layer_6_size': 106, 'layer_7_size': 186, 'layer_8_size': 158, 'layer_9_size': 49, 'layer_10_size': 228, 'layer_11_size': 140, 'layer_12_size': 171, 'dropout_rate': 0.42214678432464436, 'learning_rate': 1.7474154682067586e-05, 'batch_size': 32, 'epochs': 48}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/48, Train Loss: 1.1530, Test Loss: 1.0242\n",
      "Epoch 1/12, Train Loss: 1.1478, Test Loss: 0.8028\n",
      "Epoch 2/12, Train Loss: 1.1193, Test Loss: 0.8111\n",
      "Epoch 3/12, Train Loss: 1.1242, Test Loss: 0.7980\n",
      "Epoch 4/12, Train Loss: 1.1067, Test Loss: 0.7817\n",
      "Epoch 5/12, Train Loss: 1.0547, Test Loss: 0.7891\n",
      "Epoch 6/12, Train Loss: 1.0602, Test Loss: 0.7841\n",
      "Epoch 7/12, Train Loss: 1.0758, Test Loss: 0.7830\n",
      "Epoch 8/12, Train Loss: 1.0326, Test Loss: 0.7845\n",
      "Epoch 9/12, Train Loss: 1.0316, Test Loss: 0.7870\n",
      "Epoch 10/12, Train Loss: 1.0253, Test Loss: 0.7871\n",
      "Epoch 11/12, Train Loss: 1.0118, Test Loss: 0.7876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:46:12,689] Trial 15 finished with value: 0.7889194743973869 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 74, 'layer_1_size': 256, 'layer_2_size': 152, 'layer_3_size': 87, 'layer_4_size': 91, 'layer_5_size': 91, 'layer_6_size': 93, 'layer_7_size': 99, 'layer_8_size': 101, 'layer_9_size': 32, 'layer_10_size': 140, 'layer_11_size': 142, 'layer_12_size': 168, 'layer_13_size': 120, 'dropout_rate': 0.3049525481045946, 'learning_rate': 0.0011387640153906913, 'batch_size': 32, 'epochs': 12}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/12, Train Loss: 1.0377, Test Loss: 0.7889\n",
      "Epoch 1/8, Train Loss: 1.2130, Test Loss: 1.1062\n",
      "Epoch 2/8, Train Loss: 1.1993, Test Loss: 1.1201\n",
      "Epoch 3/8, Train Loss: 1.1496, Test Loss: 1.1346\n",
      "Epoch 4/8, Train Loss: 1.2374, Test Loss: 1.1341\n",
      "Epoch 5/8, Train Loss: 1.1685, Test Loss: 1.1296\n",
      "Epoch 6/8, Train Loss: 1.0903, Test Loss: 1.1067\n",
      "Epoch 7/8, Train Loss: 1.1191, Test Loss: 1.1022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:46:14,723] Trial 16 finished with value: 1.1141436525753565 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 72, 'layer_1_size': 253, 'layer_2_size': 163, 'layer_3_size': 72, 'layer_4_size': 84, 'layer_5_size': 95, 'layer_6_size': 113, 'layer_7_size': 89, 'layer_8_size': 186, 'layer_9_size': 33, 'layer_10_size': 196, 'layer_11_size': 117, 'layer_12_size': 161, 'layer_13_size': 58, 'dropout_rate': 0.3115197590069041, 'learning_rate': 0.00014340138071634363, 'batch_size': 32, 'epochs': 8}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/8, Train Loss: 1.1497, Test Loss: 1.1141\n",
      "Epoch 1/22, Train Loss: 1.2970, Test Loss: 1.0779\n",
      "Epoch 2/22, Train Loss: 1.1705, Test Loss: 1.1028\n",
      "Epoch 3/22, Train Loss: 1.1217, Test Loss: 1.0953\n",
      "Epoch 4/22, Train Loss: 1.1513, Test Loss: 1.0969\n",
      "Epoch 5/22, Train Loss: 1.1284, Test Loss: 1.0781\n",
      "Epoch 6/22, Train Loss: 1.1401, Test Loss: 1.0948\n",
      "Epoch 7/22, Train Loss: 1.1148, Test Loss: 1.0782\n",
      "Epoch 8/22, Train Loss: 1.1226, Test Loss: 1.0821\n",
      "Epoch 9/22, Train Loss: 1.1254, Test Loss: 1.0844\n",
      "Epoch 10/22, Train Loss: 1.1295, Test Loss: 1.0838\n",
      "Epoch 11/22, Train Loss: 1.1189, Test Loss: 1.0961\n",
      "Epoch 12/22, Train Loss: 1.1404, Test Loss: 1.0827\n",
      "Epoch 13/22, Train Loss: 1.1379, Test Loss: 1.0815\n",
      "Epoch 14/22, Train Loss: 1.1103, Test Loss: 1.0875\n",
      "Epoch 15/22, Train Loss: 1.1337, Test Loss: 1.0777\n",
      "Epoch 16/22, Train Loss: 1.1067, Test Loss: 1.0788\n",
      "Epoch 17/22, Train Loss: 1.1237, Test Loss: 1.0887\n",
      "Epoch 18/22, Train Loss: 1.1071, Test Loss: 1.0859\n",
      "Epoch 19/22, Train Loss: 1.1221, Test Loss: 1.0836\n",
      "Epoch 20/22, Train Loss: 1.1058, Test Loss: 1.0825\n",
      "Epoch 21/22, Train Loss: 1.1046, Test Loss: 1.0853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:46:18,763] Trial 17 finished with value: 1.082479681287493 and parameters: {'num_hidden_layers': 10, 'layer_0_size': 75, 'layer_1_size': 254, 'layer_2_size': 214, 'layer_3_size': 34, 'layer_4_size': 80, 'layer_5_size': 80, 'layer_6_size': 131, 'layer_7_size': 189, 'layer_8_size': 37, 'layer_9_size': 78, 'dropout_rate': 0.4326934528014709, 'learning_rate': 0.0031513797291003232, 'batch_size': 32, 'epochs': 22}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/22, Train Loss: 1.1342, Test Loss: 1.0825\n",
      "Epoch 1/22, Train Loss: 1.1757, Test Loss: 1.0016\n",
      "Epoch 2/22, Train Loss: 1.0935, Test Loss: 0.9998\n",
      "Epoch 3/22, Train Loss: 1.0398, Test Loss: 0.9964\n",
      "Epoch 4/22, Train Loss: 1.0153, Test Loss: 1.0047\n",
      "Epoch 5/22, Train Loss: 1.0107, Test Loss: 0.9969\n",
      "Epoch 6/22, Train Loss: 1.0078, Test Loss: 0.9956\n",
      "Epoch 7/22, Train Loss: 1.0250, Test Loss: 1.0008\n",
      "Epoch 8/22, Train Loss: 1.0270, Test Loss: 1.0015\n",
      "Epoch 9/22, Train Loss: 0.9851, Test Loss: 1.0001\n",
      "Epoch 10/22, Train Loss: 0.9966, Test Loss: 0.9994\n",
      "Epoch 11/22, Train Loss: 1.0097, Test Loss: 1.0011\n",
      "Epoch 12/22, Train Loss: 0.9758, Test Loss: 0.9982\n",
      "Epoch 13/22, Train Loss: 0.9784, Test Loss: 0.9968\n",
      "Epoch 14/22, Train Loss: 0.9808, Test Loss: 0.9994\n",
      "Epoch 15/22, Train Loss: 0.9814, Test Loss: 0.9987\n",
      "Epoch 16/22, Train Loss: 0.9874, Test Loss: 0.9987\n",
      "Epoch 17/22, Train Loss: 0.9885, Test Loss: 0.9981\n",
      "Epoch 18/22, Train Loss: 0.9990, Test Loss: 0.9990\n",
      "Epoch 19/22, Train Loss: 1.0028, Test Loss: 0.9983\n",
      "Epoch 20/22, Train Loss: 0.9738, Test Loss: 0.9987\n",
      "Epoch 21/22, Train Loss: 0.9915, Test Loss: 0.9982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:46:26,644] Trial 18 finished with value: 0.9991567220006671 and parameters: {'num_hidden_layers': 20, 'layer_0_size': 33, 'layer_1_size': 187, 'layer_2_size': 80, 'layer_3_size': 89, 'layer_4_size': 107, 'layer_5_size': 32, 'layer_6_size': 82, 'layer_7_size': 94, 'layer_8_size': 120, 'layer_9_size': 82, 'layer_10_size': 142, 'layer_11_size': 90, 'layer_12_size': 188, 'layer_13_size': 97, 'layer_14_size': 194, 'layer_15_size': 256, 'layer_16_size': 45, 'layer_17_size': 237, 'layer_18_size': 249, 'layer_19_size': 32, 'dropout_rate': 0.3483203673287814, 'learning_rate': 0.001599533935006653, 'batch_size': 32, 'epochs': 22}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/22, Train Loss: 0.9829, Test Loss: 0.9992\n",
      "Epoch 1/51, Train Loss: 1.2293, Test Loss: 0.7819\n",
      "Epoch 2/51, Train Loss: 1.1111, Test Loss: 0.8242\n",
      "Epoch 3/51, Train Loss: 1.0162, Test Loss: 0.7802\n",
      "Epoch 4/51, Train Loss: 1.0024, Test Loss: 0.7973\n",
      "Epoch 5/51, Train Loss: 1.0578, Test Loss: 0.7874\n",
      "Epoch 6/51, Train Loss: 1.0060, Test Loss: 0.8399\n",
      "Epoch 7/51, Train Loss: 1.0262, Test Loss: 0.7900\n",
      "Epoch 8/51, Train Loss: 1.0152, Test Loss: 0.7767\n",
      "Epoch 9/51, Train Loss: 1.0062, Test Loss: 0.7977\n",
      "Epoch 10/51, Train Loss: 1.0124, Test Loss: 0.7881\n",
      "Epoch 11/51, Train Loss: 1.0065, Test Loss: 0.7856\n",
      "Epoch 12/51, Train Loss: 0.9860, Test Loss: 0.8194\n",
      "Epoch 13/51, Train Loss: 0.9877, Test Loss: 0.8018\n",
      "Epoch 14/51, Train Loss: 0.9808, Test Loss: 0.7684\n",
      "Epoch 15/51, Train Loss: 0.9652, Test Loss: 0.7974\n",
      "Epoch 16/51, Train Loss: 0.9578, Test Loss: 0.7887\n",
      "Epoch 17/51, Train Loss: 0.9645, Test Loss: 0.7900\n",
      "Epoch 18/51, Train Loss: 0.9425, Test Loss: 0.8376\n",
      "Epoch 19/51, Train Loss: 0.9504, Test Loss: 0.8235\n",
      "Epoch 20/51, Train Loss: 0.9428, Test Loss: 0.7838\n",
      "Epoch 21/51, Train Loss: 0.9475, Test Loss: 0.7818\n",
      "Epoch 22/51, Train Loss: 0.9078, Test Loss: 0.8297\n",
      "Epoch 23/51, Train Loss: 0.9073, Test Loss: 0.7856\n",
      "Epoch 24/51, Train Loss: 0.8812, Test Loss: 0.8071\n",
      "Epoch 25/51, Train Loss: 0.8835, Test Loss: 0.7904\n",
      "Epoch 26/51, Train Loss: 0.8800, Test Loss: 0.7992\n",
      "Epoch 27/51, Train Loss: 0.8184, Test Loss: 0.8715\n",
      "Epoch 28/51, Train Loss: 0.8882, Test Loss: 0.8382\n",
      "Epoch 29/51, Train Loss: 0.8434, Test Loss: 0.8630\n",
      "Epoch 30/51, Train Loss: 0.8755, Test Loss: 0.7970\n",
      "Epoch 31/51, Train Loss: 0.8369, Test Loss: 0.8738\n",
      "Epoch 32/51, Train Loss: 0.8103, Test Loss: 0.8295\n",
      "Epoch 33/51, Train Loss: 0.7818, Test Loss: 0.8493\n",
      "Epoch 34/51, Train Loss: 0.7287, Test Loss: 0.8334\n",
      "Epoch 35/51, Train Loss: 0.7530, Test Loss: 0.8351\n",
      "Epoch 36/51, Train Loss: 0.7579, Test Loss: 0.8180\n",
      "Epoch 37/51, Train Loss: 0.7057, Test Loss: 0.8283\n",
      "Epoch 38/51, Train Loss: 0.6888, Test Loss: 0.8823\n",
      "Epoch 39/51, Train Loss: 0.7064, Test Loss: 0.8146\n",
      "Epoch 40/51, Train Loss: 0.7301, Test Loss: 0.8153\n",
      "Epoch 41/51, Train Loss: 0.6891, Test Loss: 0.8047\n",
      "Epoch 42/51, Train Loss: 0.6563, Test Loss: 0.8766\n",
      "Epoch 43/51, Train Loss: 0.6531, Test Loss: 0.9468\n",
      "Epoch 44/51, Train Loss: 0.6332, Test Loss: 0.8187\n",
      "Epoch 45/51, Train Loss: 0.5840, Test Loss: 0.8648\n",
      "Epoch 46/51, Train Loss: 0.6095, Test Loss: 0.9207\n",
      "Epoch 47/51, Train Loss: 0.6184, Test Loss: 0.8615\n",
      "Epoch 48/51, Train Loss: 0.5510, Test Loss: 0.8871\n",
      "Epoch 49/51, Train Loss: 0.5509, Test Loss: 0.8914\n",
      "Epoch 50/51, Train Loss: 0.5215, Test Loss: 0.8964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:46:34,819] Trial 19 finished with value: 0.871054008603096 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 83, 'layer_1_size': 147, 'layer_2_size': 154, 'layer_3_size': 159, 'layer_4_size': 66, 'layer_5_size': 103, 'layer_6_size': 178, 'layer_7_size': 91, 'layer_8_size': 176, 'layer_9_size': 76, 'layer_10_size': 211, 'layer_11_size': 200, 'layer_12_size': 118, 'layer_13_size': 200, 'dropout_rate': 0.3030415555076535, 'learning_rate': 0.008371584789234204, 'batch_size': 64, 'epochs': 51}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/51, Train Loss: 0.5728, Test Loss: 0.8711\n",
      "Epoch 1/25, Train Loss: 1.1921, Test Loss: 1.0055\n",
      "Epoch 2/25, Train Loss: 1.0787, Test Loss: 0.9815\n",
      "Epoch 3/25, Train Loss: 1.1959, Test Loss: 0.9574\n",
      "Epoch 4/25, Train Loss: 1.0743, Test Loss: 0.9481\n",
      "Epoch 5/25, Train Loss: 1.1003, Test Loss: 0.9450\n",
      "Epoch 6/25, Train Loss: 1.0897, Test Loss: 0.9456\n",
      "Epoch 7/25, Train Loss: 1.0742, Test Loss: 0.9456\n",
      "Epoch 8/25, Train Loss: 1.0727, Test Loss: 0.9457\n",
      "Epoch 9/25, Train Loss: 1.0893, Test Loss: 0.9462\n",
      "Epoch 10/25, Train Loss: 1.0713, Test Loss: 0.9450\n",
      "Epoch 11/25, Train Loss: 1.0872, Test Loss: 0.9455\n",
      "Epoch 12/25, Train Loss: 1.0710, Test Loss: 0.9462\n",
      "Epoch 13/25, Train Loss: 1.0811, Test Loss: 0.9462\n",
      "Epoch 14/25, Train Loss: 1.1049, Test Loss: 0.9465\n",
      "Epoch 15/25, Train Loss: 1.0818, Test Loss: 0.9468\n",
      "Epoch 16/25, Train Loss: 1.0357, Test Loss: 0.9481\n",
      "Epoch 17/25, Train Loss: 1.0663, Test Loss: 0.9479\n",
      "Epoch 18/25, Train Loss: 1.0164, Test Loss: 0.9464\n",
      "Epoch 19/25, Train Loss: 1.0339, Test Loss: 0.9447\n",
      "Epoch 20/25, Train Loss: 1.0702, Test Loss: 0.9448\n",
      "Epoch 21/25, Train Loss: 1.0222, Test Loss: 0.9457\n",
      "Epoch 22/25, Train Loss: 1.0128, Test Loss: 0.9454\n",
      "Epoch 23/25, Train Loss: 1.0331, Test Loss: 0.9452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:46:37,854] Trial 20 finished with value: 0.9462142586708069 and parameters: {'num_hidden_layers': 18, 'layer_0_size': 54, 'layer_1_size': 185, 'layer_2_size': 252, 'layer_3_size': 201, 'layer_4_size': 91, 'layer_5_size': 132, 'layer_6_size': 68, 'layer_7_size': 205, 'layer_8_size': 87, 'layer_9_size': 203, 'layer_10_size': 62, 'layer_11_size': 45, 'layer_12_size': 202, 'layer_13_size': 101, 'layer_14_size': 234, 'layer_15_size': 176, 'layer_16_size': 243, 'layer_17_size': 41, 'dropout_rate': 0.3931105695471781, 'learning_rate': 0.00022088270262702442, 'batch_size': 128, 'epochs': 25}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/25, Train Loss: 1.0274, Test Loss: 0.9454\n",
      "Epoch 25/25, Train Loss: 0.9969, Test Loss: 0.9462\n",
      "Epoch 1/94, Train Loss: 1.1627, Test Loss: 1.0059\n",
      "Epoch 2/94, Train Loss: 1.1105, Test Loss: 0.9491\n",
      "Epoch 3/94, Train Loss: 1.0764, Test Loss: 0.9480\n",
      "Epoch 4/94, Train Loss: 1.1410, Test Loss: 0.9152\n",
      "Epoch 5/94, Train Loss: 1.0727, Test Loss: 0.9209\n",
      "Epoch 6/94, Train Loss: 1.1131, Test Loss: 0.9026\n",
      "Epoch 7/94, Train Loss: 1.0804, Test Loss: 0.9543\n",
      "Epoch 8/94, Train Loss: 1.0335, Test Loss: 0.9457\n",
      "Epoch 9/94, Train Loss: 1.0852, Test Loss: 0.9131\n",
      "Epoch 10/94, Train Loss: 1.0726, Test Loss: 0.9345\n",
      "Epoch 11/94, Train Loss: 1.0401, Test Loss: 0.9167\n",
      "Epoch 12/94, Train Loss: 1.0756, Test Loss: 0.8998\n",
      "Epoch 13/94, Train Loss: 1.0661, Test Loss: 0.8956\n",
      "Epoch 14/94, Train Loss: 1.0299, Test Loss: 0.9275\n",
      "Epoch 15/94, Train Loss: 1.0426, Test Loss: 0.9038\n",
      "Epoch 16/94, Train Loss: 1.0346, Test Loss: 0.9378\n",
      "Epoch 17/94, Train Loss: 1.0363, Test Loss: 0.9123\n",
      "Epoch 18/94, Train Loss: 1.0207, Test Loss: 0.9054\n",
      "Epoch 19/94, Train Loss: 1.0418, Test Loss: 0.8858\n",
      "Epoch 20/94, Train Loss: 1.0327, Test Loss: 0.9020\n",
      "Epoch 21/94, Train Loss: 1.0520, Test Loss: 0.9126\n",
      "Epoch 22/94, Train Loss: 1.0253, Test Loss: 0.9169\n",
      "Epoch 23/94, Train Loss: 1.0456, Test Loss: 0.9004\n",
      "Epoch 24/94, Train Loss: 1.0331, Test Loss: 0.8948\n",
      "Epoch 25/94, Train Loss: 1.0372, Test Loss: 0.8873\n",
      "Epoch 26/94, Train Loss: 1.0137, Test Loss: 0.9114\n",
      "Epoch 27/94, Train Loss: 1.0198, Test Loss: 0.9108\n",
      "Epoch 28/94, Train Loss: 1.0020, Test Loss: 0.9042\n",
      "Epoch 29/94, Train Loss: 1.0396, Test Loss: 0.9140\n",
      "Epoch 30/94, Train Loss: 1.0292, Test Loss: 0.9252\n",
      "Epoch 31/94, Train Loss: 1.0395, Test Loss: 0.9401\n",
      "Epoch 32/94, Train Loss: 1.0134, Test Loss: 0.9493\n",
      "Epoch 33/94, Train Loss: 1.0202, Test Loss: 0.9157\n",
      "Epoch 34/94, Train Loss: 1.0214, Test Loss: 0.9447\n",
      "Epoch 35/94, Train Loss: 1.0074, Test Loss: 0.9378\n",
      "Epoch 36/94, Train Loss: 1.0359, Test Loss: 0.9595\n",
      "Epoch 37/94, Train Loss: 0.9899, Test Loss: 0.9705\n",
      "Epoch 38/94, Train Loss: 1.0217, Test Loss: 0.9679\n",
      "Epoch 39/94, Train Loss: 0.9878, Test Loss: 0.9337\n",
      "Epoch 40/94, Train Loss: 1.0073, Test Loss: 0.9736\n",
      "Epoch 41/94, Train Loss: 1.0217, Test Loss: 0.9297\n",
      "Epoch 42/94, Train Loss: 0.9974, Test Loss: 0.9246\n",
      "Epoch 43/94, Train Loss: 0.9613, Test Loss: 0.9536\n",
      "Epoch 44/94, Train Loss: 0.9771, Test Loss: 0.9595\n",
      "Epoch 45/94, Train Loss: 0.9730, Test Loss: 0.9745\n",
      "Epoch 46/94, Train Loss: 0.9648, Test Loss: 1.0119\n",
      "Epoch 47/94, Train Loss: 0.9632, Test Loss: 1.0627\n",
      "Epoch 48/94, Train Loss: 0.9507, Test Loss: 1.0367\n",
      "Epoch 49/94, Train Loss: 0.9738, Test Loss: 1.0179\n",
      "Epoch 50/94, Train Loss: 0.9090, Test Loss: 1.0202\n",
      "Epoch 51/94, Train Loss: 0.9348, Test Loss: 0.9999\n",
      "Epoch 52/94, Train Loss: 0.9404, Test Loss: 1.0409\n",
      "Epoch 53/94, Train Loss: 0.9317, Test Loss: 1.0393\n",
      "Epoch 54/94, Train Loss: 0.9701, Test Loss: 0.9863\n",
      "Epoch 55/94, Train Loss: 0.8939, Test Loss: 1.0560\n",
      "Epoch 56/94, Train Loss: 0.8930, Test Loss: 1.0696\n",
      "Epoch 57/94, Train Loss: 0.8795, Test Loss: 1.0726\n",
      "Epoch 58/94, Train Loss: 0.8696, Test Loss: 1.1339\n",
      "Epoch 59/94, Train Loss: 0.8573, Test Loss: 1.1262\n",
      "Epoch 60/94, Train Loss: 0.8452, Test Loss: 1.1386\n",
      "Epoch 61/94, Train Loss: 0.8579, Test Loss: 1.1296\n",
      "Epoch 62/94, Train Loss: 0.7958, Test Loss: 1.1194\n",
      "Epoch 63/94, Train Loss: 0.8143, Test Loss: 1.1162\n",
      "Epoch 64/94, Train Loss: 0.8230, Test Loss: 1.1087\n",
      "Epoch 65/94, Train Loss: 0.8492, Test Loss: 1.0652\n",
      "Epoch 66/94, Train Loss: 0.8390, Test Loss: 1.1153\n",
      "Epoch 67/94, Train Loss: 0.8171, Test Loss: 1.1852\n",
      "Epoch 68/94, Train Loss: 0.8108, Test Loss: 1.2074\n",
      "Epoch 69/94, Train Loss: 0.7730, Test Loss: 1.2000\n",
      "Epoch 70/94, Train Loss: 0.6957, Test Loss: 1.2612\n",
      "Epoch 71/94, Train Loss: 0.7503, Test Loss: 1.1918\n",
      "Epoch 72/94, Train Loss: 0.7472, Test Loss: 1.1870\n",
      "Epoch 73/94, Train Loss: 0.7430, Test Loss: 1.2686\n",
      "Epoch 74/94, Train Loss: 0.7251, Test Loss: 1.2845\n",
      "Epoch 75/94, Train Loss: 0.7208, Test Loss: 1.2308\n",
      "Epoch 76/94, Train Loss: 0.6963, Test Loss: 1.2468\n",
      "Epoch 77/94, Train Loss: 0.6597, Test Loss: 1.3041\n",
      "Epoch 78/94, Train Loss: 0.6588, Test Loss: 1.2144\n",
      "Epoch 79/94, Train Loss: 0.6771, Test Loss: 1.2221\n",
      "Epoch 80/94, Train Loss: 0.6318, Test Loss: 1.2760\n",
      "Epoch 81/94, Train Loss: 0.6283, Test Loss: 1.3588\n",
      "Epoch 82/94, Train Loss: 0.6405, Test Loss: 1.2642\n",
      "Epoch 83/94, Train Loss: 0.5803, Test Loss: 1.3750\n",
      "Epoch 84/94, Train Loss: 0.6322, Test Loss: 1.3138\n",
      "Epoch 85/94, Train Loss: 0.5824, Test Loss: 1.2569\n",
      "Epoch 86/94, Train Loss: 0.5571, Test Loss: 1.3670\n",
      "Epoch 87/94, Train Loss: 0.5835, Test Loss: 1.4179\n",
      "Epoch 88/94, Train Loss: 0.5582, Test Loss: 1.3232\n",
      "Epoch 89/94, Train Loss: 0.5515, Test Loss: 1.2943\n",
      "Epoch 90/94, Train Loss: 0.5387, Test Loss: 1.3116\n",
      "Epoch 91/94, Train Loss: 0.5658, Test Loss: 1.3401\n",
      "Epoch 92/94, Train Loss: 0.5757, Test Loss: 1.2704\n",
      "Epoch 93/94, Train Loss: 0.5461, Test Loss: 1.3751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:47:05,224] Trial 21 finished with value: 1.3358747277941023 and parameters: {'num_hidden_layers': 15, 'layer_0_size': 180, 'layer_1_size': 205, 'layer_2_size': 134, 'layer_3_size': 110, 'layer_4_size': 254, 'layer_5_size': 198, 'layer_6_size': 32, 'layer_7_size': 162, 'layer_8_size': 116, 'layer_9_size': 193, 'layer_10_size': 140, 'layer_11_size': 169, 'layer_12_size': 128, 'layer_13_size': 157, 'layer_14_size': 111, 'dropout_rate': 0.21010917277288926, 'learning_rate': 0.0007823056846800397, 'batch_size': 32, 'epochs': 94}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/94, Train Loss: 0.5603, Test Loss: 1.3359\n",
      "Epoch 1/46, Train Loss: 1.1503, Test Loss: 1.0691\n",
      "Epoch 2/46, Train Loss: 1.0947, Test Loss: 1.0697\n",
      "Epoch 3/46, Train Loss: 1.1467, Test Loss: 1.0718\n",
      "Epoch 4/46, Train Loss: 1.0676, Test Loss: 1.0755\n",
      "Epoch 5/46, Train Loss: 1.0858, Test Loss: 1.0660\n",
      "Epoch 6/46, Train Loss: 1.1094, Test Loss: 1.0693\n",
      "Epoch 7/46, Train Loss: 1.0522, Test Loss: 1.0614\n",
      "Epoch 8/46, Train Loss: 1.0830, Test Loss: 1.0855\n",
      "Epoch 9/46, Train Loss: 1.0879, Test Loss: 1.0688\n",
      "Epoch 10/46, Train Loss: 1.0590, Test Loss: 1.0666\n",
      "Epoch 11/46, Train Loss: 1.0374, Test Loss: 1.0652\n",
      "Epoch 12/46, Train Loss: 1.0451, Test Loss: 1.0640\n",
      "Epoch 13/46, Train Loss: 1.0637, Test Loss: 1.0804\n",
      "Epoch 14/46, Train Loss: 1.0735, Test Loss: 1.0894\n",
      "Epoch 15/46, Train Loss: 1.0190, Test Loss: 1.0694\n",
      "Epoch 16/46, Train Loss: 1.0455, Test Loss: 1.0700\n",
      "Epoch 17/46, Train Loss: 1.0379, Test Loss: 1.0659\n",
      "Epoch 18/46, Train Loss: 1.0162, Test Loss: 1.0660\n",
      "Epoch 19/46, Train Loss: 1.0251, Test Loss: 1.0685\n",
      "Epoch 20/46, Train Loss: 1.0026, Test Loss: 1.0625\n",
      "Epoch 21/46, Train Loss: 1.0163, Test Loss: 1.0664\n",
      "Epoch 22/46, Train Loss: 1.0156, Test Loss: 1.0678\n",
      "Epoch 23/46, Train Loss: 1.0072, Test Loss: 1.0636\n",
      "Epoch 24/46, Train Loss: 1.0306, Test Loss: 1.0654\n",
      "Epoch 25/46, Train Loss: 1.0161, Test Loss: 1.0705\n",
      "Epoch 26/46, Train Loss: 1.0420, Test Loss: 1.0645\n",
      "Epoch 27/46, Train Loss: 1.0472, Test Loss: 1.0690\n",
      "Epoch 28/46, Train Loss: 1.0052, Test Loss: 1.0674\n",
      "Epoch 29/46, Train Loss: 1.0073, Test Loss: 1.0655\n",
      "Epoch 30/46, Train Loss: 1.0020, Test Loss: 1.0663\n",
      "Epoch 31/46, Train Loss: 0.9768, Test Loss: 1.0653\n",
      "Epoch 32/46, Train Loss: 1.0133, Test Loss: 1.0668\n",
      "Epoch 33/46, Train Loss: 1.0201, Test Loss: 1.0648\n",
      "Epoch 34/46, Train Loss: 1.0023, Test Loss: 1.0614\n",
      "Epoch 35/46, Train Loss: 1.0170, Test Loss: 1.0648\n",
      "Epoch 36/46, Train Loss: 0.9856, Test Loss: 1.0646\n",
      "Epoch 37/46, Train Loss: 0.9928, Test Loss: 1.0666\n",
      "Epoch 38/46, Train Loss: 0.9992, Test Loss: 1.0704\n",
      "Epoch 39/46, Train Loss: 1.0061, Test Loss: 1.0655\n",
      "Epoch 40/46, Train Loss: 0.9978, Test Loss: 1.0669\n",
      "Epoch 41/46, Train Loss: 1.0068, Test Loss: 1.0702\n",
      "Epoch 42/46, Train Loss: 1.0129, Test Loss: 1.0670\n",
      "Epoch 43/46, Train Loss: 1.0217, Test Loss: 1.0735\n",
      "Epoch 44/46, Train Loss: 1.0105, Test Loss: 1.0840\n",
      "Epoch 45/46, Train Loss: 1.0207, Test Loss: 1.0704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:47:19,179] Trial 22 finished with value: 1.0700275983129228 and parameters: {'num_hidden_layers': 18, 'layer_0_size': 118, 'layer_1_size': 222, 'layer_2_size': 147, 'layer_3_size': 145, 'layer_4_size': 128, 'layer_5_size': 79, 'layer_6_size': 135, 'layer_7_size': 111, 'layer_8_size': 91, 'layer_9_size': 57, 'layer_10_size': 115, 'layer_11_size': 137, 'layer_12_size': 219, 'layer_13_size': 128, 'layer_14_size': 47, 'layer_15_size': 40, 'layer_16_size': 154, 'layer_17_size': 254, 'dropout_rate': 0.2856649680675406, 'learning_rate': 0.0005978838659399474, 'batch_size': 32, 'epochs': 46}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/46, Train Loss: 1.0102, Test Loss: 1.0700\n",
      "Epoch 1/61, Train Loss: 1.1523, Test Loss: 0.9575\n",
      "Epoch 2/61, Train Loss: 1.0573, Test Loss: 0.9644\n",
      "Epoch 3/61, Train Loss: 1.0504, Test Loss: 0.9546\n",
      "Epoch 4/61, Train Loss: 1.0664, Test Loss: 0.9648\n",
      "Epoch 5/61, Train Loss: 1.0254, Test Loss: 0.9353\n",
      "Epoch 6/61, Train Loss: 1.0109, Test Loss: 0.9699\n",
      "Epoch 7/61, Train Loss: 1.0642, Test Loss: 0.9488\n",
      "Epoch 8/61, Train Loss: 0.9945, Test Loss: 0.9587\n",
      "Epoch 9/61, Train Loss: 0.9863, Test Loss: 0.9931\n",
      "Epoch 10/61, Train Loss: 0.9968, Test Loss: 0.9346\n",
      "Epoch 11/61, Train Loss: 0.9929, Test Loss: 1.0458\n",
      "Epoch 12/61, Train Loss: 0.9928, Test Loss: 0.9348\n",
      "Epoch 13/61, Train Loss: 0.9864, Test Loss: 0.9516\n",
      "Epoch 14/61, Train Loss: 0.9952, Test Loss: 1.0172\n",
      "Epoch 15/61, Train Loss: 0.9824, Test Loss: 0.9817\n",
      "Epoch 16/61, Train Loss: 0.9918, Test Loss: 0.9456\n",
      "Epoch 17/61, Train Loss: 1.0094, Test Loss: 0.9384\n",
      "Epoch 18/61, Train Loss: 0.9909, Test Loss: 1.0567\n",
      "Epoch 19/61, Train Loss: 1.0001, Test Loss: 1.0164\n",
      "Epoch 20/61, Train Loss: 0.9808, Test Loss: 0.9300\n",
      "Epoch 21/61, Train Loss: 0.9895, Test Loss: 0.9926\n",
      "Epoch 22/61, Train Loss: 0.9887, Test Loss: 0.9606\n",
      "Epoch 23/61, Train Loss: 0.9900, Test Loss: 0.9555\n",
      "Epoch 24/61, Train Loss: 1.0102, Test Loss: 0.9357\n",
      "Epoch 25/61, Train Loss: 0.9705, Test Loss: 0.9625\n",
      "Epoch 26/61, Train Loss: 0.9754, Test Loss: 0.9700\n",
      "Epoch 27/61, Train Loss: 0.9666, Test Loss: 0.9831\n",
      "Epoch 28/61, Train Loss: 1.0014, Test Loss: 0.9614\n",
      "Epoch 29/61, Train Loss: 0.9891, Test Loss: 0.9247\n",
      "Epoch 30/61, Train Loss: 0.9601, Test Loss: 0.9266\n",
      "Epoch 31/61, Train Loss: 0.9656, Test Loss: 0.9180\n",
      "Epoch 32/61, Train Loss: 0.9645, Test Loss: 0.9352\n",
      "Epoch 33/61, Train Loss: 0.9589, Test Loss: 0.9662\n",
      "Epoch 34/61, Train Loss: 0.9438, Test Loss: 0.9304\n",
      "Epoch 35/61, Train Loss: 0.9644, Test Loss: 0.9472\n",
      "Epoch 36/61, Train Loss: 1.0116, Test Loss: 0.9321\n",
      "Epoch 37/61, Train Loss: 0.9591, Test Loss: 0.9553\n",
      "Epoch 38/61, Train Loss: 0.9222, Test Loss: 0.9758\n",
      "Epoch 39/61, Train Loss: 0.9292, Test Loss: 0.9748\n",
      "Epoch 40/61, Train Loss: 0.9266, Test Loss: 0.9551\n",
      "Epoch 41/61, Train Loss: 0.9107, Test Loss: 0.9191\n",
      "Epoch 42/61, Train Loss: 0.8894, Test Loss: 0.9494\n",
      "Epoch 43/61, Train Loss: 0.8878, Test Loss: 1.0103\n",
      "Epoch 44/61, Train Loss: 0.8715, Test Loss: 0.9389\n",
      "Epoch 45/61, Train Loss: 0.8595, Test Loss: 0.9345\n",
      "Epoch 46/61, Train Loss: 0.8371, Test Loss: 0.9116\n",
      "Epoch 47/61, Train Loss: 0.8239, Test Loss: 0.9647\n",
      "Epoch 48/61, Train Loss: 0.8372, Test Loss: 0.9379\n",
      "Epoch 49/61, Train Loss: 0.7868, Test Loss: 0.9241\n",
      "Epoch 50/61, Train Loss: 0.7708, Test Loss: 0.9506\n",
      "Epoch 51/61, Train Loss: 0.8054, Test Loss: 0.9219\n",
      "Epoch 52/61, Train Loss: 0.7632, Test Loss: 0.9470\n",
      "Epoch 53/61, Train Loss: 0.7409, Test Loss: 0.9451\n",
      "Epoch 54/61, Train Loss: 0.7202, Test Loss: 0.9668\n",
      "Epoch 55/61, Train Loss: 0.7713, Test Loss: 0.9592\n",
      "Epoch 56/61, Train Loss: 0.7487, Test Loss: 0.9559\n",
      "Epoch 57/61, Train Loss: 0.7250, Test Loss: 0.9833\n",
      "Epoch 58/61, Train Loss: 0.7679, Test Loss: 0.9547\n",
      "Epoch 59/61, Train Loss: 0.6808, Test Loss: 1.0136\n",
      "Epoch 60/61, Train Loss: 0.7254, Test Loss: 1.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:47:33,484] Trial 23 finished with value: 1.0019423791340418 and parameters: {'num_hidden_layers': 12, 'layer_0_size': 91, 'layer_1_size': 209, 'layer_2_size': 181, 'layer_3_size': 54, 'layer_4_size': 175, 'layer_5_size': 216, 'layer_6_size': 81, 'layer_7_size': 128, 'layer_8_size': 131, 'layer_9_size': 96, 'layer_10_size': 154, 'layer_11_size': 196, 'dropout_rate': 0.28894211840215633, 'learning_rate': 0.0020150583914580627, 'batch_size': 32, 'epochs': 61}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/61, Train Loss: 0.7265, Test Loss: 1.0019\n",
      "Epoch 1/85, Train Loss: 1.2195, Test Loss: 1.0164\n",
      "Epoch 2/85, Train Loss: 1.1317, Test Loss: 1.0265\n",
      "Epoch 3/85, Train Loss: 1.1173, Test Loss: 1.0263\n",
      "Epoch 4/85, Train Loss: 1.1693, Test Loss: 1.0247\n",
      "Epoch 5/85, Train Loss: 1.0998, Test Loss: 1.0205\n",
      "Epoch 6/85, Train Loss: 1.1246, Test Loss: 1.0165\n",
      "Epoch 7/85, Train Loss: 1.1289, Test Loss: 1.0190\n",
      "Epoch 8/85, Train Loss: 1.0898, Test Loss: 1.0223\n",
      "Epoch 9/85, Train Loss: 1.1255, Test Loss: 1.0229\n",
      "Epoch 10/85, Train Loss: 1.1212, Test Loss: 1.0275\n",
      "Epoch 11/85, Train Loss: 1.1048, Test Loss: 1.0376\n",
      "Epoch 12/85, Train Loss: 1.0983, Test Loss: 1.0273\n",
      "Epoch 13/85, Train Loss: 1.0917, Test Loss: 1.0241\n",
      "Epoch 14/85, Train Loss: 1.1154, Test Loss: 1.0272\n",
      "Epoch 15/85, Train Loss: 1.0966, Test Loss: 1.0217\n",
      "Epoch 16/85, Train Loss: 1.0810, Test Loss: 1.0237\n",
      "Epoch 17/85, Train Loss: 1.0868, Test Loss: 1.0131\n",
      "Epoch 18/85, Train Loss: 1.0965, Test Loss: 1.0195\n",
      "Epoch 19/85, Train Loss: 1.1212, Test Loss: 1.0136\n",
      "Epoch 20/85, Train Loss: 1.0839, Test Loss: 1.0183\n",
      "Epoch 21/85, Train Loss: 1.1050, Test Loss: 1.0162\n",
      "Epoch 22/85, Train Loss: 1.0724, Test Loss: 1.0155\n",
      "Epoch 23/85, Train Loss: 1.0649, Test Loss: 1.0170\n",
      "Epoch 24/85, Train Loss: 1.0872, Test Loss: 1.0353\n",
      "Epoch 25/85, Train Loss: 1.0715, Test Loss: 1.0274\n",
      "Epoch 26/85, Train Loss: 1.0978, Test Loss: 1.0270\n",
      "Epoch 27/85, Train Loss: 1.0495, Test Loss: 1.0193\n",
      "Epoch 28/85, Train Loss: 1.0602, Test Loss: 1.0226\n",
      "Epoch 29/85, Train Loss: 1.0793, Test Loss: 1.0222\n",
      "Epoch 30/85, Train Loss: 1.0832, Test Loss: 1.0292\n",
      "Epoch 31/85, Train Loss: 1.0875, Test Loss: 1.0254\n",
      "Epoch 32/85, Train Loss: 1.0982, Test Loss: 1.0346\n",
      "Epoch 33/85, Train Loss: 1.0953, Test Loss: 1.0305\n",
      "Epoch 34/85, Train Loss: 1.0727, Test Loss: 1.0311\n",
      "Epoch 35/85, Train Loss: 1.0830, Test Loss: 1.0343\n",
      "Epoch 36/85, Train Loss: 1.0794, Test Loss: 1.0244\n",
      "Epoch 37/85, Train Loss: 1.0630, Test Loss: 1.0190\n",
      "Epoch 38/85, Train Loss: 1.0629, Test Loss: 1.0300\n",
      "Epoch 39/85, Train Loss: 1.0735, Test Loss: 1.0280\n",
      "Epoch 40/85, Train Loss: 1.0476, Test Loss: 1.0201\n",
      "Epoch 41/85, Train Loss: 1.0710, Test Loss: 1.0180\n",
      "Epoch 42/85, Train Loss: 1.0731, Test Loss: 1.0117\n",
      "Epoch 43/85, Train Loss: 1.1074, Test Loss: 1.0236\n",
      "Epoch 44/85, Train Loss: 1.0764, Test Loss: 1.0264\n",
      "Epoch 45/85, Train Loss: 1.0868, Test Loss: 1.0148\n",
      "Epoch 46/85, Train Loss: 1.0581, Test Loss: 1.0184\n",
      "Epoch 47/85, Train Loss: 1.0499, Test Loss: 1.0148\n",
      "Epoch 48/85, Train Loss: 1.0785, Test Loss: 1.0142\n",
      "Epoch 49/85, Train Loss: 1.0819, Test Loss: 1.0237\n",
      "Epoch 50/85, Train Loss: 1.0624, Test Loss: 1.0178\n",
      "Epoch 51/85, Train Loss: 1.0740, Test Loss: 1.0202\n",
      "Epoch 52/85, Train Loss: 1.0619, Test Loss: 1.0335\n",
      "Epoch 53/85, Train Loss: 1.0704, Test Loss: 1.0362\n",
      "Epoch 54/85, Train Loss: 1.0791, Test Loss: 1.0237\n",
      "Epoch 55/85, Train Loss: 1.1035, Test Loss: 1.0257\n",
      "Epoch 56/85, Train Loss: 1.0623, Test Loss: 1.0229\n",
      "Epoch 57/85, Train Loss: 1.0659, Test Loss: 1.0447\n",
      "Epoch 58/85, Train Loss: 1.0547, Test Loss: 1.0507\n",
      "Epoch 59/85, Train Loss: 1.0776, Test Loss: 1.0517\n",
      "Epoch 60/85, Train Loss: 1.0693, Test Loss: 1.0520\n",
      "Epoch 61/85, Train Loss: 1.0586, Test Loss: 1.0534\n",
      "Epoch 62/85, Train Loss: 1.0572, Test Loss: 1.0534\n",
      "Epoch 63/85, Train Loss: 1.0875, Test Loss: 1.0330\n",
      "Epoch 64/85, Train Loss: 1.0505, Test Loss: 1.0405\n",
      "Epoch 65/85, Train Loss: 1.0616, Test Loss: 1.0403\n",
      "Epoch 66/85, Train Loss: 1.0680, Test Loss: 1.0333\n",
      "Epoch 67/85, Train Loss: 1.0586, Test Loss: 1.0256\n",
      "Epoch 68/85, Train Loss: 1.1057, Test Loss: 1.0333\n",
      "Epoch 69/85, Train Loss: 1.0515, Test Loss: 1.0316\n",
      "Epoch 70/85, Train Loss: 1.0646, Test Loss: 1.0286\n",
      "Epoch 71/85, Train Loss: 1.0798, Test Loss: 1.0388\n",
      "Epoch 72/85, Train Loss: 1.0930, Test Loss: 1.0530\n",
      "Epoch 73/85, Train Loss: 1.0603, Test Loss: 1.0393\n",
      "Epoch 74/85, Train Loss: 1.0504, Test Loss: 1.0352\n",
      "Epoch 75/85, Train Loss: 1.0479, Test Loss: 1.0441\n",
      "Epoch 76/85, Train Loss: 1.0819, Test Loss: 1.0346\n",
      "Epoch 77/85, Train Loss: 1.0588, Test Loss: 1.0352\n",
      "Epoch 78/85, Train Loss: 1.0507, Test Loss: 1.0344\n",
      "Epoch 79/85, Train Loss: 1.0541, Test Loss: 1.0342\n",
      "Epoch 80/85, Train Loss: 1.0668, Test Loss: 1.0249\n",
      "Epoch 81/85, Train Loss: 1.0608, Test Loss: 1.0265\n",
      "Epoch 82/85, Train Loss: 1.0785, Test Loss: 1.0352\n",
      "Epoch 83/85, Train Loss: 1.0588, Test Loss: 1.0269\n",
      "Epoch 84/85, Train Loss: 1.0688, Test Loss: 1.0276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:47:54,657] Trial 24 finished with value: 1.024799176624843 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 131, 'layer_1_size': 256, 'layer_2_size': 144, 'layer_3_size': 122, 'layer_4_size': 226, 'layer_5_size': 138, 'layer_6_size': 54, 'layer_7_size': 163, 'layer_8_size': 100, 'layer_9_size': 39, 'layer_10_size': 87, 'layer_11_size': 166, 'layer_12_size': 150, 'layer_13_size': 87, 'dropout_rate': 0.2146078398366389, 'learning_rate': 0.00019559073024928795, 'batch_size': 32, 'epochs': 85}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/85, Train Loss: 1.0585, Test Loss: 1.0248\n",
      "Epoch 1/15, Train Loss: 1.0813, Test Loss: 1.0387\n",
      "Epoch 2/15, Train Loss: 1.0325, Test Loss: 1.0248\n",
      "Epoch 3/15, Train Loss: 1.0672, Test Loss: 1.0308\n",
      "Epoch 4/15, Train Loss: 1.0442, Test Loss: 1.0164\n",
      "Epoch 5/15, Train Loss: 1.0267, Test Loss: 0.9994\n",
      "Epoch 6/15, Train Loss: 1.0074, Test Loss: 0.9982\n",
      "Epoch 7/15, Train Loss: 0.9978, Test Loss: 1.0045\n",
      "Epoch 8/15, Train Loss: 1.0081, Test Loss: 0.9971\n",
      "Epoch 9/15, Train Loss: 1.0111, Test Loss: 0.9960\n",
      "Epoch 10/15, Train Loss: 1.0006, Test Loss: 1.0011\n",
      "Epoch 11/15, Train Loss: 1.0021, Test Loss: 1.0000\n",
      "Epoch 12/15, Train Loss: 0.9981, Test Loss: 1.0001\n",
      "Epoch 13/15, Train Loss: 1.0030, Test Loss: 1.0052\n",
      "Epoch 14/15, Train Loss: 1.0101, Test Loss: 0.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:47:59,114] Trial 25 finished with value: 0.9990700909069606 and parameters: {'num_hidden_layers': 18, 'layer_0_size': 58, 'layer_1_size': 170, 'layer_2_size': 114, 'layer_3_size': 93, 'layer_4_size': 36, 'layer_5_size': 114, 'layer_6_size': 87, 'layer_7_size': 112, 'layer_8_size': 156, 'layer_9_size': 103, 'layer_10_size': 132, 'layer_11_size': 110, 'layer_12_size': 219, 'layer_13_size': 188, 'layer_14_size': 118, 'layer_15_size': 104, 'layer_16_size': 46, 'layer_17_size': 48, 'dropout_rate': 0.3299770437461258, 'learning_rate': 0.001973031860477384, 'batch_size': 32, 'epochs': 15}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15, Train Loss: 0.9912, Test Loss: 0.9991\n",
      "Epoch 1/40, Train Loss: 1.1000, Test Loss: 0.8809\n",
      "Epoch 2/40, Train Loss: 1.0543, Test Loss: 0.8677\n",
      "Epoch 3/40, Train Loss: 1.0551, Test Loss: 0.8625\n",
      "Epoch 4/40, Train Loss: 1.0696, Test Loss: 0.8636\n",
      "Epoch 5/40, Train Loss: 1.0315, Test Loss: 0.8689\n",
      "Epoch 6/40, Train Loss: 1.0169, Test Loss: 0.8669\n",
      "Epoch 7/40, Train Loss: 0.9798, Test Loss: 0.8780\n",
      "Epoch 8/40, Train Loss: 1.0035, Test Loss: 0.8840\n",
      "Epoch 9/40, Train Loss: 1.0125, Test Loss: 0.8689\n",
      "Epoch 10/40, Train Loss: 1.0292, Test Loss: 0.8893\n",
      "Epoch 11/40, Train Loss: 0.9974, Test Loss: 0.8682\n",
      "Epoch 12/40, Train Loss: 1.0010, Test Loss: 0.8635\n",
      "Epoch 13/40, Train Loss: 1.0024, Test Loss: 0.8620\n",
      "Epoch 14/40, Train Loss: 0.9904, Test Loss: 0.8671\n",
      "Epoch 15/40, Train Loss: 0.9663, Test Loss: 0.8644\n",
      "Epoch 16/40, Train Loss: 1.0285, Test Loss: 0.8644\n",
      "Epoch 17/40, Train Loss: 0.9660, Test Loss: 0.8674\n",
      "Epoch 18/40, Train Loss: 0.9415, Test Loss: 0.8639\n",
      "Epoch 19/40, Train Loss: 0.9922, Test Loss: 0.8678\n",
      "Epoch 20/40, Train Loss: 0.9811, Test Loss: 0.8602\n",
      "Epoch 21/40, Train Loss: 0.9498, Test Loss: 0.8626\n",
      "Epoch 22/40, Train Loss: 0.9693, Test Loss: 0.8621\n",
      "Epoch 23/40, Train Loss: 0.9859, Test Loss: 0.8669\n",
      "Epoch 24/40, Train Loss: 0.9757, Test Loss: 0.8620\n",
      "Epoch 25/40, Train Loss: 0.9801, Test Loss: 0.8593\n",
      "Epoch 26/40, Train Loss: 0.9371, Test Loss: 0.8614\n",
      "Epoch 27/40, Train Loss: 0.9521, Test Loss: 0.8560\n",
      "Epoch 28/40, Train Loss: 0.9815, Test Loss: 0.8561\n",
      "Epoch 29/40, Train Loss: 0.9953, Test Loss: 0.8624\n",
      "Epoch 30/40, Train Loss: 0.9626, Test Loss: 0.8615\n",
      "Epoch 31/40, Train Loss: 0.9384, Test Loss: 0.8608\n",
      "Epoch 32/40, Train Loss: 0.9537, Test Loss: 0.8588\n",
      "Epoch 33/40, Train Loss: 0.9850, Test Loss: 0.8630\n",
      "Epoch 34/40, Train Loss: 0.9492, Test Loss: 0.8595\n",
      "Epoch 35/40, Train Loss: 0.9521, Test Loss: 0.8612\n",
      "Epoch 36/40, Train Loss: 0.9852, Test Loss: 0.8606\n",
      "Epoch 37/40, Train Loss: 0.9546, Test Loss: 0.8769\n",
      "Epoch 38/40, Train Loss: 0.9287, Test Loss: 0.8675\n",
      "Epoch 39/40, Train Loss: 0.9589, Test Loss: 0.8755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:48:07,926] Trial 26 finished with value: 0.861845885004316 and parameters: {'num_hidden_layers': 11, 'layer_0_size': 133, 'layer_1_size': 232, 'layer_2_size': 179, 'layer_3_size': 144, 'layer_4_size': 131, 'layer_5_size': 82, 'layer_6_size': 189, 'layer_7_size': 209, 'layer_8_size': 68, 'layer_9_size': 167, 'layer_10_size': 168, 'dropout_rate': 0.28886678836451873, 'learning_rate': 0.0005991290590052126, 'batch_size': 32, 'epochs': 40}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/40, Train Loss: 0.9541, Test Loss: 0.8618\n",
      "Epoch 1/29, Train Loss: 1.1313, Test Loss: 1.1184\n",
      "Epoch 2/29, Train Loss: 1.1825, Test Loss: 1.1250\n",
      "Epoch 3/29, Train Loss: 1.1719, Test Loss: 1.1194\n",
      "Epoch 4/29, Train Loss: 1.2514, Test Loss: 1.1304\n",
      "Epoch 5/29, Train Loss: 1.1563, Test Loss: 1.1198\n",
      "Epoch 6/29, Train Loss: 1.1501, Test Loss: 1.1272\n",
      "Epoch 7/29, Train Loss: 1.1635, Test Loss: 1.1312\n",
      "Epoch 8/29, Train Loss: 1.1851, Test Loss: 1.1217\n",
      "Epoch 9/29, Train Loss: 1.2094, Test Loss: 1.1193\n",
      "Epoch 10/29, Train Loss: 1.1513, Test Loss: 1.1224\n",
      "Epoch 11/29, Train Loss: 1.1273, Test Loss: 1.1385\n",
      "Epoch 12/29, Train Loss: 1.1616, Test Loss: 1.1265\n",
      "Epoch 13/29, Train Loss: 1.1383, Test Loss: 1.1224\n",
      "Epoch 14/29, Train Loss: 1.1517, Test Loss: 1.1266\n",
      "Epoch 15/29, Train Loss: 1.1846, Test Loss: 1.1198\n",
      "Epoch 16/29, Train Loss: 1.1492, Test Loss: 1.1208\n",
      "Epoch 17/29, Train Loss: 1.1788, Test Loss: 1.1093\n",
      "Epoch 18/29, Train Loss: 1.1165, Test Loss: 1.1156\n",
      "Epoch 19/29, Train Loss: 1.1477, Test Loss: 1.1233\n",
      "Epoch 20/29, Train Loss: 1.1488, Test Loss: 1.1161\n",
      "Epoch 21/29, Train Loss: 1.1264, Test Loss: 1.1133\n",
      "Epoch 22/29, Train Loss: 1.1478, Test Loss: 1.1163\n",
      "Epoch 23/29, Train Loss: 1.1246, Test Loss: 1.1253\n",
      "Epoch 24/29, Train Loss: 1.1116, Test Loss: 1.1169\n",
      "Epoch 25/29, Train Loss: 1.1471, Test Loss: 1.1201\n",
      "Epoch 26/29, Train Loss: 1.1751, Test Loss: 1.1215\n",
      "Epoch 27/29, Train Loss: 1.1273, Test Loss: 1.1352\n",
      "Epoch 28/29, Train Loss: 1.1031, Test Loss: 1.1237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:48:15,899] Trial 27 finished with value: 1.1213818192481995 and parameters: {'num_hidden_layers': 16, 'layer_0_size': 170, 'layer_1_size': 205, 'layer_2_size': 83, 'layer_3_size': 77, 'layer_4_size': 99, 'layer_5_size': 60, 'layer_6_size': 130, 'layer_7_size': 73, 'layer_8_size': 138, 'layer_9_size': 62, 'layer_10_size': 82, 'layer_11_size': 131, 'layer_12_size': 108, 'layer_13_size': 219, 'layer_14_size': 147, 'layer_15_size': 123, 'dropout_rate': 0.16581148485404923, 'learning_rate': 2.6392200853089075e-05, 'batch_size': 32, 'epochs': 29}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/29, Train Loss: 1.1252, Test Loss: 1.1214\n",
      "Epoch 1/55, Train Loss: 1.2040, Test Loss: 0.7865\n",
      "Epoch 2/55, Train Loss: 1.2531, Test Loss: 0.7974\n",
      "Epoch 3/55, Train Loss: 1.2329, Test Loss: 0.7980\n",
      "Epoch 4/55, Train Loss: 1.1574, Test Loss: 0.7975\n",
      "Epoch 5/55, Train Loss: 1.1677, Test Loss: 0.8088\n",
      "Epoch 6/55, Train Loss: 1.1514, Test Loss: 0.8118\n",
      "Epoch 7/55, Train Loss: 1.1870, Test Loss: 0.8100\n",
      "Epoch 8/55, Train Loss: 1.1458, Test Loss: 0.8037\n",
      "Epoch 9/55, Train Loss: 1.1851, Test Loss: 0.8094\n",
      "Epoch 10/55, Train Loss: 1.2162, Test Loss: 0.8174\n",
      "Epoch 11/55, Train Loss: 1.1883, Test Loss: 0.8068\n",
      "Epoch 12/55, Train Loss: 1.1301, Test Loss: 0.8112\n",
      "Epoch 13/55, Train Loss: 1.2467, Test Loss: 0.8097\n",
      "Epoch 14/55, Train Loss: 1.1567, Test Loss: 0.8129\n",
      "Epoch 15/55, Train Loss: 1.1449, Test Loss: 0.8118\n",
      "Epoch 16/55, Train Loss: 1.2039, Test Loss: 0.8281\n",
      "Epoch 17/55, Train Loss: 1.1505, Test Loss: 0.8162\n",
      "Epoch 18/55, Train Loss: 1.1514, Test Loss: 0.8173\n",
      "Epoch 19/55, Train Loss: 1.1394, Test Loss: 0.8101\n",
      "Epoch 20/55, Train Loss: 1.1615, Test Loss: 0.8146\n",
      "Epoch 21/55, Train Loss: 1.1549, Test Loss: 0.8096\n",
      "Epoch 22/55, Train Loss: 1.1546, Test Loss: 0.8152\n",
      "Epoch 23/55, Train Loss: 1.1509, Test Loss: 0.8202\n",
      "Epoch 24/55, Train Loss: 1.1887, Test Loss: 0.8095\n",
      "Epoch 25/55, Train Loss: 1.1536, Test Loss: 0.8127\n",
      "Epoch 26/55, Train Loss: 1.1831, Test Loss: 0.8087\n",
      "Epoch 27/55, Train Loss: 1.1864, Test Loss: 0.8135\n",
      "Epoch 28/55, Train Loss: 1.1582, Test Loss: 0.8064\n",
      "Epoch 29/55, Train Loss: 1.1728, Test Loss: 0.8089\n",
      "Epoch 30/55, Train Loss: 1.1305, Test Loss: 0.8123\n",
      "Epoch 31/55, Train Loss: 1.1540, Test Loss: 0.8187\n",
      "Epoch 32/55, Train Loss: 1.1552, Test Loss: 0.8042\n",
      "Epoch 33/55, Train Loss: 1.1024, Test Loss: 0.8098\n",
      "Epoch 34/55, Train Loss: 1.2108, Test Loss: 0.8143\n",
      "Epoch 35/55, Train Loss: 1.1103, Test Loss: 0.8117\n",
      "Epoch 36/55, Train Loss: 1.1551, Test Loss: 0.8112\n",
      "Epoch 37/55, Train Loss: 1.1649, Test Loss: 0.8195\n",
      "Epoch 38/55, Train Loss: 1.1393, Test Loss: 0.8188\n",
      "Epoch 39/55, Train Loss: 1.1942, Test Loss: 0.8149\n",
      "Epoch 40/55, Train Loss: 1.1827, Test Loss: 0.8097\n",
      "Epoch 41/55, Train Loss: 1.1652, Test Loss: 0.8090\n",
      "Epoch 42/55, Train Loss: 1.1377, Test Loss: 0.8245\n",
      "Epoch 43/55, Train Loss: 1.1194, Test Loss: 0.8193\n",
      "Epoch 44/55, Train Loss: 1.1439, Test Loss: 0.8194\n",
      "Epoch 45/55, Train Loss: 1.1486, Test Loss: 0.8110\n",
      "Epoch 46/55, Train Loss: 1.1492, Test Loss: 0.8081\n",
      "Epoch 47/55, Train Loss: 1.1568, Test Loss: 0.8236\n",
      "Epoch 48/55, Train Loss: 1.1871, Test Loss: 0.8217\n",
      "Epoch 49/55, Train Loss: 1.1402, Test Loss: 0.8131\n",
      "Epoch 50/55, Train Loss: 1.1073, Test Loss: 0.8200\n",
      "Epoch 51/55, Train Loss: 1.1317, Test Loss: 0.8304\n",
      "Epoch 52/55, Train Loss: 1.1495, Test Loss: 0.8164\n",
      "Epoch 53/55, Train Loss: 1.1419, Test Loss: 0.8199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:48:23,935] Trial 28 finished with value: 0.8195150792598724 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 209, 'layer_1_size': 138, 'layer_2_size': 204, 'layer_3_size': 105, 'layer_4_size': 63, 'layer_5_size': 179, 'layer_6_size': 49, 'layer_7_size': 33, 'layer_8_size': 105, 'layer_9_size': 222, 'layer_10_size': 256, 'layer_11_size': 179, 'layer_12_size': 43, 'dropout_rate': 0.2155968721382211, 'learning_rate': 1.001025979002256e-05, 'batch_size': 64, 'epochs': 55}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/55, Train Loss: 1.1561, Test Loss: 0.8178\n",
      "Epoch 55/55, Train Loss: 1.1723, Test Loss: 0.8195\n",
      "Epoch 1/92, Train Loss: 1.1200, Test Loss: 0.9508\n",
      "Epoch 2/92, Train Loss: 1.1343, Test Loss: 0.9647\n",
      "Epoch 3/92, Train Loss: 1.1030, Test Loss: 0.9751\n",
      "Epoch 4/92, Train Loss: 1.0989, Test Loss: 0.9869\n",
      "Epoch 5/92, Train Loss: 1.0823, Test Loss: 0.9962\n",
      "Epoch 6/92, Train Loss: 1.1262, Test Loss: 0.9945\n",
      "Epoch 7/92, Train Loss: 1.0871, Test Loss: 0.9907\n",
      "Epoch 8/92, Train Loss: 1.1068, Test Loss: 0.9962\n",
      "Epoch 9/92, Train Loss: 1.1785, Test Loss: 0.9952\n",
      "Epoch 10/92, Train Loss: 1.0737, Test Loss: 0.9919\n",
      "Epoch 11/92, Train Loss: 1.0963, Test Loss: 0.9870\n",
      "Epoch 12/92, Train Loss: 1.0540, Test Loss: 0.9818\n",
      "Epoch 13/92, Train Loss: 1.0343, Test Loss: 0.9758\n",
      "Epoch 14/92, Train Loss: 1.1269, Test Loss: 0.9747\n",
      "Epoch 15/92, Train Loss: 1.0409, Test Loss: 0.9700\n",
      "Epoch 16/92, Train Loss: 1.0892, Test Loss: 0.9725\n",
      "Epoch 17/92, Train Loss: 1.0527, Test Loss: 0.9712\n",
      "Epoch 18/92, Train Loss: 1.0912, Test Loss: 0.9681\n",
      "Epoch 19/92, Train Loss: 1.0560, Test Loss: 0.9660\n",
      "Epoch 20/92, Train Loss: 1.0532, Test Loss: 0.9675\n",
      "Epoch 21/92, Train Loss: 1.0769, Test Loss: 0.9728\n",
      "Epoch 22/92, Train Loss: 1.0446, Test Loss: 0.9771\n",
      "Epoch 23/92, Train Loss: 1.0770, Test Loss: 0.9900\n",
      "Epoch 24/92, Train Loss: 1.0582, Test Loss: 0.9916\n",
      "Epoch 25/92, Train Loss: 1.0454, Test Loss: 0.9931\n",
      "Epoch 26/92, Train Loss: 1.0608, Test Loss: 0.9950\n",
      "Epoch 27/92, Train Loss: 1.0543, Test Loss: 0.9968\n",
      "Epoch 28/92, Train Loss: 1.0391, Test Loss: 0.9935\n",
      "Epoch 29/92, Train Loss: 1.0406, Test Loss: 0.9931\n",
      "Epoch 30/92, Train Loss: 1.0860, Test Loss: 0.9938\n",
      "Epoch 31/92, Train Loss: 1.0738, Test Loss: 0.9957\n",
      "Epoch 32/92, Train Loss: 1.0553, Test Loss: 0.9930\n",
      "Epoch 33/92, Train Loss: 1.0639, Test Loss: 0.9905\n",
      "Epoch 34/92, Train Loss: 1.0437, Test Loss: 0.9911\n",
      "Epoch 35/92, Train Loss: 1.0228, Test Loss: 0.9833\n",
      "Epoch 36/92, Train Loss: 1.0129, Test Loss: 0.9785\n",
      "Epoch 37/92, Train Loss: 1.0364, Test Loss: 0.9774\n",
      "Epoch 38/92, Train Loss: 1.0186, Test Loss: 0.9723\n",
      "Epoch 39/92, Train Loss: 1.0411, Test Loss: 0.9683\n",
      "Epoch 40/92, Train Loss: 1.0507, Test Loss: 0.9666\n",
      "Epoch 41/92, Train Loss: 1.0432, Test Loss: 0.9658\n",
      "Epoch 42/92, Train Loss: 1.0298, Test Loss: 0.9638\n",
      "Epoch 43/92, Train Loss: 1.0386, Test Loss: 0.9649\n",
      "Epoch 44/92, Train Loss: 1.0300, Test Loss: 0.9625\n",
      "Epoch 45/92, Train Loss: 1.0474, Test Loss: 0.9639\n",
      "Epoch 46/92, Train Loss: 0.9966, Test Loss: 0.9637\n",
      "Epoch 47/92, Train Loss: 1.0105, Test Loss: 0.9647\n",
      "Epoch 48/92, Train Loss: 1.0266, Test Loss: 0.9581\n",
      "Epoch 49/92, Train Loss: 1.0022, Test Loss: 0.9577\n",
      "Epoch 50/92, Train Loss: 1.0157, Test Loss: 0.9603\n",
      "Epoch 51/92, Train Loss: 1.0326, Test Loss: 0.9603\n",
      "Epoch 52/92, Train Loss: 1.0379, Test Loss: 0.9620\n",
      "Epoch 53/92, Train Loss: 1.0055, Test Loss: 0.9640\n",
      "Epoch 54/92, Train Loss: 1.0289, Test Loss: 0.9681\n",
      "Epoch 55/92, Train Loss: 1.0255, Test Loss: 0.9703\n",
      "Epoch 56/92, Train Loss: 0.9950, Test Loss: 0.9682\n",
      "Epoch 57/92, Train Loss: 1.0327, Test Loss: 0.9734\n",
      "Epoch 58/92, Train Loss: 1.0301, Test Loss: 0.9740\n",
      "Epoch 59/92, Train Loss: 1.0110, Test Loss: 0.9727\n",
      "Epoch 60/92, Train Loss: 1.0488, Test Loss: 0.9740\n",
      "Epoch 61/92, Train Loss: 1.0598, Test Loss: 0.9708\n",
      "Epoch 62/92, Train Loss: 1.0129, Test Loss: 0.9701\n",
      "Epoch 63/92, Train Loss: 1.0596, Test Loss: 0.9648\n",
      "Epoch 64/92, Train Loss: 1.0071, Test Loss: 0.9652\n",
      "Epoch 65/92, Train Loss: 0.9888, Test Loss: 0.9646\n",
      "Epoch 66/92, Train Loss: 1.0197, Test Loss: 0.9661\n",
      "Epoch 67/92, Train Loss: 1.0383, Test Loss: 0.9601\n",
      "Epoch 68/92, Train Loss: 1.0243, Test Loss: 0.9629\n",
      "Epoch 69/92, Train Loss: 0.9911, Test Loss: 0.9639\n",
      "Epoch 70/92, Train Loss: 1.0188, Test Loss: 0.9654\n",
      "Epoch 71/92, Train Loss: 1.0217, Test Loss: 0.9664\n",
      "Epoch 72/92, Train Loss: 1.0001, Test Loss: 0.9644\n",
      "Epoch 73/92, Train Loss: 1.0096, Test Loss: 0.9594\n",
      "Epoch 74/92, Train Loss: 1.0304, Test Loss: 0.9580\n",
      "Epoch 75/92, Train Loss: 1.0116, Test Loss: 0.9576\n",
      "Epoch 76/92, Train Loss: 0.9917, Test Loss: 0.9574\n",
      "Epoch 77/92, Train Loss: 1.0117, Test Loss: 0.9583\n",
      "Epoch 78/92, Train Loss: 1.0434, Test Loss: 0.9577\n",
      "Epoch 79/92, Train Loss: 1.0238, Test Loss: 0.9593\n",
      "Epoch 80/92, Train Loss: 0.9817, Test Loss: 0.9611\n",
      "Epoch 81/92, Train Loss: 0.9852, Test Loss: 0.9615\n",
      "Epoch 82/92, Train Loss: 1.0080, Test Loss: 0.9616\n",
      "Epoch 83/92, Train Loss: 0.9881, Test Loss: 0.9586\n",
      "Epoch 84/92, Train Loss: 0.9935, Test Loss: 0.9579\n",
      "Epoch 85/92, Train Loss: 0.9760, Test Loss: 0.9549\n",
      "Epoch 86/92, Train Loss: 0.9841, Test Loss: 0.9516\n",
      "Epoch 87/92, Train Loss: 0.9924, Test Loss: 0.9512\n",
      "Epoch 88/92, Train Loss: 1.0137, Test Loss: 0.9537\n",
      "Epoch 89/92, Train Loss: 0.9887, Test Loss: 0.9538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:48:30,332] Trial 29 finished with value: 0.9563828110694885 and parameters: {'num_hidden_layers': 9, 'layer_0_size': 98, 'layer_1_size': 244, 'layer_2_size': 160, 'layer_3_size': 190, 'layer_4_size': 131, 'layer_5_size': 151, 'layer_6_size': 201, 'layer_7_size': 177, 'layer_8_size': 172, 'dropout_rate': 0.3780541986365221, 'learning_rate': 8.00033487190863e-05, 'batch_size': 128, 'epochs': 92}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/92, Train Loss: 0.9907, Test Loss: 0.9538\n",
      "Epoch 91/92, Train Loss: 1.0066, Test Loss: 0.9538\n",
      "Epoch 92/92, Train Loss: 1.0342, Test Loss: 0.9564\n",
      "Epoch 1/71, Train Loss: 1.2316, Test Loss: 1.0500\n",
      "Epoch 2/71, Train Loss: 1.1856, Test Loss: 1.0502\n",
      "Epoch 3/71, Train Loss: 1.1525, Test Loss: 1.0464\n",
      "Epoch 4/71, Train Loss: 1.1206, Test Loss: 1.0466\n",
      "Epoch 5/71, Train Loss: 1.1404, Test Loss: 1.0519\n",
      "Epoch 6/71, Train Loss: 1.0933, Test Loss: 1.0538\n",
      "Epoch 7/71, Train Loss: 1.1133, Test Loss: 1.0521\n",
      "Epoch 8/71, Train Loss: 1.1229, Test Loss: 1.0509\n",
      "Epoch 9/71, Train Loss: 1.1022, Test Loss: 1.0531\n",
      "Epoch 10/71, Train Loss: 1.1514, Test Loss: 1.0469\n",
      "Epoch 11/71, Train Loss: 1.0909, Test Loss: 1.0471\n",
      "Epoch 12/71, Train Loss: 1.0989, Test Loss: 1.0454\n",
      "Epoch 13/71, Train Loss: 1.0844, Test Loss: 1.0506\n",
      "Epoch 14/71, Train Loss: 1.1427, Test Loss: 1.0489\n",
      "Epoch 15/71, Train Loss: 1.1114, Test Loss: 1.0479\n",
      "Epoch 16/71, Train Loss: 1.0955, Test Loss: 1.0434\n",
      "Epoch 17/71, Train Loss: 1.1042, Test Loss: 1.0439\n",
      "Epoch 18/71, Train Loss: 1.1157, Test Loss: 1.0451\n",
      "Epoch 19/71, Train Loss: 1.1179, Test Loss: 1.0441\n",
      "Epoch 20/71, Train Loss: 1.1327, Test Loss: 1.0420\n",
      "Epoch 21/71, Train Loss: 1.0666, Test Loss: 1.0416\n",
      "Epoch 22/71, Train Loss: 1.0908, Test Loss: 1.0411\n",
      "Epoch 23/71, Train Loss: 1.0687, Test Loss: 1.0398\n",
      "Epoch 24/71, Train Loss: 1.0993, Test Loss: 1.0412\n",
      "Epoch 25/71, Train Loss: 1.1316, Test Loss: 1.0456\n",
      "Epoch 26/71, Train Loss: 1.0677, Test Loss: 1.0408\n",
      "Epoch 27/71, Train Loss: 1.1013, Test Loss: 1.0411\n",
      "Epoch 28/71, Train Loss: 1.0847, Test Loss: 1.0408\n",
      "Epoch 29/71, Train Loss: 1.0815, Test Loss: 1.0431\n",
      "Epoch 30/71, Train Loss: 1.0992, Test Loss: 1.0448\n",
      "Epoch 31/71, Train Loss: 1.0729, Test Loss: 1.0524\n",
      "Epoch 32/71, Train Loss: 1.0601, Test Loss: 1.0557\n",
      "Epoch 33/71, Train Loss: 1.0699, Test Loss: 1.0581\n",
      "Epoch 34/71, Train Loss: 1.0817, Test Loss: 1.0612\n",
      "Epoch 35/71, Train Loss: 1.0895, Test Loss: 1.0604\n",
      "Epoch 36/71, Train Loss: 1.0531, Test Loss: 1.0579\n",
      "Epoch 37/71, Train Loss: 1.0957, Test Loss: 1.0596\n",
      "Epoch 38/71, Train Loss: 1.0756, Test Loss: 1.0523\n",
      "Epoch 39/71, Train Loss: 1.0987, Test Loss: 1.0512\n",
      "Epoch 40/71, Train Loss: 1.0732, Test Loss: 1.0478\n",
      "Epoch 41/71, Train Loss: 1.0798, Test Loss: 1.0453\n",
      "Epoch 42/71, Train Loss: 1.0469, Test Loss: 1.0450\n",
      "Epoch 43/71, Train Loss: 1.0714, Test Loss: 1.0475\n",
      "Epoch 44/71, Train Loss: 1.0773, Test Loss: 1.0461\n",
      "Epoch 45/71, Train Loss: 1.0809, Test Loss: 1.0477\n",
      "Epoch 46/71, Train Loss: 1.0706, Test Loss: 1.0536\n",
      "Epoch 47/71, Train Loss: 1.0661, Test Loss: 1.0604\n",
      "Epoch 48/71, Train Loss: 1.0788, Test Loss: 1.0624\n",
      "Epoch 49/71, Train Loss: 1.0419, Test Loss: 1.0623\n",
      "Epoch 50/71, Train Loss: 1.0635, Test Loss: 1.0587\n",
      "Epoch 51/71, Train Loss: 1.0594, Test Loss: 1.0566\n",
      "Epoch 52/71, Train Loss: 1.0665, Test Loss: 1.0497\n",
      "Epoch 53/71, Train Loss: 1.0701, Test Loss: 1.0477\n",
      "Epoch 54/71, Train Loss: 1.0758, Test Loss: 1.0484\n",
      "Epoch 55/71, Train Loss: 1.0737, Test Loss: 1.0495\n",
      "Epoch 56/71, Train Loss: 1.0631, Test Loss: 1.0522\n",
      "Epoch 57/71, Train Loss: 1.0736, Test Loss: 1.0551\n",
      "Epoch 58/71, Train Loss: 1.0584, Test Loss: 1.0528\n",
      "Epoch 59/71, Train Loss: 1.0601, Test Loss: 1.0483\n",
      "Epoch 60/71, Train Loss: 1.0352, Test Loss: 1.0467\n",
      "Epoch 61/71, Train Loss: 1.0741, Test Loss: 1.0461\n",
      "Epoch 62/71, Train Loss: 1.0687, Test Loss: 1.0489\n",
      "Epoch 63/71, Train Loss: 1.0639, Test Loss: 1.0484\n",
      "Epoch 64/71, Train Loss: 1.0794, Test Loss: 1.0494\n",
      "Epoch 65/71, Train Loss: 1.0573, Test Loss: 1.0490\n",
      "Epoch 66/71, Train Loss: 1.0525, Test Loss: 1.0503\n",
      "Epoch 67/71, Train Loss: 1.0584, Test Loss: 1.0505\n",
      "Epoch 68/71, Train Loss: 1.0641, Test Loss: 1.0518\n",
      "Epoch 69/71, Train Loss: 1.0498, Test Loss: 1.0523\n",
      "Epoch 70/71, Train Loss: 1.0508, Test Loss: 1.0525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:48:37,149] Trial 30 finished with value: 1.0498037934303284 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 232, 'layer_1_size': 110, 'layer_2_size': 62, 'layer_3_size': 214, 'layer_4_size': 234, 'layer_5_size': 236, 'layer_6_size': 149, 'layer_7_size': 149, 'layer_8_size': 52, 'layer_9_size': 106, 'layer_10_size': 121, 'layer_11_size': 230, 'layer_12_size': 180, 'dropout_rate': 0.27347279242162326, 'learning_rate': 0.00024346263692384747, 'batch_size': 128, 'epochs': 71}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/71, Train Loss: 1.0358, Test Loss: 1.0498\n",
      "Epoch 1/55, Train Loss: 1.2410, Test Loss: 1.1393\n",
      "Epoch 2/55, Train Loss: 1.2057, Test Loss: 1.1496\n",
      "Epoch 3/55, Train Loss: 1.1922, Test Loss: 1.1238\n",
      "Epoch 4/55, Train Loss: 1.2424, Test Loss: 1.1143\n",
      "Epoch 5/55, Train Loss: 1.1579, Test Loss: 1.1113\n",
      "Epoch 6/55, Train Loss: 1.1632, Test Loss: 1.1219\n",
      "Epoch 7/55, Train Loss: 1.2266, Test Loss: 1.1195\n",
      "Epoch 8/55, Train Loss: 1.1656, Test Loss: 1.1120\n",
      "Epoch 9/55, Train Loss: 1.1830, Test Loss: 1.1084\n",
      "Epoch 10/55, Train Loss: 1.1194, Test Loss: 1.1094\n",
      "Epoch 11/55, Train Loss: 1.1845, Test Loss: 1.1165\n",
      "Epoch 12/55, Train Loss: 1.1691, Test Loss: 1.0912\n",
      "Epoch 13/55, Train Loss: 1.1670, Test Loss: 1.0962\n",
      "Epoch 14/55, Train Loss: 1.1918, Test Loss: 1.0988\n",
      "Epoch 15/55, Train Loss: 1.1664, Test Loss: 1.0964\n",
      "Epoch 16/55, Train Loss: 1.1460, Test Loss: 1.1059\n",
      "Epoch 17/55, Train Loss: 1.1819, Test Loss: 1.1003\n",
      "Epoch 18/55, Train Loss: 1.1564, Test Loss: 1.0914\n",
      "Epoch 19/55, Train Loss: 1.1818, Test Loss: 1.1008\n",
      "Epoch 20/55, Train Loss: 1.1687, Test Loss: 1.0983\n",
      "Epoch 21/55, Train Loss: 1.1627, Test Loss: 1.0845\n",
      "Epoch 22/55, Train Loss: 1.1738, Test Loss: 1.0880\n",
      "Epoch 23/55, Train Loss: 1.1464, Test Loss: 1.0883\n",
      "Epoch 24/55, Train Loss: 1.1409, Test Loss: 1.0897\n",
      "Epoch 25/55, Train Loss: 1.1115, Test Loss: 1.0992\n",
      "Epoch 26/55, Train Loss: 1.1722, Test Loss: 1.0904\n",
      "Epoch 27/55, Train Loss: 1.1519, Test Loss: 1.0845\n",
      "Epoch 28/55, Train Loss: 1.1611, Test Loss: 1.1048\n",
      "Epoch 29/55, Train Loss: 1.1516, Test Loss: 1.0904\n",
      "Epoch 30/55, Train Loss: 1.1281, Test Loss: 1.0943\n",
      "Epoch 31/55, Train Loss: 1.1364, Test Loss: 1.0880\n",
      "Epoch 32/55, Train Loss: 1.1350, Test Loss: 1.0859\n",
      "Epoch 33/55, Train Loss: 1.1459, Test Loss: 1.0906\n",
      "Epoch 34/55, Train Loss: 1.0994, Test Loss: 1.0866\n",
      "Epoch 35/55, Train Loss: 1.1654, Test Loss: 1.0970\n",
      "Epoch 36/55, Train Loss: 1.1335, Test Loss: 1.0888\n",
      "Epoch 37/55, Train Loss: 1.0958, Test Loss: 1.0926\n",
      "Epoch 38/55, Train Loss: 1.1442, Test Loss: 1.0899\n",
      "Epoch 39/55, Train Loss: 1.1214, Test Loss: 1.0887\n",
      "Epoch 40/55, Train Loss: 1.1729, Test Loss: 1.0862\n",
      "Epoch 41/55, Train Loss: 1.1486, Test Loss: 1.0937\n",
      "Epoch 42/55, Train Loss: 1.1871, Test Loss: 1.1040\n",
      "Epoch 43/55, Train Loss: 1.1580, Test Loss: 1.0978\n",
      "Epoch 44/55, Train Loss: 1.1765, Test Loss: 1.0985\n",
      "Epoch 45/55, Train Loss: 1.1889, Test Loss: 1.0940\n",
      "Epoch 46/55, Train Loss: 1.1211, Test Loss: 1.0957\n",
      "Epoch 47/55, Train Loss: 1.0759, Test Loss: 1.0910\n",
      "Epoch 48/55, Train Loss: 1.1276, Test Loss: 1.0913\n",
      "Epoch 49/55, Train Loss: 1.1434, Test Loss: 1.0991\n",
      "Epoch 50/55, Train Loss: 1.1494, Test Loss: 1.0909\n",
      "Epoch 51/55, Train Loss: 1.1137, Test Loss: 1.1000\n",
      "Epoch 52/55, Train Loss: 1.1252, Test Loss: 1.0905\n",
      "Epoch 53/55, Train Loss: 1.1371, Test Loss: 1.0982\n",
      "Epoch 54/55, Train Loss: 1.1697, Test Loss: 1.0943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:48:46,189] Trial 31 finished with value: 1.0945087000727654 and parameters: {'num_hidden_layers': 15, 'layer_0_size': 202, 'layer_1_size': 137, 'layer_2_size': 210, 'layer_3_size': 106, 'layer_4_size': 66, 'layer_5_size': 183, 'layer_6_size': 48, 'layer_7_size': 38, 'layer_8_size': 102, 'layer_9_size': 237, 'layer_10_size': 246, 'layer_11_size': 178, 'layer_12_size': 32, 'layer_13_size': 149, 'layer_14_size': 200, 'dropout_rate': 0.22615415734392377, 'learning_rate': 1.0447486389310892e-05, 'batch_size': 64, 'epochs': 55}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/55, Train Loss: 1.1165, Test Loss: 1.0945\n",
      "Epoch 1/81, Train Loss: 1.1702, Test Loss: 1.0388\n",
      "Epoch 2/81, Train Loss: 1.1920, Test Loss: 1.0807\n",
      "Epoch 3/81, Train Loss: 1.2293, Test Loss: 1.0861\n",
      "Epoch 4/81, Train Loss: 1.2241, Test Loss: 1.0809\n",
      "Epoch 5/81, Train Loss: 1.1715, Test Loss: 1.0964\n",
      "Epoch 6/81, Train Loss: 1.2450, Test Loss: 1.0898\n",
      "Epoch 7/81, Train Loss: 1.2407, Test Loss: 1.1014\n",
      "Epoch 8/81, Train Loss: 1.2076, Test Loss: 1.1312\n",
      "Epoch 9/81, Train Loss: 1.2501, Test Loss: 1.1159\n",
      "Epoch 10/81, Train Loss: 1.1848, Test Loss: 1.1183\n",
      "Epoch 11/81, Train Loss: 1.1877, Test Loss: 1.1143\n",
      "Epoch 12/81, Train Loss: 1.1976, Test Loss: 1.1119\n",
      "Epoch 13/81, Train Loss: 1.1880, Test Loss: 1.1165\n",
      "Epoch 14/81, Train Loss: 1.1392, Test Loss: 1.1208\n",
      "Epoch 15/81, Train Loss: 1.1824, Test Loss: 1.1143\n",
      "Epoch 16/81, Train Loss: 1.1615, Test Loss: 1.1199\n",
      "Epoch 17/81, Train Loss: 1.1833, Test Loss: 1.1026\n",
      "Epoch 18/81, Train Loss: 1.2157, Test Loss: 1.1042\n",
      "Epoch 19/81, Train Loss: 1.2041, Test Loss: 1.0952\n",
      "Epoch 20/81, Train Loss: 1.1700, Test Loss: 1.0943\n",
      "Epoch 21/81, Train Loss: 1.1721, Test Loss: 1.0939\n",
      "Epoch 22/81, Train Loss: 1.1598, Test Loss: 1.0958\n",
      "Epoch 23/81, Train Loss: 1.2000, Test Loss: 1.0820\n",
      "Epoch 24/81, Train Loss: 1.1828, Test Loss: 1.0987\n",
      "Epoch 25/81, Train Loss: 1.1572, Test Loss: 1.0833\n",
      "Epoch 26/81, Train Loss: 1.1704, Test Loss: 1.0812\n",
      "Epoch 27/81, Train Loss: 1.1706, Test Loss: 1.0860\n",
      "Epoch 28/81, Train Loss: 1.2053, Test Loss: 1.0775\n",
      "Epoch 29/81, Train Loss: 1.2137, Test Loss: 1.0963\n",
      "Epoch 30/81, Train Loss: 1.1846, Test Loss: 1.0799\n",
      "Epoch 31/81, Train Loss: 1.1815, Test Loss: 1.1009\n",
      "Epoch 32/81, Train Loss: 1.1561, Test Loss: 1.0835\n",
      "Epoch 33/81, Train Loss: 1.1724, Test Loss: 1.0925\n",
      "Epoch 34/81, Train Loss: 1.1552, Test Loss: 1.0911\n",
      "Epoch 35/81, Train Loss: 1.1958, Test Loss: 1.0865\n",
      "Epoch 36/81, Train Loss: 1.1389, Test Loss: 1.0926\n",
      "Epoch 37/81, Train Loss: 1.1479, Test Loss: 1.1025\n",
      "Epoch 38/81, Train Loss: 1.1750, Test Loss: 1.1033\n",
      "Epoch 39/81, Train Loss: 1.1560, Test Loss: 1.0923\n",
      "Epoch 40/81, Train Loss: 1.1748, Test Loss: 1.0768\n",
      "Epoch 41/81, Train Loss: 1.1666, Test Loss: 1.0863\n",
      "Epoch 42/81, Train Loss: 1.2004, Test Loss: 1.0899\n",
      "Epoch 43/81, Train Loss: 1.1631, Test Loss: 1.0835\n",
      "Epoch 44/81, Train Loss: 1.1673, Test Loss: 1.0857\n",
      "Epoch 45/81, Train Loss: 1.1791, Test Loss: 1.0752\n",
      "Epoch 46/81, Train Loss: 1.1840, Test Loss: 1.0896\n",
      "Epoch 47/81, Train Loss: 1.1654, Test Loss: 1.0936\n",
      "Epoch 48/81, Train Loss: 1.1502, Test Loss: 1.0958\n",
      "Epoch 49/81, Train Loss: 1.1981, Test Loss: 1.0845\n",
      "Epoch 50/81, Train Loss: 1.1773, Test Loss: 1.1031\n",
      "Epoch 51/81, Train Loss: 1.1887, Test Loss: 1.0897\n",
      "Epoch 52/81, Train Loss: 1.1543, Test Loss: 1.0873\n",
      "Epoch 53/81, Train Loss: 1.1933, Test Loss: 1.0983\n",
      "Epoch 54/81, Train Loss: 1.2017, Test Loss: 1.0850\n",
      "Epoch 55/81, Train Loss: 1.1646, Test Loss: 1.0836\n",
      "Epoch 56/81, Train Loss: 1.1124, Test Loss: 1.0863\n",
      "Epoch 57/81, Train Loss: 1.1127, Test Loss: 1.0858\n",
      "Epoch 58/81, Train Loss: 1.1321, Test Loss: 1.0835\n",
      "Epoch 59/81, Train Loss: 1.1466, Test Loss: 1.0902\n",
      "Epoch 60/81, Train Loss: 1.1415, Test Loss: 1.0852\n",
      "Epoch 61/81, Train Loss: 1.1199, Test Loss: 1.0797\n",
      "Epoch 62/81, Train Loss: 1.1596, Test Loss: 1.0865\n",
      "Epoch 63/81, Train Loss: 1.1733, Test Loss: 1.0829\n",
      "Epoch 64/81, Train Loss: 1.1446, Test Loss: 1.0935\n",
      "Epoch 65/81, Train Loss: 1.1694, Test Loss: 1.0812\n",
      "Epoch 66/81, Train Loss: 1.1315, Test Loss: 1.0830\n",
      "Epoch 67/81, Train Loss: 1.1887, Test Loss: 1.0873\n",
      "Epoch 68/81, Train Loss: 1.1273, Test Loss: 1.0879\n",
      "Epoch 69/81, Train Loss: 1.1677, Test Loss: 1.0943\n",
      "Epoch 70/81, Train Loss: 1.1807, Test Loss: 1.0972\n",
      "Epoch 71/81, Train Loss: 1.1373, Test Loss: 1.1077\n",
      "Epoch 72/81, Train Loss: 1.1504, Test Loss: 1.1053\n",
      "Epoch 73/81, Train Loss: 1.1502, Test Loss: 1.0931\n",
      "Epoch 74/81, Train Loss: 1.1268, Test Loss: 1.0906\n",
      "Epoch 75/81, Train Loss: 1.1175, Test Loss: 1.0844\n",
      "Epoch 76/81, Train Loss: 1.1668, Test Loss: 1.0898\n",
      "Epoch 77/81, Train Loss: 1.1427, Test Loss: 1.0844\n",
      "Epoch 78/81, Train Loss: 1.1284, Test Loss: 1.0854\n",
      "Epoch 79/81, Train Loss: 1.1350, Test Loss: 1.0808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:48:58,001] Trial 32 finished with value: 1.0932123214006424 and parameters: {'num_hidden_layers': 12, 'layer_0_size': 197, 'layer_1_size': 159, 'layer_2_size': 238, 'layer_3_size': 134, 'layer_4_size': 74, 'layer_5_size': 224, 'layer_6_size': 67, 'layer_7_size': 68, 'layer_8_size': 109, 'layer_9_size': 224, 'layer_10_size': 231, 'layer_11_size': 154, 'dropout_rate': 0.2289473220464075, 'learning_rate': 2.012539742058359e-05, 'batch_size': 64, 'epochs': 81}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/81, Train Loss: 1.1577, Test Loss: 1.0846\n",
      "Epoch 81/81, Train Loss: 1.1101, Test Loss: 1.0932\n",
      "Epoch 1/43, Train Loss: 1.0964, Test Loss: 0.9241\n",
      "Epoch 2/43, Train Loss: 1.0320, Test Loss: 0.9389\n",
      "Epoch 3/43, Train Loss: 1.0570, Test Loss: 0.9250\n",
      "Epoch 4/43, Train Loss: 1.0430, Test Loss: 0.9450\n",
      "Epoch 5/43, Train Loss: 1.0248, Test Loss: 0.9457\n",
      "Epoch 6/43, Train Loss: 1.0212, Test Loss: 0.9493\n",
      "Epoch 7/43, Train Loss: 1.0595, Test Loss: 0.9394\n",
      "Epoch 8/43, Train Loss: 1.0182, Test Loss: 0.9470\n",
      "Epoch 9/43, Train Loss: 1.0431, Test Loss: 0.9456\n",
      "Epoch 10/43, Train Loss: 1.0059, Test Loss: 0.9344\n",
      "Epoch 11/43, Train Loss: 1.0161, Test Loss: 0.9389\n",
      "Epoch 12/43, Train Loss: 1.0071, Test Loss: 0.9409\n",
      "Epoch 13/43, Train Loss: 1.0111, Test Loss: 0.9314\n",
      "Epoch 14/43, Train Loss: 1.0258, Test Loss: 0.9351\n",
      "Epoch 15/43, Train Loss: 0.9712, Test Loss: 0.9421\n",
      "Epoch 16/43, Train Loss: 1.0033, Test Loss: 0.9428\n",
      "Epoch 17/43, Train Loss: 1.0308, Test Loss: 0.9424\n",
      "Epoch 18/43, Train Loss: 0.9860, Test Loss: 0.9445\n",
      "Epoch 19/43, Train Loss: 0.9844, Test Loss: 0.9347\n",
      "Epoch 20/43, Train Loss: 1.0404, Test Loss: 0.9396\n",
      "Epoch 21/43, Train Loss: 0.9905, Test Loss: 0.9367\n",
      "Epoch 22/43, Train Loss: 0.9872, Test Loss: 0.9437\n",
      "Epoch 23/43, Train Loss: 1.0007, Test Loss: 0.9374\n",
      "Epoch 24/43, Train Loss: 0.9877, Test Loss: 0.9305\n",
      "Epoch 25/43, Train Loss: 1.0186, Test Loss: 0.9383\n",
      "Epoch 26/43, Train Loss: 0.9910, Test Loss: 0.9364\n",
      "Epoch 27/43, Train Loss: 1.0008, Test Loss: 0.9238\n",
      "Epoch 28/43, Train Loss: 0.9893, Test Loss: 0.9226\n",
      "Epoch 29/43, Train Loss: 0.9581, Test Loss: 0.9226\n",
      "Epoch 30/43, Train Loss: 0.9635, Test Loss: 0.9223\n",
      "Epoch 31/43, Train Loss: 0.9719, Test Loss: 0.9250\n",
      "Epoch 32/43, Train Loss: 0.9793, Test Loss: 0.9247\n",
      "Epoch 33/43, Train Loss: 0.9721, Test Loss: 0.9245\n",
      "Epoch 34/43, Train Loss: 1.0163, Test Loss: 0.9220\n",
      "Epoch 35/43, Train Loss: 0.9582, Test Loss: 0.9126\n",
      "Epoch 36/43, Train Loss: 0.9698, Test Loss: 0.9200\n",
      "Epoch 37/43, Train Loss: 0.9825, Test Loss: 0.9274\n",
      "Epoch 38/43, Train Loss: 0.9810, Test Loss: 0.9181\n",
      "Epoch 39/43, Train Loss: 0.9553, Test Loss: 0.9241\n",
      "Epoch 40/43, Train Loss: 1.0045, Test Loss: 0.9240\n",
      "Epoch 41/43, Train Loss: 0.9926, Test Loss: 0.9305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:49:02,794] Trial 33 finished with value: 0.9293600022792816 and parameters: {'num_hidden_layers': 10, 'layer_0_size': 229, 'layer_1_size': 117, 'layer_2_size': 136, 'layer_3_size': 161, 'layer_4_size': 52, 'layer_5_size': 184, 'layer_6_size': 64, 'layer_7_size': 112, 'layer_8_size': 132, 'layer_9_size': 220, 'dropout_rate': 0.18437654102671644, 'learning_rate': 3.713850577686851e-05, 'batch_size': 64, 'epochs': 43}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/43, Train Loss: 1.0003, Test Loss: 0.9287\n",
      "Epoch 43/43, Train Loss: 0.9849, Test Loss: 0.9294\n",
      "Epoch 1/66, Train Loss: 1.1767, Test Loss: 1.1705\n",
      "Epoch 2/66, Train Loss: 1.1189, Test Loss: 1.1778\n",
      "Epoch 3/66, Train Loss: 1.1050, Test Loss: 1.1770\n",
      "Epoch 4/66, Train Loss: 1.0634, Test Loss: 1.1567\n",
      "Epoch 5/66, Train Loss: 1.0547, Test Loss: 1.1506\n",
      "Epoch 6/66, Train Loss: 1.0791, Test Loss: 1.1656\n",
      "Epoch 7/66, Train Loss: 1.0413, Test Loss: 1.1710\n",
      "Epoch 8/66, Train Loss: 1.0162, Test Loss: 1.1907\n",
      "Epoch 9/66, Train Loss: 1.0599, Test Loss: 1.2128\n",
      "Epoch 10/66, Train Loss: 1.0224, Test Loss: 1.1816\n",
      "Epoch 11/66, Train Loss: 1.0574, Test Loss: 1.1786\n",
      "Epoch 12/66, Train Loss: 1.0577, Test Loss: 1.1591\n",
      "Epoch 13/66, Train Loss: 1.0339, Test Loss: 1.1456\n",
      "Epoch 14/66, Train Loss: 1.0368, Test Loss: 1.1487\n",
      "Epoch 15/66, Train Loss: 1.0070, Test Loss: 1.1457\n",
      "Epoch 16/66, Train Loss: 1.0198, Test Loss: 1.1499\n",
      "Epoch 17/66, Train Loss: 1.0219, Test Loss: 1.1478\n",
      "Epoch 18/66, Train Loss: 1.0310, Test Loss: 1.1519\n",
      "Epoch 19/66, Train Loss: 1.0380, Test Loss: 1.1343\n",
      "Epoch 20/66, Train Loss: 1.0022, Test Loss: 1.1345\n",
      "Epoch 21/66, Train Loss: 1.0207, Test Loss: 1.1504\n",
      "Epoch 22/66, Train Loss: 1.0107, Test Loss: 1.1592\n",
      "Epoch 23/66, Train Loss: 1.0205, Test Loss: 1.1606\n",
      "Epoch 24/66, Train Loss: 1.0234, Test Loss: 1.1433\n",
      "Epoch 25/66, Train Loss: 1.0212, Test Loss: 1.1415\n",
      "Epoch 26/66, Train Loss: 1.0207, Test Loss: 1.1374\n",
      "Epoch 27/66, Train Loss: 1.0086, Test Loss: 1.1310\n",
      "Epoch 28/66, Train Loss: 1.0039, Test Loss: 1.1305\n",
      "Epoch 29/66, Train Loss: 1.0175, Test Loss: 1.1363\n",
      "Epoch 30/66, Train Loss: 1.0055, Test Loss: 1.1356\n",
      "Epoch 31/66, Train Loss: 1.0161, Test Loss: 1.1404\n",
      "Epoch 32/66, Train Loss: 1.0071, Test Loss: 1.1420\n",
      "Epoch 33/66, Train Loss: 1.0085, Test Loss: 1.1464\n",
      "Epoch 34/66, Train Loss: 1.0020, Test Loss: 1.1449\n",
      "Epoch 35/66, Train Loss: 1.0192, Test Loss: 1.1459\n",
      "Epoch 36/66, Train Loss: 1.0142, Test Loss: 1.1337\n",
      "Epoch 37/66, Train Loss: 0.9950, Test Loss: 1.1467\n",
      "Epoch 38/66, Train Loss: 0.9951, Test Loss: 1.1678\n",
      "Epoch 39/66, Train Loss: 1.0162, Test Loss: 1.1577\n",
      "Epoch 40/66, Train Loss: 1.0071, Test Loss: 1.1443\n",
      "Epoch 41/66, Train Loss: 0.9944, Test Loss: 1.1508\n",
      "Epoch 42/66, Train Loss: 1.0157, Test Loss: 1.1453\n",
      "Epoch 43/66, Train Loss: 1.0066, Test Loss: 1.1408\n",
      "Epoch 44/66, Train Loss: 1.0047, Test Loss: 1.1454\n",
      "Epoch 45/66, Train Loss: 1.0039, Test Loss: 1.1529\n",
      "Epoch 46/66, Train Loss: 1.0122, Test Loss: 1.1495\n",
      "Epoch 47/66, Train Loss: 0.9971, Test Loss: 1.1512\n",
      "Epoch 48/66, Train Loss: 1.0008, Test Loss: 1.1619\n",
      "Epoch 49/66, Train Loss: 1.0117, Test Loss: 1.1628\n",
      "Epoch 50/66, Train Loss: 1.0015, Test Loss: 1.1536\n",
      "Epoch 51/66, Train Loss: 1.0006, Test Loss: 1.1627\n",
      "Epoch 52/66, Train Loss: 0.9980, Test Loss: 1.1698\n",
      "Epoch 53/66, Train Loss: 0.9956, Test Loss: 1.1784\n",
      "Epoch 54/66, Train Loss: 1.0081, Test Loss: 1.1632\n",
      "Epoch 55/66, Train Loss: 1.0005, Test Loss: 1.1540\n",
      "Epoch 56/66, Train Loss: 1.0043, Test Loss: 1.1591\n",
      "Epoch 57/66, Train Loss: 1.0035, Test Loss: 1.1597\n",
      "Epoch 58/66, Train Loss: 1.0004, Test Loss: 1.1522\n",
      "Epoch 59/66, Train Loss: 1.0076, Test Loss: 1.1563\n",
      "Epoch 60/66, Train Loss: 0.9988, Test Loss: 1.1577\n",
      "Epoch 61/66, Train Loss: 0.9789, Test Loss: 1.1649\n",
      "Epoch 62/66, Train Loss: 0.9934, Test Loss: 1.1681\n",
      "Epoch 63/66, Train Loss: 1.0053, Test Loss: 1.1570\n",
      "Epoch 64/66, Train Loss: 0.9809, Test Loss: 1.1572\n",
      "Epoch 65/66, Train Loss: 1.0069, Test Loss: 1.1634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:49:14,287] Trial 34 finished with value: 1.1671329885721207 and parameters: {'num_hidden_layers': 15, 'layer_0_size': 178, 'layer_1_size': 178, 'layer_2_size': 200, 'layer_3_size': 118, 'layer_4_size': 98, 'layer_5_size': 112, 'layer_6_size': 49, 'layer_7_size': 217, 'layer_8_size': 78, 'layer_9_size': 199, 'layer_10_size': 211, 'layer_11_size': 188, 'layer_12_size': 136, 'layer_13_size': 117, 'layer_14_size': 108, 'dropout_rate': 0.32121369087257445, 'learning_rate': 0.0009511005245872829, 'batch_size': 64, 'epochs': 66}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/66, Train Loss: 0.9943, Test Loss: 1.1671\n",
      "Epoch 1/15, Train Loss: 1.2307, Test Loss: 0.9851\n",
      "Epoch 2/15, Train Loss: 1.2146, Test Loss: 0.9934\n",
      "Epoch 3/15, Train Loss: 1.1897, Test Loss: 1.0044\n",
      "Epoch 4/15, Train Loss: 1.2445, Test Loss: 1.0153\n",
      "Epoch 5/15, Train Loss: 1.2043, Test Loss: 1.0282\n",
      "Epoch 6/15, Train Loss: 1.1591, Test Loss: 1.0379\n",
      "Epoch 7/15, Train Loss: 1.2147, Test Loss: 1.0419\n",
      "Epoch 8/15, Train Loss: 1.1987, Test Loss: 1.0410\n",
      "Epoch 9/15, Train Loss: 1.1709, Test Loss: 1.0458\n",
      "Epoch 10/15, Train Loss: 1.1750, Test Loss: 1.0510\n",
      "Epoch 11/15, Train Loss: 1.1565, Test Loss: 1.0620\n",
      "Epoch 12/15, Train Loss: 1.1757, Test Loss: 1.0587\n",
      "Epoch 13/15, Train Loss: 1.1943, Test Loss: 1.0557\n",
      "Epoch 14/15, Train Loss: 1.2035, Test Loss: 1.0538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:49:15,167] Trial 35 finished with value: 1.0533274412155151 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 145, 'layer_1_size': 149, 'layer_2_size': 122, 'layer_3_size': 65, 'layer_4_size': 117, 'layer_5_size': 59, 'layer_6_size': 93, 'layer_7_size': 42, 'layer_8_size': 199, 'layer_9_size': 177, 'layer_10_size': 160, 'layer_11_size': 149, 'layer_12_size': 42, 'dropout_rate': 0.12239489565057327, 'learning_rate': 1.551405710971993e-05, 'batch_size': 256, 'epochs': 15}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15, Train Loss: 1.2167, Test Loss: 1.0533\n",
      "Epoch 1/56, Train Loss: 1.2247, Test Loss: 1.0290\n",
      "Epoch 2/56, Train Loss: 1.1495, Test Loss: 0.9527\n",
      "Epoch 3/56, Train Loss: 1.0848, Test Loss: 0.9364\n",
      "Epoch 4/56, Train Loss: 1.0191, Test Loss: 0.9245\n",
      "Epoch 5/56, Train Loss: 1.0250, Test Loss: 0.8915\n",
      "Epoch 6/56, Train Loss: 1.1032, Test Loss: 0.8997\n",
      "Epoch 7/56, Train Loss: 1.0356, Test Loss: 0.8893\n",
      "Epoch 8/56, Train Loss: 1.0531, Test Loss: 0.9047\n",
      "Epoch 9/56, Train Loss: 1.0302, Test Loss: 0.9027\n",
      "Epoch 10/56, Train Loss: 1.0420, Test Loss: 0.8939\n",
      "Epoch 11/56, Train Loss: 1.0060, Test Loss: 0.8989\n",
      "Epoch 12/56, Train Loss: 1.0354, Test Loss: 0.9003\n",
      "Epoch 13/56, Train Loss: 0.9862, Test Loss: 0.8966\n",
      "Epoch 14/56, Train Loss: 0.9906, Test Loss: 0.8922\n",
      "Epoch 15/56, Train Loss: 1.0159, Test Loss: 0.8955\n",
      "Epoch 16/56, Train Loss: 0.9942, Test Loss: 0.8868\n",
      "Epoch 17/56, Train Loss: 0.9983, Test Loss: 0.8919\n",
      "Epoch 18/56, Train Loss: 1.0148, Test Loss: 0.8897\n",
      "Epoch 19/56, Train Loss: 0.9994, Test Loss: 0.8799\n",
      "Epoch 20/56, Train Loss: 0.9916, Test Loss: 0.8812\n",
      "Epoch 21/56, Train Loss: 1.0026, Test Loss: 0.8768\n",
      "Epoch 22/56, Train Loss: 1.0045, Test Loss: 0.8847\n",
      "Epoch 23/56, Train Loss: 0.9840, Test Loss: 0.8858\n",
      "Epoch 24/56, Train Loss: 0.9658, Test Loss: 0.8809\n",
      "Epoch 25/56, Train Loss: 0.9759, Test Loss: 0.8837\n",
      "Epoch 26/56, Train Loss: 0.9942, Test Loss: 0.8709\n",
      "Epoch 27/56, Train Loss: 0.9546, Test Loss: 0.8773\n",
      "Epoch 28/56, Train Loss: 0.9862, Test Loss: 0.8774\n",
      "Epoch 29/56, Train Loss: 0.9679, Test Loss: 0.8805\n",
      "Epoch 30/56, Train Loss: 0.9715, Test Loss: 0.8774\n",
      "Epoch 31/56, Train Loss: 0.9820, Test Loss: 0.8810\n",
      "Epoch 32/56, Train Loss: 0.9678, Test Loss: 0.8809\n",
      "Epoch 33/56, Train Loss: 0.9496, Test Loss: 0.8796\n",
      "Epoch 34/56, Train Loss: 1.0023, Test Loss: 0.8841\n",
      "Epoch 35/56, Train Loss: 1.0093, Test Loss: 0.8757\n",
      "Epoch 36/56, Train Loss: 0.9777, Test Loss: 0.8660\n",
      "Epoch 37/56, Train Loss: 0.9198, Test Loss: 0.8728\n",
      "Epoch 38/56, Train Loss: 0.9990, Test Loss: 0.8717\n",
      "Epoch 39/56, Train Loss: 0.9761, Test Loss: 0.8679\n",
      "Epoch 40/56, Train Loss: 0.9773, Test Loss: 0.8748\n",
      "Epoch 41/56, Train Loss: 0.9383, Test Loss: 0.8689\n",
      "Epoch 42/56, Train Loss: 0.9890, Test Loss: 0.8713\n",
      "Epoch 43/56, Train Loss: 0.9519, Test Loss: 0.8615\n",
      "Epoch 44/56, Train Loss: 0.9386, Test Loss: 0.8783\n",
      "Epoch 45/56, Train Loss: 0.9538, Test Loss: 0.8718\n",
      "Epoch 46/56, Train Loss: 0.9509, Test Loss: 0.8761\n",
      "Epoch 47/56, Train Loss: 0.9165, Test Loss: 0.8889\n",
      "Epoch 48/56, Train Loss: 0.9403, Test Loss: 0.8814\n",
      "Epoch 49/56, Train Loss: 0.9188, Test Loss: 0.8720\n",
      "Epoch 50/56, Train Loss: 0.9283, Test Loss: 0.8798\n",
      "Epoch 51/56, Train Loss: 0.9506, Test Loss: 0.8675\n",
      "Epoch 52/56, Train Loss: 0.9288, Test Loss: 0.8623\n",
      "Epoch 53/56, Train Loss: 0.9357, Test Loss: 0.8691\n",
      "Epoch 54/56, Train Loss: 0.9555, Test Loss: 0.8694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:49:22,018] Trial 36 finished with value: 0.8749158020530429 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 67, 'layer_1_size': 90, 'layer_2_size': 172, 'layer_3_size': 87, 'layer_4_size': 181, 'layer_5_size': 92, 'layer_6_size': 123, 'dropout_rate': 0.15729643313467545, 'learning_rate': 0.00012443947897220536, 'batch_size': 32, 'epochs': 56}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/56, Train Loss: 0.9326, Test Loss: 0.8626\n",
      "Epoch 56/56, Train Loss: 0.9124, Test Loss: 0.8749\n",
      "Epoch 1/32, Train Loss: 1.1886, Test Loss: 1.0632\n",
      "Epoch 2/32, Train Loss: 1.1132, Test Loss: 1.0706\n",
      "Epoch 3/32, Train Loss: 1.1063, Test Loss: 1.0647\n",
      "Epoch 4/32, Train Loss: 1.1393, Test Loss: 1.0597\n",
      "Epoch 5/32, Train Loss: 1.1357, Test Loss: 1.0547\n",
      "Epoch 6/32, Train Loss: 1.0885, Test Loss: 1.0612\n",
      "Epoch 7/32, Train Loss: 1.1426, Test Loss: 1.0575\n",
      "Epoch 8/32, Train Loss: 1.1459, Test Loss: 1.0556\n",
      "Epoch 9/32, Train Loss: 1.0830, Test Loss: 1.0554\n",
      "Epoch 10/32, Train Loss: 1.1419, Test Loss: 1.0493\n",
      "Epoch 11/32, Train Loss: 1.1046, Test Loss: 1.0474\n",
      "Epoch 12/32, Train Loss: 1.1044, Test Loss: 1.0511\n",
      "Epoch 13/32, Train Loss: 1.1162, Test Loss: 1.0461\n",
      "Epoch 14/32, Train Loss: 1.0953, Test Loss: 1.0550\n",
      "Epoch 15/32, Train Loss: 1.0901, Test Loss: 1.0504\n",
      "Epoch 16/32, Train Loss: 1.0878, Test Loss: 1.0442\n",
      "Epoch 17/32, Train Loss: 1.0499, Test Loss: 1.0419\n",
      "Epoch 18/32, Train Loss: 1.1009, Test Loss: 1.0398\n",
      "Epoch 19/32, Train Loss: 1.0706, Test Loss: 1.0425\n",
      "Epoch 20/32, Train Loss: 1.0708, Test Loss: 1.0465\n",
      "Epoch 21/32, Train Loss: 1.0955, Test Loss: 1.0466\n",
      "Epoch 22/32, Train Loss: 1.0960, Test Loss: 1.0464\n",
      "Epoch 23/32, Train Loss: 1.1065, Test Loss: 1.0470\n",
      "Epoch 24/32, Train Loss: 1.0685, Test Loss: 1.0527\n",
      "Epoch 25/32, Train Loss: 1.0891, Test Loss: 1.0499\n",
      "Epoch 26/32, Train Loss: 1.0987, Test Loss: 1.0483\n",
      "Epoch 27/32, Train Loss: 1.0953, Test Loss: 1.0480\n",
      "Epoch 28/32, Train Loss: 1.0876, Test Loss: 1.0410\n",
      "Epoch 29/32, Train Loss: 1.0956, Test Loss: 1.0423\n",
      "Epoch 30/32, Train Loss: 1.1338, Test Loss: 1.0461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:49:27,910] Trial 37 finished with value: 1.0451400727033615 and parameters: {'num_hidden_layers': 17, 'layer_0_size': 246, 'layer_1_size': 129, 'layer_2_size': 226, 'layer_3_size': 104, 'layer_4_size': 70, 'layer_5_size': 172, 'layer_6_size': 72, 'layer_7_size': 149, 'layer_8_size': 128, 'layer_9_size': 229, 'layer_10_size': 254, 'layer_11_size': 222, 'layer_12_size': 199, 'layer_13_size': 72, 'layer_14_size': 32, 'layer_15_size': 37, 'layer_16_size': 161, 'dropout_rate': 0.27226221530204336, 'learning_rate': 2.7046008614394122e-05, 'batch_size': 64, 'epochs': 32}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/32, Train Loss: 1.0766, Test Loss: 1.0460\n",
      "Epoch 32/32, Train Loss: 1.0445, Test Loss: 1.0451\n",
      "Epoch 1/66, Train Loss: 1.2760, Test Loss: 0.9296\n",
      "Epoch 2/66, Train Loss: 1.2246, Test Loss: 0.9299\n",
      "Epoch 3/66, Train Loss: 1.2223, Test Loss: 0.9317\n",
      "Epoch 4/66, Train Loss: 1.2197, Test Loss: 0.9338\n",
      "Epoch 5/66, Train Loss: 1.1753, Test Loss: 0.9330\n",
      "Epoch 6/66, Train Loss: 1.2202, Test Loss: 0.9307\n",
      "Epoch 7/66, Train Loss: 1.1259, Test Loss: 0.9298\n",
      "Epoch 8/66, Train Loss: 1.1481, Test Loss: 0.9300\n",
      "Epoch 9/66, Train Loss: 1.2102, Test Loss: 0.9303\n",
      "Epoch 10/66, Train Loss: 1.0941, Test Loss: 0.9306\n",
      "Epoch 11/66, Train Loss: 1.1217, Test Loss: 0.9312\n",
      "Epoch 12/66, Train Loss: 1.1325, Test Loss: 0.9312\n",
      "Epoch 13/66, Train Loss: 1.1662, Test Loss: 0.9313\n",
      "Epoch 14/66, Train Loss: 1.1184, Test Loss: 0.9343\n",
      "Epoch 15/66, Train Loss: 1.1209, Test Loss: 0.9369\n",
      "Epoch 16/66, Train Loss: 1.1340, Test Loss: 0.9371\n",
      "Epoch 17/66, Train Loss: 1.1449, Test Loss: 0.9353\n",
      "Epoch 18/66, Train Loss: 1.1102, Test Loss: 0.9348\n",
      "Epoch 19/66, Train Loss: 1.1042, Test Loss: 0.9345\n",
      "Epoch 20/66, Train Loss: 1.0980, Test Loss: 0.9336\n",
      "Epoch 21/66, Train Loss: 1.1063, Test Loss: 0.9323\n",
      "Epoch 22/66, Train Loss: 1.0943, Test Loss: 0.9325\n",
      "Epoch 23/66, Train Loss: 1.1085, Test Loss: 0.9323\n",
      "Epoch 24/66, Train Loss: 1.0933, Test Loss: 0.9322\n",
      "Epoch 25/66, Train Loss: 1.1003, Test Loss: 0.9317\n",
      "Epoch 26/66, Train Loss: 1.0964, Test Loss: 0.9312\n",
      "Epoch 27/66, Train Loss: 1.1317, Test Loss: 0.9312\n",
      "Epoch 28/66, Train Loss: 1.0946, Test Loss: 0.9307\n",
      "Epoch 29/66, Train Loss: 1.0828, Test Loss: 0.9301\n",
      "Epoch 30/66, Train Loss: 1.1099, Test Loss: 0.9301\n",
      "Epoch 31/66, Train Loss: 1.0935, Test Loss: 0.9305\n",
      "Epoch 32/66, Train Loss: 1.1119, Test Loss: 0.9299\n",
      "Epoch 33/66, Train Loss: 1.0774, Test Loss: 0.9300\n",
      "Epoch 34/66, Train Loss: 1.0957, Test Loss: 0.9294\n",
      "Epoch 35/66, Train Loss: 1.0834, Test Loss: 0.9295\n",
      "Epoch 36/66, Train Loss: 1.0634, Test Loss: 0.9301\n",
      "Epoch 37/66, Train Loss: 1.0657, Test Loss: 0.9303\n",
      "Epoch 38/66, Train Loss: 1.0756, Test Loss: 0.9303\n",
      "Epoch 39/66, Train Loss: 1.0744, Test Loss: 0.9303\n",
      "Epoch 40/66, Train Loss: 1.0704, Test Loss: 0.9300\n",
      "Epoch 41/66, Train Loss: 1.0939, Test Loss: 0.9301\n",
      "Epoch 42/66, Train Loss: 1.0537, Test Loss: 0.9303\n",
      "Epoch 43/66, Train Loss: 1.0377, Test Loss: 0.9305\n",
      "Epoch 44/66, Train Loss: 1.0819, Test Loss: 0.9303\n",
      "Epoch 45/66, Train Loss: 1.0875, Test Loss: 0.9302\n",
      "Epoch 46/66, Train Loss: 1.0689, Test Loss: 0.9300\n",
      "Epoch 47/66, Train Loss: 1.0721, Test Loss: 0.9297\n",
      "Epoch 48/66, Train Loss: 1.0511, Test Loss: 0.9292\n",
      "Epoch 49/66, Train Loss: 1.0680, Test Loss: 0.9294\n",
      "Epoch 50/66, Train Loss: 1.0769, Test Loss: 0.9295\n",
      "Epoch 51/66, Train Loss: 1.0592, Test Loss: 0.9293\n",
      "Epoch 52/66, Train Loss: 1.0624, Test Loss: 0.9292\n",
      "Epoch 53/66, Train Loss: 1.0700, Test Loss: 0.9300\n",
      "Epoch 54/66, Train Loss: 1.0394, Test Loss: 0.9315\n",
      "Epoch 55/66, Train Loss: 1.0663, Test Loss: 0.9328\n",
      "Epoch 56/66, Train Loss: 1.0506, Test Loss: 0.9348\n",
      "Epoch 57/66, Train Loss: 1.0840, Test Loss: 0.9367\n",
      "Epoch 58/66, Train Loss: 1.0602, Test Loss: 0.9362\n",
      "Epoch 59/66, Train Loss: 1.0582, Test Loss: 0.9338\n",
      "Epoch 60/66, Train Loss: 1.0720, Test Loss: 0.9313\n",
      "Epoch 61/66, Train Loss: 1.0700, Test Loss: 0.9296\n",
      "Epoch 62/66, Train Loss: 1.0609, Test Loss: 0.9295\n",
      "Epoch 63/66, Train Loss: 1.0790, Test Loss: 0.9299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:49:32,716] Trial 38 finished with value: 0.9298310875892639 and parameters: {'num_hidden_layers': 16, 'layer_0_size': 47, 'layer_1_size': 218, 'layer_2_size': 101, 'layer_3_size': 148, 'layer_4_size': 118, 'layer_5_size': 124, 'layer_6_size': 39, 'layer_7_size': 74, 'layer_8_size': 217, 'layer_9_size': 256, 'layer_10_size': 186, 'layer_11_size': 120, 'layer_12_size': 96, 'layer_13_size': 165, 'layer_14_size': 146, 'layer_15_size': 225, 'dropout_rate': 0.3458509137657138, 'learning_rate': 0.000488876242508228, 'batch_size': 256, 'epochs': 66}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/66, Train Loss: 1.0527, Test Loss: 0.9299\n",
      "Epoch 65/66, Train Loss: 1.0680, Test Loss: 0.9300\n",
      "Epoch 66/66, Train Loss: 1.0581, Test Loss: 0.9298\n",
      "Epoch 1/73, Train Loss: 1.4039, Test Loss: 0.9937\n",
      "Epoch 2/73, Train Loss: 1.2568, Test Loss: 1.0034\n",
      "Epoch 3/73, Train Loss: 1.2148, Test Loss: 0.9976\n",
      "Epoch 4/73, Train Loss: 1.1761, Test Loss: 1.0184\n",
      "Epoch 5/73, Train Loss: 1.1589, Test Loss: 1.0087\n",
      "Epoch 6/73, Train Loss: 1.1673, Test Loss: 1.0019\n",
      "Epoch 7/73, Train Loss: 1.1332, Test Loss: 1.0153\n",
      "Epoch 8/73, Train Loss: 1.0915, Test Loss: 1.0249\n",
      "Epoch 9/73, Train Loss: 1.1592, Test Loss: 1.0147\n",
      "Epoch 10/73, Train Loss: 1.0980, Test Loss: 1.0190\n",
      "Epoch 11/73, Train Loss: 1.1385, Test Loss: 1.0296\n",
      "Epoch 12/73, Train Loss: 1.1260, Test Loss: 1.0214\n",
      "Epoch 13/73, Train Loss: 1.0797, Test Loss: 1.0181\n",
      "Epoch 14/73, Train Loss: 1.0882, Test Loss: 1.0352\n",
      "Epoch 15/73, Train Loss: 1.1073, Test Loss: 1.0457\n",
      "Epoch 16/73, Train Loss: 1.1157, Test Loss: 1.0325\n",
      "Epoch 17/73, Train Loss: 1.0752, Test Loss: 1.0481\n",
      "Epoch 18/73, Train Loss: 1.1665, Test Loss: 1.0318\n",
      "Epoch 19/73, Train Loss: 1.0493, Test Loss: 1.0299\n",
      "Epoch 20/73, Train Loss: 1.0686, Test Loss: 1.0276\n",
      "Epoch 21/73, Train Loss: 1.0816, Test Loss: 1.0529\n",
      "Epoch 22/73, Train Loss: 1.0916, Test Loss: 1.0513\n",
      "Epoch 23/73, Train Loss: 1.0933, Test Loss: 1.0338\n",
      "Epoch 24/73, Train Loss: 1.1285, Test Loss: 1.0470\n",
      "Epoch 25/73, Train Loss: 1.1150, Test Loss: 1.0455\n",
      "Epoch 26/73, Train Loss: 1.0943, Test Loss: 1.0503\n",
      "Epoch 27/73, Train Loss: 1.0845, Test Loss: 1.0593\n",
      "Epoch 28/73, Train Loss: 1.0798, Test Loss: 1.0827\n",
      "Epoch 29/73, Train Loss: 1.0910, Test Loss: 1.0456\n",
      "Epoch 30/73, Train Loss: 1.0415, Test Loss: 1.0373\n",
      "Epoch 31/73, Train Loss: 1.0676, Test Loss: 1.0438\n",
      "Epoch 32/73, Train Loss: 1.0643, Test Loss: 1.0522\n",
      "Epoch 33/73, Train Loss: 1.0559, Test Loss: 1.0576\n",
      "Epoch 34/73, Train Loss: 1.0553, Test Loss: 1.0521\n",
      "Epoch 35/73, Train Loss: 1.1415, Test Loss: 1.0387\n",
      "Epoch 36/73, Train Loss: 1.0662, Test Loss: 1.0247\n",
      "Epoch 37/73, Train Loss: 1.0605, Test Loss: 1.0313\n",
      "Epoch 38/73, Train Loss: 1.0618, Test Loss: 1.0417\n",
      "Epoch 39/73, Train Loss: 1.0265, Test Loss: 1.0298\n",
      "Epoch 40/73, Train Loss: 1.0244, Test Loss: 1.0291\n",
      "Epoch 41/73, Train Loss: 1.0595, Test Loss: 1.0252\n",
      "Epoch 42/73, Train Loss: 1.0693, Test Loss: 1.0379\n",
      "Epoch 43/73, Train Loss: 1.1057, Test Loss: 1.0209\n",
      "Epoch 44/73, Train Loss: 1.0547, Test Loss: 1.0222\n",
      "Epoch 45/73, Train Loss: 1.0682, Test Loss: 1.0215\n",
      "Epoch 46/73, Train Loss: 1.1509, Test Loss: 1.0265\n",
      "Epoch 47/73, Train Loss: 1.0840, Test Loss: 1.0096\n",
      "Epoch 48/73, Train Loss: 1.0488, Test Loss: 1.0231\n",
      "Epoch 49/73, Train Loss: 1.0667, Test Loss: 1.0300\n",
      "Epoch 50/73, Train Loss: 1.0520, Test Loss: 1.0176\n",
      "Epoch 51/73, Train Loss: 1.0619, Test Loss: 1.0204\n",
      "Epoch 52/73, Train Loss: 1.0498, Test Loss: 1.0156\n",
      "Epoch 53/73, Train Loss: 1.0703, Test Loss: 1.0211\n",
      "Epoch 54/73, Train Loss: 1.0495, Test Loss: 1.0318\n",
      "Epoch 55/73, Train Loss: 1.0489, Test Loss: 1.0264\n",
      "Epoch 56/73, Train Loss: 1.0264, Test Loss: 1.0183\n",
      "Epoch 57/73, Train Loss: 1.0671, Test Loss: 1.0244\n",
      "Epoch 58/73, Train Loss: 1.0287, Test Loss: 1.0310\n",
      "Epoch 59/73, Train Loss: 1.0672, Test Loss: 1.0387\n",
      "Epoch 60/73, Train Loss: 1.0673, Test Loss: 1.0291\n",
      "Epoch 61/73, Train Loss: 1.0106, Test Loss: 1.0260\n",
      "Epoch 62/73, Train Loss: 1.0654, Test Loss: 1.0328\n",
      "Epoch 63/73, Train Loss: 1.0306, Test Loss: 1.0224\n",
      "Epoch 64/73, Train Loss: 1.0690, Test Loss: 1.0257\n",
      "Epoch 65/73, Train Loss: 1.0764, Test Loss: 1.0315\n",
      "Epoch 66/73, Train Loss: 1.0567, Test Loss: 1.0349\n",
      "Epoch 67/73, Train Loss: 1.0585, Test Loss: 1.0322\n",
      "Epoch 68/73, Train Loss: 1.0462, Test Loss: 1.0224\n",
      "Epoch 69/73, Train Loss: 1.0435, Test Loss: 1.0237\n",
      "Epoch 70/73, Train Loss: 1.0702, Test Loss: 1.0296\n",
      "Epoch 71/73, Train Loss: 1.0794, Test Loss: 1.0212\n",
      "Epoch 72/73, Train Loss: 1.0203, Test Loss: 1.0291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:49:50,753] Trial 39 finished with value: 1.0297404442514693 and parameters: {'num_hidden_layers': 12, 'layer_0_size': 193, 'layer_1_size': 239, 'layer_2_size': 192, 'layer_3_size': 125, 'layer_4_size': 154, 'layer_5_size': 148, 'layer_6_size': 115, 'layer_7_size': 175, 'layer_8_size': 253, 'layer_9_size': 151, 'layer_10_size': 235, 'layer_11_size': 184, 'dropout_rate': 0.24345030638483342, 'learning_rate': 6.013929862365195e-05, 'batch_size': 32, 'epochs': 73}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/73, Train Loss: 1.0214, Test Loss: 1.0297\n",
      "Epoch 1/84, Train Loss: 1.0941, Test Loss: 1.0274\n",
      "Epoch 2/84, Train Loss: 1.0597, Test Loss: 1.0296\n",
      "Epoch 3/84, Train Loss: 1.0566, Test Loss: 1.0372\n",
      "Epoch 4/84, Train Loss: 1.0038, Test Loss: 1.0473\n",
      "Epoch 5/84, Train Loss: 1.0233, Test Loss: 1.0522\n",
      "Epoch 6/84, Train Loss: 1.0087, Test Loss: 1.0512\n",
      "Epoch 7/84, Train Loss: 1.0120, Test Loss: 1.0435\n",
      "Epoch 8/84, Train Loss: 0.9492, Test Loss: 1.0381\n",
      "Epoch 9/84, Train Loss: 0.9767, Test Loss: 1.0379\n",
      "Epoch 10/84, Train Loss: 0.9814, Test Loss: 1.0356\n",
      "Epoch 11/84, Train Loss: 0.9786, Test Loss: 1.0276\n",
      "Epoch 12/84, Train Loss: 0.9598, Test Loss: 1.0271\n",
      "Epoch 13/84, Train Loss: 0.9716, Test Loss: 1.0299\n",
      "Epoch 14/84, Train Loss: 0.9628, Test Loss: 1.0258\n",
      "Epoch 15/84, Train Loss: 0.9289, Test Loss: 1.0302\n",
      "Epoch 16/84, Train Loss: 0.9400, Test Loss: 1.0488\n",
      "Epoch 17/84, Train Loss: 0.9430, Test Loss: 1.0517\n",
      "Epoch 18/84, Train Loss: 0.9167, Test Loss: 1.0519\n",
      "Epoch 19/84, Train Loss: 0.9041, Test Loss: 1.0759\n",
      "Epoch 20/84, Train Loss: 0.8669, Test Loss: 1.1111\n",
      "Epoch 21/84, Train Loss: 0.8645, Test Loss: 1.1061\n",
      "Epoch 22/84, Train Loss: 0.8319, Test Loss: 1.1037\n",
      "Epoch 23/84, Train Loss: 0.7772, Test Loss: 1.1378\n",
      "Epoch 24/84, Train Loss: 0.7946, Test Loss: 1.1412\n",
      "Epoch 25/84, Train Loss: 0.7626, Test Loss: 1.1552\n",
      "Epoch 26/84, Train Loss: 0.7238, Test Loss: 1.1659\n",
      "Epoch 27/84, Train Loss: 0.6845, Test Loss: 1.2375\n",
      "Epoch 28/84, Train Loss: 0.7210, Test Loss: 1.2291\n",
      "Epoch 29/84, Train Loss: 0.6357, Test Loss: 1.2564\n",
      "Epoch 30/84, Train Loss: 0.5963, Test Loss: 1.2722\n",
      "Epoch 31/84, Train Loss: 0.6279, Test Loss: 1.2748\n",
      "Epoch 32/84, Train Loss: 0.5691, Test Loss: 1.2798\n",
      "Epoch 33/84, Train Loss: 0.5817, Test Loss: 1.2978\n",
      "Epoch 34/84, Train Loss: 0.5697, Test Loss: 1.2941\n",
      "Epoch 35/84, Train Loss: 0.5499, Test Loss: 1.3234\n",
      "Epoch 36/84, Train Loss: 0.5336, Test Loss: 1.2875\n",
      "Epoch 37/84, Train Loss: 0.5127, Test Loss: 1.3134\n",
      "Epoch 38/84, Train Loss: 0.4838, Test Loss: 1.3766\n",
      "Epoch 39/84, Train Loss: 0.4491, Test Loss: 1.3710\n",
      "Epoch 40/84, Train Loss: 0.4599, Test Loss: 1.3960\n",
      "Epoch 41/84, Train Loss: 0.4731, Test Loss: 1.3794\n",
      "Epoch 42/84, Train Loss: 0.4368, Test Loss: 1.4030\n",
      "Epoch 43/84, Train Loss: 0.4310, Test Loss: 1.4088\n",
      "Epoch 44/84, Train Loss: 0.4467, Test Loss: 1.3828\n",
      "Epoch 45/84, Train Loss: 0.4004, Test Loss: 1.3650\n",
      "Epoch 46/84, Train Loss: 0.4270, Test Loss: 1.3532\n",
      "Epoch 47/84, Train Loss: 0.3840, Test Loss: 1.3508\n",
      "Epoch 48/84, Train Loss: 0.3726, Test Loss: 1.4275\n",
      "Epoch 49/84, Train Loss: 0.3878, Test Loss: 1.4171\n",
      "Epoch 50/84, Train Loss: 0.4089, Test Loss: 1.4061\n",
      "Epoch 51/84, Train Loss: 0.3378, Test Loss: 1.4125\n",
      "Epoch 52/84, Train Loss: 0.3368, Test Loss: 1.3978\n",
      "Epoch 53/84, Train Loss: 0.3461, Test Loss: 1.3624\n",
      "Epoch 54/84, Train Loss: 0.3146, Test Loss: 1.3632\n",
      "Epoch 55/84, Train Loss: 0.3342, Test Loss: 1.3559\n",
      "Epoch 56/84, Train Loss: 0.3243, Test Loss: 1.2824\n",
      "Epoch 57/84, Train Loss: 0.3268, Test Loss: 1.3287\n",
      "Epoch 58/84, Train Loss: 0.3133, Test Loss: 1.3527\n",
      "Epoch 59/84, Train Loss: 0.3143, Test Loss: 1.3364\n",
      "Epoch 60/84, Train Loss: 0.3286, Test Loss: 1.3073\n",
      "Epoch 61/84, Train Loss: 0.3272, Test Loss: 1.2841\n",
      "Epoch 62/84, Train Loss: 0.3150, Test Loss: 1.2528\n",
      "Epoch 63/84, Train Loss: 0.3084, Test Loss: 1.3161\n",
      "Epoch 64/84, Train Loss: 0.2780, Test Loss: 1.3020\n",
      "Epoch 65/84, Train Loss: 0.2918, Test Loss: 1.2890\n",
      "Epoch 66/84, Train Loss: 0.2767, Test Loss: 1.2976\n",
      "Epoch 67/84, Train Loss: 0.2900, Test Loss: 1.3108\n",
      "Epoch 68/84, Train Loss: 0.3093, Test Loss: 1.3179\n",
      "Epoch 69/84, Train Loss: 0.2870, Test Loss: 1.3365\n",
      "Epoch 70/84, Train Loss: 0.2718, Test Loss: 1.2989\n",
      "Epoch 71/84, Train Loss: 0.2787, Test Loss: 1.2789\n",
      "Epoch 72/84, Train Loss: 0.2806, Test Loss: 1.3279\n",
      "Epoch 73/84, Train Loss: 0.2558, Test Loss: 1.3073\n",
      "Epoch 74/84, Train Loss: 0.2717, Test Loss: 1.3397\n",
      "Epoch 75/84, Train Loss: 0.2648, Test Loss: 1.2905\n",
      "Epoch 76/84, Train Loss: 0.2660, Test Loss: 1.3133\n",
      "Epoch 77/84, Train Loss: 0.2474, Test Loss: 1.3188\n",
      "Epoch 78/84, Train Loss: 0.2331, Test Loss: 1.3125\n",
      "Epoch 79/84, Train Loss: 0.2387, Test Loss: 1.3282\n",
      "Epoch 80/84, Train Loss: 0.2416, Test Loss: 1.2610\n",
      "Epoch 81/84, Train Loss: 0.2613, Test Loss: 1.2805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:49:55,143] Trial 40 finished with value: 1.2814431190490723 and parameters: {'num_hidden_layers': 9, 'layer_0_size': 212, 'layer_1_size': 156, 'layer_2_size': 129, 'layer_3_size': 54, 'layer_4_size': 36, 'layer_5_size': 68, 'layer_6_size': 253, 'layer_7_size': 136, 'layer_8_size': 93, 'dropout_rate': 0.19038610007950657, 'learning_rate': 0.0014427660198381013, 'batch_size': 128, 'epochs': 84}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/84, Train Loss: 0.2425, Test Loss: 1.2742\n",
      "Epoch 83/84, Train Loss: 0.2258, Test Loss: 1.2928\n",
      "Epoch 84/84, Train Loss: 0.2486, Test Loss: 1.2814\n",
      "Epoch 1/64, Train Loss: 1.3197, Test Loss: 1.0794\n",
      "Epoch 2/64, Train Loss: 1.2967, Test Loss: 1.0800\n",
      "Epoch 3/64, Train Loss: 1.3084, Test Loss: 1.0821\n",
      "Epoch 4/64, Train Loss: 1.3090, Test Loss: 1.0834\n",
      "Epoch 5/64, Train Loss: 1.3148, Test Loss: 1.0849\n",
      "Epoch 6/64, Train Loss: 1.2969, Test Loss: 1.0832\n",
      "Epoch 7/64, Train Loss: 1.2985, Test Loss: 1.0829\n",
      "Epoch 8/64, Train Loss: 1.2677, Test Loss: 1.0829\n",
      "Epoch 9/64, Train Loss: 1.2764, Test Loss: 1.0868\n",
      "Epoch 10/64, Train Loss: 1.2759, Test Loss: 1.0903\n",
      "Epoch 11/64, Train Loss: 1.2325, Test Loss: 1.0917\n",
      "Epoch 12/64, Train Loss: 1.2397, Test Loss: 1.0954\n",
      "Epoch 13/64, Train Loss: 1.2118, Test Loss: 1.0977\n",
      "Epoch 14/64, Train Loss: 1.2426, Test Loss: 1.0998\n",
      "Epoch 15/64, Train Loss: 1.2452, Test Loss: 1.1044\n",
      "Epoch 16/64, Train Loss: 1.2622, Test Loss: 1.1059\n",
      "Epoch 17/64, Train Loss: 1.2611, Test Loss: 1.1083\n",
      "Epoch 18/64, Train Loss: 1.2570, Test Loss: 1.1093\n",
      "Epoch 19/64, Train Loss: 1.2094, Test Loss: 1.1087\n",
      "Epoch 20/64, Train Loss: 1.2059, Test Loss: 1.1069\n",
      "Epoch 21/64, Train Loss: 1.1901, Test Loss: 1.1091\n",
      "Epoch 22/64, Train Loss: 1.1968, Test Loss: 1.1091\n",
      "Epoch 23/64, Train Loss: 1.1414, Test Loss: 1.1090\n",
      "Epoch 24/64, Train Loss: 1.1685, Test Loss: 1.1095\n",
      "Epoch 25/64, Train Loss: 1.1892, Test Loss: 1.1075\n",
      "Epoch 26/64, Train Loss: 1.1958, Test Loss: 1.1075\n",
      "Epoch 27/64, Train Loss: 1.1926, Test Loss: 1.1082\n",
      "Epoch 28/64, Train Loss: 1.1885, Test Loss: 1.1101\n",
      "Epoch 29/64, Train Loss: 1.1799, Test Loss: 1.1090\n",
      "Epoch 30/64, Train Loss: 1.2035, Test Loss: 1.1102\n",
      "Epoch 31/64, Train Loss: 1.1589, Test Loss: 1.1127\n",
      "Epoch 32/64, Train Loss: 1.1903, Test Loss: 1.1115\n",
      "Epoch 33/64, Train Loss: 1.1785, Test Loss: 1.1134\n",
      "Epoch 34/64, Train Loss: 1.1788, Test Loss: 1.1092\n",
      "Epoch 35/64, Train Loss: 1.2150, Test Loss: 1.1096\n",
      "Epoch 36/64, Train Loss: 1.1899, Test Loss: 1.1085\n",
      "Epoch 37/64, Train Loss: 1.2177, Test Loss: 1.1090\n",
      "Epoch 38/64, Train Loss: 1.1506, Test Loss: 1.1121\n",
      "Epoch 39/64, Train Loss: 1.1699, Test Loss: 1.1128\n",
      "Epoch 40/64, Train Loss: 1.1663, Test Loss: 1.1115\n",
      "Epoch 41/64, Train Loss: 1.1284, Test Loss: 1.1104\n",
      "Epoch 42/64, Train Loss: 1.1016, Test Loss: 1.1099\n",
      "Epoch 43/64, Train Loss: 1.1072, Test Loss: 1.1141\n",
      "Epoch 44/64, Train Loss: 1.1433, Test Loss: 1.1156\n",
      "Epoch 45/64, Train Loss: 1.1293, Test Loss: 1.1131\n",
      "Epoch 46/64, Train Loss: 1.1409, Test Loss: 1.1092\n",
      "Epoch 47/64, Train Loss: 1.0966, Test Loss: 1.1082\n",
      "Epoch 48/64, Train Loss: 1.1201, Test Loss: 1.1063\n",
      "Epoch 49/64, Train Loss: 1.1308, Test Loss: 1.1060\n",
      "Epoch 50/64, Train Loss: 1.1238, Test Loss: 1.1060\n",
      "Epoch 51/64, Train Loss: 1.1183, Test Loss: 1.1054\n",
      "Epoch 52/64, Train Loss: 1.1219, Test Loss: 1.1073\n",
      "Epoch 53/64, Train Loss: 1.1265, Test Loss: 1.1077\n",
      "Epoch 54/64, Train Loss: 1.1359, Test Loss: 1.1036\n",
      "Epoch 55/64, Train Loss: 1.1291, Test Loss: 1.1031\n",
      "Epoch 56/64, Train Loss: 1.1242, Test Loss: 1.1003\n",
      "Epoch 57/64, Train Loss: 1.1304, Test Loss: 1.1023\n",
      "Epoch 58/64, Train Loss: 1.1145, Test Loss: 1.1009\n",
      "Epoch 59/64, Train Loss: 1.1574, Test Loss: 1.1037\n",
      "Epoch 60/64, Train Loss: 1.1277, Test Loss: 1.1052\n",
      "Epoch 61/64, Train Loss: 1.1607, Test Loss: 1.1029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:49:57,616] Trial 41 finished with value: 1.0993684530258179 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 145, 'layer_1_size': 61, 'layer_2_size': 144, 'layer_3_size': 237, 'layer_4_size': 143, 'layer_5_size': 164, 'layer_6_size': 210, 'dropout_rate': 0.20173898501336468, 'learning_rate': 1.4583753342650157e-05, 'batch_size': 256, 'epochs': 64}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/64, Train Loss: 1.0869, Test Loss: 1.1014\n",
      "Epoch 63/64, Train Loss: 1.1373, Test Loss: 1.0996\n",
      "Epoch 64/64, Train Loss: 1.0784, Test Loss: 1.0994\n",
      "Epoch 1/78, Train Loss: 1.0691, Test Loss: 1.1416\n",
      "Epoch 2/78, Train Loss: 1.0413, Test Loss: 1.1337\n",
      "Epoch 3/78, Train Loss: 1.0231, Test Loss: 1.1292\n",
      "Epoch 4/78, Train Loss: 1.0474, Test Loss: 1.1284\n",
      "Epoch 5/78, Train Loss: 1.0469, Test Loss: 1.1293\n",
      "Epoch 6/78, Train Loss: 1.0674, Test Loss: 1.1316\n",
      "Epoch 7/78, Train Loss: 1.0200, Test Loss: 1.1344\n",
      "Epoch 8/78, Train Loss: 1.0212, Test Loss: 1.1371\n",
      "Epoch 9/78, Train Loss: 0.9883, Test Loss: 1.1385\n",
      "Epoch 10/78, Train Loss: 1.0093, Test Loss: 1.1399\n",
      "Epoch 11/78, Train Loss: 1.0282, Test Loss: 1.1410\n",
      "Epoch 12/78, Train Loss: 0.9734, Test Loss: 1.1401\n",
      "Epoch 13/78, Train Loss: 0.9889, Test Loss: 1.1405\n",
      "Epoch 14/78, Train Loss: 0.9703, Test Loss: 1.1430\n",
      "Epoch 15/78, Train Loss: 0.9738, Test Loss: 1.1424\n",
      "Epoch 16/78, Train Loss: 0.9833, Test Loss: 1.1427\n",
      "Epoch 17/78, Train Loss: 0.9670, Test Loss: 1.1433\n",
      "Epoch 18/78, Train Loss: 0.9668, Test Loss: 1.1434\n",
      "Epoch 19/78, Train Loss: 0.9897, Test Loss: 1.1459\n",
      "Epoch 20/78, Train Loss: 0.9903, Test Loss: 1.1451\n",
      "Epoch 21/78, Train Loss: 0.9385, Test Loss: 1.1458\n",
      "Epoch 22/78, Train Loss: 0.9458, Test Loss: 1.1471\n",
      "Epoch 23/78, Train Loss: 0.9652, Test Loss: 1.1478\n",
      "Epoch 24/78, Train Loss: 0.9608, Test Loss: 1.1480\n",
      "Epoch 25/78, Train Loss: 0.9661, Test Loss: 1.1495\n",
      "Epoch 26/78, Train Loss: 0.9541, Test Loss: 1.1490\n",
      "Epoch 27/78, Train Loss: 0.9701, Test Loss: 1.1504\n",
      "Epoch 28/78, Train Loss: 0.9969, Test Loss: 1.1506\n",
      "Epoch 29/78, Train Loss: 0.9956, Test Loss: 1.1529\n",
      "Epoch 30/78, Train Loss: 0.9503, Test Loss: 1.1520\n",
      "Epoch 31/78, Train Loss: 0.9818, Test Loss: 1.1533\n",
      "Epoch 32/78, Train Loss: 0.9684, Test Loss: 1.1535\n",
      "Epoch 33/78, Train Loss: 0.9802, Test Loss: 1.1533\n",
      "Epoch 34/78, Train Loss: 0.9855, Test Loss: 1.1537\n",
      "Epoch 35/78, Train Loss: 0.9431, Test Loss: 1.1554\n",
      "Epoch 36/78, Train Loss: 0.9349, Test Loss: 1.1563\n",
      "Epoch 37/78, Train Loss: 0.9792, Test Loss: 1.1560\n",
      "Epoch 38/78, Train Loss: 0.9527, Test Loss: 1.1566\n",
      "Epoch 39/78, Train Loss: 0.9683, Test Loss: 1.1558\n",
      "Epoch 40/78, Train Loss: 0.9445, Test Loss: 1.1559\n",
      "Epoch 41/78, Train Loss: 0.9301, Test Loss: 1.1579\n",
      "Epoch 42/78, Train Loss: 0.9341, Test Loss: 1.1572\n",
      "Epoch 43/78, Train Loss: 0.9190, Test Loss: 1.1583\n",
      "Epoch 44/78, Train Loss: 0.9541, Test Loss: 1.1545\n",
      "Epoch 45/78, Train Loss: 0.9722, Test Loss: 1.1561\n",
      "Epoch 46/78, Train Loss: 0.9552, Test Loss: 1.1574\n",
      "Epoch 47/78, Train Loss: 0.9288, Test Loss: 1.1567\n",
      "Epoch 48/78, Train Loss: 0.9259, Test Loss: 1.1558\n",
      "Epoch 49/78, Train Loss: 0.9571, Test Loss: 1.1574\n",
      "Epoch 50/78, Train Loss: 0.9274, Test Loss: 1.1567\n",
      "Epoch 51/78, Train Loss: 0.9231, Test Loss: 1.1559\n",
      "Epoch 52/78, Train Loss: 0.9595, Test Loss: 1.1568\n",
      "Epoch 53/78, Train Loss: 0.9279, Test Loss: 1.1595\n",
      "Epoch 54/78, Train Loss: 0.9256, Test Loss: 1.1606\n",
      "Epoch 55/78, Train Loss: 0.9792, Test Loss: 1.1620\n",
      "Epoch 56/78, Train Loss: 0.9114, Test Loss: 1.1603\n",
      "Epoch 57/78, Train Loss: 0.9275, Test Loss: 1.1616\n",
      "Epoch 58/78, Train Loss: 0.9549, Test Loss: 1.1612\n",
      "Epoch 59/78, Train Loss: 0.9023, Test Loss: 1.1602\n",
      "Epoch 60/78, Train Loss: 0.9235, Test Loss: 1.1612\n",
      "Epoch 61/78, Train Loss: 0.9386, Test Loss: 1.1616\n",
      "Epoch 62/78, Train Loss: 0.9298, Test Loss: 1.1614\n",
      "Epoch 63/78, Train Loss: 0.9358, Test Loss: 1.1625\n",
      "Epoch 64/78, Train Loss: 0.9414, Test Loss: 1.1632\n",
      "Epoch 65/78, Train Loss: 0.9544, Test Loss: 1.1619\n",
      "Epoch 66/78, Train Loss: 0.9530, Test Loss: 1.1620\n",
      "Epoch 67/78, Train Loss: 0.9022, Test Loss: 1.1641\n",
      "Epoch 68/78, Train Loss: 0.9470, Test Loss: 1.1660\n",
      "Epoch 69/78, Train Loss: 0.9071, Test Loss: 1.1671\n",
      "Epoch 70/78, Train Loss: 0.9503, Test Loss: 1.1675\n",
      "Epoch 71/78, Train Loss: 0.8783, Test Loss: 1.1686\n",
      "Epoch 72/78, Train Loss: 0.9319, Test Loss: 1.1679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:50:00,094] Trial 42 finished with value: 1.169878363609314 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 160, 'layer_1_size': 66, 'layer_2_size': 125, 'layer_3_size': 177, 'layer_4_size': 201, 'layer_5_size': 195, 'dropout_rate': 0.2655380815314358, 'learning_rate': 3.963178028136617e-05, 'batch_size': 256, 'epochs': 78}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/78, Train Loss: 0.9339, Test Loss: 1.1687\n",
      "Epoch 74/78, Train Loss: 0.9147, Test Loss: 1.1685\n",
      "Epoch 75/78, Train Loss: 0.9369, Test Loss: 1.1680\n",
      "Epoch 76/78, Train Loss: 0.9253, Test Loss: 1.1670\n",
      "Epoch 77/78, Train Loss: 0.9453, Test Loss: 1.1681\n",
      "Epoch 78/78, Train Loss: 0.9296, Test Loss: 1.1699\n",
      "Epoch 1/75, Train Loss: 1.2639, Test Loss: 0.8652\n",
      "Epoch 2/75, Train Loss: 1.2320, Test Loss: 0.8680\n",
      "Epoch 3/75, Train Loss: 1.1745, Test Loss: 0.8699\n",
      "Epoch 4/75, Train Loss: 1.2468, Test Loss: 0.8719\n",
      "Epoch 5/75, Train Loss: 1.2433, Test Loss: 0.8717\n",
      "Epoch 6/75, Train Loss: 1.2258, Test Loss: 0.8671\n",
      "Epoch 7/75, Train Loss: 1.2298, Test Loss: 0.8665\n",
      "Epoch 8/75, Train Loss: 1.2491, Test Loss: 0.8671\n",
      "Epoch 9/75, Train Loss: 1.1982, Test Loss: 0.8671\n",
      "Epoch 10/75, Train Loss: 1.1816, Test Loss: 0.8685\n",
      "Epoch 11/75, Train Loss: 1.2351, Test Loss: 0.8686\n",
      "Epoch 12/75, Train Loss: 1.2415, Test Loss: 0.8679\n",
      "Epoch 13/75, Train Loss: 1.2175, Test Loss: 0.8677\n",
      "Epoch 14/75, Train Loss: 1.1249, Test Loss: 0.8660\n",
      "Epoch 15/75, Train Loss: 1.1826, Test Loss: 0.8682\n",
      "Epoch 16/75, Train Loss: 1.1815, Test Loss: 0.8684\n",
      "Epoch 17/75, Train Loss: 1.2044, Test Loss: 0.8679\n",
      "Epoch 18/75, Train Loss: 1.2244, Test Loss: 0.8671\n",
      "Epoch 19/75, Train Loss: 1.1417, Test Loss: 0.8664\n",
      "Epoch 20/75, Train Loss: 1.1937, Test Loss: 0.8668\n",
      "Epoch 21/75, Train Loss: 1.2473, Test Loss: 0.8670\n",
      "Epoch 22/75, Train Loss: 1.1833, Test Loss: 0.8687\n",
      "Epoch 23/75, Train Loss: 1.1710, Test Loss: 0.8670\n",
      "Epoch 24/75, Train Loss: 1.1584, Test Loss: 0.8660\n",
      "Epoch 25/75, Train Loss: 1.2095, Test Loss: 0.8642\n",
      "Epoch 26/75, Train Loss: 1.1425, Test Loss: 0.8639\n",
      "Epoch 27/75, Train Loss: 1.2048, Test Loss: 0.8642\n",
      "Epoch 28/75, Train Loss: 1.1980, Test Loss: 0.8633\n",
      "Epoch 29/75, Train Loss: 1.1441, Test Loss: 0.8623\n",
      "Epoch 30/75, Train Loss: 1.1281, Test Loss: 0.8625\n",
      "Epoch 31/75, Train Loss: 1.1543, Test Loss: 0.8615\n",
      "Epoch 32/75, Train Loss: 1.1595, Test Loss: 0.8620\n",
      "Epoch 33/75, Train Loss: 1.2121, Test Loss: 0.8621\n",
      "Epoch 34/75, Train Loss: 1.1933, Test Loss: 0.8625\n",
      "Epoch 35/75, Train Loss: 1.1668, Test Loss: 0.8621\n",
      "Epoch 36/75, Train Loss: 1.1110, Test Loss: 0.8613\n",
      "Epoch 37/75, Train Loss: 1.1965, Test Loss: 0.8599\n",
      "Epoch 38/75, Train Loss: 1.1391, Test Loss: 0.8604\n",
      "Epoch 39/75, Train Loss: 1.1560, Test Loss: 0.8606\n",
      "Epoch 40/75, Train Loss: 1.1555, Test Loss: 0.8608\n",
      "Epoch 41/75, Train Loss: 1.1390, Test Loss: 0.8604\n",
      "Epoch 42/75, Train Loss: 1.1664, Test Loss: 0.8608\n",
      "Epoch 43/75, Train Loss: 1.1433, Test Loss: 0.8603\n",
      "Epoch 44/75, Train Loss: 1.1352, Test Loss: 0.8612\n",
      "Epoch 45/75, Train Loss: 1.1505, Test Loss: 0.8619\n",
      "Epoch 46/75, Train Loss: 1.1545, Test Loss: 0.8624\n",
      "Epoch 47/75, Train Loss: 1.1496, Test Loss: 0.8617\n",
      "Epoch 48/75, Train Loss: 1.1400, Test Loss: 0.8626\n",
      "Epoch 49/75, Train Loss: 1.1404, Test Loss: 0.8610\n",
      "Epoch 50/75, Train Loss: 1.1792, Test Loss: 0.8595\n",
      "Epoch 51/75, Train Loss: 1.1691, Test Loss: 0.8599\n",
      "Epoch 52/75, Train Loss: 1.1415, Test Loss: 0.8605\n",
      "Epoch 53/75, Train Loss: 1.1463, Test Loss: 0.8607\n",
      "Epoch 54/75, Train Loss: 1.1204, Test Loss: 0.8596\n",
      "Epoch 55/75, Train Loss: 1.1178, Test Loss: 0.8598\n",
      "Epoch 56/75, Train Loss: 1.1715, Test Loss: 0.8611\n",
      "Epoch 57/75, Train Loss: 1.1530, Test Loss: 0.8600\n",
      "Epoch 58/75, Train Loss: 1.1307, Test Loss: 0.8595\n",
      "Epoch 59/75, Train Loss: 1.1327, Test Loss: 0.8599\n",
      "Epoch 60/75, Train Loss: 1.1494, Test Loss: 0.8584\n",
      "Epoch 61/75, Train Loss: 1.1287, Test Loss: 0.8594\n",
      "Epoch 62/75, Train Loss: 1.1604, Test Loss: 0.8581\n",
      "Epoch 63/75, Train Loss: 1.1341, Test Loss: 0.8587\n",
      "Epoch 64/75, Train Loss: 1.1211, Test Loss: 0.8585\n",
      "Epoch 65/75, Train Loss: 1.1382, Test Loss: 0.8584\n",
      "Epoch 66/75, Train Loss: 1.1167, Test Loss: 0.8582\n",
      "Epoch 67/75, Train Loss: 1.1363, Test Loss: 0.8570\n",
      "Epoch 68/75, Train Loss: 1.0935, Test Loss: 0.8567\n",
      "Epoch 69/75, Train Loss: 1.1209, Test Loss: 0.8574\n",
      "Epoch 70/75, Train Loss: 1.1507, Test Loss: 0.8568\n",
      "Epoch 71/75, Train Loss: 1.1414, Test Loss: 0.8573\n",
      "Epoch 72/75, Train Loss: 1.1405, Test Loss: 0.8575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:50:03,031] Trial 43 finished with value: 0.8573423624038696 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 119, 'layer_1_size': 197, 'layer_2_size': 117, 'layer_3_size': 227, 'layer_4_size': 87, 'layer_5_size': 164, 'layer_6_size': 98, 'layer_7_size': 256, 'dropout_rate': 0.24517393151997768, 'learning_rate': 2.4363567986571435e-05, 'batch_size': 256, 'epochs': 75}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/75, Train Loss: 1.1466, Test Loss: 0.8569\n",
      "Epoch 74/75, Train Loss: 1.1369, Test Loss: 0.8566\n",
      "Epoch 75/75, Train Loss: 1.1121, Test Loss: 0.8573\n",
      "Epoch 1/58, Train Loss: 1.1602, Test Loss: 0.9651\n",
      "Epoch 2/58, Train Loss: 1.1948, Test Loss: 0.9630\n",
      "Epoch 3/58, Train Loss: 1.1919, Test Loss: 0.9623\n",
      "Epoch 4/58, Train Loss: 1.2061, Test Loss: 0.9649\n",
      "Epoch 5/58, Train Loss: 1.2292, Test Loss: 0.9695\n",
      "Epoch 6/58, Train Loss: 1.1669, Test Loss: 0.9753\n",
      "Epoch 7/58, Train Loss: 1.1643, Test Loss: 0.9807\n",
      "Epoch 8/58, Train Loss: 1.2202, Test Loss: 0.9860\n",
      "Epoch 9/58, Train Loss: 1.1894, Test Loss: 0.9903\n",
      "Epoch 10/58, Train Loss: 1.1657, Test Loss: 0.9941\n",
      "Epoch 11/58, Train Loss: 1.1923, Test Loss: 0.9961\n",
      "Epoch 12/58, Train Loss: 1.1915, Test Loss: 0.9969\n",
      "Epoch 13/58, Train Loss: 1.1853, Test Loss: 0.9985\n",
      "Epoch 14/58, Train Loss: 1.2179, Test Loss: 0.9996\n",
      "Epoch 15/58, Train Loss: 1.1887, Test Loss: 0.9996\n",
      "Epoch 16/58, Train Loss: 1.1503, Test Loss: 1.0002\n",
      "Epoch 17/58, Train Loss: 1.1792, Test Loss: 1.0009\n",
      "Epoch 18/58, Train Loss: 1.1590, Test Loss: 1.0006\n",
      "Epoch 19/58, Train Loss: 1.1618, Test Loss: 1.0012\n",
      "Epoch 20/58, Train Loss: 1.1226, Test Loss: 1.0010\n",
      "Epoch 21/58, Train Loss: 1.1460, Test Loss: 1.0006\n",
      "Epoch 22/58, Train Loss: 1.2392, Test Loss: 1.0002\n",
      "Epoch 23/58, Train Loss: 1.1711, Test Loss: 0.9999\n",
      "Epoch 24/58, Train Loss: 1.2124, Test Loss: 0.9991\n",
      "Epoch 25/58, Train Loss: 1.1923, Test Loss: 0.9990\n",
      "Epoch 26/58, Train Loss: 1.1935, Test Loss: 0.9988\n",
      "Epoch 27/58, Train Loss: 1.1744, Test Loss: 0.9986\n",
      "Epoch 28/58, Train Loss: 1.1961, Test Loss: 0.9978\n",
      "Epoch 29/58, Train Loss: 1.1822, Test Loss: 0.9979\n",
      "Epoch 30/58, Train Loss: 1.1747, Test Loss: 0.9980\n",
      "Epoch 31/58, Train Loss: 1.1694, Test Loss: 0.9983\n",
      "Epoch 32/58, Train Loss: 1.1875, Test Loss: 0.9988\n",
      "Epoch 33/58, Train Loss: 1.1841, Test Loss: 0.9984\n",
      "Epoch 34/58, Train Loss: 1.1592, Test Loss: 0.9985\n",
      "Epoch 35/58, Train Loss: 1.1558, Test Loss: 0.9988\n",
      "Epoch 36/58, Train Loss: 1.1557, Test Loss: 0.9980\n",
      "Epoch 37/58, Train Loss: 1.1784, Test Loss: 0.9977\n",
      "Epoch 38/58, Train Loss: 1.1186, Test Loss: 0.9982\n",
      "Epoch 39/58, Train Loss: 1.1500, Test Loss: 0.9986\n",
      "Epoch 40/58, Train Loss: 1.1763, Test Loss: 0.9978\n",
      "Epoch 41/58, Train Loss: 1.1713, Test Loss: 0.9983\n",
      "Epoch 42/58, Train Loss: 1.1528, Test Loss: 0.9984\n",
      "Epoch 43/58, Train Loss: 1.1595, Test Loss: 0.9975\n",
      "Epoch 44/58, Train Loss: 1.1372, Test Loss: 0.9968\n",
      "Epoch 45/58, Train Loss: 1.1329, Test Loss: 0.9968\n",
      "Epoch 46/58, Train Loss: 1.1597, Test Loss: 0.9974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:50:03,968] Trial 44 finished with value: 0.9940518140792847 and parameters: {'num_hidden_layers': 3, 'layer_0_size': 112, 'layer_1_size': 92, 'layer_2_size': 98, 'dropout_rate': 0.2345604544645991, 'learning_rate': 1.2367404566371054e-05, 'batch_size': 256, 'epochs': 58}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/58, Train Loss: 1.1480, Test Loss: 0.9965\n",
      "Epoch 48/58, Train Loss: 1.1425, Test Loss: 0.9970\n",
      "Epoch 49/58, Train Loss: 1.1398, Test Loss: 0.9969\n",
      "Epoch 50/58, Train Loss: 1.1659, Test Loss: 0.9959\n",
      "Epoch 51/58, Train Loss: 1.1272, Test Loss: 0.9961\n",
      "Epoch 52/58, Train Loss: 1.1599, Test Loss: 0.9953\n",
      "Epoch 53/58, Train Loss: 1.0960, Test Loss: 0.9953\n",
      "Epoch 54/58, Train Loss: 1.1297, Test Loss: 0.9956\n",
      "Epoch 55/58, Train Loss: 1.1411, Test Loss: 0.9952\n",
      "Epoch 56/58, Train Loss: 1.1669, Test Loss: 0.9949\n",
      "Epoch 57/58, Train Loss: 1.1247, Test Loss: 0.9951\n",
      "Epoch 58/58, Train Loss: 1.1457, Test Loss: 0.9941\n",
      "Epoch 1/51, Train Loss: 1.0832, Test Loss: 1.1315\n",
      "Epoch 2/51, Train Loss: 1.0884, Test Loss: 1.1316\n",
      "Epoch 3/51, Train Loss: 1.0585, Test Loss: 1.1258\n",
      "Epoch 4/51, Train Loss: 1.0058, Test Loss: 1.1245\n",
      "Epoch 5/51, Train Loss: 0.9834, Test Loss: 1.1268\n",
      "Epoch 6/51, Train Loss: 1.0286, Test Loss: 1.1086\n",
      "Epoch 7/51, Train Loss: 1.0022, Test Loss: 1.1097\n",
      "Epoch 8/51, Train Loss: 0.9641, Test Loss: 1.1135\n",
      "Epoch 9/51, Train Loss: 0.9680, Test Loss: 1.1094\n",
      "Epoch 10/51, Train Loss: 1.0185, Test Loss: 1.0982\n",
      "Epoch 11/51, Train Loss: 0.9617, Test Loss: 1.1050\n",
      "Epoch 12/51, Train Loss: 0.9525, Test Loss: 1.1044\n",
      "Epoch 13/51, Train Loss: 0.9671, Test Loss: 1.1065\n",
      "Epoch 14/51, Train Loss: 0.9442, Test Loss: 1.1132\n",
      "Epoch 15/51, Train Loss: 0.9387, Test Loss: 1.1052\n",
      "Epoch 16/51, Train Loss: 0.9347, Test Loss: 1.1031\n",
      "Epoch 17/51, Train Loss: 0.9384, Test Loss: 1.1132\n",
      "Epoch 18/51, Train Loss: 0.9358, Test Loss: 1.1029\n",
      "Epoch 19/51, Train Loss: 0.9765, Test Loss: 1.1158\n",
      "Epoch 20/51, Train Loss: 0.9116, Test Loss: 1.1008\n",
      "Epoch 21/51, Train Loss: 0.9252, Test Loss: 1.1062\n",
      "Epoch 22/51, Train Loss: 0.9241, Test Loss: 1.1093\n",
      "Epoch 23/51, Train Loss: 0.9059, Test Loss: 1.1135\n",
      "Epoch 24/51, Train Loss: 0.9428, Test Loss: 1.1106\n",
      "Epoch 25/51, Train Loss: 0.9085, Test Loss: 1.1101\n",
      "Epoch 26/51, Train Loss: 0.8975, Test Loss: 1.1136\n",
      "Epoch 27/51, Train Loss: 0.9021, Test Loss: 1.1177\n",
      "Epoch 28/51, Train Loss: 0.8868, Test Loss: 1.1187\n",
      "Epoch 29/51, Train Loss: 0.8768, Test Loss: 1.1367\n",
      "Epoch 30/51, Train Loss: 0.8693, Test Loss: 1.1402\n",
      "Epoch 31/51, Train Loss: 0.8462, Test Loss: 1.1376\n",
      "Epoch 32/51, Train Loss: 0.8903, Test Loss: 1.1369\n",
      "Epoch 33/51, Train Loss: 0.8506, Test Loss: 1.1480\n",
      "Epoch 34/51, Train Loss: 0.8578, Test Loss: 1.1500\n",
      "Epoch 35/51, Train Loss: 0.8521, Test Loss: 1.1392\n",
      "Epoch 36/51, Train Loss: 0.8303, Test Loss: 1.1617\n",
      "Epoch 37/51, Train Loss: 0.8588, Test Loss: 1.1709\n",
      "Epoch 38/51, Train Loss: 0.8406, Test Loss: 1.1604\n",
      "Epoch 39/51, Train Loss: 0.8371, Test Loss: 1.1665\n",
      "Epoch 40/51, Train Loss: 0.8053, Test Loss: 1.1634\n",
      "Epoch 41/51, Train Loss: 0.8095, Test Loss: 1.1659\n",
      "Epoch 42/51, Train Loss: 0.8042, Test Loss: 1.1660\n",
      "Epoch 43/51, Train Loss: 0.8238, Test Loss: 1.1642\n",
      "Epoch 44/51, Train Loss: 0.7721, Test Loss: 1.1671\n",
      "Epoch 45/51, Train Loss: 0.7889, Test Loss: 1.1815\n",
      "Epoch 46/51, Train Loss: 0.8217, Test Loss: 1.1822\n",
      "Epoch 47/51, Train Loss: 0.7593, Test Loss: 1.1829\n",
      "Epoch 48/51, Train Loss: 0.7472, Test Loss: 1.1893\n",
      "Epoch 49/51, Train Loss: 0.7680, Test Loss: 1.1933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:50:09,190] Trial 45 finished with value: 1.1974629334041051 and parameters: {'num_hidden_layers': 5, 'layer_0_size': 152, 'layer_1_size': 73, 'layer_2_size': 152, 'layer_3_size': 255, 'layer_4_size': 166, 'dropout_rate': 0.3074385061121416, 'learning_rate': 0.00032400697806586035, 'batch_size': 32, 'epochs': 51}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/51, Train Loss: 0.7732, Test Loss: 1.1805\n",
      "Epoch 51/51, Train Loss: 0.7608, Test Loss: 1.1975\n",
      "Epoch 1/65, Train Loss: 1.3054, Test Loss: 0.9731\n",
      "Epoch 2/65, Train Loss: 1.2370, Test Loss: 0.9719\n",
      "Epoch 3/65, Train Loss: 1.1951, Test Loss: 0.9714\n",
      "Epoch 4/65, Train Loss: 1.2652, Test Loss: 0.9716\n",
      "Epoch 5/65, Train Loss: 1.2211, Test Loss: 0.9725\n",
      "Epoch 6/65, Train Loss: 1.1560, Test Loss: 0.9748\n",
      "Epoch 7/65, Train Loss: 1.2020, Test Loss: 0.9774\n",
      "Epoch 8/65, Train Loss: 1.2520, Test Loss: 0.9776\n",
      "Epoch 9/65, Train Loss: 1.1949, Test Loss: 0.9777\n",
      "Epoch 10/65, Train Loss: 1.1947, Test Loss: 0.9775\n",
      "Epoch 11/65, Train Loss: 1.2590, Test Loss: 0.9783\n",
      "Epoch 12/65, Train Loss: 1.2283, Test Loss: 0.9774\n",
      "Epoch 13/65, Train Loss: 1.2431, Test Loss: 0.9810\n",
      "Epoch 14/65, Train Loss: 1.2051, Test Loss: 0.9825\n",
      "Epoch 15/65, Train Loss: 1.2003, Test Loss: 0.9839\n",
      "Epoch 16/65, Train Loss: 1.2153, Test Loss: 0.9836\n",
      "Epoch 17/65, Train Loss: 1.2206, Test Loss: 0.9826\n",
      "Epoch 18/65, Train Loss: 1.1216, Test Loss: 0.9862\n",
      "Epoch 19/65, Train Loss: 1.2048, Test Loss: 0.9878\n",
      "Epoch 20/65, Train Loss: 1.1886, Test Loss: 0.9864\n",
      "Epoch 21/65, Train Loss: 1.1838, Test Loss: 0.9863\n",
      "Epoch 22/65, Train Loss: 1.1434, Test Loss: 0.9898\n",
      "Epoch 23/65, Train Loss: 1.1425, Test Loss: 0.9893\n",
      "Epoch 24/65, Train Loss: 1.1871, Test Loss: 0.9896\n",
      "Epoch 25/65, Train Loss: 1.1635, Test Loss: 0.9892\n",
      "Epoch 26/65, Train Loss: 1.1802, Test Loss: 0.9898\n",
      "Epoch 27/65, Train Loss: 1.1746, Test Loss: 0.9914\n",
      "Epoch 28/65, Train Loss: 1.1803, Test Loss: 0.9924\n",
      "Epoch 29/65, Train Loss: 1.1525, Test Loss: 0.9892\n",
      "Epoch 30/65, Train Loss: 1.1595, Test Loss: 0.9875\n",
      "Epoch 31/65, Train Loss: 1.1911, Test Loss: 0.9887\n",
      "Epoch 32/65, Train Loss: 1.1582, Test Loss: 0.9880\n",
      "Epoch 33/65, Train Loss: 1.1318, Test Loss: 0.9859\n",
      "Epoch 34/65, Train Loss: 1.1518, Test Loss: 0.9847\n",
      "Epoch 35/65, Train Loss: 1.1700, Test Loss: 0.9846\n",
      "Epoch 36/65, Train Loss: 1.1767, Test Loss: 0.9830\n",
      "Epoch 37/65, Train Loss: 1.1743, Test Loss: 0.9795\n",
      "Epoch 38/65, Train Loss: 1.1865, Test Loss: 0.9798\n",
      "Epoch 39/65, Train Loss: 1.2137, Test Loss: 0.9791\n",
      "Epoch 40/65, Train Loss: 1.1377, Test Loss: 0.9781\n",
      "Epoch 41/65, Train Loss: 1.1967, Test Loss: 0.9774\n",
      "Epoch 42/65, Train Loss: 1.1330, Test Loss: 0.9775\n",
      "Epoch 43/65, Train Loss: 1.1195, Test Loss: 0.9769\n",
      "Epoch 44/65, Train Loss: 1.1470, Test Loss: 0.9787\n",
      "Epoch 45/65, Train Loss: 1.1721, Test Loss: 0.9789\n",
      "Epoch 46/65, Train Loss: 1.1271, Test Loss: 0.9784\n",
      "Epoch 47/65, Train Loss: 1.1745, Test Loss: 0.9796\n",
      "Epoch 48/65, Train Loss: 1.1587, Test Loss: 0.9806\n",
      "Epoch 49/65, Train Loss: 1.1808, Test Loss: 0.9793\n",
      "Epoch 50/65, Train Loss: 1.1282, Test Loss: 0.9803\n",
      "Epoch 51/65, Train Loss: 1.1507, Test Loss: 0.9809\n",
      "Epoch 52/65, Train Loss: 1.1592, Test Loss: 0.9801\n",
      "Epoch 53/65, Train Loss: 1.1297, Test Loss: 0.9800\n",
      "Epoch 54/65, Train Loss: 1.1494, Test Loss: 0.9799\n",
      "Epoch 55/65, Train Loss: 1.1633, Test Loss: 0.9784\n",
      "Epoch 56/65, Train Loss: 1.1431, Test Loss: 0.9806\n",
      "Epoch 57/65, Train Loss: 1.1066, Test Loss: 0.9812\n",
      "Epoch 58/65, Train Loss: 1.1623, Test Loss: 0.9818\n",
      "Epoch 59/65, Train Loss: 1.1233, Test Loss: 0.9812\n",
      "Epoch 60/65, Train Loss: 1.1725, Test Loss: 0.9820\n",
      "Epoch 61/65, Train Loss: 1.1444, Test Loss: 0.9826\n",
      "Epoch 62/65, Train Loss: 1.1430, Test Loss: 0.9811\n",
      "Epoch 63/65, Train Loss: 1.1189, Test Loss: 0.9808\n",
      "Epoch 64/65, Train Loss: 1.1146, Test Loss: 0.9808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:50:13,233] Trial 46 finished with value: 0.9832490682601929 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 87, 'layer_1_size': 44, 'layer_2_size': 170, 'layer_3_size': 117, 'layer_4_size': 59, 'layer_5_size': 213, 'layer_6_size': 56, 'layer_7_size': 221, 'layer_8_size': 147, 'layer_9_size': 239, 'layer_10_size': 215, 'layer_11_size': 156, 'layer_12_size': 149, 'dropout_rate': 0.2598438625697399, 'learning_rate': 5.614073923172478e-05, 'batch_size': 256, 'epochs': 65}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/65, Train Loss: 1.1272, Test Loss: 0.9832\n",
      "Epoch 1/69, Train Loss: 1.1537, Test Loss: 1.0624\n",
      "Epoch 2/69, Train Loss: 1.1417, Test Loss: 1.0738\n",
      "Epoch 3/69, Train Loss: 1.1550, Test Loss: 1.0993\n",
      "Epoch 4/69, Train Loss: 1.0998, Test Loss: 1.0863\n",
      "Epoch 5/69, Train Loss: 1.0980, Test Loss: 1.0699\n",
      "Epoch 6/69, Train Loss: 1.1039, Test Loss: 1.0702\n",
      "Epoch 7/69, Train Loss: 1.1460, Test Loss: 1.0730\n",
      "Epoch 8/69, Train Loss: 1.1148, Test Loss: 1.0754\n",
      "Epoch 9/69, Train Loss: 1.1089, Test Loss: 1.0629\n",
      "Epoch 10/69, Train Loss: 1.0540, Test Loss: 1.0721\n",
      "Epoch 11/69, Train Loss: 1.0633, Test Loss: 1.0618\n",
      "Epoch 12/69, Train Loss: 1.1000, Test Loss: 1.0684\n",
      "Epoch 13/69, Train Loss: 1.0411, Test Loss: 1.0601\n",
      "Epoch 14/69, Train Loss: 1.0684, Test Loss: 1.0483\n",
      "Epoch 15/69, Train Loss: 1.0552, Test Loss: 1.0557\n",
      "Epoch 16/69, Train Loss: 1.0411, Test Loss: 1.0479\n",
      "Epoch 17/69, Train Loss: 1.0647, Test Loss: 1.0438\n",
      "Epoch 18/69, Train Loss: 1.0291, Test Loss: 1.0410\n",
      "Epoch 19/69, Train Loss: 1.0577, Test Loss: 1.0348\n",
      "Epoch 20/69, Train Loss: 1.0753, Test Loss: 1.0345\n",
      "Epoch 21/69, Train Loss: 1.0461, Test Loss: 1.0406\n",
      "Epoch 22/69, Train Loss: 1.0520, Test Loss: 1.0375\n",
      "Epoch 23/69, Train Loss: 1.0332, Test Loss: 1.0348\n",
      "Epoch 24/69, Train Loss: 1.0253, Test Loss: 1.0383\n",
      "Epoch 25/69, Train Loss: 0.9948, Test Loss: 1.0420\n",
      "Epoch 26/69, Train Loss: 1.0649, Test Loss: 1.0411\n",
      "Epoch 27/69, Train Loss: 1.0047, Test Loss: 1.0467\n",
      "Epoch 28/69, Train Loss: 1.0295, Test Loss: 1.0482\n",
      "Epoch 29/69, Train Loss: 1.0436, Test Loss: 1.0498\n",
      "Epoch 30/69, Train Loss: 1.0107, Test Loss: 1.0519\n",
      "Epoch 31/69, Train Loss: 1.0126, Test Loss: 1.0480\n",
      "Epoch 32/69, Train Loss: 1.0144, Test Loss: 1.0368\n",
      "Epoch 33/69, Train Loss: 0.9972, Test Loss: 1.0350\n",
      "Epoch 34/69, Train Loss: 1.0499, Test Loss: 1.0396\n",
      "Epoch 35/69, Train Loss: 1.0331, Test Loss: 1.0418\n",
      "Epoch 36/69, Train Loss: 1.0069, Test Loss: 1.0371\n",
      "Epoch 37/69, Train Loss: 1.0054, Test Loss: 1.0411\n",
      "Epoch 38/69, Train Loss: 1.0067, Test Loss: 1.0360\n",
      "Epoch 39/69, Train Loss: 1.0500, Test Loss: 1.0402\n",
      "Epoch 40/69, Train Loss: 1.0085, Test Loss: 1.0382\n",
      "Epoch 41/69, Train Loss: 1.0164, Test Loss: 1.0347\n",
      "Epoch 42/69, Train Loss: 1.0002, Test Loss: 1.0343\n",
      "Epoch 43/69, Train Loss: 0.9925, Test Loss: 1.0374\n",
      "Epoch 44/69, Train Loss: 0.9720, Test Loss: 1.0286\n",
      "Epoch 45/69, Train Loss: 1.0401, Test Loss: 1.0279\n",
      "Epoch 46/69, Train Loss: 1.0241, Test Loss: 1.0292\n",
      "Epoch 47/69, Train Loss: 1.0327, Test Loss: 1.0277\n",
      "Epoch 48/69, Train Loss: 1.0035, Test Loss: 1.0349\n",
      "Epoch 49/69, Train Loss: 0.9814, Test Loss: 1.0312\n",
      "Epoch 50/69, Train Loss: 0.9968, Test Loss: 1.0302\n",
      "Epoch 51/69, Train Loss: 0.9920, Test Loss: 1.0291\n",
      "Epoch 52/69, Train Loss: 1.0056, Test Loss: 1.0320\n",
      "Epoch 53/69, Train Loss: 0.9960, Test Loss: 1.0261\n",
      "Epoch 54/69, Train Loss: 1.0233, Test Loss: 1.0303\n",
      "Epoch 55/69, Train Loss: 0.9945, Test Loss: 1.0209\n",
      "Epoch 56/69, Train Loss: 0.9801, Test Loss: 1.0273\n",
      "Epoch 57/69, Train Loss: 0.9907, Test Loss: 1.0276\n",
      "Epoch 58/69, Train Loss: 1.0206, Test Loss: 1.0251\n",
      "Epoch 59/69, Train Loss: 1.0224, Test Loss: 1.0242\n",
      "Epoch 60/69, Train Loss: 0.9537, Test Loss: 1.0263\n",
      "Epoch 61/69, Train Loss: 1.0045, Test Loss: 1.0287\n",
      "Epoch 62/69, Train Loss: 0.9935, Test Loss: 1.0274\n",
      "Epoch 63/69, Train Loss: 0.9885, Test Loss: 1.0298\n",
      "Epoch 64/69, Train Loss: 0.9912, Test Loss: 1.0308\n",
      "Epoch 65/69, Train Loss: 0.9808, Test Loss: 1.0327\n",
      "Epoch 66/69, Train Loss: 0.9706, Test Loss: 1.0267\n",
      "Epoch 67/69, Train Loss: 1.0225, Test Loss: 1.0282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:50:20,769] Trial 47 finished with value: 1.034315139055252 and parameters: {'num_hidden_layers': 11, 'layer_0_size': 174, 'layer_1_size': 174, 'layer_2_size': 136, 'layer_3_size': 133, 'layer_4_size': 51, 'layer_5_size': 231, 'layer_6_size': 155, 'layer_7_size': 121, 'layer_8_size': 75, 'layer_9_size': 119, 'layer_10_size': 115, 'dropout_rate': 0.3495774893177823, 'learning_rate': 0.00010742188839835265, 'batch_size': 64, 'epochs': 69}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/69, Train Loss: 0.9956, Test Loss: 1.0278\n",
      "Epoch 69/69, Train Loss: 0.9831, Test Loss: 1.0343\n",
      "Epoch 1/5, Train Loss: 1.2230, Test Loss: 1.0311\n",
      "Epoch 2/5, Train Loss: 1.1961, Test Loss: 1.0369\n",
      "Epoch 3/5, Train Loss: 1.2079, Test Loss: 1.0553\n",
      "Epoch 4/5, Train Loss: 1.2234, Test Loss: 1.0575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:50:22,195] Trial 48 finished with value: 1.0452173735414232 and parameters: {'num_hidden_layers': 17, 'layer_0_size': 135, 'layer_1_size': 212, 'layer_2_size': 193, 'layer_3_size': 158, 'layer_4_size': 141, 'layer_5_size': 121, 'layer_6_size': 33, 'layer_7_size': 243, 'layer_8_size': 106, 'layer_9_size': 50, 'layer_10_size': 153, 'layer_11_size': 103, 'layer_12_size': 228, 'layer_13_size': 141, 'layer_14_size': 74, 'layer_15_size': 73, 'layer_16_size': 103, 'dropout_rate': 0.2804470115928093, 'learning_rate': 1.9528948551422928e-05, 'batch_size': 32, 'epochs': 5}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Train Loss: 1.1669, Test Loss: 1.0452\n",
      "Epoch 1/78, Train Loss: 1.2108, Test Loss: 1.3974\n",
      "Epoch 2/78, Train Loss: 1.2037, Test Loss: 1.4344\n",
      "Epoch 3/78, Train Loss: 1.1756, Test Loss: 1.4550\n",
      "Epoch 4/78, Train Loss: 1.1563, Test Loss: 1.4090\n",
      "Epoch 5/78, Train Loss: 1.1868, Test Loss: 1.4248\n",
      "Epoch 6/78, Train Loss: 1.1551, Test Loss: 1.4136\n",
      "Epoch 7/78, Train Loss: 1.2054, Test Loss: 1.4370\n",
      "Epoch 8/78, Train Loss: 1.2023, Test Loss: 1.4137\n",
      "Epoch 9/78, Train Loss: 1.2306, Test Loss: 1.4177\n",
      "Epoch 10/78, Train Loss: 1.1208, Test Loss: 1.4225\n",
      "Epoch 11/78, Train Loss: 1.1046, Test Loss: 1.4256\n",
      "Epoch 12/78, Train Loss: 1.1391, Test Loss: 1.4251\n",
      "Epoch 13/78, Train Loss: 1.1412, Test Loss: 1.4294\n",
      "Epoch 14/78, Train Loss: 1.1033, Test Loss: 1.4279\n",
      "Epoch 15/78, Train Loss: 1.0960, Test Loss: 1.4233\n",
      "Epoch 16/78, Train Loss: 1.0841, Test Loss: 1.4034\n",
      "Epoch 17/78, Train Loss: 1.1862, Test Loss: 1.4192\n",
      "Epoch 18/78, Train Loss: 1.0868, Test Loss: 1.4071\n",
      "Epoch 19/78, Train Loss: 1.0880, Test Loss: 1.3986\n",
      "Epoch 20/78, Train Loss: 1.1517, Test Loss: 1.4033\n",
      "Epoch 21/78, Train Loss: 1.1387, Test Loss: 1.4105\n",
      "Epoch 22/78, Train Loss: 1.0502, Test Loss: 1.3999\n",
      "Epoch 23/78, Train Loss: 1.0844, Test Loss: 1.4087\n",
      "Epoch 24/78, Train Loss: 1.0688, Test Loss: 1.4078\n",
      "Epoch 25/78, Train Loss: 1.0984, Test Loss: 1.3867\n",
      "Epoch 26/78, Train Loss: 1.1209, Test Loss: 1.4166\n",
      "Epoch 27/78, Train Loss: 1.1169, Test Loss: 1.4138\n",
      "Epoch 28/78, Train Loss: 1.0835, Test Loss: 1.3952\n",
      "Epoch 29/78, Train Loss: 1.1319, Test Loss: 1.4011\n",
      "Epoch 30/78, Train Loss: 1.0776, Test Loss: 1.3943\n",
      "Epoch 31/78, Train Loss: 1.1141, Test Loss: 1.3820\n",
      "Epoch 32/78, Train Loss: 1.1210, Test Loss: 1.3849\n",
      "Epoch 33/78, Train Loss: 1.0803, Test Loss: 1.4004\n",
      "Epoch 34/78, Train Loss: 1.0976, Test Loss: 1.4062\n",
      "Epoch 35/78, Train Loss: 1.1076, Test Loss: 1.3980\n",
      "Epoch 36/78, Train Loss: 1.0902, Test Loss: 1.3881\n",
      "Epoch 37/78, Train Loss: 1.0897, Test Loss: 1.3992\n",
      "Epoch 38/78, Train Loss: 1.0688, Test Loss: 1.3938\n",
      "Epoch 39/78, Train Loss: 1.0935, Test Loss: 1.3870\n",
      "Epoch 40/78, Train Loss: 1.0843, Test Loss: 1.3946\n",
      "Epoch 41/78, Train Loss: 1.0985, Test Loss: 1.4015\n",
      "Epoch 42/78, Train Loss: 1.1158, Test Loss: 1.3906\n",
      "Epoch 43/78, Train Loss: 1.1084, Test Loss: 1.4005\n",
      "Epoch 44/78, Train Loss: 1.0756, Test Loss: 1.3974\n",
      "Epoch 45/78, Train Loss: 1.0629, Test Loss: 1.3878\n",
      "Epoch 46/78, Train Loss: 1.1031, Test Loss: 1.3697\n",
      "Epoch 47/78, Train Loss: 1.0959, Test Loss: 1.3928\n",
      "Epoch 48/78, Train Loss: 1.1043, Test Loss: 1.3725\n",
      "Epoch 49/78, Train Loss: 1.0954, Test Loss: 1.3846\n",
      "Epoch 50/78, Train Loss: 1.1003, Test Loss: 1.3907\n",
      "Epoch 51/78, Train Loss: 1.0522, Test Loss: 1.3991\n",
      "Epoch 52/78, Train Loss: 1.0776, Test Loss: 1.3852\n",
      "Epoch 53/78, Train Loss: 1.0625, Test Loss: 1.3910\n",
      "Epoch 54/78, Train Loss: 1.0597, Test Loss: 1.3813\n",
      "Epoch 55/78, Train Loss: 1.0574, Test Loss: 1.3838\n",
      "Epoch 56/78, Train Loss: 1.0841, Test Loss: 1.3800\n",
      "Epoch 57/78, Train Loss: 1.0522, Test Loss: 1.3915\n",
      "Epoch 58/78, Train Loss: 1.0593, Test Loss: 1.3829\n",
      "Epoch 59/78, Train Loss: 1.0860, Test Loss: 1.3914\n",
      "Epoch 60/78, Train Loss: 1.0596, Test Loss: 1.3870\n",
      "Epoch 61/78, Train Loss: 1.0690, Test Loss: 1.3887\n",
      "Epoch 62/78, Train Loss: 1.1045, Test Loss: 1.4033\n",
      "Epoch 63/78, Train Loss: 1.0822, Test Loss: 1.3869\n",
      "Epoch 64/78, Train Loss: 1.1099, Test Loss: 1.3856\n",
      "Epoch 65/78, Train Loss: 1.0146, Test Loss: 1.3885\n",
      "Epoch 66/78, Train Loss: 1.0489, Test Loss: 1.3810\n",
      "Epoch 67/78, Train Loss: 1.0724, Test Loss: 1.3902\n",
      "Epoch 68/78, Train Loss: 1.1419, Test Loss: 1.3866\n",
      "Epoch 69/78, Train Loss: 1.0403, Test Loss: 1.3791\n",
      "Epoch 70/78, Train Loss: 1.0851, Test Loss: 1.3949\n",
      "Epoch 71/78, Train Loss: 1.0923, Test Loss: 1.3818\n",
      "Epoch 72/78, Train Loss: 1.0450, Test Loss: 1.4003\n",
      "Epoch 73/78, Train Loss: 1.0723, Test Loss: 1.3960\n",
      "Epoch 74/78, Train Loss: 1.1217, Test Loss: 1.3833\n",
      "Epoch 75/78, Train Loss: 1.0586, Test Loss: 1.3918\n",
      "Epoch 76/78, Train Loss: 1.0843, Test Loss: 1.3858\n",
      "Epoch 77/78, Train Loss: 1.1098, Test Loss: 1.3824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:50:40,668] Trial 49 finished with value: 1.3859471763883318 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 166, 'layer_1_size': 36, 'layer_2_size': 107, 'layer_3_size': 100, 'layer_4_size': 103, 'layer_5_size': 253, 'layer_6_size': 107, 'layer_7_size': 55, 'layer_8_size': 166, 'layer_9_size': 209, 'layer_10_size': 128, 'layer_11_size': 127, 'layer_12_size': 69, 'layer_13_size': 208, 'dropout_rate': 0.16735362895587502, 'learning_rate': 1.0183947225408868e-05, 'batch_size': 32, 'epochs': 78}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/78, Train Loss: 1.0399, Test Loss: 1.3859\n",
      "Epoch 1/99, Train Loss: 1.0719, Test Loss: 0.8247\n",
      "Epoch 2/99, Train Loss: 1.0494, Test Loss: 0.7921\n",
      "Epoch 3/99, Train Loss: 1.0230, Test Loss: 0.8198\n",
      "Epoch 4/99, Train Loss: 0.9704, Test Loss: 0.8011\n",
      "Epoch 5/99, Train Loss: 0.9732, Test Loss: 0.8172\n",
      "Epoch 6/99, Train Loss: 0.9585, Test Loss: 0.8013\n",
      "Epoch 7/99, Train Loss: 0.9710, Test Loss: 0.7982\n",
      "Epoch 8/99, Train Loss: 0.9981, Test Loss: 0.7991\n",
      "Epoch 9/99, Train Loss: 0.9844, Test Loss: 0.8112\n",
      "Epoch 10/99, Train Loss: 0.9832, Test Loss: 0.8134\n",
      "Epoch 11/99, Train Loss: 0.9687, Test Loss: 0.8050\n",
      "Epoch 12/99, Train Loss: 0.9501, Test Loss: 0.8014\n",
      "Epoch 13/99, Train Loss: 0.9630, Test Loss: 0.8098\n",
      "Epoch 14/99, Train Loss: 0.9366, Test Loss: 0.8138\n",
      "Epoch 15/99, Train Loss: 0.9239, Test Loss: 0.8086\n",
      "Epoch 16/99, Train Loss: 0.9193, Test Loss: 0.8249\n",
      "Epoch 17/99, Train Loss: 0.9334, Test Loss: 0.8082\n",
      "Epoch 18/99, Train Loss: 0.9289, Test Loss: 0.8534\n",
      "Epoch 19/99, Train Loss: 0.9056, Test Loss: 0.8320\n",
      "Epoch 20/99, Train Loss: 0.9233, Test Loss: 0.8285\n",
      "Epoch 21/99, Train Loss: 0.8835, Test Loss: 0.8789\n",
      "Epoch 22/99, Train Loss: 0.8725, Test Loss: 0.8893\n",
      "Epoch 23/99, Train Loss: 0.8460, Test Loss: 0.9158\n",
      "Epoch 24/99, Train Loss: 0.8582, Test Loss: 0.8193\n",
      "Epoch 25/99, Train Loss: 0.8246, Test Loss: 0.9012\n",
      "Epoch 26/99, Train Loss: 0.8031, Test Loss: 0.8687\n",
      "Epoch 27/99, Train Loss: 0.8052, Test Loss: 0.8343\n",
      "Epoch 28/99, Train Loss: 0.7861, Test Loss: 0.9151\n",
      "Epoch 29/99, Train Loss: 0.7699, Test Loss: 0.9122\n",
      "Epoch 30/99, Train Loss: 0.7137, Test Loss: 0.8674\n",
      "Epoch 31/99, Train Loss: 0.7574, Test Loss: 0.9137\n",
      "Epoch 32/99, Train Loss: 0.7243, Test Loss: 0.9825\n",
      "Epoch 33/99, Train Loss: 0.6852, Test Loss: 0.8992\n",
      "Epoch 34/99, Train Loss: 0.6782, Test Loss: 0.9814\n",
      "Epoch 35/99, Train Loss: 0.7208, Test Loss: 0.8975\n",
      "Epoch 36/99, Train Loss: 0.6639, Test Loss: 0.9560\n",
      "Epoch 37/99, Train Loss: 0.6647, Test Loss: 0.9651\n",
      "Epoch 38/99, Train Loss: 0.6698, Test Loss: 0.9917\n",
      "Epoch 39/99, Train Loss: 0.6370, Test Loss: 0.9038\n",
      "Epoch 40/99, Train Loss: 0.6200, Test Loss: 0.9394\n",
      "Epoch 41/99, Train Loss: 0.6147, Test Loss: 0.9932\n",
      "Epoch 42/99, Train Loss: 0.5839, Test Loss: 1.0161\n",
      "Epoch 43/99, Train Loss: 0.5424, Test Loss: 1.0221\n",
      "Epoch 44/99, Train Loss: 0.6410, Test Loss: 0.8985\n",
      "Epoch 45/99, Train Loss: 0.6250, Test Loss: 0.8522\n",
      "Epoch 46/99, Train Loss: 0.6145, Test Loss: 0.8956\n",
      "Epoch 47/99, Train Loss: 0.5856, Test Loss: 1.0175\n",
      "Epoch 48/99, Train Loss: 0.5387, Test Loss: 0.9638\n",
      "Epoch 49/99, Train Loss: 0.5199, Test Loss: 0.9537\n",
      "Epoch 50/99, Train Loss: 0.5246, Test Loss: 0.8804\n",
      "Epoch 51/99, Train Loss: 0.5319, Test Loss: 0.9156\n",
      "Epoch 52/99, Train Loss: 0.5177, Test Loss: 0.9563\n",
      "Epoch 53/99, Train Loss: 0.5276, Test Loss: 0.9103\n",
      "Epoch 54/99, Train Loss: 0.5490, Test Loss: 0.9608\n",
      "Epoch 55/99, Train Loss: 0.4796, Test Loss: 1.0093\n",
      "Epoch 56/99, Train Loss: 0.4910, Test Loss: 0.9856\n",
      "Epoch 57/99, Train Loss: 0.4972, Test Loss: 0.8492\n",
      "Epoch 58/99, Train Loss: 0.4472, Test Loss: 0.8957\n",
      "Epoch 59/99, Train Loss: 0.4754, Test Loss: 0.9154\n",
      "Epoch 60/99, Train Loss: 0.4625, Test Loss: 0.9730\n",
      "Epoch 61/99, Train Loss: 0.4477, Test Loss: 0.9525\n",
      "Epoch 62/99, Train Loss: 0.4171, Test Loss: 0.9808\n",
      "Epoch 63/99, Train Loss: 0.4519, Test Loss: 0.9368\n",
      "Epoch 64/99, Train Loss: 0.4294, Test Loss: 1.0152\n",
      "Epoch 65/99, Train Loss: 0.4336, Test Loss: 0.9242\n",
      "Epoch 66/99, Train Loss: 0.4269, Test Loss: 0.9967\n",
      "Epoch 67/99, Train Loss: 0.3952, Test Loss: 1.0224\n",
      "Epoch 68/99, Train Loss: 0.3909, Test Loss: 0.9360\n",
      "Epoch 69/99, Train Loss: 0.4141, Test Loss: 0.9694\n",
      "Epoch 70/99, Train Loss: 0.3952, Test Loss: 1.0497\n",
      "Epoch 71/99, Train Loss: 0.4373, Test Loss: 1.0173\n",
      "Epoch 72/99, Train Loss: 0.3757, Test Loss: 1.0186\n",
      "Epoch 73/99, Train Loss: 0.3572, Test Loss: 1.0931\n",
      "Epoch 74/99, Train Loss: 0.3562, Test Loss: 1.0324\n",
      "Epoch 75/99, Train Loss: 0.3963, Test Loss: 1.0356\n",
      "Epoch 76/99, Train Loss: 0.3563, Test Loss: 1.0386\n",
      "Epoch 77/99, Train Loss: 0.3223, Test Loss: 1.0890\n",
      "Epoch 78/99, Train Loss: 0.4003, Test Loss: 1.0510\n",
      "Epoch 79/99, Train Loss: 0.3572, Test Loss: 1.0414\n",
      "Epoch 80/99, Train Loss: 0.3897, Test Loss: 0.9329\n",
      "Epoch 81/99, Train Loss: 0.3685, Test Loss: 1.0420\n",
      "Epoch 82/99, Train Loss: 0.3745, Test Loss: 1.0000\n",
      "Epoch 83/99, Train Loss: 0.3590, Test Loss: 1.0498\n",
      "Epoch 84/99, Train Loss: 0.3404, Test Loss: 0.9480\n",
      "Epoch 85/99, Train Loss: 0.3381, Test Loss: 0.9366\n",
      "Epoch 86/99, Train Loss: 0.3599, Test Loss: 0.9637\n",
      "Epoch 87/99, Train Loss: 0.3792, Test Loss: 0.9995\n",
      "Epoch 88/99, Train Loss: 0.3556, Test Loss: 0.9271\n",
      "Epoch 89/99, Train Loss: 0.3641, Test Loss: 1.0479\n",
      "Epoch 90/99, Train Loss: 0.3561, Test Loss: 0.9917\n",
      "Epoch 91/99, Train Loss: 0.3245, Test Loss: 1.0418\n",
      "Epoch 92/99, Train Loss: 0.3502, Test Loss: 0.9063\n",
      "Epoch 93/99, Train Loss: 0.3171, Test Loss: 0.9931\n",
      "Epoch 94/99, Train Loss: 0.3309, Test Loss: 0.9988\n",
      "Epoch 95/99, Train Loss: 0.3174, Test Loss: 1.0031\n",
      "Epoch 96/99, Train Loss: 0.3483, Test Loss: 0.9950\n",
      "Epoch 97/99, Train Loss: 0.3827, Test Loss: 0.9581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:50:58,266] Trial 50 finished with value: 1.1123542104448592 and parameters: {'num_hidden_layers': 10, 'layer_0_size': 187, 'layer_1_size': 129, 'layer_2_size': 161, 'layer_3_size': 84, 'layer_4_size': 77, 'layer_5_size': 139, 'layer_6_size': 228, 'layer_7_size': 101, 'layer_8_size': 53, 'layer_9_size': 184, 'dropout_rate': 0.3192088903527409, 'learning_rate': 0.0031194241223218735, 'batch_size': 32, 'epochs': 99}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/99, Train Loss: 0.3197, Test Loss: 1.0309\n",
      "Epoch 99/99, Train Loss: 0.3285, Test Loss: 1.1124\n",
      "Epoch 1/86, Train Loss: 1.2780, Test Loss: 0.9819\n",
      "Epoch 2/86, Train Loss: 1.2780, Test Loss: 0.9814\n",
      "Epoch 3/86, Train Loss: 1.2787, Test Loss: 0.9812\n",
      "Epoch 4/86, Train Loss: 1.2317, Test Loss: 0.9816\n",
      "Epoch 5/86, Train Loss: 1.1941, Test Loss: 0.9832\n",
      "Epoch 6/86, Train Loss: 1.2133, Test Loss: 0.9899\n",
      "Epoch 7/86, Train Loss: 1.2187, Test Loss: 0.9970\n",
      "Epoch 8/86, Train Loss: 1.1880, Test Loss: 1.0020\n",
      "Epoch 9/86, Train Loss: 1.2166, Test Loss: 1.0064\n",
      "Epoch 10/86, Train Loss: 1.2209, Test Loss: 1.0112\n",
      "Epoch 11/86, Train Loss: 1.2035, Test Loss: 1.0158\n",
      "Epoch 12/86, Train Loss: 1.2897, Test Loss: 1.0175\n",
      "Epoch 13/86, Train Loss: 1.2278, Test Loss: 1.0168\n",
      "Epoch 14/86, Train Loss: 1.1942, Test Loss: 1.0184\n",
      "Epoch 15/86, Train Loss: 1.2362, Test Loss: 1.0188\n",
      "Epoch 16/86, Train Loss: 1.2027, Test Loss: 1.0155\n",
      "Epoch 17/86, Train Loss: 1.2070, Test Loss: 1.0170\n",
      "Epoch 18/86, Train Loss: 1.2217, Test Loss: 1.0165\n",
      "Epoch 19/86, Train Loss: 1.1637, Test Loss: 1.0180\n",
      "Epoch 20/86, Train Loss: 1.2100, Test Loss: 1.0196\n",
      "Epoch 21/86, Train Loss: 1.2117, Test Loss: 1.0205\n",
      "Epoch 22/86, Train Loss: 1.1572, Test Loss: 1.0204\n",
      "Epoch 23/86, Train Loss: 1.1691, Test Loss: 1.0201\n",
      "Epoch 24/86, Train Loss: 1.2051, Test Loss: 1.0193\n",
      "Epoch 25/86, Train Loss: 1.1633, Test Loss: 1.0188\n",
      "Epoch 26/86, Train Loss: 1.1813, Test Loss: 1.0174\n",
      "Epoch 27/86, Train Loss: 1.1956, Test Loss: 1.0177\n",
      "Epoch 28/86, Train Loss: 1.1666, Test Loss: 1.0180\n",
      "Epoch 29/86, Train Loss: 1.1866, Test Loss: 1.0188\n",
      "Epoch 30/86, Train Loss: 1.2056, Test Loss: 1.0188\n",
      "Epoch 31/86, Train Loss: 1.1832, Test Loss: 1.0175\n",
      "Epoch 32/86, Train Loss: 1.1841, Test Loss: 1.0182\n",
      "Epoch 33/86, Train Loss: 1.1751, Test Loss: 1.0203\n",
      "Epoch 34/86, Train Loss: 1.1839, Test Loss: 1.0195\n",
      "Epoch 35/86, Train Loss: 1.1714, Test Loss: 1.0184\n",
      "Epoch 36/86, Train Loss: 1.1764, Test Loss: 1.0182\n",
      "Epoch 37/86, Train Loss: 1.1693, Test Loss: 1.0175\n",
      "Epoch 38/86, Train Loss: 1.1867, Test Loss: 1.0182\n",
      "Epoch 39/86, Train Loss: 1.1964, Test Loss: 1.0164\n",
      "Epoch 40/86, Train Loss: 1.1600, Test Loss: 1.0157\n",
      "Epoch 41/86, Train Loss: 1.2086, Test Loss: 1.0167\n",
      "Epoch 42/86, Train Loss: 1.1646, Test Loss: 1.0162\n",
      "Epoch 43/86, Train Loss: 1.1705, Test Loss: 1.0163\n",
      "Epoch 44/86, Train Loss: 1.1877, Test Loss: 1.0165\n",
      "Epoch 45/86, Train Loss: 1.1842, Test Loss: 1.0171\n",
      "Epoch 46/86, Train Loss: 1.1874, Test Loss: 1.0150\n",
      "Epoch 47/86, Train Loss: 1.1683, Test Loss: 1.0141\n",
      "Epoch 48/86, Train Loss: 1.1472, Test Loss: 1.0148\n",
      "Epoch 49/86, Train Loss: 1.1490, Test Loss: 1.0164\n",
      "Epoch 50/86, Train Loss: 1.1635, Test Loss: 1.0158\n",
      "Epoch 51/86, Train Loss: 1.1307, Test Loss: 1.0174\n",
      "Epoch 52/86, Train Loss: 1.1703, Test Loss: 1.0175\n",
      "Epoch 53/86, Train Loss: 1.1207, Test Loss: 1.0163\n",
      "Epoch 54/86, Train Loss: 1.2011, Test Loss: 1.0162\n",
      "Epoch 55/86, Train Loss: 1.1255, Test Loss: 1.0148\n",
      "Epoch 56/86, Train Loss: 1.1807, Test Loss: 1.0162\n",
      "Epoch 57/86, Train Loss: 1.1772, Test Loss: 1.0144\n",
      "Epoch 58/86, Train Loss: 1.2051, Test Loss: 1.0139\n",
      "Epoch 59/86, Train Loss: 1.1668, Test Loss: 1.0165\n",
      "Epoch 60/86, Train Loss: 1.1666, Test Loss: 1.0163\n",
      "Epoch 61/86, Train Loss: 1.1998, Test Loss: 1.0172\n",
      "Epoch 62/86, Train Loss: 1.1725, Test Loss: 1.0164\n",
      "Epoch 63/86, Train Loss: 1.1927, Test Loss: 1.0164\n",
      "Epoch 64/86, Train Loss: 1.1390, Test Loss: 1.0154\n",
      "Epoch 65/86, Train Loss: 1.1249, Test Loss: 1.0142\n",
      "Epoch 66/86, Train Loss: 1.1827, Test Loss: 1.0144\n",
      "Epoch 67/86, Train Loss: 1.1362, Test Loss: 1.0159\n",
      "Epoch 68/86, Train Loss: 1.1708, Test Loss: 1.0160\n",
      "Epoch 69/86, Train Loss: 1.1683, Test Loss: 1.0184\n",
      "Epoch 70/86, Train Loss: 1.1867, Test Loss: 1.0176\n",
      "Epoch 71/86, Train Loss: 1.1893, Test Loss: 1.0169\n",
      "Epoch 72/86, Train Loss: 1.1584, Test Loss: 1.0173\n",
      "Epoch 73/86, Train Loss: 1.1351, Test Loss: 1.0156\n",
      "Epoch 74/86, Train Loss: 1.1970, Test Loss: 1.0167\n",
      "Epoch 75/86, Train Loss: 1.1431, Test Loss: 1.0177\n",
      "Epoch 76/86, Train Loss: 1.1578, Test Loss: 1.0187\n",
      "Epoch 77/86, Train Loss: 1.1360, Test Loss: 1.0169\n",
      "Epoch 78/86, Train Loss: 1.1695, Test Loss: 1.0161\n",
      "Epoch 79/86, Train Loss: 1.1362, Test Loss: 1.0150\n",
      "Epoch 80/86, Train Loss: 1.2108, Test Loss: 1.0165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:51:01,863] Trial 51 finished with value: 1.0183501243591309 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 118, 'layer_1_size': 196, 'layer_2_size': 115, 'layer_3_size': 228, 'layer_4_size': 89, 'layer_5_size': 167, 'layer_6_size': 95, 'layer_7_size': 253, 'dropout_rate': 0.23953863076799997, 'learning_rate': 2.616723594257425e-05, 'batch_size': 256, 'epochs': 86}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/86, Train Loss: 1.1233, Test Loss: 1.0173\n",
      "Epoch 82/86, Train Loss: 1.1764, Test Loss: 1.0165\n",
      "Epoch 83/86, Train Loss: 1.1399, Test Loss: 1.0146\n",
      "Epoch 84/86, Train Loss: 1.2009, Test Loss: 1.0157\n",
      "Epoch 85/86, Train Loss: 1.1813, Test Loss: 1.0173\n",
      "Epoch 86/86, Train Loss: 1.1377, Test Loss: 1.0184\n",
      "Epoch 1/75, Train Loss: 1.2028, Test Loss: 1.0684\n",
      "Epoch 2/75, Train Loss: 1.2026, Test Loss: 1.0670\n",
      "Epoch 3/75, Train Loss: 1.2457, Test Loss: 1.0677\n",
      "Epoch 4/75, Train Loss: 1.2428, Test Loss: 1.0701\n",
      "Epoch 5/75, Train Loss: 1.2100, Test Loss: 1.0736\n",
      "Epoch 6/75, Train Loss: 1.2581, Test Loss: 1.0766\n",
      "Epoch 7/75, Train Loss: 1.1805, Test Loss: 1.0810\n",
      "Epoch 8/75, Train Loss: 1.2435, Test Loss: 1.0854\n",
      "Epoch 9/75, Train Loss: 1.1635, Test Loss: 1.0851\n",
      "Epoch 10/75, Train Loss: 1.2108, Test Loss: 1.0854\n",
      "Epoch 11/75, Train Loss: 1.2214, Test Loss: 1.0859\n",
      "Epoch 12/75, Train Loss: 1.1430, Test Loss: 1.0875\n",
      "Epoch 13/75, Train Loss: 1.1777, Test Loss: 1.0878\n",
      "Epoch 14/75, Train Loss: 1.1703, Test Loss: 1.0882\n",
      "Epoch 15/75, Train Loss: 1.1726, Test Loss: 1.0871\n",
      "Epoch 16/75, Train Loss: 1.1951, Test Loss: 1.0877\n",
      "Epoch 17/75, Train Loss: 1.2460, Test Loss: 1.0878\n",
      "Epoch 18/75, Train Loss: 1.1426, Test Loss: 1.0873\n",
      "Epoch 19/75, Train Loss: 1.2011, Test Loss: 1.0877\n",
      "Epoch 20/75, Train Loss: 1.1711, Test Loss: 1.0891\n",
      "Epoch 21/75, Train Loss: 1.1928, Test Loss: 1.0892\n",
      "Epoch 22/75, Train Loss: 1.1643, Test Loss: 1.0911\n",
      "Epoch 23/75, Train Loss: 1.1618, Test Loss: 1.0919\n",
      "Epoch 24/75, Train Loss: 1.2474, Test Loss: 1.0913\n",
      "Epoch 25/75, Train Loss: 1.2109, Test Loss: 1.0917\n",
      "Epoch 26/75, Train Loss: 1.1730, Test Loss: 1.0883\n",
      "Epoch 27/75, Train Loss: 1.1948, Test Loss: 1.0910\n",
      "Epoch 28/75, Train Loss: 1.1355, Test Loss: 1.0897\n",
      "Epoch 29/75, Train Loss: 1.2079, Test Loss: 1.0912\n",
      "Epoch 30/75, Train Loss: 1.1913, Test Loss: 1.0895\n",
      "Epoch 31/75, Train Loss: 1.1864, Test Loss: 1.0872\n",
      "Epoch 32/75, Train Loss: 1.1756, Test Loss: 1.0863\n",
      "Epoch 33/75, Train Loss: 1.2208, Test Loss: 1.0858\n",
      "Epoch 34/75, Train Loss: 1.1309, Test Loss: 1.0859\n",
      "Epoch 35/75, Train Loss: 1.1627, Test Loss: 1.0872\n",
      "Epoch 36/75, Train Loss: 1.1881, Test Loss: 1.0899\n",
      "Epoch 37/75, Train Loss: 1.1931, Test Loss: 1.0877\n",
      "Epoch 38/75, Train Loss: 1.1876, Test Loss: 1.0885\n",
      "Epoch 39/75, Train Loss: 1.1950, Test Loss: 1.0864\n",
      "Epoch 40/75, Train Loss: 1.1960, Test Loss: 1.0865\n",
      "Epoch 41/75, Train Loss: 1.2051, Test Loss: 1.0863\n",
      "Epoch 42/75, Train Loss: 1.1484, Test Loss: 1.0851\n",
      "Epoch 43/75, Train Loss: 1.1768, Test Loss: 1.0847\n",
      "Epoch 44/75, Train Loss: 1.1678, Test Loss: 1.0840\n",
      "Epoch 45/75, Train Loss: 1.1773, Test Loss: 1.0847\n",
      "Epoch 46/75, Train Loss: 1.1633, Test Loss: 1.0854\n",
      "Epoch 47/75, Train Loss: 1.1638, Test Loss: 1.0840\n",
      "Epoch 48/75, Train Loss: 1.1261, Test Loss: 1.0848\n",
      "Epoch 49/75, Train Loss: 1.2003, Test Loss: 1.0863\n",
      "Epoch 50/75, Train Loss: 1.1495, Test Loss: 1.0858\n",
      "Epoch 51/75, Train Loss: 1.2056, Test Loss: 1.0855\n",
      "Epoch 52/75, Train Loss: 1.1519, Test Loss: 1.0842\n",
      "Epoch 53/75, Train Loss: 1.1900, Test Loss: 1.0861\n",
      "Epoch 54/75, Train Loss: 1.2120, Test Loss: 1.0867\n",
      "Epoch 55/75, Train Loss: 1.1540, Test Loss: 1.0851\n",
      "Epoch 56/75, Train Loss: 1.1321, Test Loss: 1.0845\n",
      "Epoch 57/75, Train Loss: 1.1783, Test Loss: 1.0832\n",
      "Epoch 58/75, Train Loss: 1.1321, Test Loss: 1.0828\n",
      "Epoch 59/75, Train Loss: 1.1751, Test Loss: 1.0810\n",
      "Epoch 60/75, Train Loss: 1.1421, Test Loss: 1.0814\n",
      "Epoch 61/75, Train Loss: 1.1423, Test Loss: 1.0819\n",
      "Epoch 62/75, Train Loss: 1.1204, Test Loss: 1.0833\n",
      "Epoch 63/75, Train Loss: 1.1398, Test Loss: 1.0843\n",
      "Epoch 64/75, Train Loss: 1.1553, Test Loss: 1.0838\n",
      "Epoch 65/75, Train Loss: 1.2118, Test Loss: 1.0833\n",
      "Epoch 66/75, Train Loss: 1.1300, Test Loss: 1.0850\n",
      "Epoch 67/75, Train Loss: 1.1088, Test Loss: 1.0837\n",
      "Epoch 68/75, Train Loss: 1.1115, Test Loss: 1.0843\n",
      "Epoch 69/75, Train Loss: 1.1506, Test Loss: 1.0815\n",
      "Epoch 70/75, Train Loss: 1.1738, Test Loss: 1.0846\n",
      "Epoch 71/75, Train Loss: 1.1618, Test Loss: 1.0836\n",
      "Epoch 72/75, Train Loss: 1.1288, Test Loss: 1.0837\n",
      "Epoch 73/75, Train Loss: 1.1319, Test Loss: 1.0842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:51:04,863] Trial 52 finished with value: 1.086253046989441 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 106, 'layer_1_size': 196, 'layer_2_size': 118, 'layer_3_size': 221, 'layer_4_size': 91, 'layer_5_size': 178, 'layer_6_size': 78, 'layer_7_size': 256, 'dropout_rate': 0.24898558064510032, 'learning_rate': 2.239787813168144e-05, 'batch_size': 256, 'epochs': 75}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/75, Train Loss: 1.1146, Test Loss: 1.0848\n",
      "Epoch 75/75, Train Loss: 1.1360, Test Loss: 1.0863\n",
      "Epoch 1/80, Train Loss: 1.0824, Test Loss: 0.9401\n",
      "Epoch 2/80, Train Loss: 1.0703, Test Loss: 0.9399\n",
      "Epoch 3/80, Train Loss: 1.1553, Test Loss: 0.9392\n",
      "Epoch 4/80, Train Loss: 1.1302, Test Loss: 0.9376\n",
      "Epoch 5/80, Train Loss: 1.0920, Test Loss: 0.9369\n",
      "Epoch 6/80, Train Loss: 1.0934, Test Loss: 0.9353\n",
      "Epoch 7/80, Train Loss: 1.1072, Test Loss: 0.9355\n",
      "Epoch 8/80, Train Loss: 1.1060, Test Loss: 0.9372\n",
      "Epoch 9/80, Train Loss: 1.0925, Test Loss: 0.9392\n",
      "Epoch 10/80, Train Loss: 1.0755, Test Loss: 0.9408\n",
      "Epoch 11/80, Train Loss: 1.0820, Test Loss: 0.9404\n",
      "Epoch 12/80, Train Loss: 1.0843, Test Loss: 0.9406\n",
      "Epoch 13/80, Train Loss: 1.0939, Test Loss: 0.9421\n",
      "Epoch 14/80, Train Loss: 1.0828, Test Loss: 0.9434\n",
      "Epoch 15/80, Train Loss: 1.1113, Test Loss: 0.9429\n",
      "Epoch 16/80, Train Loss: 1.1071, Test Loss: 0.9422\n",
      "Epoch 17/80, Train Loss: 1.0458, Test Loss: 0.9425\n",
      "Epoch 18/80, Train Loss: 1.0701, Test Loss: 0.9419\n",
      "Epoch 19/80, Train Loss: 1.1121, Test Loss: 0.9410\n",
      "Epoch 20/80, Train Loss: 1.0658, Test Loss: 0.9420\n",
      "Epoch 21/80, Train Loss: 1.0749, Test Loss: 0.9435\n",
      "Epoch 22/80, Train Loss: 1.0709, Test Loss: 0.9425\n",
      "Epoch 23/80, Train Loss: 1.0797, Test Loss: 0.9417\n",
      "Epoch 24/80, Train Loss: 1.1291, Test Loss: 0.9420\n",
      "Epoch 25/80, Train Loss: 1.0876, Test Loss: 0.9425\n",
      "Epoch 26/80, Train Loss: 1.0631, Test Loss: 0.9433\n",
      "Epoch 27/80, Train Loss: 1.0816, Test Loss: 0.9425\n",
      "Epoch 28/80, Train Loss: 1.0715, Test Loss: 0.9444\n",
      "Epoch 29/80, Train Loss: 1.0750, Test Loss: 0.9437\n",
      "Epoch 30/80, Train Loss: 1.0908, Test Loss: 0.9428\n",
      "Epoch 31/80, Train Loss: 1.0877, Test Loss: 0.9425\n",
      "Epoch 32/80, Train Loss: 1.0276, Test Loss: 0.9417\n",
      "Epoch 33/80, Train Loss: 1.0376, Test Loss: 0.9425\n",
      "Epoch 34/80, Train Loss: 1.0949, Test Loss: 0.9424\n",
      "Epoch 35/80, Train Loss: 1.0785, Test Loss: 0.9414\n",
      "Epoch 36/80, Train Loss: 1.0911, Test Loss: 0.9426\n",
      "Epoch 37/80, Train Loss: 1.1016, Test Loss: 0.9428\n",
      "Epoch 38/80, Train Loss: 1.0460, Test Loss: 0.9419\n",
      "Epoch 39/80, Train Loss: 1.0969, Test Loss: 0.9421\n",
      "Epoch 40/80, Train Loss: 1.0841, Test Loss: 0.9427\n",
      "Epoch 41/80, Train Loss: 1.0867, Test Loss: 0.9417\n",
      "Epoch 42/80, Train Loss: 1.0699, Test Loss: 0.9417\n",
      "Epoch 43/80, Train Loss: 1.0802, Test Loss: 0.9430\n",
      "Epoch 44/80, Train Loss: 1.0569, Test Loss: 0.9417\n",
      "Epoch 45/80, Train Loss: 1.0661, Test Loss: 0.9423\n",
      "Epoch 46/80, Train Loss: 1.0587, Test Loss: 0.9427\n",
      "Epoch 47/80, Train Loss: 1.0601, Test Loss: 0.9430\n",
      "Epoch 48/80, Train Loss: 1.0216, Test Loss: 0.9436\n",
      "Epoch 49/80, Train Loss: 1.0511, Test Loss: 0.9435\n",
      "Epoch 50/80, Train Loss: 1.0652, Test Loss: 0.9439\n",
      "Epoch 51/80, Train Loss: 1.0706, Test Loss: 0.9435\n",
      "Epoch 52/80, Train Loss: 1.0528, Test Loss: 0.9421\n",
      "Epoch 53/80, Train Loss: 1.0959, Test Loss: 0.9416\n",
      "Epoch 54/80, Train Loss: 1.0525, Test Loss: 0.9421\n",
      "Epoch 55/80, Train Loss: 1.0512, Test Loss: 0.9417\n",
      "Epoch 56/80, Train Loss: 1.0252, Test Loss: 0.9428\n",
      "Epoch 57/80, Train Loss: 1.0425, Test Loss: 0.9429\n",
      "Epoch 58/80, Train Loss: 1.0560, Test Loss: 0.9440\n",
      "Epoch 59/80, Train Loss: 1.0460, Test Loss: 0.9437\n",
      "Epoch 60/80, Train Loss: 1.0601, Test Loss: 0.9447\n",
      "Epoch 61/80, Train Loss: 1.0681, Test Loss: 0.9432\n",
      "Epoch 62/80, Train Loss: 1.0567, Test Loss: 0.9442\n",
      "Epoch 63/80, Train Loss: 1.0624, Test Loss: 0.9444\n",
      "Epoch 64/80, Train Loss: 1.0390, Test Loss: 0.9435\n",
      "Epoch 65/80, Train Loss: 1.0421, Test Loss: 0.9441\n",
      "Epoch 66/80, Train Loss: 1.0720, Test Loss: 0.9439\n",
      "Epoch 67/80, Train Loss: 1.0953, Test Loss: 0.9442\n",
      "Epoch 68/80, Train Loss: 1.0434, Test Loss: 0.9437\n",
      "Epoch 69/80, Train Loss: 1.0200, Test Loss: 0.9426\n",
      "Epoch 70/80, Train Loss: 1.0615, Test Loss: 0.9435\n",
      "Epoch 71/80, Train Loss: 1.0663, Test Loss: 0.9421\n",
      "Epoch 72/80, Train Loss: 1.0514, Test Loss: 0.9426\n",
      "Epoch 73/80, Train Loss: 1.0195, Test Loss: 0.9420\n",
      "Epoch 74/80, Train Loss: 1.0897, Test Loss: 0.9421\n",
      "Epoch 75/80, Train Loss: 1.0532, Test Loss: 0.9439\n",
      "Epoch 76/80, Train Loss: 1.0614, Test Loss: 0.9440\n",
      "Epoch 77/80, Train Loss: 1.0530, Test Loss: 0.9469\n",
      "Epoch 78/80, Train Loss: 1.0394, Test Loss: 0.9454\n",
      "Epoch 79/80, Train Loss: 1.0559, Test Loss: 0.9439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:51:08,211] Trial 53 finished with value: 0.9438804388046265 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 79, 'layer_1_size': 185, 'layer_2_size': 136, 'layer_3_size': 246, 'layer_4_size': 108, 'layer_5_size': 153, 'layer_6_size': 99, 'layer_7_size': 229, 'dropout_rate': 0.21666831004591097, 'learning_rate': 3.477893335046762e-05, 'batch_size': 256, 'epochs': 80}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/80, Train Loss: 1.0550, Test Loss: 0.9439\n",
      "Epoch 1/47, Train Loss: 1.2561, Test Loss: 0.8944\n",
      "Epoch 2/47, Train Loss: 1.1927, Test Loss: 0.8981\n",
      "Epoch 3/47, Train Loss: 1.2353, Test Loss: 0.9022\n",
      "Epoch 4/47, Train Loss: 1.2577, Test Loss: 0.9043\n",
      "Epoch 5/47, Train Loss: 1.2026, Test Loss: 0.9080\n",
      "Epoch 6/47, Train Loss: 1.1874, Test Loss: 0.9092\n",
      "Epoch 7/47, Train Loss: 1.1924, Test Loss: 0.9109\n",
      "Epoch 8/47, Train Loss: 1.2337, Test Loss: 0.9096\n",
      "Epoch 9/47, Train Loss: 1.2383, Test Loss: 0.9098\n",
      "Epoch 10/47, Train Loss: 1.2660, Test Loss: 0.9092\n",
      "Epoch 11/47, Train Loss: 1.2420, Test Loss: 0.9082\n",
      "Epoch 12/47, Train Loss: 1.2337, Test Loss: 0.9066\n",
      "Epoch 13/47, Train Loss: 1.2173, Test Loss: 0.9084\n",
      "Epoch 14/47, Train Loss: 1.2767, Test Loss: 0.9059\n",
      "Epoch 15/47, Train Loss: 1.1948, Test Loss: 0.9066\n",
      "Epoch 16/47, Train Loss: 1.1956, Test Loss: 0.9079\n",
      "Epoch 17/47, Train Loss: 1.3222, Test Loss: 0.9073\n",
      "Epoch 18/47, Train Loss: 1.2213, Test Loss: 0.9080\n",
      "Epoch 19/47, Train Loss: 1.2316, Test Loss: 0.9083\n",
      "Epoch 20/47, Train Loss: 1.2388, Test Loss: 0.9086\n",
      "Epoch 21/47, Train Loss: 1.2527, Test Loss: 0.9091\n",
      "Epoch 22/47, Train Loss: 1.2244, Test Loss: 0.9081\n",
      "Epoch 23/47, Train Loss: 1.2785, Test Loss: 0.9062\n",
      "Epoch 24/47, Train Loss: 1.2531, Test Loss: 0.9048\n",
      "Epoch 25/47, Train Loss: 1.2474, Test Loss: 0.9054\n",
      "Epoch 26/47, Train Loss: 1.2576, Test Loss: 0.9039\n",
      "Epoch 27/47, Train Loss: 1.2122, Test Loss: 0.9042\n",
      "Epoch 28/47, Train Loss: 1.2204, Test Loss: 0.9035\n",
      "Epoch 29/47, Train Loss: 1.2387, Test Loss: 0.9019\n",
      "Epoch 30/47, Train Loss: 1.2087, Test Loss: 0.9027\n",
      "Epoch 31/47, Train Loss: 1.2254, Test Loss: 0.9040\n",
      "Epoch 32/47, Train Loss: 1.2374, Test Loss: 0.9035\n",
      "Epoch 33/47, Train Loss: 1.2321, Test Loss: 0.9021\n",
      "Epoch 34/47, Train Loss: 1.2208, Test Loss: 0.9037\n",
      "Epoch 35/47, Train Loss: 1.1606, Test Loss: 0.9037\n",
      "Epoch 36/47, Train Loss: 1.1947, Test Loss: 0.9013\n",
      "Epoch 37/47, Train Loss: 1.2348, Test Loss: 0.9019\n",
      "Epoch 38/47, Train Loss: 1.2137, Test Loss: 0.9010\n",
      "Epoch 39/47, Train Loss: 1.2632, Test Loss: 0.8995\n",
      "Epoch 40/47, Train Loss: 1.2384, Test Loss: 0.8988\n",
      "Epoch 41/47, Train Loss: 1.1657, Test Loss: 0.8977\n",
      "Epoch 42/47, Train Loss: 1.2674, Test Loss: 0.8985\n",
      "Epoch 43/47, Train Loss: 1.2238, Test Loss: 0.8993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:51:11,413] Trial 54 finished with value: 0.899110734462738 and parameters: {'num_hidden_layers': 15, 'layer_0_size': 67, 'layer_1_size': 229, 'layer_2_size': 34, 'layer_3_size': 212, 'layer_4_size': 84, 'layer_5_size': 101, 'layer_6_size': 140, 'layer_7_size': 234, 'layer_8_size': 141, 'layer_9_size': 151, 'layer_10_size': 102, 'layer_11_size': 83, 'layer_12_size': 170, 'layer_13_size': 116, 'layer_14_size': 171, 'dropout_rate': 0.29684705959019114, 'learning_rate': 1.4804838137248919e-05, 'batch_size': 256, 'epochs': 47}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/47, Train Loss: 1.2198, Test Loss: 0.8985\n",
      "Epoch 45/47, Train Loss: 1.2200, Test Loss: 0.8980\n",
      "Epoch 46/47, Train Loss: 1.2218, Test Loss: 0.8987\n",
      "Epoch 47/47, Train Loss: 1.2402, Test Loss: 0.8991\n",
      "Epoch 1/91, Train Loss: 1.0689, Test Loss: 1.0308\n",
      "Epoch 2/91, Train Loss: 1.0780, Test Loss: 1.0295\n",
      "Epoch 3/91, Train Loss: 1.0076, Test Loss: 1.0291\n",
      "Epoch 4/91, Train Loss: 1.0495, Test Loss: 1.0290\n",
      "Epoch 5/91, Train Loss: 1.0260, Test Loss: 1.0297\n",
      "Epoch 6/91, Train Loss: 1.0139, Test Loss: 1.0285\n",
      "Epoch 7/91, Train Loss: 0.9826, Test Loss: 1.0256\n",
      "Epoch 8/91, Train Loss: 0.9691, Test Loss: 1.0243\n",
      "Epoch 9/91, Train Loss: 1.0530, Test Loss: 1.0238\n",
      "Epoch 10/91, Train Loss: 0.9801, Test Loss: 1.0220\n",
      "Epoch 11/91, Train Loss: 1.0227, Test Loss: 1.0225\n",
      "Epoch 12/91, Train Loss: 1.0193, Test Loss: 1.0270\n",
      "Epoch 13/91, Train Loss: 0.9927, Test Loss: 1.0287\n",
      "Epoch 14/91, Train Loss: 0.9854, Test Loss: 1.0278\n",
      "Epoch 15/91, Train Loss: 0.9768, Test Loss: 1.0240\n",
      "Epoch 16/91, Train Loss: 1.0046, Test Loss: 1.0215\n",
      "Epoch 17/91, Train Loss: 0.9879, Test Loss: 1.0218\n",
      "Epoch 18/91, Train Loss: 0.9853, Test Loss: 1.0217\n",
      "Epoch 19/91, Train Loss: 0.9720, Test Loss: 1.0222\n",
      "Epoch 20/91, Train Loss: 1.0069, Test Loss: 1.0218\n",
      "Epoch 21/91, Train Loss: 0.9747, Test Loss: 1.0201\n",
      "Epoch 22/91, Train Loss: 0.9652, Test Loss: 1.0237\n",
      "Epoch 23/91, Train Loss: 0.9688, Test Loss: 1.0299\n",
      "Epoch 24/91, Train Loss: 0.9765, Test Loss: 1.0308\n",
      "Epoch 25/91, Train Loss: 0.9744, Test Loss: 1.0302\n",
      "Epoch 26/91, Train Loss: 0.9597, Test Loss: 1.0277\n",
      "Epoch 27/91, Train Loss: 0.9606, Test Loss: 1.0234\n",
      "Epoch 28/91, Train Loss: 0.9827, Test Loss: 1.0219\n",
      "Epoch 29/91, Train Loss: 0.9685, Test Loss: 1.0238\n",
      "Epoch 30/91, Train Loss: 0.9555, Test Loss: 1.0267\n",
      "Epoch 31/91, Train Loss: 0.9822, Test Loss: 1.0257\n",
      "Epoch 32/91, Train Loss: 0.9632, Test Loss: 1.0282\n",
      "Epoch 33/91, Train Loss: 0.9648, Test Loss: 1.0300\n",
      "Epoch 34/91, Train Loss: 0.9720, Test Loss: 1.0277\n",
      "Epoch 35/91, Train Loss: 0.9482, Test Loss: 1.0263\n",
      "Epoch 36/91, Train Loss: 0.9559, Test Loss: 1.0237\n",
      "Epoch 37/91, Train Loss: 0.9711, Test Loss: 1.0219\n",
      "Epoch 38/91, Train Loss: 0.9532, Test Loss: 1.0196\n",
      "Epoch 39/91, Train Loss: 0.9824, Test Loss: 1.0174\n",
      "Epoch 40/91, Train Loss: 0.9475, Test Loss: 1.0167\n",
      "Epoch 41/91, Train Loss: 0.9575, Test Loss: 1.0145\n",
      "Epoch 42/91, Train Loss: 0.9247, Test Loss: 1.0109\n",
      "Epoch 43/91, Train Loss: 0.9519, Test Loss: 1.0095\n",
      "Epoch 44/91, Train Loss: 0.9660, Test Loss: 1.0109\n",
      "Epoch 45/91, Train Loss: 0.9426, Test Loss: 1.0137\n",
      "Epoch 46/91, Train Loss: 0.9421, Test Loss: 1.0152\n",
      "Epoch 47/91, Train Loss: 0.9277, Test Loss: 1.0131\n",
      "Epoch 48/91, Train Loss: 0.9384, Test Loss: 1.0136\n",
      "Epoch 49/91, Train Loss: 0.9267, Test Loss: 1.0118\n",
      "Epoch 50/91, Train Loss: 0.9250, Test Loss: 1.0077\n",
      "Epoch 51/91, Train Loss: 0.9265, Test Loss: 1.0071\n",
      "Epoch 52/91, Train Loss: 0.9405, Test Loss: 1.0085\n",
      "Epoch 53/91, Train Loss: 0.9333, Test Loss: 1.0107\n",
      "Epoch 54/91, Train Loss: 0.9382, Test Loss: 1.0068\n",
      "Epoch 55/91, Train Loss: 0.9316, Test Loss: 1.0049\n",
      "Epoch 56/91, Train Loss: 0.9318, Test Loss: 1.0053\n",
      "Epoch 57/91, Train Loss: 0.9118, Test Loss: 1.0083\n",
      "Epoch 58/91, Train Loss: 0.9205, Test Loss: 1.0114\n",
      "Epoch 59/91, Train Loss: 0.9103, Test Loss: 1.0167\n",
      "Epoch 60/91, Train Loss: 0.9074, Test Loss: 1.0223\n",
      "Epoch 61/91, Train Loss: 0.8672, Test Loss: 1.0247\n",
      "Epoch 62/91, Train Loss: 0.9008, Test Loss: 1.0248\n",
      "Epoch 63/91, Train Loss: 0.9116, Test Loss: 1.0241\n",
      "Epoch 64/91, Train Loss: 0.8657, Test Loss: 1.0235\n",
      "Epoch 65/91, Train Loss: 0.8733, Test Loss: 1.0259\n",
      "Epoch 66/91, Train Loss: 0.9034, Test Loss: 1.0266\n",
      "Epoch 67/91, Train Loss: 0.8819, Test Loss: 1.0318\n",
      "Epoch 68/91, Train Loss: 0.8775, Test Loss: 1.0324\n",
      "Epoch 69/91, Train Loss: 0.8842, Test Loss: 1.0292\n",
      "Epoch 70/91, Train Loss: 0.8925, Test Loss: 1.0284\n",
      "Epoch 71/91, Train Loss: 0.8954, Test Loss: 1.0315\n",
      "Epoch 72/91, Train Loss: 0.8896, Test Loss: 1.0328\n",
      "Epoch 73/91, Train Loss: 0.8478, Test Loss: 1.0313\n",
      "Epoch 74/91, Train Loss: 0.8335, Test Loss: 1.0379\n",
      "Epoch 75/91, Train Loss: 0.8509, Test Loss: 1.0453\n",
      "Epoch 76/91, Train Loss: 0.8319, Test Loss: 1.0540\n",
      "Epoch 77/91, Train Loss: 0.8425, Test Loss: 1.0525\n",
      "Epoch 78/91, Train Loss: 0.8195, Test Loss: 1.0565\n",
      "Epoch 79/91, Train Loss: 0.8404, Test Loss: 1.0607\n",
      "Epoch 80/91, Train Loss: 0.8403, Test Loss: 1.0570\n",
      "Epoch 81/91, Train Loss: 0.8103, Test Loss: 1.0576\n",
      "Epoch 82/91, Train Loss: 0.8038, Test Loss: 1.0595\n",
      "Epoch 83/91, Train Loss: 0.7998, Test Loss: 1.0601\n",
      "Epoch 84/91, Train Loss: 0.8058, Test Loss: 1.0694\n",
      "Epoch 85/91, Train Loss: 0.7656, Test Loss: 1.0837\n",
      "Epoch 86/91, Train Loss: 0.7690, Test Loss: 1.1103\n",
      "Epoch 87/91, Train Loss: 0.7666, Test Loss: 1.1196\n",
      "Epoch 88/91, Train Loss: 0.8013, Test Loss: 1.1121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:51:16,814] Trial 55 finished with value: 1.1419193744659424 and parameters: {'num_hidden_layers': 12, 'layer_0_size': 94, 'layer_1_size': 222, 'layer_2_size': 89, 'layer_3_size': 111, 'layer_4_size': 125, 'layer_5_size': 161, 'layer_6_size': 121, 'layer_7_size': 196, 'layer_8_size': 117, 'layer_9_size': 215, 'layer_10_size': 201, 'layer_11_size': 222, 'dropout_rate': 0.2034793018409774, 'learning_rate': 0.0007323931457443007, 'batch_size': 256, 'epochs': 91}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/91, Train Loss: 0.7265, Test Loss: 1.1210\n",
      "Epoch 90/91, Train Loss: 0.7352, Test Loss: 1.1305\n",
      "Epoch 91/91, Train Loss: 0.7405, Test Loss: 1.1419\n",
      "Epoch 1/75, Train Loss: 1.3004, Test Loss: 1.1141\n",
      "Epoch 2/75, Train Loss: 1.2445, Test Loss: 1.0864\n",
      "Epoch 3/75, Train Loss: 1.2032, Test Loss: 1.1099\n",
      "Epoch 4/75, Train Loss: 1.1771, Test Loss: 1.0890\n",
      "Epoch 5/75, Train Loss: 1.1842, Test Loss: 1.0941\n",
      "Epoch 6/75, Train Loss: 1.1448, Test Loss: 1.1117\n",
      "Epoch 7/75, Train Loss: 1.1323, Test Loss: 1.0906\n",
      "Epoch 8/75, Train Loss: 1.1450, Test Loss: 1.0969\n",
      "Epoch 9/75, Train Loss: 1.1214, Test Loss: 1.0884\n",
      "Epoch 10/75, Train Loss: 1.1181, Test Loss: 1.0883\n",
      "Epoch 11/75, Train Loss: 1.1117, Test Loss: 1.0791\n",
      "Epoch 12/75, Train Loss: 1.1172, Test Loss: 1.0820\n",
      "Epoch 13/75, Train Loss: 1.0954, Test Loss: 1.0746\n",
      "Epoch 14/75, Train Loss: 1.1163, Test Loss: 1.1266\n",
      "Epoch 15/75, Train Loss: 1.0893, Test Loss: 1.1210\n",
      "Epoch 16/75, Train Loss: 1.1070, Test Loss: 1.0892\n",
      "Epoch 17/75, Train Loss: 1.1023, Test Loss: 1.1206\n",
      "Epoch 18/75, Train Loss: 1.0997, Test Loss: 1.1097\n",
      "Epoch 19/75, Train Loss: 1.0878, Test Loss: 1.1498\n",
      "Epoch 20/75, Train Loss: 1.0926, Test Loss: 1.1435\n",
      "Epoch 21/75, Train Loss: 1.0882, Test Loss: 1.1385\n",
      "Epoch 22/75, Train Loss: 1.0726, Test Loss: 1.1453\n",
      "Epoch 23/75, Train Loss: 1.1007, Test Loss: 1.1291\n",
      "Epoch 24/75, Train Loss: 1.0826, Test Loss: 1.1114\n",
      "Epoch 25/75, Train Loss: 1.1104, Test Loss: 1.1431\n",
      "Epoch 26/75, Train Loss: 1.0735, Test Loss: 1.1319\n",
      "Epoch 27/75, Train Loss: 1.0669, Test Loss: 1.1536\n",
      "Epoch 28/75, Train Loss: 1.0581, Test Loss: 1.1542\n",
      "Epoch 29/75, Train Loss: 1.0705, Test Loss: 1.1796\n",
      "Epoch 30/75, Train Loss: 1.0459, Test Loss: 1.1663\n",
      "Epoch 31/75, Train Loss: 1.0246, Test Loss: 1.1790\n",
      "Epoch 32/75, Train Loss: 1.0370, Test Loss: 1.1461\n",
      "Epoch 33/75, Train Loss: 1.0370, Test Loss: 1.1770\n",
      "Epoch 34/75, Train Loss: 0.9830, Test Loss: 1.1609\n",
      "Epoch 35/75, Train Loss: 1.0000, Test Loss: 1.1948\n",
      "Epoch 36/75, Train Loss: 1.0096, Test Loss: 1.2148\n",
      "Epoch 37/75, Train Loss: 0.9949, Test Loss: 1.1790\n",
      "Epoch 38/75, Train Loss: 0.9427, Test Loss: 1.2632\n",
      "Epoch 39/75, Train Loss: 0.9912, Test Loss: 1.1660\n",
      "Epoch 40/75, Train Loss: 0.9761, Test Loss: 1.2882\n",
      "Epoch 41/75, Train Loss: 0.9162, Test Loss: 1.3098\n",
      "Epoch 42/75, Train Loss: 0.9320, Test Loss: 1.2599\n",
      "Epoch 43/75, Train Loss: 0.9128, Test Loss: 1.2517\n",
      "Epoch 44/75, Train Loss: 0.9031, Test Loss: 1.2740\n",
      "Epoch 45/75, Train Loss: 0.9001, Test Loss: 1.2674\n",
      "Epoch 46/75, Train Loss: 0.9018, Test Loss: 1.2589\n",
      "Epoch 47/75, Train Loss: 0.8872, Test Loss: 1.2567\n",
      "Epoch 48/75, Train Loss: 0.8104, Test Loss: 1.3858\n",
      "Epoch 49/75, Train Loss: 0.8469, Test Loss: 1.2085\n",
      "Epoch 50/75, Train Loss: 0.8048, Test Loss: 1.3176\n",
      "Epoch 51/75, Train Loss: 0.8221, Test Loss: 1.2886\n",
      "Epoch 52/75, Train Loss: 0.8413, Test Loss: 1.2567\n",
      "Epoch 53/75, Train Loss: 0.8401, Test Loss: 1.2725\n",
      "Epoch 54/75, Train Loss: 0.7385, Test Loss: 1.3490\n",
      "Epoch 55/75, Train Loss: 0.7658, Test Loss: 1.4201\n",
      "Epoch 56/75, Train Loss: 0.7457, Test Loss: 1.2978\n",
      "Epoch 57/75, Train Loss: 0.7984, Test Loss: 1.3819\n",
      "Epoch 58/75, Train Loss: 0.7585, Test Loss: 1.2858\n",
      "Epoch 59/75, Train Loss: 0.7817, Test Loss: 1.3293\n",
      "Epoch 60/75, Train Loss: 0.7403, Test Loss: 1.2990\n",
      "Epoch 61/75, Train Loss: 0.7489, Test Loss: 1.4173\n",
      "Epoch 62/75, Train Loss: 0.6937, Test Loss: 1.4001\n",
      "Epoch 63/75, Train Loss: 0.6997, Test Loss: 1.3728\n",
      "Epoch 64/75, Train Loss: 0.7190, Test Loss: 1.3632\n",
      "Epoch 65/75, Train Loss: 0.7399, Test Loss: 1.3923\n",
      "Epoch 66/75, Train Loss: 0.7173, Test Loss: 1.3567\n",
      "Epoch 67/75, Train Loss: 0.6694, Test Loss: 1.4131\n",
      "Epoch 68/75, Train Loss: 0.6852, Test Loss: 1.4218\n",
      "Epoch 69/75, Train Loss: 0.7119, Test Loss: 1.2983\n",
      "Epoch 70/75, Train Loss: 0.5995, Test Loss: 1.4304\n",
      "Epoch 71/75, Train Loss: 0.6699, Test Loss: 1.3524\n",
      "Epoch 72/75, Train Loss: 0.7029, Test Loss: 1.3445\n",
      "Epoch 73/75, Train Loss: 0.6409, Test Loss: 1.4314\n",
      "Epoch 74/75, Train Loss: 0.6085, Test Loss: 1.3028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:51:30,934] Trial 56 finished with value: 1.314091614314488 and parameters: {'num_hidden_layers': 9, 'layer_0_size': 43, 'layer_1_size': 242, 'layer_2_size': 146, 'layer_3_size': 195, 'layer_4_size': 246, 'layer_5_size': 191, 'layer_6_size': 45, 'layer_7_size': 247, 'layer_8_size': 151, 'dropout_rate': 0.2594646821354342, 'learning_rate': 0.0011322135239917829, 'batch_size': 32, 'epochs': 75}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/75, Train Loss: 0.6411, Test Loss: 1.3141\n",
      "Epoch 1/62, Train Loss: 1.2733, Test Loss: 0.9822\n",
      "Epoch 2/62, Train Loss: 1.2370, Test Loss: 1.0184\n",
      "Epoch 3/62, Train Loss: 1.2614, Test Loss: 1.0239\n",
      "Epoch 4/62, Train Loss: 1.2731, Test Loss: 1.0296\n",
      "Epoch 5/62, Train Loss: 1.2945, Test Loss: 1.0297\n",
      "Epoch 6/62, Train Loss: 1.2710, Test Loss: 1.0278\n",
      "Epoch 7/62, Train Loss: 1.2464, Test Loss: 1.0295\n",
      "Epoch 8/62, Train Loss: 1.3334, Test Loss: 1.0289\n",
      "Epoch 9/62, Train Loss: 1.3363, Test Loss: 1.0274\n",
      "Epoch 10/62, Train Loss: 1.2461, Test Loss: 1.0262\n",
      "Epoch 11/62, Train Loss: 1.2757, Test Loss: 1.0240\n",
      "Epoch 12/62, Train Loss: 1.2883, Test Loss: 1.0257\n",
      "Epoch 13/62, Train Loss: 1.2867, Test Loss: 1.0219\n",
      "Epoch 14/62, Train Loss: 1.2568, Test Loss: 1.0238\n",
      "Epoch 15/62, Train Loss: 1.2502, Test Loss: 1.0230\n",
      "Epoch 16/62, Train Loss: 1.2546, Test Loss: 1.0208\n",
      "Epoch 17/62, Train Loss: 1.2506, Test Loss: 1.0218\n",
      "Epoch 18/62, Train Loss: 1.2559, Test Loss: 1.0202\n",
      "Epoch 19/62, Train Loss: 1.2247, Test Loss: 1.0215\n",
      "Epoch 20/62, Train Loss: 1.2421, Test Loss: 1.0170\n",
      "Epoch 21/62, Train Loss: 1.2503, Test Loss: 1.0198\n",
      "Epoch 22/62, Train Loss: 1.2509, Test Loss: 1.0184\n",
      "Epoch 23/62, Train Loss: 1.2363, Test Loss: 1.0178\n",
      "Epoch 24/62, Train Loss: 1.2143, Test Loss: 1.0151\n",
      "Epoch 25/62, Train Loss: 1.2120, Test Loss: 1.0230\n",
      "Epoch 26/62, Train Loss: 1.2057, Test Loss: 1.0184\n",
      "Epoch 27/62, Train Loss: 1.3140, Test Loss: 1.0194\n",
      "Epoch 28/62, Train Loss: 1.2731, Test Loss: 1.0163\n",
      "Epoch 29/62, Train Loss: 1.2059, Test Loss: 1.0118\n",
      "Epoch 30/62, Train Loss: 1.2313, Test Loss: 1.0161\n",
      "Epoch 31/62, Train Loss: 1.2790, Test Loss: 1.0139\n",
      "Epoch 32/62, Train Loss: 1.2575, Test Loss: 1.0149\n",
      "Epoch 33/62, Train Loss: 1.2374, Test Loss: 1.0141\n",
      "Epoch 34/62, Train Loss: 1.1595, Test Loss: 1.0153\n",
      "Epoch 35/62, Train Loss: 1.2032, Test Loss: 1.0148\n",
      "Epoch 36/62, Train Loss: 1.1771, Test Loss: 1.0094\n",
      "Epoch 37/62, Train Loss: 1.1859, Test Loss: 1.0115\n",
      "Epoch 38/62, Train Loss: 1.2267, Test Loss: 1.0117\n",
      "Epoch 39/62, Train Loss: 1.2268, Test Loss: 1.0139\n",
      "Epoch 40/62, Train Loss: 1.2091, Test Loss: 1.0117\n",
      "Epoch 41/62, Train Loss: 1.2178, Test Loss: 1.0083\n",
      "Epoch 42/62, Train Loss: 1.2013, Test Loss: 1.0109\n",
      "Epoch 43/62, Train Loss: 1.2412, Test Loss: 1.0115\n",
      "Epoch 44/62, Train Loss: 1.1989, Test Loss: 1.0112\n",
      "Epoch 45/62, Train Loss: 1.2247, Test Loss: 1.0088\n",
      "Epoch 46/62, Train Loss: 1.2012, Test Loss: 1.0131\n",
      "Epoch 47/62, Train Loss: 1.2524, Test Loss: 1.0100\n",
      "Epoch 48/62, Train Loss: 1.2124, Test Loss: 1.0094\n",
      "Epoch 49/62, Train Loss: 1.2163, Test Loss: 1.0099\n",
      "Epoch 50/62, Train Loss: 1.2105, Test Loss: 1.0071\n",
      "Epoch 51/62, Train Loss: 1.2167, Test Loss: 1.0081\n",
      "Epoch 52/62, Train Loss: 1.2080, Test Loss: 1.0084\n",
      "Epoch 53/62, Train Loss: 1.1887, Test Loss: 1.0052\n",
      "Epoch 54/62, Train Loss: 1.2485, Test Loss: 1.0053\n",
      "Epoch 55/62, Train Loss: 1.1994, Test Loss: 1.0056\n",
      "Epoch 56/62, Train Loss: 1.2226, Test Loss: 1.0011\n",
      "Epoch 57/62, Train Loss: 1.1959, Test Loss: 1.0051\n",
      "Epoch 58/62, Train Loss: 1.2145, Test Loss: 1.0089\n",
      "Epoch 59/62, Train Loss: 1.2341, Test Loss: 1.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:51:33,191] Trial 57 finished with value: 1.0069830566644669 and parameters: {'num_hidden_layers': 3, 'layer_0_size': 119, 'layer_1_size': 202, 'layer_2_size': 130, 'dropout_rate': 0.404381694751246, 'learning_rate': 1.2728142553439462e-05, 'batch_size': 64, 'epochs': 62}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/62, Train Loss: 1.2005, Test Loss: 1.0032\n",
      "Epoch 61/62, Train Loss: 1.2006, Test Loss: 1.0041\n",
      "Epoch 62/62, Train Loss: 1.1776, Test Loss: 1.0070\n",
      "Epoch 1/69, Train Loss: 1.2691, Test Loss: 0.8578\n",
      "Epoch 2/69, Train Loss: 1.2338, Test Loss: 0.8575\n",
      "Epoch 3/69, Train Loss: 1.2573, Test Loss: 0.8617\n",
      "Epoch 4/69, Train Loss: 1.2239, Test Loss: 0.8743\n",
      "Epoch 5/69, Train Loss: 1.2035, Test Loss: 0.8894\n",
      "Epoch 6/69, Train Loss: 1.2546, Test Loss: 0.8898\n",
      "Epoch 7/69, Train Loss: 1.2081, Test Loss: 0.8871\n",
      "Epoch 8/69, Train Loss: 1.2339, Test Loss: 0.8901\n",
      "Epoch 9/69, Train Loss: 1.1587, Test Loss: 0.8858\n",
      "Epoch 10/69, Train Loss: 1.2085, Test Loss: 0.8888\n",
      "Epoch 11/69, Train Loss: 1.1898, Test Loss: 0.8782\n",
      "Epoch 12/69, Train Loss: 1.1945, Test Loss: 0.8794\n",
      "Epoch 13/69, Train Loss: 1.2099, Test Loss: 0.8827\n",
      "Epoch 14/69, Train Loss: 1.2113, Test Loss: 0.8812\n",
      "Epoch 15/69, Train Loss: 1.2053, Test Loss: 0.8807\n",
      "Epoch 16/69, Train Loss: 1.1654, Test Loss: 0.8792\n",
      "Epoch 17/69, Train Loss: 1.1736, Test Loss: 0.8765\n",
      "Epoch 18/69, Train Loss: 1.2193, Test Loss: 0.8780\n",
      "Epoch 19/69, Train Loss: 1.1878, Test Loss: 0.8776\n",
      "Epoch 20/69, Train Loss: 1.2270, Test Loss: 0.8842\n",
      "Epoch 21/69, Train Loss: 1.1618, Test Loss: 0.8910\n",
      "Epoch 22/69, Train Loss: 1.1729, Test Loss: 0.8890\n",
      "Epoch 23/69, Train Loss: 1.1724, Test Loss: 0.8884\n",
      "Epoch 24/69, Train Loss: 1.1509, Test Loss: 0.8806\n",
      "Epoch 25/69, Train Loss: 1.1503, Test Loss: 0.8834\n",
      "Epoch 26/69, Train Loss: 1.1375, Test Loss: 0.8831\n",
      "Epoch 27/69, Train Loss: 1.1605, Test Loss: 0.8753\n",
      "Epoch 28/69, Train Loss: 1.1603, Test Loss: 0.8717\n",
      "Epoch 29/69, Train Loss: 1.1488, Test Loss: 0.8688\n",
      "Epoch 30/69, Train Loss: 1.1840, Test Loss: 0.8672\n",
      "Epoch 31/69, Train Loss: 1.1417, Test Loss: 0.8658\n",
      "Epoch 32/69, Train Loss: 1.1722, Test Loss: 0.8688\n",
      "Epoch 33/69, Train Loss: 1.1562, Test Loss: 0.8691\n",
      "Epoch 34/69, Train Loss: 1.1167, Test Loss: 0.8736\n",
      "Epoch 35/69, Train Loss: 1.1614, Test Loss: 0.8782\n",
      "Epoch 36/69, Train Loss: 1.1608, Test Loss: 0.8797\n",
      "Epoch 37/69, Train Loss: 1.1191, Test Loss: 0.8731\n",
      "Epoch 38/69, Train Loss: 1.1495, Test Loss: 0.8669\n",
      "Epoch 39/69, Train Loss: 1.1429, Test Loss: 0.8725\n",
      "Epoch 40/69, Train Loss: 1.1131, Test Loss: 0.8699\n",
      "Epoch 41/69, Train Loss: 1.1174, Test Loss: 0.8750\n",
      "Epoch 42/69, Train Loss: 1.1684, Test Loss: 0.8656\n",
      "Epoch 43/69, Train Loss: 1.1456, Test Loss: 0.8708\n",
      "Epoch 44/69, Train Loss: 1.1740, Test Loss: 0.8683\n",
      "Epoch 45/69, Train Loss: 1.1550, Test Loss: 0.8737\n",
      "Epoch 46/69, Train Loss: 1.1452, Test Loss: 0.8668\n",
      "Epoch 47/69, Train Loss: 1.1486, Test Loss: 0.8638\n",
      "Epoch 48/69, Train Loss: 1.1697, Test Loss: 0.8628\n",
      "Epoch 49/69, Train Loss: 1.1144, Test Loss: 0.8661\n",
      "Epoch 50/69, Train Loss: 1.1689, Test Loss: 0.8628\n",
      "Epoch 51/69, Train Loss: 1.1768, Test Loss: 0.8647\n",
      "Epoch 52/69, Train Loss: 1.1340, Test Loss: 0.8655\n",
      "Epoch 53/69, Train Loss: 1.1525, Test Loss: 0.8676\n",
      "Epoch 54/69, Train Loss: 1.1360, Test Loss: 0.8693\n",
      "Epoch 55/69, Train Loss: 1.1311, Test Loss: 0.8675\n",
      "Epoch 56/69, Train Loss: 1.1335, Test Loss: 0.8659\n",
      "Epoch 57/69, Train Loss: 1.1456, Test Loss: 0.8629\n",
      "Epoch 58/69, Train Loss: 1.1384, Test Loss: 0.8649\n",
      "Epoch 59/69, Train Loss: 1.1207, Test Loss: 0.8651\n",
      "Epoch 60/69, Train Loss: 1.1234, Test Loss: 0.8669\n",
      "Epoch 61/69, Train Loss: 1.1223, Test Loss: 0.8639\n",
      "Epoch 62/69, Train Loss: 1.1132, Test Loss: 0.8630\n",
      "Epoch 63/69, Train Loss: 1.1114, Test Loss: 0.8637\n",
      "Epoch 64/69, Train Loss: 1.1151, Test Loss: 0.8643\n",
      "Epoch 65/69, Train Loss: 1.1055, Test Loss: 0.8669\n",
      "Epoch 66/69, Train Loss: 1.1191, Test Loss: 0.8658\n",
      "Epoch 67/69, Train Loss: 1.1393, Test Loss: 0.8644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:51:41,640] Trial 58 finished with value: 0.8698719441890717 and parameters: {'num_hidden_layers': 19, 'layer_0_size': 105, 'layer_1_size': 165, 'layer_2_size': 154, 'layer_3_size': 135, 'layer_4_size': 203, 'layer_5_size': 204, 'layer_6_size': 57, 'layer_7_size': 83, 'layer_8_size': 187, 'layer_9_size': 242, 'layer_10_size': 32, 'layer_11_size': 173, 'layer_12_size': 236, 'layer_13_size': 78, 'layer_14_size': 218, 'layer_15_size': 137, 'layer_16_size': 206, 'layer_17_size': 152, 'layer_18_size': 61, 'dropout_rate': 0.22571135105796727, 'learning_rate': 4.732971458102315e-05, 'batch_size': 128, 'epochs': 69}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/69, Train Loss: 1.1097, Test Loss: 0.8694\n",
      "Epoch 69/69, Train Loss: 1.1377, Test Loss: 0.8699\n",
      "Epoch 1/39, Train Loss: 1.2364, Test Loss: 0.9177\n",
      "Epoch 2/39, Train Loss: 1.2458, Test Loss: 0.9210\n",
      "Epoch 3/39, Train Loss: 1.1971, Test Loss: 0.9185\n",
      "Epoch 4/39, Train Loss: 1.1986, Test Loss: 0.9228\n",
      "Epoch 5/39, Train Loss: 1.2252, Test Loss: 0.9147\n",
      "Epoch 6/39, Train Loss: 1.2650, Test Loss: 0.9171\n",
      "Epoch 7/39, Train Loss: 1.2204, Test Loss: 0.9175\n",
      "Epoch 8/39, Train Loss: 1.2331, Test Loss: 0.9235\n",
      "Epoch 9/39, Train Loss: 1.2027, Test Loss: 0.9188\n",
      "Epoch 10/39, Train Loss: 1.1555, Test Loss: 0.9186\n",
      "Epoch 11/39, Train Loss: 1.1699, Test Loss: 0.9194\n",
      "Epoch 12/39, Train Loss: 1.2240, Test Loss: 0.9170\n",
      "Epoch 13/39, Train Loss: 1.2536, Test Loss: 0.9217\n",
      "Epoch 14/39, Train Loss: 1.1832, Test Loss: 0.9196\n",
      "Epoch 15/39, Train Loss: 1.1715, Test Loss: 0.9204\n",
      "Epoch 16/39, Train Loss: 1.1637, Test Loss: 0.9178\n",
      "Epoch 17/39, Train Loss: 1.1408, Test Loss: 0.9148\n",
      "Epoch 18/39, Train Loss: 1.1701, Test Loss: 0.9160\n",
      "Epoch 19/39, Train Loss: 1.1854, Test Loss: 0.9162\n",
      "Epoch 20/39, Train Loss: 1.1253, Test Loss: 0.9201\n",
      "Epoch 21/39, Train Loss: 1.1559, Test Loss: 0.9153\n",
      "Epoch 22/39, Train Loss: 1.1683, Test Loss: 0.9180\n",
      "Epoch 23/39, Train Loss: 1.1757, Test Loss: 0.9181\n",
      "Epoch 24/39, Train Loss: 1.1465, Test Loss: 0.9165\n",
      "Epoch 25/39, Train Loss: 1.1575, Test Loss: 0.9152\n",
      "Epoch 26/39, Train Loss: 1.1495, Test Loss: 0.9216\n",
      "Epoch 27/39, Train Loss: 1.1517, Test Loss: 0.9157\n",
      "Epoch 28/39, Train Loss: 1.1182, Test Loss: 0.9176\n",
      "Epoch 29/39, Train Loss: 1.0779, Test Loss: 0.9208\n",
      "Epoch 30/39, Train Loss: 1.1285, Test Loss: 0.9191\n",
      "Epoch 31/39, Train Loss: 1.1209, Test Loss: 0.9229\n",
      "Epoch 32/39, Train Loss: 1.1251, Test Loss: 0.9214\n",
      "Epoch 33/39, Train Loss: 1.1281, Test Loss: 0.9180\n",
      "Epoch 34/39, Train Loss: 1.1134, Test Loss: 0.9234\n",
      "Epoch 35/39, Train Loss: 1.1437, Test Loss: 0.9184\n",
      "Epoch 36/39, Train Loss: 1.1000, Test Loss: 0.9183\n",
      "Epoch 37/39, Train Loss: 1.1390, Test Loss: 0.9168\n",
      "Epoch 38/39, Train Loss: 1.1156, Test Loss: 0.9191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:51:49,466] Trial 59 finished with value: 0.9190362180982318 and parameters: {'num_hidden_layers': 11, 'layer_0_size': 151, 'layer_1_size': 183, 'layer_2_size': 183, 'layer_3_size': 175, 'layer_4_size': 154, 'layer_5_size': 46, 'layer_6_size': 172, 'layer_7_size': 233, 'layer_8_size': 85, 'layer_9_size': 132, 'layer_10_size': 184, 'dropout_rate': 0.46220649877942316, 'learning_rate': 8.101994673873074e-05, 'batch_size': 32, 'epochs': 39}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/39, Train Loss: 1.1144, Test Loss: 0.9190\n",
      "Epoch 1/51, Train Loss: 1.2842, Test Loss: 1.2110\n",
      "Epoch 2/51, Train Loss: 1.3024, Test Loss: 1.2097\n",
      "Epoch 3/51, Train Loss: 1.2756, Test Loss: 1.2092\n",
      "Epoch 4/51, Train Loss: 1.3031, Test Loss: 1.2090\n",
      "Epoch 5/51, Train Loss: 1.2643, Test Loss: 1.2091\n",
      "Epoch 6/51, Train Loss: 1.3198, Test Loss: 1.2094\n",
      "Epoch 7/51, Train Loss: 1.2686, Test Loss: 1.2099\n",
      "Epoch 8/51, Train Loss: 1.2603, Test Loss: 1.2109\n",
      "Epoch 9/51, Train Loss: 1.2762, Test Loss: 1.2125\n",
      "Epoch 10/51, Train Loss: 1.2801, Test Loss: 1.2121\n",
      "Epoch 11/51, Train Loss: 1.2848, Test Loss: 1.2140\n",
      "Epoch 12/51, Train Loss: 1.2482, Test Loss: 1.2156\n",
      "Epoch 13/51, Train Loss: 1.2824, Test Loss: 1.2173\n",
      "Epoch 14/51, Train Loss: 1.2411, Test Loss: 1.2180\n",
      "Epoch 15/51, Train Loss: 1.2891, Test Loss: 1.2187\n",
      "Epoch 16/51, Train Loss: 1.2972, Test Loss: 1.2203\n",
      "Epoch 17/51, Train Loss: 1.2543, Test Loss: 1.2223\n",
      "Epoch 18/51, Train Loss: 1.3038, Test Loss: 1.2253\n",
      "Epoch 19/51, Train Loss: 1.2499, Test Loss: 1.2253\n",
      "Epoch 20/51, Train Loss: 1.1968, Test Loss: 1.2267\n",
      "Epoch 21/51, Train Loss: 1.3081, Test Loss: 1.2260\n",
      "Epoch 22/51, Train Loss: 1.2457, Test Loss: 1.2261\n",
      "Epoch 23/51, Train Loss: 1.2822, Test Loss: 1.2256\n",
      "Epoch 24/51, Train Loss: 1.2163, Test Loss: 1.2292\n",
      "Epoch 25/51, Train Loss: 1.2716, Test Loss: 1.2291\n",
      "Epoch 26/51, Train Loss: 1.2234, Test Loss: 1.2278\n",
      "Epoch 27/51, Train Loss: 1.2643, Test Loss: 1.2297\n",
      "Epoch 28/51, Train Loss: 1.2177, Test Loss: 1.2302\n",
      "Epoch 29/51, Train Loss: 1.2500, Test Loss: 1.2309\n",
      "Epoch 30/51, Train Loss: 1.1838, Test Loss: 1.2334\n",
      "Epoch 31/51, Train Loss: 1.2366, Test Loss: 1.2355\n",
      "Epoch 32/51, Train Loss: 1.2622, Test Loss: 1.2339\n",
      "Epoch 33/51, Train Loss: 1.2631, Test Loss: 1.2338\n",
      "Epoch 34/51, Train Loss: 1.1856, Test Loss: 1.2318\n",
      "Epoch 35/51, Train Loss: 1.2123, Test Loss: 1.2346\n",
      "Epoch 36/51, Train Loss: 1.2542, Test Loss: 1.2327\n",
      "Epoch 37/51, Train Loss: 1.2245, Test Loss: 1.2337\n",
      "Epoch 38/51, Train Loss: 1.2938, Test Loss: 1.2336\n",
      "Epoch 39/51, Train Loss: 1.2613, Test Loss: 1.2360\n",
      "Epoch 40/51, Train Loss: 1.1844, Test Loss: 1.2380\n",
      "Epoch 41/51, Train Loss: 1.2310, Test Loss: 1.2388\n",
      "Epoch 42/51, Train Loss: 1.2111, Test Loss: 1.2381\n",
      "Epoch 43/51, Train Loss: 1.2145, Test Loss: 1.2381\n",
      "Epoch 44/51, Train Loss: 1.2250, Test Loss: 1.2393\n",
      "Epoch 45/51, Train Loss: 1.2213, Test Loss: 1.2375\n",
      "Epoch 46/51, Train Loss: 1.2580, Test Loss: 1.2376\n",
      "Epoch 47/51, Train Loss: 1.2297, Test Loss: 1.2373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:51:52,595] Trial 60 finished with value: 1.2373261451721191 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 124, 'layer_1_size': 213, 'layer_2_size': 58, 'layer_3_size': 236, 'layer_4_size': 62, 'layer_5_size': 133, 'layer_6_size': 89, 'layer_7_size': 141, 'layer_8_size': 63, 'layer_9_size': 32, 'layer_10_size': 236, 'layer_11_size': 152, 'layer_12_size': 201, 'layer_13_size': 42, 'dropout_rate': 0.33787436139717986, 'learning_rate': 3.060038895862856e-05, 'batch_size': 256, 'epochs': 51}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/51, Train Loss: 1.2108, Test Loss: 1.2376\n",
      "Epoch 49/51, Train Loss: 1.2216, Test Loss: 1.2381\n",
      "Epoch 50/51, Train Loss: 1.1828, Test Loss: 1.2385\n",
      "Epoch 51/51, Train Loss: 1.2365, Test Loss: 1.2373\n",
      "Epoch 1/40, Train Loss: 1.2060, Test Loss: 1.1099\n",
      "Epoch 2/40, Train Loss: 1.1601, Test Loss: 1.1050\n",
      "Epoch 3/40, Train Loss: 1.1431, Test Loss: 1.0977\n",
      "Epoch 4/40, Train Loss: 1.1367, Test Loss: 1.1243\n",
      "Epoch 5/40, Train Loss: 1.1410, Test Loss: 1.0997\n",
      "Epoch 6/40, Train Loss: 1.1647, Test Loss: 1.1106\n",
      "Epoch 7/40, Train Loss: 1.1227, Test Loss: 1.1271\n",
      "Epoch 8/40, Train Loss: 1.1045, Test Loss: 1.1049\n",
      "Epoch 9/40, Train Loss: 1.1145, Test Loss: 1.1073\n",
      "Epoch 10/40, Train Loss: 1.1020, Test Loss: 1.1123\n",
      "Epoch 11/40, Train Loss: 1.0639, Test Loss: 1.1064\n",
      "Epoch 12/40, Train Loss: 1.1189, Test Loss: 1.1391\n",
      "Epoch 13/40, Train Loss: 1.0753, Test Loss: 1.1312\n",
      "Epoch 14/40, Train Loss: 1.1030, Test Loss: 1.1185\n",
      "Epoch 15/40, Train Loss: 1.1006, Test Loss: 1.1197\n",
      "Epoch 16/40, Train Loss: 1.1022, Test Loss: 1.1171\n",
      "Epoch 17/40, Train Loss: 1.0648, Test Loss: 1.1089\n",
      "Epoch 18/40, Train Loss: 1.0761, Test Loss: 1.1028\n",
      "Epoch 19/40, Train Loss: 1.0785, Test Loss: 1.0983\n",
      "Epoch 20/40, Train Loss: 1.0709, Test Loss: 1.1108\n",
      "Epoch 21/40, Train Loss: 1.0842, Test Loss: 1.1014\n",
      "Epoch 22/40, Train Loss: 1.0500, Test Loss: 1.1127\n",
      "Epoch 23/40, Train Loss: 1.0845, Test Loss: 1.1128\n",
      "Epoch 24/40, Train Loss: 1.0692, Test Loss: 1.1110\n",
      "Epoch 25/40, Train Loss: 1.0744, Test Loss: 1.1097\n",
      "Epoch 26/40, Train Loss: 1.0943, Test Loss: 1.1062\n",
      "Epoch 27/40, Train Loss: 1.0829, Test Loss: 1.1145\n",
      "Epoch 28/40, Train Loss: 1.0469, Test Loss: 1.1191\n",
      "Epoch 29/40, Train Loss: 1.0846, Test Loss: 1.1066\n",
      "Epoch 30/40, Train Loss: 1.0504, Test Loss: 1.1043\n",
      "Epoch 31/40, Train Loss: 1.0602, Test Loss: 1.1027\n",
      "Epoch 32/40, Train Loss: 1.0457, Test Loss: 1.1033\n",
      "Epoch 33/40, Train Loss: 1.0500, Test Loss: 1.1042\n",
      "Epoch 34/40, Train Loss: 1.0718, Test Loss: 1.1061\n",
      "Epoch 35/40, Train Loss: 1.0742, Test Loss: 1.1026\n",
      "Epoch 36/40, Train Loss: 1.0459, Test Loss: 1.1120\n",
      "Epoch 37/40, Train Loss: 1.0418, Test Loss: 1.1199\n",
      "Epoch 38/40, Train Loss: 1.0531, Test Loss: 1.1221\n",
      "Epoch 39/40, Train Loss: 1.0371, Test Loss: 1.1088\n",
      "Epoch 40/40, Train Loss: 1.0619, Test Loss: 1.1067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:52:01,382] Trial 61 finished with value: 1.106673036302839 and parameters: {'num_hidden_layers': 11, 'layer_0_size': 137, 'layer_1_size': 250, 'layer_2_size': 184, 'layer_3_size': 152, 'layer_4_size': 138, 'layer_5_size': 97, 'layer_6_size': 244, 'layer_7_size': 202, 'layer_8_size': 67, 'layer_9_size': 173, 'layer_10_size': 162, 'dropout_rate': 0.29995469589359536, 'learning_rate': 0.0004927032934618599, 'batch_size': 32, 'epochs': 40}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/44, Train Loss: 1.1258, Test Loss: 1.0602\n",
      "Epoch 2/44, Train Loss: 1.0120, Test Loss: 1.1032\n",
      "Epoch 3/44, Train Loss: 1.0621, Test Loss: 1.1040\n",
      "Epoch 4/44, Train Loss: 1.0356, Test Loss: 1.1189\n",
      "Epoch 5/44, Train Loss: 1.0088, Test Loss: 1.0287\n",
      "Epoch 6/44, Train Loss: 0.9999, Test Loss: 1.0172\n",
      "Epoch 7/44, Train Loss: 0.9479, Test Loss: 1.1164\n",
      "Epoch 8/44, Train Loss: 0.9465, Test Loss: 1.0290\n",
      "Epoch 9/44, Train Loss: 0.9385, Test Loss: 1.0639\n",
      "Epoch 10/44, Train Loss: 0.9411, Test Loss: 1.0244\n",
      "Epoch 11/44, Train Loss: 0.9562, Test Loss: 1.0120\n",
      "Epoch 12/44, Train Loss: 0.9567, Test Loss: 1.0130\n",
      "Epoch 13/44, Train Loss: 0.9330, Test Loss: 1.0277\n",
      "Epoch 14/44, Train Loss: 0.9476, Test Loss: 1.0132\n",
      "Epoch 15/44, Train Loss: 0.9272, Test Loss: 1.0148\n",
      "Epoch 16/44, Train Loss: 0.9416, Test Loss: 1.0122\n",
      "Epoch 17/44, Train Loss: 0.9175, Test Loss: 1.0309\n",
      "Epoch 18/44, Train Loss: 0.9118, Test Loss: 1.0180\n",
      "Epoch 19/44, Train Loss: 0.9507, Test Loss: 1.0238\n",
      "Epoch 20/44, Train Loss: 0.9468, Test Loss: 1.0237\n",
      "Epoch 21/44, Train Loss: 0.9268, Test Loss: 1.0180\n",
      "Epoch 22/44, Train Loss: 0.9149, Test Loss: 1.0212\n",
      "Epoch 23/44, Train Loss: 0.9180, Test Loss: 1.0191\n",
      "Epoch 24/44, Train Loss: 0.9400, Test Loss: 1.0184\n",
      "Epoch 25/44, Train Loss: 0.9107, Test Loss: 1.0207\n",
      "Epoch 26/44, Train Loss: 0.9192, Test Loss: 1.0128\n",
      "Epoch 27/44, Train Loss: 0.9329, Test Loss: 1.0198\n",
      "Epoch 28/44, Train Loss: 0.9120, Test Loss: 1.0221\n",
      "Epoch 29/44, Train Loss: 0.9308, Test Loss: 1.0271\n",
      "Epoch 30/44, Train Loss: 0.9152, Test Loss: 1.0146\n",
      "Epoch 31/44, Train Loss: 0.9195, Test Loss: 1.0166\n",
      "Epoch 32/44, Train Loss: 0.9177, Test Loss: 1.0207\n",
      "Epoch 33/44, Train Loss: 0.9466, Test Loss: 1.0164\n",
      "Epoch 34/44, Train Loss: 0.9167, Test Loss: 1.0201\n",
      "Epoch 35/44, Train Loss: 0.9282, Test Loss: 1.0206\n",
      "Epoch 36/44, Train Loss: 0.9153, Test Loss: 1.0266\n",
      "Epoch 37/44, Train Loss: 0.9168, Test Loss: 1.0209\n",
      "Epoch 38/44, Train Loss: 0.8978, Test Loss: 1.0229\n",
      "Epoch 39/44, Train Loss: 0.9257, Test Loss: 1.0212\n",
      "Epoch 40/44, Train Loss: 0.9250, Test Loss: 1.0118\n",
      "Epoch 41/44, Train Loss: 0.9216, Test Loss: 1.0138\n",
      "Epoch 42/44, Train Loss: 0.8992, Test Loss: 1.0153\n",
      "Epoch 43/44, Train Loss: 0.9314, Test Loss: 1.0126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:52:11,887] Trial 62 finished with value: 1.0150966729436601 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 138, 'layer_1_size': 235, 'layer_2_size': 201, 'layer_3_size': 141, 'layer_4_size': 132, 'layer_5_size': 83, 'layer_6_size': 188, 'layer_7_size': 216, 'layer_8_size': 36, 'layer_9_size': 168, 'layer_10_size': 173, 'layer_11_size': 245, 'layer_12_size': 62, 'dropout_rate': 0.2880576157203042, 'learning_rate': 0.0007459449547159865, 'batch_size': 32, 'epochs': 44}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/44, Train Loss: 0.9193, Test Loss: 1.0151\n",
      "Epoch 1/54, Train Loss: 1.1622, Test Loss: 0.9358\n",
      "Epoch 2/54, Train Loss: 1.0820, Test Loss: 0.9441\n",
      "Epoch 3/54, Train Loss: 1.0966, Test Loss: 0.9297\n",
      "Epoch 4/54, Train Loss: 1.1189, Test Loss: 0.9235\n",
      "Epoch 5/54, Train Loss: 1.0972, Test Loss: 0.9317\n",
      "Epoch 6/54, Train Loss: 1.0747, Test Loss: 0.9295\n",
      "Epoch 7/54, Train Loss: 1.0780, Test Loss: 0.9392\n",
      "Epoch 8/54, Train Loss: 1.0438, Test Loss: 0.9159\n",
      "Epoch 9/54, Train Loss: 1.0504, Test Loss: 0.9152\n",
      "Epoch 10/54, Train Loss: 1.0679, Test Loss: 0.9179\n",
      "Epoch 11/54, Train Loss: 1.0468, Test Loss: 0.9155\n",
      "Epoch 12/54, Train Loss: 1.0483, Test Loss: 0.9226\n",
      "Epoch 13/54, Train Loss: 1.0740, Test Loss: 0.9152\n",
      "Epoch 14/54, Train Loss: 1.0566, Test Loss: 0.9167\n",
      "Epoch 15/54, Train Loss: 1.0483, Test Loss: 0.9223\n",
      "Epoch 16/54, Train Loss: 1.0460, Test Loss: 0.9233\n",
      "Epoch 17/54, Train Loss: 1.0443, Test Loss: 0.9268\n",
      "Epoch 18/54, Train Loss: 1.0309, Test Loss: 0.9377\n",
      "Epoch 19/54, Train Loss: 1.0560, Test Loss: 0.9429\n",
      "Epoch 20/54, Train Loss: 1.0506, Test Loss: 0.9303\n",
      "Epoch 21/54, Train Loss: 1.0504, Test Loss: 0.9365\n",
      "Epoch 22/54, Train Loss: 1.0572, Test Loss: 0.9387\n",
      "Epoch 23/54, Train Loss: 1.0177, Test Loss: 0.9446\n",
      "Epoch 24/54, Train Loss: 1.0330, Test Loss: 0.9465\n",
      "Epoch 25/54, Train Loss: 1.0574, Test Loss: 0.9377\n",
      "Epoch 26/54, Train Loss: 1.0316, Test Loss: 0.9333\n",
      "Epoch 27/54, Train Loss: 1.0165, Test Loss: 0.9380\n",
      "Epoch 28/54, Train Loss: 1.0363, Test Loss: 0.9346\n",
      "Epoch 29/54, Train Loss: 1.0116, Test Loss: 0.9331\n",
      "Epoch 30/54, Train Loss: 1.0364, Test Loss: 0.9329\n",
      "Epoch 31/54, Train Loss: 1.0324, Test Loss: 0.9394\n",
      "Epoch 32/54, Train Loss: 1.0420, Test Loss: 0.9343\n",
      "Epoch 33/54, Train Loss: 1.0224, Test Loss: 0.9388\n",
      "Epoch 34/54, Train Loss: 1.0321, Test Loss: 0.9392\n",
      "Epoch 35/54, Train Loss: 1.0052, Test Loss: 0.9537\n",
      "Epoch 36/54, Train Loss: 1.0235, Test Loss: 0.9507\n",
      "Epoch 37/54, Train Loss: 1.0253, Test Loss: 0.9482\n",
      "Epoch 38/54, Train Loss: 1.0257, Test Loss: 0.9503\n",
      "Epoch 39/54, Train Loss: 1.0548, Test Loss: 0.9438\n",
      "Epoch 40/54, Train Loss: 1.0214, Test Loss: 0.9347\n",
      "Epoch 41/54, Train Loss: 1.0094, Test Loss: 0.9346\n",
      "Epoch 42/54, Train Loss: 1.0219, Test Loss: 0.9382\n",
      "Epoch 43/54, Train Loss: 1.0260, Test Loss: 0.9362\n",
      "Epoch 44/54, Train Loss: 1.0135, Test Loss: 0.9350\n",
      "Epoch 45/54, Train Loss: 1.0185, Test Loss: 0.9340\n",
      "Epoch 46/54, Train Loss: 1.0108, Test Loss: 0.9350\n",
      "Epoch 47/54, Train Loss: 1.0069, Test Loss: 0.9388\n",
      "Epoch 48/54, Train Loss: 1.0136, Test Loss: 0.9462\n",
      "Epoch 49/54, Train Loss: 0.9975, Test Loss: 0.9568\n",
      "Epoch 50/54, Train Loss: 1.0061, Test Loss: 0.9558\n",
      "Epoch 51/54, Train Loss: 1.0014, Test Loss: 0.9661\n",
      "Epoch 52/54, Train Loss: 0.9919, Test Loss: 0.9694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:52:21,683] Trial 63 finished with value: 0.9734586477279663 and parameters: {'num_hidden_layers': 10, 'layer_0_size': 128, 'layer_1_size': 231, 'layer_2_size': 169, 'layer_3_size': 93, 'layer_4_size': 116, 'layer_5_size': 81, 'layer_6_size': 219, 'layer_7_size': 175, 'layer_8_size': 93, 'layer_9_size': 67, 'dropout_rate': 0.27646328498750444, 'learning_rate': 0.00043668636568069844, 'batch_size': 32, 'epochs': 54}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/54, Train Loss: 1.0042, Test Loss: 0.9713\n",
      "Epoch 54/54, Train Loss: 0.9673, Test Loss: 0.9735\n",
      "Epoch 1/36, Train Loss: 1.2364, Test Loss: 1.0748\n",
      "Epoch 2/36, Train Loss: 1.2128, Test Loss: 1.0661\n",
      "Epoch 3/36, Train Loss: 1.1418, Test Loss: 1.0594\n",
      "Epoch 4/36, Train Loss: 1.2209, Test Loss: 1.0561\n",
      "Epoch 5/36, Train Loss: 1.1819, Test Loss: 1.0585\n",
      "Epoch 6/36, Train Loss: 1.1846, Test Loss: 1.0554\n",
      "Epoch 7/36, Train Loss: 1.1944, Test Loss: 1.0608\n",
      "Epoch 8/36, Train Loss: 1.1601, Test Loss: 1.0610\n",
      "Epoch 9/36, Train Loss: 1.1122, Test Loss: 1.0601\n",
      "Epoch 10/36, Train Loss: 1.1833, Test Loss: 1.0608\n",
      "Epoch 11/36, Train Loss: 1.1338, Test Loss: 1.0630\n",
      "Epoch 12/36, Train Loss: 1.1821, Test Loss: 1.0596\n",
      "Epoch 13/36, Train Loss: 1.1353, Test Loss: 1.0610\n",
      "Epoch 14/36, Train Loss: 1.1433, Test Loss: 1.0582\n",
      "Epoch 15/36, Train Loss: 1.1653, Test Loss: 1.0544\n",
      "Epoch 16/36, Train Loss: 1.1565, Test Loss: 1.0521\n",
      "Epoch 17/36, Train Loss: 1.1335, Test Loss: 1.0497\n",
      "Epoch 18/36, Train Loss: 1.1641, Test Loss: 1.0511\n",
      "Epoch 19/36, Train Loss: 1.1201, Test Loss: 1.0539\n",
      "Epoch 20/36, Train Loss: 1.0978, Test Loss: 1.0505\n",
      "Epoch 21/36, Train Loss: 1.1091, Test Loss: 1.0499\n",
      "Epoch 22/36, Train Loss: 1.1224, Test Loss: 1.0482\n",
      "Epoch 23/36, Train Loss: 1.0963, Test Loss: 1.0507\n",
      "Epoch 24/36, Train Loss: 1.1348, Test Loss: 1.0520\n",
      "Epoch 25/36, Train Loss: 1.0936, Test Loss: 1.0504\n",
      "Epoch 26/36, Train Loss: 1.1315, Test Loss: 1.0518\n",
      "Epoch 27/36, Train Loss: 1.1047, Test Loss: 1.0495\n",
      "Epoch 28/36, Train Loss: 1.0817, Test Loss: 1.0514\n",
      "Epoch 29/36, Train Loss: 1.0987, Test Loss: 1.0518\n",
      "Epoch 30/36, Train Loss: 1.0791, Test Loss: 1.0534\n",
      "Epoch 31/36, Train Loss: 1.0908, Test Loss: 1.0548\n",
      "Epoch 32/36, Train Loss: 1.1045, Test Loss: 1.0560\n",
      "Epoch 33/36, Train Loss: 1.0936, Test Loss: 1.0543\n",
      "Epoch 34/36, Train Loss: 1.0754, Test Loss: 1.0531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:52:26,140] Trial 64 finished with value: 1.0504891446658544 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 155, 'layer_1_size': 224, 'layer_2_size': 154, 'layer_3_size': 139, 'layer_4_size': 96, 'layer_5_size': 106, 'dropout_rate': 0.3592299802463818, 'learning_rate': 0.0002664257956389387, 'batch_size': 32, 'epochs': 36}. Best is trial 13 with value: 0.6764646470546722.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/36, Train Loss: 1.1067, Test Loss: 1.0515\n",
      "Epoch 36/36, Train Loss: 1.0953, Test Loss: 1.0505\n",
      "Epoch 1/57, Train Loss: 1.1455, Test Loss: 0.6775\n",
      "Epoch 2/57, Train Loss: 1.0626, Test Loss: 0.6886\n",
      "Epoch 3/57, Train Loss: 1.0695, Test Loss: 0.6899\n",
      "Epoch 4/57, Train Loss: 1.0581, Test Loss: 0.6757\n",
      "Epoch 5/57, Train Loss: 1.0474, Test Loss: 0.6788\n",
      "Epoch 6/57, Train Loss: 1.0616, Test Loss: 0.6754\n",
      "Epoch 7/57, Train Loss: 1.0116, Test Loss: 0.6799\n",
      "Epoch 8/57, Train Loss: 1.0827, Test Loss: 0.6847\n",
      "Epoch 9/57, Train Loss: 1.0510, Test Loss: 0.6847\n",
      "Epoch 10/57, Train Loss: 1.0370, Test Loss: 0.6843\n",
      "Epoch 11/57, Train Loss: 1.0725, Test Loss: 0.6811\n",
      "Epoch 12/57, Train Loss: 1.0654, Test Loss: 0.6830\n",
      "Epoch 13/57, Train Loss: 1.0194, Test Loss: 0.6723\n",
      "Epoch 14/57, Train Loss: 1.0086, Test Loss: 0.6746\n",
      "Epoch 15/57, Train Loss: 1.0296, Test Loss: 0.6699\n",
      "Epoch 16/57, Train Loss: 1.0258, Test Loss: 0.6688\n",
      "Epoch 17/57, Train Loss: 1.0250, Test Loss: 0.6723\n",
      "Epoch 18/57, Train Loss: 1.0498, Test Loss: 0.6701\n",
      "Epoch 19/57, Train Loss: 0.9939, Test Loss: 0.6740\n",
      "Epoch 20/57, Train Loss: 1.0084, Test Loss: 0.6733\n",
      "Epoch 21/57, Train Loss: 1.0121, Test Loss: 0.6743\n",
      "Epoch 22/57, Train Loss: 1.0166, Test Loss: 0.6783\n",
      "Epoch 23/57, Train Loss: 0.9942, Test Loss: 0.6762\n",
      "Epoch 24/57, Train Loss: 0.9780, Test Loss: 0.6810\n",
      "Epoch 25/57, Train Loss: 0.9972, Test Loss: 0.6843\n",
      "Epoch 26/57, Train Loss: 1.0018, Test Loss: 0.6854\n",
      "Epoch 27/57, Train Loss: 0.9945, Test Loss: 0.6816\n",
      "Epoch 28/57, Train Loss: 0.9938, Test Loss: 0.6806\n",
      "Epoch 29/57, Train Loss: 1.0128, Test Loss: 0.6750\n",
      "Epoch 30/57, Train Loss: 1.0095, Test Loss: 0.6770\n",
      "Epoch 31/57, Train Loss: 1.0289, Test Loss: 0.6764\n",
      "Epoch 32/57, Train Loss: 1.0068, Test Loss: 0.6753\n",
      "Epoch 33/57, Train Loss: 0.9832, Test Loss: 0.6762\n",
      "Epoch 34/57, Train Loss: 1.0517, Test Loss: 0.6749\n",
      "Epoch 35/57, Train Loss: 0.9776, Test Loss: 0.6758\n",
      "Epoch 36/57, Train Loss: 0.9954, Test Loss: 0.6742\n",
      "Epoch 37/57, Train Loss: 0.9572, Test Loss: 0.6730\n",
      "Epoch 38/57, Train Loss: 0.9772, Test Loss: 0.6703\n",
      "Epoch 39/57, Train Loss: 1.0148, Test Loss: 0.6721\n",
      "Epoch 40/57, Train Loss: 1.0004, Test Loss: 0.6687\n",
      "Epoch 41/57, Train Loss: 0.9731, Test Loss: 0.6705\n",
      "Epoch 42/57, Train Loss: 0.9844, Test Loss: 0.6725\n",
      "Epoch 43/57, Train Loss: 0.9890, Test Loss: 0.6775\n",
      "Epoch 44/57, Train Loss: 0.9877, Test Loss: 0.6743\n",
      "Epoch 45/57, Train Loss: 0.9838, Test Loss: 0.6754\n",
      "Epoch 46/57, Train Loss: 1.0068, Test Loss: 0.6743\n",
      "Epoch 47/57, Train Loss: 0.9644, Test Loss: 0.6740\n",
      "Epoch 48/57, Train Loss: 0.9958, Test Loss: 0.6724\n",
      "Epoch 49/57, Train Loss: 1.0279, Test Loss: 0.6724\n",
      "Epoch 50/57, Train Loss: 0.9816, Test Loss: 0.6741\n",
      "Epoch 51/57, Train Loss: 0.9752, Test Loss: 0.6761\n",
      "Epoch 52/57, Train Loss: 0.9764, Test Loss: 0.6746\n",
      "Epoch 53/57, Train Loss: 0.9784, Test Loss: 0.6762\n",
      "Epoch 54/57, Train Loss: 0.9501, Test Loss: 0.6797\n",
      "Epoch 55/57, Train Loss: 0.9817, Test Loss: 0.6731\n",
      "Epoch 56/57, Train Loss: 0.9992, Test Loss: 0.6722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:52:38,776] Trial 65 finished with value: 0.6746163879122052 and parameters: {'num_hidden_layers': 12, 'layer_0_size': 164, 'layer_1_size': 191, 'layer_2_size': 224, 'layer_3_size': 114, 'layer_4_size': 164, 'layer_5_size': 90, 'layer_6_size': 182, 'layer_7_size': 208, 'layer_8_size': 72, 'layer_9_size': 43, 'layer_10_size': 248, 'layer_11_size': 76, 'dropout_rate': 0.2542002795679332, 'learning_rate': 0.00016799883676835427, 'batch_size': 32, 'epochs': 57}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/57, Train Loss: 0.9867, Test Loss: 0.6746\n",
      "Epoch 1/61, Train Loss: 1.2285, Test Loss: 0.9456\n",
      "Epoch 2/61, Train Loss: 1.2076, Test Loss: 0.9756\n",
      "Epoch 3/61, Train Loss: 1.2674, Test Loss: 0.9653\n",
      "Epoch 4/61, Train Loss: 1.1650, Test Loss: 0.9600\n",
      "Epoch 5/61, Train Loss: 1.1563, Test Loss: 0.9293\n",
      "Epoch 6/61, Train Loss: 1.1254, Test Loss: 0.9340\n",
      "Epoch 7/61, Train Loss: 1.1238, Test Loss: 0.9404\n",
      "Epoch 8/61, Train Loss: 1.1490, Test Loss: 0.9252\n",
      "Epoch 9/61, Train Loss: 1.1699, Test Loss: 0.9545\n",
      "Epoch 10/61, Train Loss: 1.1862, Test Loss: 0.9351\n",
      "Epoch 11/61, Train Loss: 1.1096, Test Loss: 0.9443\n",
      "Epoch 12/61, Train Loss: 1.1779, Test Loss: 0.9366\n",
      "Epoch 13/61, Train Loss: 1.1811, Test Loss: 0.9297\n",
      "Epoch 14/61, Train Loss: 1.1574, Test Loss: 0.9227\n",
      "Epoch 15/61, Train Loss: 1.1631, Test Loss: 0.9160\n",
      "Epoch 16/61, Train Loss: 1.1214, Test Loss: 0.9088\n",
      "Epoch 17/61, Train Loss: 1.1536, Test Loss: 0.8983\n",
      "Epoch 18/61, Train Loss: 1.1274, Test Loss: 0.9118\n",
      "Epoch 19/61, Train Loss: 1.1482, Test Loss: 0.9026\n",
      "Epoch 20/61, Train Loss: 1.1798, Test Loss: 0.8923\n",
      "Epoch 21/61, Train Loss: 1.1199, Test Loss: 0.8977\n",
      "Epoch 22/61, Train Loss: 1.1684, Test Loss: 0.8945\n",
      "Epoch 23/61, Train Loss: 1.1106, Test Loss: 0.9083\n",
      "Epoch 24/61, Train Loss: 1.1361, Test Loss: 0.9068\n",
      "Epoch 25/61, Train Loss: 1.1588, Test Loss: 0.8934\n",
      "Epoch 26/61, Train Loss: 1.1314, Test Loss: 0.8930\n",
      "Epoch 27/61, Train Loss: 1.1383, Test Loss: 0.8886\n",
      "Epoch 28/61, Train Loss: 1.1531, Test Loss: 0.9044\n",
      "Epoch 29/61, Train Loss: 1.1026, Test Loss: 0.8890\n",
      "Epoch 30/61, Train Loss: 1.0973, Test Loss: 0.8936\n",
      "Epoch 31/61, Train Loss: 1.1395, Test Loss: 0.8882\n",
      "Epoch 32/61, Train Loss: 1.1224, Test Loss: 0.8952\n",
      "Epoch 33/61, Train Loss: 1.1745, Test Loss: 0.8995\n",
      "Epoch 34/61, Train Loss: 1.1549, Test Loss: 0.9114\n",
      "Epoch 35/61, Train Loss: 1.1091, Test Loss: 0.8907\n",
      "Epoch 36/61, Train Loss: 1.1252, Test Loss: 0.9115\n",
      "Epoch 37/61, Train Loss: 1.1005, Test Loss: 0.9041\n",
      "Epoch 38/61, Train Loss: 1.1284, Test Loss: 0.9127\n",
      "Epoch 39/61, Train Loss: 1.0883, Test Loss: 0.8890\n",
      "Epoch 40/61, Train Loss: 1.0845, Test Loss: 0.8901\n",
      "Epoch 41/61, Train Loss: 1.1561, Test Loss: 0.8982\n",
      "Epoch 42/61, Train Loss: 1.1209, Test Loss: 0.8914\n",
      "Epoch 43/61, Train Loss: 1.0762, Test Loss: 0.8952\n",
      "Epoch 44/61, Train Loss: 1.1637, Test Loss: 0.8895\n",
      "Epoch 45/61, Train Loss: 1.1676, Test Loss: 0.8886\n",
      "Epoch 46/61, Train Loss: 1.1303, Test Loss: 0.8802\n",
      "Epoch 47/61, Train Loss: 1.1096, Test Loss: 0.8917\n",
      "Epoch 48/61, Train Loss: 1.1003, Test Loss: 0.8913\n",
      "Epoch 49/61, Train Loss: 1.0993, Test Loss: 0.8792\n",
      "Epoch 50/61, Train Loss: 1.1256, Test Loss: 0.8806\n",
      "Epoch 51/61, Train Loss: 1.1152, Test Loss: 0.9053\n",
      "Epoch 52/61, Train Loss: 1.1275, Test Loss: 0.8913\n",
      "Epoch 53/61, Train Loss: 1.1044, Test Loss: 0.8800\n",
      "Epoch 54/61, Train Loss: 1.0998, Test Loss: 0.8885\n",
      "Epoch 55/61, Train Loss: 1.0876, Test Loss: 0.8918\n",
      "Epoch 56/61, Train Loss: 1.1208, Test Loss: 0.8888\n",
      "Epoch 57/61, Train Loss: 1.1167, Test Loss: 0.8904\n",
      "Epoch 58/61, Train Loss: 1.0998, Test Loss: 0.9006\n",
      "Epoch 59/61, Train Loss: 1.0759, Test Loss: 0.8917\n",
      "Epoch 60/61, Train Loss: 1.1392, Test Loss: 0.8989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:52:53,903] Trial 66 finished with value: 0.8918633886745998 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 165, 'layer_1_size': 199, 'layer_2_size': 241, 'layer_3_size': 113, 'layer_4_size': 212, 'layer_5_size': 141, 'layer_6_size': 75, 'layer_7_size': 155, 'layer_8_size': 47, 'layer_9_size': 49, 'layer_10_size': 245, 'layer_11_size': 73, 'layer_12_size': 163, 'layer_13_size': 228, 'dropout_rate': 0.24449595329250406, 'learning_rate': 1.7214211285967676e-05, 'batch_size': 32, 'epochs': 61}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/61, Train Loss: 1.1251, Test Loss: 0.8919\n",
      "Epoch 1/69, Train Loss: 1.0662, Test Loss: 1.1448\n",
      "Epoch 2/69, Train Loss: 1.0648, Test Loss: 1.1342\n",
      "Epoch 3/69, Train Loss: 1.0714, Test Loss: 1.1340\n",
      "Epoch 4/69, Train Loss: 1.0334, Test Loss: 1.1247\n",
      "Epoch 5/69, Train Loss: 1.0200, Test Loss: 1.1296\n",
      "Epoch 6/69, Train Loss: 1.0502, Test Loss: 1.1376\n",
      "Epoch 7/69, Train Loss: 1.0542, Test Loss: 1.1266\n",
      "Epoch 8/69, Train Loss: 1.0528, Test Loss: 1.1278\n",
      "Epoch 9/69, Train Loss: 1.0257, Test Loss: 1.1404\n",
      "Epoch 10/69, Train Loss: 1.0198, Test Loss: 1.1482\n",
      "Epoch 11/69, Train Loss: 1.0199, Test Loss: 1.1557\n",
      "Epoch 12/69, Train Loss: 1.0018, Test Loss: 1.1519\n",
      "Epoch 13/69, Train Loss: 1.0001, Test Loss: 1.1466\n",
      "Epoch 14/69, Train Loss: 0.9588, Test Loss: 1.1384\n",
      "Epoch 15/69, Train Loss: 1.0172, Test Loss: 1.1449\n",
      "Epoch 16/69, Train Loss: 0.9916, Test Loss: 1.1457\n",
      "Epoch 17/69, Train Loss: 1.0236, Test Loss: 1.1489\n",
      "Epoch 18/69, Train Loss: 1.0227, Test Loss: 1.1421\n",
      "Epoch 19/69, Train Loss: 0.9869, Test Loss: 1.1522\n",
      "Epoch 20/69, Train Loss: 0.9849, Test Loss: 1.1478\n",
      "Epoch 21/69, Train Loss: 1.0335, Test Loss: 1.1549\n",
      "Epoch 22/69, Train Loss: 0.9803, Test Loss: 1.1479\n",
      "Epoch 23/69, Train Loss: 1.0073, Test Loss: 1.1437\n",
      "Epoch 24/69, Train Loss: 0.9908, Test Loss: 1.1438\n",
      "Epoch 25/69, Train Loss: 0.9801, Test Loss: 1.1357\n",
      "Epoch 26/69, Train Loss: 1.0021, Test Loss: 1.1290\n",
      "Epoch 27/69, Train Loss: 0.9851, Test Loss: 1.1380\n",
      "Epoch 28/69, Train Loss: 0.9932, Test Loss: 1.1455\n",
      "Epoch 29/69, Train Loss: 0.9865, Test Loss: 1.1435\n",
      "Epoch 30/69, Train Loss: 0.9737, Test Loss: 1.1398\n",
      "Epoch 31/69, Train Loss: 0.9961, Test Loss: 1.1378\n",
      "Epoch 32/69, Train Loss: 0.9760, Test Loss: 1.1358\n",
      "Epoch 33/69, Train Loss: 0.9979, Test Loss: 1.1382\n",
      "Epoch 34/69, Train Loss: 1.0171, Test Loss: 1.1367\n",
      "Epoch 35/69, Train Loss: 1.0035, Test Loss: 1.1364\n",
      "Epoch 36/69, Train Loss: 1.0061, Test Loss: 1.1457\n",
      "Epoch 37/69, Train Loss: 1.0023, Test Loss: 1.1403\n",
      "Epoch 38/69, Train Loss: 0.9538, Test Loss: 1.1315\n",
      "Epoch 39/69, Train Loss: 0.9675, Test Loss: 1.1323\n",
      "Epoch 40/69, Train Loss: 1.0027, Test Loss: 1.1242\n",
      "Epoch 41/69, Train Loss: 0.9904, Test Loss: 1.1220\n",
      "Epoch 42/69, Train Loss: 1.0029, Test Loss: 1.1188\n",
      "Epoch 43/69, Train Loss: 0.9883, Test Loss: 1.1240\n",
      "Epoch 44/69, Train Loss: 0.9807, Test Loss: 1.1294\n",
      "Epoch 45/69, Train Loss: 1.0075, Test Loss: 1.1337\n",
      "Epoch 46/69, Train Loss: 1.0003, Test Loss: 1.1241\n",
      "Epoch 47/69, Train Loss: 0.9691, Test Loss: 1.1257\n",
      "Epoch 48/69, Train Loss: 0.9812, Test Loss: 1.1268\n",
      "Epoch 49/69, Train Loss: 0.9819, Test Loss: 1.1275\n",
      "Epoch 50/69, Train Loss: 0.9817, Test Loss: 1.1268\n",
      "Epoch 51/69, Train Loss: 0.9633, Test Loss: 1.1275\n",
      "Epoch 52/69, Train Loss: 0.9661, Test Loss: 1.1284\n",
      "Epoch 53/69, Train Loss: 0.9527, Test Loss: 1.1310\n",
      "Epoch 54/69, Train Loss: 0.9876, Test Loss: 1.1315\n",
      "Epoch 55/69, Train Loss: 0.9849, Test Loss: 1.1282\n",
      "Epoch 56/69, Train Loss: 0.9643, Test Loss: 1.1273\n",
      "Epoch 57/69, Train Loss: 0.9884, Test Loss: 1.1268\n",
      "Epoch 58/69, Train Loss: 0.9660, Test Loss: 1.1243\n",
      "Epoch 59/69, Train Loss: 0.9670, Test Loss: 1.1279\n",
      "Epoch 60/69, Train Loss: 0.9781, Test Loss: 1.1307\n",
      "Epoch 61/69, Train Loss: 0.9967, Test Loss: 1.1215\n",
      "Epoch 62/69, Train Loss: 0.9731, Test Loss: 1.1255\n",
      "Epoch 63/69, Train Loss: 0.9628, Test Loss: 1.1274\n",
      "Epoch 64/69, Train Loss: 0.9816, Test Loss: 1.1257\n",
      "Epoch 65/69, Train Loss: 0.9835, Test Loss: 1.1192\n",
      "Epoch 66/69, Train Loss: 0.9529, Test Loss: 1.1207\n",
      "Epoch 67/69, Train Loss: 0.9809, Test Loss: 1.1251\n",
      "Epoch 68/69, Train Loss: 0.9776, Test Loss: 1.1222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:53:05,181] Trial 67 finished with value: 1.121548593044281 and parameters: {'num_hidden_layers': 16, 'layer_0_size': 214, 'layer_1_size': 188, 'layer_2_size': 215, 'layer_3_size': 125, 'layer_4_size': 226, 'layer_5_size': 91, 'layer_6_size': 163, 'layer_7_size': 187, 'layer_8_size': 112, 'layer_9_size': 43, 'layer_10_size': 255, 'layer_11_size': 67, 'layer_12_size': 85, 'layer_13_size': 173, 'layer_14_size': 124, 'layer_15_size': 168, 'dropout_rate': 0.21888592389971318, 'learning_rate': 0.00016774884614033088, 'batch_size': 64, 'epochs': 69}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/69, Train Loss: 0.9849, Test Loss: 1.1215\n",
      "Epoch 1/18, Train Loss: 1.1638, Test Loss: 1.1061\n",
      "Epoch 2/18, Train Loss: 1.0552, Test Loss: 1.1313\n",
      "Epoch 3/18, Train Loss: 1.0054, Test Loss: 1.1180\n",
      "Epoch 4/18, Train Loss: 1.0357, Test Loss: 1.1227\n",
      "Epoch 5/18, Train Loss: 0.9485, Test Loss: 1.1263\n",
      "Epoch 6/18, Train Loss: 0.9662, Test Loss: 1.1486\n",
      "Epoch 7/18, Train Loss: 0.9721, Test Loss: 1.1039\n",
      "Epoch 8/18, Train Loss: 0.9673, Test Loss: 1.1243\n",
      "Epoch 9/18, Train Loss: 0.9589, Test Loss: 1.1162\n",
      "Epoch 10/18, Train Loss: 0.9961, Test Loss: 1.1096\n",
      "Epoch 11/18, Train Loss: 0.9728, Test Loss: 1.1174\n",
      "Epoch 12/18, Train Loss: 0.9449, Test Loss: 1.1122\n",
      "Epoch 13/18, Train Loss: 0.9495, Test Loss: 1.1170\n",
      "Epoch 14/18, Train Loss: 0.9444, Test Loss: 1.1161\n",
      "Epoch 15/18, Train Loss: 0.9481, Test Loss: 1.1291\n",
      "Epoch 16/18, Train Loss: 0.9436, Test Loss: 1.1221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:53:08,739] Trial 68 finished with value: 1.12299667937415 and parameters: {'num_hidden_layers': 12, 'layer_0_size': 63, 'layer_1_size': 140, 'layer_2_size': 234, 'layer_3_size': 81, 'layer_4_size': 44, 'layer_5_size': 69, 'layer_6_size': 106, 'layer_7_size': 226, 'layer_8_size': 122, 'layer_9_size': 89, 'layer_10_size': 221, 'layer_11_size': 41, 'dropout_rate': 0.25556695257473966, 'learning_rate': 0.0009507439328769248, 'batch_size': 32, 'epochs': 18}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/18, Train Loss: 0.9346, Test Loss: 1.1326\n",
      "Epoch 18/18, Train Loss: 0.9276, Test Loss: 1.1230\n",
      "Epoch 1/58, Train Loss: 1.2829, Test Loss: 0.7815\n",
      "Epoch 2/58, Train Loss: 1.2694, Test Loss: 0.8347\n",
      "Epoch 3/58, Train Loss: 1.1594, Test Loss: 0.8012\n",
      "Epoch 4/58, Train Loss: 1.1223, Test Loss: 0.7809\n",
      "Epoch 5/58, Train Loss: 1.1483, Test Loss: 0.7783\n",
      "Epoch 6/58, Train Loss: 1.0939, Test Loss: 0.7994\n",
      "Epoch 7/58, Train Loss: 1.1171, Test Loss: 0.7767\n",
      "Epoch 8/58, Train Loss: 1.0862, Test Loss: 0.7775\n",
      "Epoch 9/58, Train Loss: 1.0893, Test Loss: 0.7813\n",
      "Epoch 10/58, Train Loss: 1.0793, Test Loss: 0.7882\n",
      "Epoch 11/58, Train Loss: 1.0901, Test Loss: 0.7782\n",
      "Epoch 12/58, Train Loss: 1.0714, Test Loss: 0.7813\n",
      "Epoch 13/58, Train Loss: 1.0949, Test Loss: 0.7798\n",
      "Epoch 14/58, Train Loss: 1.0753, Test Loss: 0.7791\n",
      "Epoch 15/58, Train Loss: 1.0733, Test Loss: 0.7795\n",
      "Epoch 16/58, Train Loss: 1.0788, Test Loss: 0.7819\n",
      "Epoch 17/58, Train Loss: 1.0578, Test Loss: 0.7793\n",
      "Epoch 18/58, Train Loss: 1.0601, Test Loss: 0.7959\n",
      "Epoch 19/58, Train Loss: 1.0680, Test Loss: 0.7783\n",
      "Epoch 20/58, Train Loss: 1.0836, Test Loss: 0.7804\n",
      "Epoch 21/58, Train Loss: 1.0980, Test Loss: 0.7846\n",
      "Epoch 22/58, Train Loss: 1.0872, Test Loss: 0.7965\n",
      "Epoch 23/58, Train Loss: 1.0783, Test Loss: 0.7867\n",
      "Epoch 24/58, Train Loss: 1.0851, Test Loss: 0.7837\n",
      "Epoch 25/58, Train Loss: 1.0610, Test Loss: 0.7903\n",
      "Epoch 26/58, Train Loss: 1.0747, Test Loss: 0.7891\n",
      "Epoch 27/58, Train Loss: 1.0728, Test Loss: 0.7826\n",
      "Epoch 28/58, Train Loss: 1.0661, Test Loss: 0.7951\n",
      "Epoch 29/58, Train Loss: 1.0651, Test Loss: 0.7876\n",
      "Epoch 30/58, Train Loss: 1.0776, Test Loss: 0.7851\n",
      "Epoch 31/58, Train Loss: 1.0680, Test Loss: 0.7805\n",
      "Epoch 32/58, Train Loss: 1.0722, Test Loss: 0.7922\n",
      "Epoch 33/58, Train Loss: 1.0927, Test Loss: 0.7845\n",
      "Epoch 34/58, Train Loss: 1.0857, Test Loss: 0.7828\n",
      "Epoch 35/58, Train Loss: 1.0679, Test Loss: 0.7774\n",
      "Epoch 36/58, Train Loss: 1.0806, Test Loss: 0.8025\n",
      "Epoch 37/58, Train Loss: 1.0838, Test Loss: 0.7817\n",
      "Epoch 38/58, Train Loss: 1.0694, Test Loss: 0.7919\n",
      "Epoch 39/58, Train Loss: 1.0751, Test Loss: 0.7796\n",
      "Epoch 40/58, Train Loss: 1.0711, Test Loss: 0.7901\n",
      "Epoch 41/58, Train Loss: 1.0697, Test Loss: 0.7820\n",
      "Epoch 42/58, Train Loss: 1.0618, Test Loss: 0.7853\n",
      "Epoch 43/58, Train Loss: 1.0518, Test Loss: 0.7937\n",
      "Epoch 44/58, Train Loss: 1.0614, Test Loss: 0.7909\n",
      "Epoch 45/58, Train Loss: 1.0251, Test Loss: 0.8034\n",
      "Epoch 46/58, Train Loss: 1.0530, Test Loss: 0.7911\n",
      "Epoch 47/58, Train Loss: 1.0319, Test Loss: 0.8101\n",
      "Epoch 48/58, Train Loss: 1.0269, Test Loss: 0.8108\n",
      "Epoch 49/58, Train Loss: 1.0086, Test Loss: 0.7993\n",
      "Epoch 50/58, Train Loss: 0.9969, Test Loss: 0.8266\n",
      "Epoch 51/58, Train Loss: 1.0050, Test Loss: 0.8233\n",
      "Epoch 52/58, Train Loss: 0.9808, Test Loss: 0.8553\n",
      "Epoch 53/58, Train Loss: 0.9532, Test Loss: 0.8604\n",
      "Epoch 54/58, Train Loss: 0.9597, Test Loss: 0.8679\n",
      "Epoch 55/58, Train Loss: 0.9361, Test Loss: 0.8652\n",
      "Epoch 56/58, Train Loss: 0.9235, Test Loss: 0.9195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:53:16,581] Trial 69 finished with value: 0.9221409261226654 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 183, 'layer_1_size': 82, 'layer_2_size': 226, 'layer_3_size': 69, 'layer_4_size': 193, 'layer_5_size': 118, 'layer_6_size': 143, 'layer_7_size': 126, 'layer_8_size': 74, 'layer_9_size': 74, 'layer_10_size': 244, 'layer_11_size': 208, 'layer_12_size': 189, 'dropout_rate': 0.3140306347452875, 'learning_rate': 0.002017798258278493, 'batch_size': 64, 'epochs': 58}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/58, Train Loss: 0.9277, Test Loss: 0.8797\n",
      "Epoch 58/58, Train Loss: 0.9000, Test Loss: 0.9221\n",
      "Epoch 1/82, Train Loss: 1.3157, Test Loss: 1.2400\n",
      "Epoch 2/82, Train Loss: 1.2512, Test Loss: 1.2468\n",
      "Epoch 3/82, Train Loss: 1.2554, Test Loss: 1.2546\n",
      "Epoch 4/82, Train Loss: 1.2735, Test Loss: 1.2630\n",
      "Epoch 5/82, Train Loss: 1.2581, Test Loss: 1.2722\n",
      "Epoch 6/82, Train Loss: 1.2733, Test Loss: 1.2689\n",
      "Epoch 7/82, Train Loss: 1.2388, Test Loss: 1.2730\n",
      "Epoch 8/82, Train Loss: 1.2582, Test Loss: 1.2745\n",
      "Epoch 9/82, Train Loss: 1.2835, Test Loss: 1.2754\n",
      "Epoch 10/82, Train Loss: 1.3066, Test Loss: 1.2820\n",
      "Epoch 11/82, Train Loss: 1.2706, Test Loss: 1.2962\n",
      "Epoch 12/82, Train Loss: 1.2493, Test Loss: 1.2922\n",
      "Epoch 13/82, Train Loss: 1.2875, Test Loss: 1.2901\n",
      "Epoch 14/82, Train Loss: 1.2739, Test Loss: 1.2928\n",
      "Epoch 15/82, Train Loss: 1.2173, Test Loss: 1.2958\n",
      "Epoch 16/82, Train Loss: 1.2288, Test Loss: 1.2972\n",
      "Epoch 17/82, Train Loss: 1.2280, Test Loss: 1.2981\n",
      "Epoch 18/82, Train Loss: 1.2210, Test Loss: 1.3028\n",
      "Epoch 19/82, Train Loss: 1.1817, Test Loss: 1.3047\n",
      "Epoch 20/82, Train Loss: 1.2015, Test Loss: 1.3022\n",
      "Epoch 21/82, Train Loss: 1.2774, Test Loss: 1.3023\n",
      "Epoch 22/82, Train Loss: 1.2255, Test Loss: 1.2982\n",
      "Epoch 23/82, Train Loss: 1.1847, Test Loss: 1.2970\n",
      "Epoch 24/82, Train Loss: 1.2260, Test Loss: 1.2921\n",
      "Epoch 25/82, Train Loss: 1.1995, Test Loss: 1.2942\n",
      "Epoch 26/82, Train Loss: 1.1942, Test Loss: 1.2981\n",
      "Epoch 27/82, Train Loss: 1.2264, Test Loss: 1.2945\n",
      "Epoch 28/82, Train Loss: 1.2079, Test Loss: 1.2869\n",
      "Epoch 29/82, Train Loss: 1.1214, Test Loss: 1.2909\n",
      "Epoch 30/82, Train Loss: 1.1909, Test Loss: 1.2906\n",
      "Epoch 31/82, Train Loss: 1.2157, Test Loss: 1.2945\n",
      "Epoch 32/82, Train Loss: 1.2203, Test Loss: 1.2926\n",
      "Epoch 33/82, Train Loss: 1.2019, Test Loss: 1.2835\n",
      "Epoch 34/82, Train Loss: 1.1871, Test Loss: 1.2756\n",
      "Epoch 35/82, Train Loss: 1.1873, Test Loss: 1.2808\n",
      "Epoch 36/82, Train Loss: 1.1726, Test Loss: 1.2809\n",
      "Epoch 37/82, Train Loss: 1.1552, Test Loss: 1.2822\n",
      "Epoch 38/82, Train Loss: 1.1446, Test Loss: 1.2804\n",
      "Epoch 39/82, Train Loss: 1.2005, Test Loss: 1.2825\n",
      "Epoch 40/82, Train Loss: 1.2476, Test Loss: 1.2857\n",
      "Epoch 41/82, Train Loss: 1.1778, Test Loss: 1.2803\n",
      "Epoch 42/82, Train Loss: 1.1724, Test Loss: 1.2795\n",
      "Epoch 43/82, Train Loss: 1.1620, Test Loss: 1.2762\n",
      "Epoch 44/82, Train Loss: 1.2149, Test Loss: 1.2748\n",
      "Epoch 45/82, Train Loss: 1.1748, Test Loss: 1.2796\n",
      "Epoch 46/82, Train Loss: 1.1861, Test Loss: 1.2782\n",
      "Epoch 47/82, Train Loss: 1.1474, Test Loss: 1.2787\n",
      "Epoch 48/82, Train Loss: 1.2090, Test Loss: 1.2764\n",
      "Epoch 49/82, Train Loss: 1.1479, Test Loss: 1.2765\n",
      "Epoch 50/82, Train Loss: 1.1724, Test Loss: 1.2794\n",
      "Epoch 51/82, Train Loss: 1.1979, Test Loss: 1.2734\n",
      "Epoch 52/82, Train Loss: 1.1723, Test Loss: 1.2765\n",
      "Epoch 53/82, Train Loss: 1.1771, Test Loss: 1.2758\n",
      "Epoch 54/82, Train Loss: 1.1853, Test Loss: 1.2768\n",
      "Epoch 55/82, Train Loss: 1.1448, Test Loss: 1.2733\n",
      "Epoch 56/82, Train Loss: 1.2087, Test Loss: 1.2698\n",
      "Epoch 57/82, Train Loss: 1.1751, Test Loss: 1.2666\n",
      "Epoch 58/82, Train Loss: 1.1691, Test Loss: 1.2634\n",
      "Epoch 59/82, Train Loss: 1.1810, Test Loss: 1.2626\n",
      "Epoch 60/82, Train Loss: 1.1836, Test Loss: 1.2705\n",
      "Epoch 61/82, Train Loss: 1.1353, Test Loss: 1.2658\n",
      "Epoch 62/82, Train Loss: 1.1931, Test Loss: 1.2697\n",
      "Epoch 63/82, Train Loss: 1.1898, Test Loss: 1.2683\n",
      "Epoch 64/82, Train Loss: 1.1463, Test Loss: 1.2629\n",
      "Epoch 65/82, Train Loss: 1.1576, Test Loss: 1.2626\n",
      "Epoch 66/82, Train Loss: 1.1590, Test Loss: 1.2629\n",
      "Epoch 67/82, Train Loss: 1.1071, Test Loss: 1.2661\n",
      "Epoch 68/82, Train Loss: 1.1180, Test Loss: 1.2620\n",
      "Epoch 69/82, Train Loss: 1.1879, Test Loss: 1.2608\n",
      "Epoch 70/82, Train Loss: 1.1802, Test Loss: 1.2612\n",
      "Epoch 71/82, Train Loss: 1.1669, Test Loss: 1.2608\n",
      "Epoch 72/82, Train Loss: 1.1848, Test Loss: 1.2596\n",
      "Epoch 73/82, Train Loss: 1.1557, Test Loss: 1.2543\n",
      "Epoch 74/82, Train Loss: 1.1392, Test Loss: 1.2569\n",
      "Epoch 75/82, Train Loss: 1.1435, Test Loss: 1.2556\n",
      "Epoch 76/82, Train Loss: 1.1623, Test Loss: 1.2566\n",
      "Epoch 77/82, Train Loss: 1.1575, Test Loss: 1.2558\n",
      "Epoch 78/82, Train Loss: 1.1536, Test Loss: 1.2522\n",
      "Epoch 79/82, Train Loss: 1.1284, Test Loss: 1.2530\n",
      "Epoch 80/82, Train Loss: 1.1191, Test Loss: 1.2509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:53:24,310] Trial 70 finished with value: 1.249118447303772 and parameters: {'num_hidden_layers': 15, 'layer_0_size': 33, 'layer_1_size': 119, 'layer_2_size': 142, 'layer_3_size': 102, 'layer_4_size': 164, 'layer_5_size': 157, 'layer_6_size': 116, 'layer_7_size': 240, 'layer_8_size': 101, 'layer_9_size': 57, 'layer_10_size': 224, 'layer_11_size': 141, 'layer_12_size': 255, 'layer_13_size': 146, 'layer_14_size': 90, 'dropout_rate': 0.2700105796856488, 'learning_rate': 2.305370936680047e-05, 'batch_size': 128, 'epochs': 82}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/82, Train Loss: 1.1034, Test Loss: 1.2488\n",
      "Epoch 82/82, Train Loss: 1.1537, Test Loss: 1.2491\n",
      "Epoch 1/50, Train Loss: 1.2241, Test Loss: 1.1062\n",
      "Epoch 2/50, Train Loss: 1.1166, Test Loss: 1.0853\n",
      "Epoch 3/50, Train Loss: 1.0842, Test Loss: 1.0608\n",
      "Epoch 4/50, Train Loss: 1.0869, Test Loss: 1.0597\n",
      "Epoch 5/50, Train Loss: 1.0784, Test Loss: 1.0605\n",
      "Epoch 6/50, Train Loss: 1.0544, Test Loss: 1.0934\n",
      "Epoch 7/50, Train Loss: 1.1003, Test Loss: 1.0764\n",
      "Epoch 8/50, Train Loss: 1.0927, Test Loss: 1.0615\n",
      "Epoch 9/50, Train Loss: 1.0651, Test Loss: 1.0725\n",
      "Epoch 10/50, Train Loss: 1.0597, Test Loss: 1.0620\n",
      "Epoch 11/50, Train Loss: 1.0470, Test Loss: 1.0803\n",
      "Epoch 12/50, Train Loss: 1.0624, Test Loss: 1.0745\n",
      "Epoch 13/50, Train Loss: 1.0443, Test Loss: 1.0748\n",
      "Epoch 14/50, Train Loss: 1.0408, Test Loss: 1.0769\n",
      "Epoch 15/50, Train Loss: 1.0275, Test Loss: 1.0832\n",
      "Epoch 16/50, Train Loss: 1.0300, Test Loss: 1.0782\n",
      "Epoch 17/50, Train Loss: 1.0373, Test Loss: 1.0782\n",
      "Epoch 18/50, Train Loss: 1.0317, Test Loss: 1.0885\n",
      "Epoch 19/50, Train Loss: 1.0236, Test Loss: 1.0750\n",
      "Epoch 20/50, Train Loss: 1.0216, Test Loss: 1.0749\n",
      "Epoch 21/50, Train Loss: 0.9950, Test Loss: 1.0767\n",
      "Epoch 22/50, Train Loss: 0.9976, Test Loss: 1.0662\n",
      "Epoch 23/50, Train Loss: 1.0313, Test Loss: 1.0694\n",
      "Epoch 24/50, Train Loss: 1.0245, Test Loss: 1.0728\n",
      "Epoch 25/50, Train Loss: 1.0022, Test Loss: 1.0731\n",
      "Epoch 26/50, Train Loss: 0.9939, Test Loss: 1.0667\n",
      "Epoch 27/50, Train Loss: 1.0042, Test Loss: 1.0710\n",
      "Epoch 28/50, Train Loss: 0.9981, Test Loss: 1.0783\n",
      "Epoch 29/50, Train Loss: 1.0147, Test Loss: 1.0799\n",
      "Epoch 30/50, Train Loss: 0.9886, Test Loss: 1.0797\n",
      "Epoch 31/50, Train Loss: 1.0130, Test Loss: 1.0787\n",
      "Epoch 32/50, Train Loss: 1.0124, Test Loss: 1.0783\n",
      "Epoch 33/50, Train Loss: 0.9938, Test Loss: 1.0836\n",
      "Epoch 34/50, Train Loss: 1.0351, Test Loss: 1.0956\n",
      "Epoch 35/50, Train Loss: 1.0022, Test Loss: 1.1068\n",
      "Epoch 36/50, Train Loss: 0.9769, Test Loss: 1.1121\n",
      "Epoch 37/50, Train Loss: 1.0288, Test Loss: 1.1086\n",
      "Epoch 38/50, Train Loss: 0.9895, Test Loss: 1.1070\n",
      "Epoch 39/50, Train Loss: 1.0035, Test Loss: 1.1094\n",
      "Epoch 40/50, Train Loss: 0.9834, Test Loss: 1.1016\n",
      "Epoch 41/50, Train Loss: 1.0369, Test Loss: 1.1210\n",
      "Epoch 42/50, Train Loss: 1.0000, Test Loss: 1.1405\n",
      "Epoch 43/50, Train Loss: 0.9626, Test Loss: 1.1627\n",
      "Epoch 44/50, Train Loss: 0.9835, Test Loss: 1.1209\n",
      "Epoch 45/50, Train Loss: 0.9875, Test Loss: 1.1140\n",
      "Epoch 46/50, Train Loss: 0.9661, Test Loss: 1.1280\n",
      "Epoch 47/50, Train Loss: 0.9519, Test Loss: 1.1387\n",
      "Epoch 48/50, Train Loss: 0.9497, Test Loss: 1.1438\n",
      "Epoch 49/50, Train Loss: 0.9362, Test Loss: 1.1584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:53:34,632] Trial 71 finished with value: 1.1495364904403687 and parameters: {'num_hidden_layers': 11, 'layer_0_size': 146, 'layer_1_size': 249, 'layer_2_size': 210, 'layer_3_size': 130, 'layer_4_size': 178, 'layer_5_size': 71, 'layer_6_size': 191, 'layer_7_size': 210, 'layer_8_size': 72, 'layer_9_size': 248, 'layer_10_size': 139, 'dropout_rate': 0.2922140523154036, 'learning_rate': 0.0006060082437629288, 'batch_size': 32, 'epochs': 50}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss: 0.9502, Test Loss: 1.1495\n",
      "Epoch 1/58, Train Loss: 1.1821, Test Loss: 0.9893\n",
      "Epoch 2/58, Train Loss: 1.1011, Test Loss: 0.9957\n",
      "Epoch 3/58, Train Loss: 1.1090, Test Loss: 1.0157\n",
      "Epoch 4/58, Train Loss: 1.0500, Test Loss: 0.9934\n",
      "Epoch 5/58, Train Loss: 1.0933, Test Loss: 0.9879\n",
      "Epoch 6/58, Train Loss: 1.0325, Test Loss: 0.9861\n",
      "Epoch 7/58, Train Loss: 1.0373, Test Loss: 0.9948\n",
      "Epoch 8/58, Train Loss: 1.0289, Test Loss: 0.9909\n",
      "Epoch 9/58, Train Loss: 1.0314, Test Loss: 1.0005\n",
      "Epoch 10/58, Train Loss: 1.0347, Test Loss: 0.9983\n",
      "Epoch 11/58, Train Loss: 1.0148, Test Loss: 1.0060\n",
      "Epoch 12/58, Train Loss: 1.0584, Test Loss: 1.0081\n",
      "Epoch 13/58, Train Loss: 1.0471, Test Loss: 1.0022\n",
      "Epoch 14/58, Train Loss: 1.0361, Test Loss: 1.0038\n",
      "Epoch 15/58, Train Loss: 1.0432, Test Loss: 1.0020\n",
      "Epoch 16/58, Train Loss: 1.0335, Test Loss: 0.9971\n",
      "Epoch 17/58, Train Loss: 1.0506, Test Loss: 0.9983\n",
      "Epoch 18/58, Train Loss: 1.0371, Test Loss: 0.9942\n",
      "Epoch 19/58, Train Loss: 1.0263, Test Loss: 0.9950\n",
      "Epoch 20/58, Train Loss: 1.0044, Test Loss: 0.9926\n",
      "Epoch 21/58, Train Loss: 1.0464, Test Loss: 0.9922\n",
      "Epoch 22/58, Train Loss: 1.0052, Test Loss: 0.9923\n",
      "Epoch 23/58, Train Loss: 1.0126, Test Loss: 0.9898\n",
      "Epoch 24/58, Train Loss: 1.0258, Test Loss: 0.9893\n",
      "Epoch 25/58, Train Loss: 1.0104, Test Loss: 0.9895\n",
      "Epoch 26/58, Train Loss: 1.0390, Test Loss: 0.9911\n",
      "Epoch 27/58, Train Loss: 0.9973, Test Loss: 0.9950\n",
      "Epoch 28/58, Train Loss: 1.0074, Test Loss: 0.9931\n",
      "Epoch 29/58, Train Loss: 0.9983, Test Loss: 0.9948\n",
      "Epoch 30/58, Train Loss: 1.0126, Test Loss: 0.9946\n",
      "Epoch 31/58, Train Loss: 0.9717, Test Loss: 0.9964\n",
      "Epoch 32/58, Train Loss: 1.0170, Test Loss: 0.9964\n",
      "Epoch 33/58, Train Loss: 0.9663, Test Loss: 1.0021\n",
      "Epoch 34/58, Train Loss: 0.9760, Test Loss: 1.0163\n",
      "Epoch 35/58, Train Loss: 0.9800, Test Loss: 1.0105\n",
      "Epoch 36/58, Train Loss: 1.0038, Test Loss: 1.0098\n",
      "Epoch 37/58, Train Loss: 0.9744, Test Loss: 1.0067\n",
      "Epoch 38/58, Train Loss: 0.9812, Test Loss: 1.0153\n",
      "Epoch 39/58, Train Loss: 0.9575, Test Loss: 1.0096\n",
      "Epoch 40/58, Train Loss: 0.9572, Test Loss: 1.0236\n",
      "Epoch 41/58, Train Loss: 0.9419, Test Loss: 1.0224\n",
      "Epoch 42/58, Train Loss: 0.8961, Test Loss: 1.0224\n",
      "Epoch 43/58, Train Loss: 0.9215, Test Loss: 1.0069\n",
      "Epoch 44/58, Train Loss: 0.9521, Test Loss: 1.0126\n",
      "Epoch 45/58, Train Loss: 0.9037, Test Loss: 1.0127\n",
      "Epoch 46/58, Train Loss: 0.8901, Test Loss: 1.0079\n",
      "Epoch 47/58, Train Loss: 0.8810, Test Loss: 1.0205\n",
      "Epoch 48/58, Train Loss: 0.8976, Test Loss: 1.0056\n",
      "Epoch 49/58, Train Loss: 0.8576, Test Loss: 1.0208\n",
      "Epoch 50/58, Train Loss: 0.8448, Test Loss: 1.0487\n",
      "Epoch 51/58, Train Loss: 0.8383, Test Loss: 1.0352\n",
      "Epoch 52/58, Train Loss: 0.8258, Test Loss: 1.0211\n",
      "Epoch 53/58, Train Loss: 0.8337, Test Loss: 1.0606\n",
      "Epoch 54/58, Train Loss: 0.7906, Test Loss: 1.0258\n",
      "Epoch 55/58, Train Loss: 0.8380, Test Loss: 1.1444\n",
      "Epoch 56/58, Train Loss: 0.8334, Test Loss: 1.0469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:53:44,051] Trial 72 finished with value: 1.0713280950273787 and parameters: {'num_hidden_layers': 9, 'layer_0_size': 51, 'layer_1_size': 208, 'layer_2_size': 122, 'layer_3_size': 153, 'layer_4_size': 152, 'layer_5_size': 89, 'layer_6_size': 215, 'layer_7_size': 194, 'layer_8_size': 46, 'dropout_rate': 0.33009074492468576, 'learning_rate': 0.0013569240623218242, 'batch_size': 32, 'epochs': 58}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/58, Train Loss: 0.7946, Test Loss: 1.0889\n",
      "Epoch 58/58, Train Loss: 0.8226, Test Loss: 1.0713\n",
      "Epoch 1/32, Train Loss: 1.2465, Test Loss: 1.0185\n",
      "Epoch 2/32, Train Loss: 1.1457, Test Loss: 1.0123\n",
      "Epoch 3/32, Train Loss: 1.1485, Test Loss: 1.0058\n",
      "Epoch 4/32, Train Loss: 1.1569, Test Loss: 1.0284\n",
      "Epoch 5/32, Train Loss: 1.1488, Test Loss: 1.0142\n",
      "Epoch 6/32, Train Loss: 1.1460, Test Loss: 1.0248\n",
      "Epoch 7/32, Train Loss: 1.1064, Test Loss: 1.0164\n",
      "Epoch 8/32, Train Loss: 1.1550, Test Loss: 1.0113\n",
      "Epoch 9/32, Train Loss: 1.1135, Test Loss: 1.0071\n",
      "Epoch 10/32, Train Loss: 1.1148, Test Loss: 1.0189\n",
      "Epoch 11/32, Train Loss: 1.0800, Test Loss: 1.0170\n",
      "Epoch 12/32, Train Loss: 1.1026, Test Loss: 1.0487\n",
      "Epoch 13/32, Train Loss: 1.1031, Test Loss: 1.0515\n",
      "Epoch 14/32, Train Loss: 1.0823, Test Loss: 1.0426\n",
      "Epoch 15/32, Train Loss: 1.0834, Test Loss: 1.0331\n",
      "Epoch 16/32, Train Loss: 1.0914, Test Loss: 1.0189\n",
      "Epoch 17/32, Train Loss: 1.0730, Test Loss: 1.0185\n",
      "Epoch 18/32, Train Loss: 1.0762, Test Loss: 1.0093\n",
      "Epoch 19/32, Train Loss: 1.0476, Test Loss: 1.0086\n",
      "Epoch 20/32, Train Loss: 1.1028, Test Loss: 1.0169\n",
      "Epoch 21/32, Train Loss: 1.1089, Test Loss: 1.0072\n",
      "Epoch 22/32, Train Loss: 1.0667, Test Loss: 1.0116\n",
      "Epoch 23/32, Train Loss: 1.1119, Test Loss: 1.0083\n",
      "Epoch 24/32, Train Loss: 1.1001, Test Loss: 1.0155\n",
      "Epoch 25/32, Train Loss: 1.0580, Test Loss: 1.0174\n",
      "Epoch 26/32, Train Loss: 1.0772, Test Loss: 1.0310\n",
      "Epoch 27/32, Train Loss: 1.0534, Test Loss: 1.0346\n",
      "Epoch 28/32, Train Loss: 1.0683, Test Loss: 1.0313\n",
      "Epoch 29/32, Train Loss: 1.0755, Test Loss: 1.0197\n",
      "Epoch 30/32, Train Loss: 1.0642, Test Loss: 1.0223\n",
      "Epoch 31/32, Train Loss: 1.0954, Test Loss: 1.0197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:53:53,831] Trial 73 finished with value: 1.0166477731295995 and parameters: {'num_hidden_layers': 16, 'layer_0_size': 204, 'layer_1_size': 216, 'layer_2_size': 224, 'layer_3_size': 168, 'layer_4_size': 122, 'layer_5_size': 244, 'layer_6_size': 199, 'layer_7_size': 181, 'layer_8_size': 81, 'layer_9_size': 44, 'layer_10_size': 199, 'layer_11_size': 96, 'layer_12_size': 138, 'layer_13_size': 137, 'layer_14_size': 168, 'layer_15_size': 69, 'dropout_rate': 0.20137361718516059, 'learning_rate': 0.0003756461830764719, 'batch_size': 32, 'epochs': 32}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/32, Train Loss: 1.0838, Test Loss: 1.0166\n",
      "Epoch 1/72, Train Loss: 1.1228, Test Loss: 1.1786\n",
      "Epoch 2/72, Train Loss: 1.1127, Test Loss: 1.1913\n",
      "Epoch 3/72, Train Loss: 1.1268, Test Loss: 1.1704\n",
      "Epoch 4/72, Train Loss: 1.0608, Test Loss: 1.1534\n",
      "Epoch 5/72, Train Loss: 1.0460, Test Loss: 1.1545\n",
      "Epoch 6/72, Train Loss: 1.0855, Test Loss: 1.1460\n",
      "Epoch 7/72, Train Loss: 1.0672, Test Loss: 1.1501\n",
      "Epoch 8/72, Train Loss: 1.0757, Test Loss: 1.1566\n",
      "Epoch 9/72, Train Loss: 1.0807, Test Loss: 1.1787\n",
      "Epoch 10/72, Train Loss: 1.0507, Test Loss: 1.1677\n",
      "Epoch 11/72, Train Loss: 1.0633, Test Loss: 1.1809\n",
      "Epoch 12/72, Train Loss: 1.0681, Test Loss: 1.1952\n",
      "Epoch 13/72, Train Loss: 1.0671, Test Loss: 1.1929\n",
      "Epoch 14/72, Train Loss: 1.0819, Test Loss: 1.1851\n",
      "Epoch 15/72, Train Loss: 1.0201, Test Loss: 1.1828\n",
      "Epoch 16/72, Train Loss: 1.0549, Test Loss: 1.1602\n",
      "Epoch 17/72, Train Loss: 1.0387, Test Loss: 1.1658\n",
      "Epoch 18/72, Train Loss: 1.0438, Test Loss: 1.1638\n",
      "Epoch 19/72, Train Loss: 1.0540, Test Loss: 1.1698\n",
      "Epoch 20/72, Train Loss: 1.0463, Test Loss: 1.1731\n",
      "Epoch 21/72, Train Loss: 1.0420, Test Loss: 1.1597\n",
      "Epoch 22/72, Train Loss: 1.0291, Test Loss: 1.1557\n",
      "Epoch 23/72, Train Loss: 1.0270, Test Loss: 1.1569\n",
      "Epoch 24/72, Train Loss: 1.0130, Test Loss: 1.1609\n",
      "Epoch 25/72, Train Loss: 1.0202, Test Loss: 1.1444\n",
      "Epoch 26/72, Train Loss: 1.0595, Test Loss: 1.1598\n",
      "Epoch 27/72, Train Loss: 1.0546, Test Loss: 1.1657\n",
      "Epoch 28/72, Train Loss: 1.0484, Test Loss: 1.1599\n",
      "Epoch 29/72, Train Loss: 1.0401, Test Loss: 1.1592\n",
      "Epoch 30/72, Train Loss: 1.0346, Test Loss: 1.1605\n",
      "Epoch 31/72, Train Loss: 1.0432, Test Loss: 1.1737\n",
      "Epoch 32/72, Train Loss: 1.0319, Test Loss: 1.1660\n",
      "Epoch 33/72, Train Loss: 1.0388, Test Loss: 1.1665\n",
      "Epoch 34/72, Train Loss: 1.0313, Test Loss: 1.1628\n",
      "Epoch 35/72, Train Loss: 0.9947, Test Loss: 1.1617\n",
      "Epoch 36/72, Train Loss: 1.0713, Test Loss: 1.1613\n",
      "Epoch 37/72, Train Loss: 1.0154, Test Loss: 1.1649\n",
      "Epoch 38/72, Train Loss: 1.0383, Test Loss: 1.1671\n",
      "Epoch 39/72, Train Loss: 1.0291, Test Loss: 1.1524\n",
      "Epoch 40/72, Train Loss: 1.0513, Test Loss: 1.1494\n",
      "Epoch 41/72, Train Loss: 1.0115, Test Loss: 1.1615\n",
      "Epoch 42/72, Train Loss: 1.0330, Test Loss: 1.1539\n",
      "Epoch 43/72, Train Loss: 1.0041, Test Loss: 1.1604\n",
      "Epoch 44/72, Train Loss: 1.0183, Test Loss: 1.1548\n",
      "Epoch 45/72, Train Loss: 1.0231, Test Loss: 1.1554\n",
      "Epoch 46/72, Train Loss: 1.0559, Test Loss: 1.1550\n",
      "Epoch 47/72, Train Loss: 1.0431, Test Loss: 1.1606\n",
      "Epoch 48/72, Train Loss: 1.0256, Test Loss: 1.1551\n",
      "Epoch 49/72, Train Loss: 1.0329, Test Loss: 1.1694\n",
      "Epoch 50/72, Train Loss: 1.0113, Test Loss: 1.1646\n",
      "Epoch 51/72, Train Loss: 1.0615, Test Loss: 1.1716\n",
      "Epoch 52/72, Train Loss: 1.0615, Test Loss: 1.1693\n",
      "Epoch 53/72, Train Loss: 1.0563, Test Loss: 1.1647\n",
      "Epoch 54/72, Train Loss: 1.0398, Test Loss: 1.1678\n",
      "Epoch 55/72, Train Loss: 1.0337, Test Loss: 1.1665\n",
      "Epoch 56/72, Train Loss: 1.0013, Test Loss: 1.1675\n",
      "Epoch 57/72, Train Loss: 1.0207, Test Loss: 1.1588\n",
      "Epoch 58/72, Train Loss: 1.0579, Test Loss: 1.1668\n",
      "Epoch 59/72, Train Loss: 1.0204, Test Loss: 1.1698\n",
      "Epoch 60/72, Train Loss: 1.0283, Test Loss: 1.1650\n",
      "Epoch 61/72, Train Loss: 0.9886, Test Loss: 1.1595\n",
      "Epoch 62/72, Train Loss: 1.0272, Test Loss: 1.1606\n",
      "Epoch 63/72, Train Loss: 1.0031, Test Loss: 1.1651\n",
      "Epoch 64/72, Train Loss: 1.0169, Test Loss: 1.1662\n",
      "Epoch 65/72, Train Loss: 0.9988, Test Loss: 1.1608\n",
      "Epoch 66/72, Train Loss: 1.0047, Test Loss: 1.1578\n",
      "Epoch 67/72, Train Loss: 1.0449, Test Loss: 1.1685\n",
      "Epoch 68/72, Train Loss: 1.0310, Test Loss: 1.1734\n",
      "Epoch 69/72, Train Loss: 1.0105, Test Loss: 1.1690\n",
      "Epoch 70/72, Train Loss: 1.0208, Test Loss: 1.1731\n",
      "Epoch 71/72, Train Loss: 1.0255, Test Loss: 1.1741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:54:08,016] Trial 74 finished with value: 1.1888665642057146 and parameters: {'num_hidden_layers': 12, 'layer_0_size': 162, 'layer_1_size': 179, 'layer_2_size': 112, 'layer_3_size': 94, 'layer_4_size': 109, 'layer_5_size': 109, 'layer_6_size': 152, 'layer_7_size': 250, 'layer_8_size': 59, 'layer_9_size': 32, 'layer_10_size': 88, 'layer_11_size': 163, 'dropout_rate': 0.23218394720893937, 'learning_rate': 0.00027178758191437466, 'batch_size': 32, 'epochs': 72}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/72, Train Loss: 1.0238, Test Loss: 1.1889\n",
      "Epoch 1/26, Train Loss: 1.1476, Test Loss: 1.0531\n",
      "Epoch 2/26, Train Loss: 1.1645, Test Loss: 1.0497\n",
      "Epoch 3/26, Train Loss: 1.0987, Test Loss: 1.0562\n",
      "Epoch 4/26, Train Loss: 1.0521, Test Loss: 1.0625\n",
      "Epoch 5/26, Train Loss: 1.0957, Test Loss: 1.0610\n",
      "Epoch 6/26, Train Loss: 1.0632, Test Loss: 1.0658\n",
      "Epoch 7/26, Train Loss: 1.0480, Test Loss: 1.0644\n",
      "Epoch 8/26, Train Loss: 1.0365, Test Loss: 1.0677\n",
      "Epoch 9/26, Train Loss: 1.0351, Test Loss: 1.0737\n",
      "Epoch 10/26, Train Loss: 1.0907, Test Loss: 1.0680\n",
      "Epoch 11/26, Train Loss: 1.0362, Test Loss: 1.0604\n",
      "Epoch 12/26, Train Loss: 1.0238, Test Loss: 1.0624\n",
      "Epoch 13/26, Train Loss: 1.0317, Test Loss: 1.0585\n",
      "Epoch 14/26, Train Loss: 1.0285, Test Loss: 1.0573\n",
      "Epoch 15/26, Train Loss: 1.0423, Test Loss: 1.0589\n",
      "Epoch 16/26, Train Loss: 1.0189, Test Loss: 1.0610\n",
      "Epoch 17/26, Train Loss: 1.0400, Test Loss: 1.0614\n",
      "Epoch 18/26, Train Loss: 1.0333, Test Loss: 1.0607\n",
      "Epoch 19/26, Train Loss: 1.0493, Test Loss: 1.0585\n",
      "Epoch 20/26, Train Loss: 1.0110, Test Loss: 1.0617\n",
      "Epoch 21/26, Train Loss: 1.0299, Test Loss: 1.0600\n",
      "Epoch 22/26, Train Loss: 1.0335, Test Loss: 1.0631\n",
      "Epoch 23/26, Train Loss: 1.0174, Test Loss: 1.0587\n",
      "Epoch 24/26, Train Loss: 1.0284, Test Loss: 1.0596\n",
      "Epoch 25/26, Train Loss: 1.0181, Test Loss: 1.0579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:54:14,467] Trial 75 finished with value: 1.0587186472756522 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 228, 'layer_1_size': 153, 'layer_2_size': 178, 'layer_3_size': 118, 'layer_4_size': 83, 'layer_5_size': 52, 'layer_6_size': 181, 'layer_7_size': 163, 'layer_8_size': 96, 'layer_9_size': 188, 'layer_10_size': 150, 'layer_11_size': 118, 'layer_12_size': 215, 'layer_13_size': 38, 'dropout_rate': 0.28138683556646, 'learning_rate': 0.0005930662075624573, 'batch_size': 32, 'epochs': 26}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/26, Train Loss: 0.9990, Test Loss: 1.0587\n",
      "Epoch 1/43, Train Loss: 1.1875, Test Loss: 0.9882\n",
      "Epoch 2/43, Train Loss: 1.2586, Test Loss: 0.9857\n",
      "Epoch 3/43, Train Loss: 1.2447, Test Loss: 0.9838\n",
      "Epoch 4/43, Train Loss: 1.1984, Test Loss: 0.9827\n",
      "Epoch 5/43, Train Loss: 1.2429, Test Loss: 0.9828\n",
      "Epoch 6/43, Train Loss: 1.2362, Test Loss: 0.9834\n",
      "Epoch 7/43, Train Loss: 1.1942, Test Loss: 0.9835\n",
      "Epoch 8/43, Train Loss: 1.1895, Test Loss: 0.9843\n",
      "Epoch 9/43, Train Loss: 1.2126, Test Loss: 0.9866\n",
      "Epoch 10/43, Train Loss: 1.1614, Test Loss: 0.9865\n",
      "Epoch 11/43, Train Loss: 1.2086, Test Loss: 0.9893\n",
      "Epoch 12/43, Train Loss: 1.1885, Test Loss: 0.9899\n",
      "Epoch 13/43, Train Loss: 1.1741, Test Loss: 0.9892\n",
      "Epoch 14/43, Train Loss: 1.2240, Test Loss: 0.9897\n",
      "Epoch 15/43, Train Loss: 1.2299, Test Loss: 0.9899\n",
      "Epoch 16/43, Train Loss: 1.1909, Test Loss: 0.9920\n",
      "Epoch 17/43, Train Loss: 1.2015, Test Loss: 0.9914\n",
      "Epoch 18/43, Train Loss: 1.1710, Test Loss: 0.9918\n",
      "Epoch 19/43, Train Loss: 1.1881, Test Loss: 0.9924\n",
      "Epoch 20/43, Train Loss: 1.2254, Test Loss: 0.9896\n",
      "Epoch 21/43, Train Loss: 1.2323, Test Loss: 0.9894\n",
      "Epoch 22/43, Train Loss: 1.1909, Test Loss: 0.9927\n",
      "Epoch 23/43, Train Loss: 1.1779, Test Loss: 0.9924\n",
      "Epoch 24/43, Train Loss: 1.1906, Test Loss: 0.9915\n",
      "Epoch 25/43, Train Loss: 1.2090, Test Loss: 0.9908\n",
      "Epoch 26/43, Train Loss: 1.2331, Test Loss: 0.9883\n",
      "Epoch 27/43, Train Loss: 1.2172, Test Loss: 0.9907\n",
      "Epoch 28/43, Train Loss: 1.2161, Test Loss: 0.9900\n",
      "Epoch 29/43, Train Loss: 1.1919, Test Loss: 0.9893\n",
      "Epoch 30/43, Train Loss: 1.2208, Test Loss: 0.9893\n",
      "Epoch 31/43, Train Loss: 1.2322, Test Loss: 0.9894\n",
      "Epoch 32/43, Train Loss: 1.1849, Test Loss: 0.9926\n",
      "Epoch 33/43, Train Loss: 1.2166, Test Loss: 0.9934\n",
      "Epoch 34/43, Train Loss: 1.2127, Test Loss: 0.9928\n",
      "Epoch 35/43, Train Loss: 1.2143, Test Loss: 0.9922\n",
      "Epoch 36/43, Train Loss: 1.1833, Test Loss: 0.9919\n",
      "Epoch 37/43, Train Loss: 1.1876, Test Loss: 0.9939\n",
      "Epoch 38/43, Train Loss: 1.2505, Test Loss: 0.9939\n",
      "Epoch 39/43, Train Loss: 1.1679, Test Loss: 0.9927\n",
      "Epoch 40/43, Train Loss: 1.2347, Test Loss: 0.9917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:54:17,591] Trial 76 finished with value: 0.9900336265563965 and parameters: {'num_hidden_layers': 17, 'layer_0_size': 170, 'layer_1_size': 239, 'layer_2_size': 201, 'layer_3_size': 108, 'layer_4_size': 72, 'layer_5_size': 174, 'layer_6_size': 84, 'layer_7_size': 204, 'layer_8_size': 86, 'layer_9_size': 68, 'layer_10_size': 242, 'layer_11_size': 32, 'layer_12_size': 112, 'layer_13_size': 105, 'layer_14_size': 129, 'layer_15_size': 114, 'layer_16_size': 82, 'dropout_rate': 0.2550931972240799, 'learning_rate': 1.1861739511764381e-05, 'batch_size': 256, 'epochs': 43}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/43, Train Loss: 1.2580, Test Loss: 0.9925\n",
      "Epoch 42/43, Train Loss: 1.1755, Test Loss: 0.9914\n",
      "Epoch 43/43, Train Loss: 1.2415, Test Loss: 0.9900\n",
      "Epoch 1/53, Train Loss: 1.1359, Test Loss: 0.9493\n",
      "Epoch 2/53, Train Loss: 1.2067, Test Loss: 0.9477\n",
      "Epoch 3/53, Train Loss: 1.1519, Test Loss: 0.9126\n",
      "Epoch 4/53, Train Loss: 1.1450, Test Loss: 0.9189\n",
      "Epoch 5/53, Train Loss: 1.1373, Test Loss: 0.9391\n",
      "Epoch 6/53, Train Loss: 1.1323, Test Loss: 0.9393\n",
      "Epoch 7/53, Train Loss: 1.0922, Test Loss: 0.9515\n",
      "Epoch 8/53, Train Loss: 1.0990, Test Loss: 0.9430\n",
      "Epoch 9/53, Train Loss: 1.1319, Test Loss: 0.9505\n",
      "Epoch 10/53, Train Loss: 1.1339, Test Loss: 0.9524\n",
      "Epoch 11/53, Train Loss: 1.0852, Test Loss: 0.9539\n",
      "Epoch 12/53, Train Loss: 1.1271, Test Loss: 0.9671\n",
      "Epoch 13/53, Train Loss: 1.0692, Test Loss: 0.9392\n",
      "Epoch 14/53, Train Loss: 1.1413, Test Loss: 0.9332\n",
      "Epoch 15/53, Train Loss: 1.0850, Test Loss: 0.9500\n",
      "Epoch 16/53, Train Loss: 1.1343, Test Loss: 0.9256\n",
      "Epoch 17/53, Train Loss: 1.1260, Test Loss: 0.9367\n",
      "Epoch 18/53, Train Loss: 1.0784, Test Loss: 0.9737\n",
      "Epoch 19/53, Train Loss: 1.0561, Test Loss: 0.9829\n",
      "Epoch 20/53, Train Loss: 1.0965, Test Loss: 0.9483\n",
      "Epoch 21/53, Train Loss: 1.1051, Test Loss: 0.9484\n",
      "Epoch 22/53, Train Loss: 1.0924, Test Loss: 0.9557\n",
      "Epoch 23/53, Train Loss: 1.0880, Test Loss: 0.9584\n",
      "Epoch 24/53, Train Loss: 1.0737, Test Loss: 0.9613\n",
      "Epoch 25/53, Train Loss: 1.0606, Test Loss: 0.9368\n",
      "Epoch 26/53, Train Loss: 1.0551, Test Loss: 0.9502\n",
      "Epoch 27/53, Train Loss: 1.0801, Test Loss: 0.9325\n",
      "Epoch 28/53, Train Loss: 1.0791, Test Loss: 0.9279\n",
      "Epoch 29/53, Train Loss: 1.0749, Test Loss: 0.9433\n",
      "Epoch 30/53, Train Loss: 1.0725, Test Loss: 0.9459\n",
      "Epoch 31/53, Train Loss: 1.0623, Test Loss: 0.9276\n",
      "Epoch 32/53, Train Loss: 1.0929, Test Loss: 0.9116\n",
      "Epoch 33/53, Train Loss: 1.0613, Test Loss: 0.9129\n",
      "Epoch 34/53, Train Loss: 1.0791, Test Loss: 0.9145\n",
      "Epoch 35/53, Train Loss: 1.0353, Test Loss: 0.9341\n",
      "Epoch 36/53, Train Loss: 1.0696, Test Loss: 0.9236\n",
      "Epoch 37/53, Train Loss: 1.0680, Test Loss: 0.9191\n",
      "Epoch 38/53, Train Loss: 1.0697, Test Loss: 0.9178\n",
      "Epoch 39/53, Train Loss: 1.0675, Test Loss: 0.9168\n",
      "Epoch 40/53, Train Loss: 1.1040, Test Loss: 0.9190\n",
      "Epoch 41/53, Train Loss: 1.0625, Test Loss: 0.9126\n",
      "Epoch 42/53, Train Loss: 1.0912, Test Loss: 0.9238\n",
      "Epoch 43/53, Train Loss: 1.0640, Test Loss: 0.9220\n",
      "Epoch 44/53, Train Loss: 1.1005, Test Loss: 0.9233\n",
      "Epoch 45/53, Train Loss: 1.0469, Test Loss: 0.9219\n",
      "Epoch 46/53, Train Loss: 1.0634, Test Loss: 0.9209\n",
      "Epoch 47/53, Train Loss: 1.0397, Test Loss: 0.9174\n",
      "Epoch 48/53, Train Loss: 1.0748, Test Loss: 0.9275\n",
      "Epoch 49/53, Train Loss: 1.0891, Test Loss: 0.9121\n",
      "Epoch 50/53, Train Loss: 1.0580, Test Loss: 0.9156\n",
      "Epoch 51/53, Train Loss: 1.0905, Test Loss: 0.9168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:54:28,314] Trial 77 finished with value: 0.9154122131211417 and parameters: {'num_hidden_layers': 10, 'layer_0_size': 74, 'layer_1_size': 256, 'layer_2_size': 165, 'layer_3_size': 207, 'layer_4_size': 134, 'layer_5_size': 186, 'layer_6_size': 236, 'layer_7_size': 170, 'layer_8_size': 127, 'layer_9_size': 231, 'dropout_rate': 0.3026615286226391, 'learning_rate': 0.0001820953039556604, 'batch_size': 32, 'epochs': 53}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/53, Train Loss: 1.0417, Test Loss: 0.9217\n",
      "Epoch 53/53, Train Loss: 1.0511, Test Loss: 0.9154\n",
      "Epoch 1/67, Train Loss: 1.1995, Test Loss: 1.0374\n",
      "Epoch 2/67, Train Loss: 1.2571, Test Loss: 1.0477\n",
      "Epoch 3/67, Train Loss: 1.1322, Test Loss: 1.0550\n",
      "Epoch 4/67, Train Loss: 1.1312, Test Loss: 1.0555\n",
      "Epoch 5/67, Train Loss: 1.1453, Test Loss: 1.0595\n",
      "Epoch 6/67, Train Loss: 1.1636, Test Loss: 1.0567\n",
      "Epoch 7/67, Train Loss: 1.1137, Test Loss: 1.0539\n",
      "Epoch 8/67, Train Loss: 1.1343, Test Loss: 1.0669\n",
      "Epoch 9/67, Train Loss: 1.1381, Test Loss: 1.0654\n",
      "Epoch 10/67, Train Loss: 1.1337, Test Loss: 1.0643\n",
      "Epoch 11/67, Train Loss: 1.1248, Test Loss: 1.0643\n",
      "Epoch 12/67, Train Loss: 1.0917, Test Loss: 1.0703\n",
      "Epoch 13/67, Train Loss: 1.1071, Test Loss: 1.0650\n",
      "Epoch 14/67, Train Loss: 1.0800, Test Loss: 1.0716\n",
      "Epoch 15/67, Train Loss: 1.0882, Test Loss: 1.0723\n",
      "Epoch 16/67, Train Loss: 1.1312, Test Loss: 1.0819\n",
      "Epoch 17/67, Train Loss: 1.0871, Test Loss: 1.0837\n",
      "Epoch 18/67, Train Loss: 1.1196, Test Loss: 1.0903\n",
      "Epoch 19/67, Train Loss: 1.0436, Test Loss: 1.0848\n",
      "Epoch 20/67, Train Loss: 1.1053, Test Loss: 1.0776\n",
      "Epoch 21/67, Train Loss: 1.0562, Test Loss: 1.0811\n",
      "Epoch 22/67, Train Loss: 1.1087, Test Loss: 1.0888\n",
      "Epoch 23/67, Train Loss: 1.1161, Test Loss: 1.0830\n",
      "Epoch 24/67, Train Loss: 1.0812, Test Loss: 1.0876\n",
      "Epoch 25/67, Train Loss: 1.0862, Test Loss: 1.0900\n",
      "Epoch 26/67, Train Loss: 1.0771, Test Loss: 1.0793\n",
      "Epoch 27/67, Train Loss: 1.0907, Test Loss: 1.0869\n",
      "Epoch 28/67, Train Loss: 1.1338, Test Loss: 1.0888\n",
      "Epoch 29/67, Train Loss: 1.0469, Test Loss: 1.0834\n",
      "Epoch 30/67, Train Loss: 1.0983, Test Loss: 1.0814\n",
      "Epoch 31/67, Train Loss: 1.0800, Test Loss: 1.0868\n",
      "Epoch 32/67, Train Loss: 1.1066, Test Loss: 1.0868\n",
      "Epoch 33/67, Train Loss: 1.0464, Test Loss: 1.0782\n",
      "Epoch 34/67, Train Loss: 1.0777, Test Loss: 1.0816\n",
      "Epoch 35/67, Train Loss: 1.0990, Test Loss: 1.0808\n",
      "Epoch 36/67, Train Loss: 1.0753, Test Loss: 1.0883\n",
      "Epoch 37/67, Train Loss: 1.0666, Test Loss: 1.0870\n",
      "Epoch 38/67, Train Loss: 1.0657, Test Loss: 1.0792\n",
      "Epoch 39/67, Train Loss: 1.0698, Test Loss: 1.0780\n",
      "Epoch 40/67, Train Loss: 1.0632, Test Loss: 1.0826\n",
      "Epoch 41/67, Train Loss: 1.0713, Test Loss: 1.0781\n",
      "Epoch 42/67, Train Loss: 1.0572, Test Loss: 1.0784\n",
      "Epoch 43/67, Train Loss: 1.0858, Test Loss: 1.0751\n",
      "Epoch 44/67, Train Loss: 1.0369, Test Loss: 1.0760\n",
      "Epoch 45/67, Train Loss: 1.0842, Test Loss: 1.0673\n",
      "Epoch 46/67, Train Loss: 1.0350, Test Loss: 1.0721\n",
      "Epoch 47/67, Train Loss: 1.0351, Test Loss: 1.0789\n",
      "Epoch 48/67, Train Loss: 1.0792, Test Loss: 1.0732\n",
      "Epoch 49/67, Train Loss: 1.0918, Test Loss: 1.0744\n",
      "Epoch 50/67, Train Loss: 1.0779, Test Loss: 1.0856\n",
      "Epoch 51/67, Train Loss: 1.0602, Test Loss: 1.0749\n",
      "Epoch 52/67, Train Loss: 1.0718, Test Loss: 1.0827\n",
      "Epoch 53/67, Train Loss: 1.1059, Test Loss: 1.0837\n",
      "Epoch 54/67, Train Loss: 1.0422, Test Loss: 1.0874\n",
      "Epoch 55/67, Train Loss: 1.0647, Test Loss: 1.0872\n",
      "Epoch 56/67, Train Loss: 1.0528, Test Loss: 1.0876\n",
      "Epoch 57/67, Train Loss: 1.0655, Test Loss: 1.0782\n",
      "Epoch 58/67, Train Loss: 1.0667, Test Loss: 1.0904\n",
      "Epoch 59/67, Train Loss: 1.0423, Test Loss: 1.0872\n",
      "Epoch 60/67, Train Loss: 1.0773, Test Loss: 1.0809\n",
      "Epoch 61/67, Train Loss: 1.0750, Test Loss: 1.0754\n",
      "Epoch 62/67, Train Loss: 1.0555, Test Loss: 1.0807\n",
      "Epoch 63/67, Train Loss: 1.0663, Test Loss: 1.0860\n",
      "Epoch 64/67, Train Loss: 1.0297, Test Loss: 1.0910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:54:36,082] Trial 78 finished with value: 1.0886693894863129 and parameters: {'num_hidden_layers': 12, 'layer_0_size': 85, 'layer_1_size': 189, 'layer_2_size': 175, 'layer_3_size': 146, 'layer_4_size': 160, 'layer_5_size': 129, 'layer_6_size': 41, 'layer_7_size': 103, 'layer_8_size': 109, 'layer_9_size': 159, 'layer_10_size': 133, 'layer_11_size': 188, 'dropout_rate': 0.24804407863465575, 'learning_rate': 4.522224511353811e-05, 'batch_size': 64, 'epochs': 67}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/67, Train Loss: 1.0825, Test Loss: 1.0840\n",
      "Epoch 66/67, Train Loss: 1.0224, Test Loss: 1.0896\n",
      "Epoch 67/67, Train Loss: 1.0366, Test Loss: 1.0887\n",
      "Epoch 1/49, Train Loss: 1.1436, Test Loss: 1.1121\n",
      "Epoch 2/49, Train Loss: 1.1071, Test Loss: 1.0975\n",
      "Epoch 3/49, Train Loss: 1.0779, Test Loss: 1.0791\n",
      "Epoch 4/49, Train Loss: 1.0779, Test Loss: 1.0905\n",
      "Epoch 5/49, Train Loss: 1.0843, Test Loss: 1.0758\n",
      "Epoch 6/49, Train Loss: 1.0345, Test Loss: 1.0687\n",
      "Epoch 7/49, Train Loss: 1.0702, Test Loss: 1.1146\n",
      "Epoch 8/49, Train Loss: 1.0117, Test Loss: 1.0688\n",
      "Epoch 9/49, Train Loss: 1.0368, Test Loss: 1.0815\n",
      "Epoch 10/49, Train Loss: 1.0538, Test Loss: 1.0788\n",
      "Epoch 11/49, Train Loss: 0.9995, Test Loss: 1.0758\n",
      "Epoch 12/49, Train Loss: 1.0122, Test Loss: 1.0746\n",
      "Epoch 13/49, Train Loss: 1.0404, Test Loss: 1.0721\n",
      "Epoch 14/49, Train Loss: 1.0136, Test Loss: 1.0831\n",
      "Epoch 15/49, Train Loss: 0.9905, Test Loss: 1.0769\n",
      "Epoch 16/49, Train Loss: 0.9969, Test Loss: 1.0677\n",
      "Epoch 17/49, Train Loss: 0.9891, Test Loss: 1.0658\n",
      "Epoch 18/49, Train Loss: 0.9976, Test Loss: 1.0585\n",
      "Epoch 19/49, Train Loss: 1.0048, Test Loss: 1.0624\n",
      "Epoch 20/49, Train Loss: 0.9972, Test Loss: 1.0611\n",
      "Epoch 21/49, Train Loss: 1.0083, Test Loss: 1.0660\n",
      "Epoch 22/49, Train Loss: 1.0110, Test Loss: 1.0692\n",
      "Epoch 23/49, Train Loss: 1.0089, Test Loss: 1.0728\n",
      "Epoch 24/49, Train Loss: 1.0077, Test Loss: 1.0733\n",
      "Epoch 25/49, Train Loss: 0.9905, Test Loss: 1.0928\n",
      "Epoch 26/49, Train Loss: 0.9926, Test Loss: 1.0731\n",
      "Epoch 27/49, Train Loss: 0.9926, Test Loss: 1.0769\n",
      "Epoch 28/49, Train Loss: 1.0045, Test Loss: 1.0831\n",
      "Epoch 29/49, Train Loss: 0.9904, Test Loss: 1.0775\n",
      "Epoch 30/49, Train Loss: 0.9905, Test Loss: 1.0788\n",
      "Epoch 31/49, Train Loss: 1.0001, Test Loss: 1.0697\n",
      "Epoch 32/49, Train Loss: 1.0029, Test Loss: 1.0738\n",
      "Epoch 33/49, Train Loss: 1.0100, Test Loss: 1.0705\n",
      "Epoch 34/49, Train Loss: 0.9870, Test Loss: 1.0703\n",
      "Epoch 35/49, Train Loss: 0.9794, Test Loss: 1.0677\n",
      "Epoch 36/49, Train Loss: 1.0016, Test Loss: 1.0668\n",
      "Epoch 37/49, Train Loss: 0.9794, Test Loss: 1.0657\n",
      "Epoch 38/49, Train Loss: 0.9798, Test Loss: 1.0664\n",
      "Epoch 39/49, Train Loss: 0.9862, Test Loss: 1.0670\n",
      "Epoch 40/49, Train Loss: 0.9864, Test Loss: 1.0670\n",
      "Epoch 41/49, Train Loss: 0.9882, Test Loss: 1.0679\n",
      "Epoch 42/49, Train Loss: 0.9978, Test Loss: 1.0804\n",
      "Epoch 43/49, Train Loss: 0.9758, Test Loss: 1.0740\n",
      "Epoch 44/49, Train Loss: 0.9710, Test Loss: 1.0651\n",
      "Epoch 45/49, Train Loss: 0.9753, Test Loss: 1.0680\n",
      "Epoch 46/49, Train Loss: 0.9730, Test Loss: 1.0702\n",
      "Epoch 47/49, Train Loss: 0.9713, Test Loss: 1.0724\n",
      "Epoch 48/49, Train Loss: 0.9791, Test Loss: 1.0795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:54:47,734] Trial 79 finished with value: 1.071360775402614 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 142, 'layer_1_size': 102, 'layer_2_size': 130, 'layer_3_size': 230, 'layer_4_size': 172, 'layer_5_size': 76, 'layer_6_size': 127, 'layer_7_size': 213, 'layer_8_size': 137, 'layer_9_size': 125, 'layer_10_size': 113, 'layer_11_size': 206, 'layer_12_size': 160, 'dropout_rate': 0.26412089832790075, 'learning_rate': 0.0008695882579013078, 'batch_size': 32, 'epochs': 49}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/49, Train Loss: 0.9612, Test Loss: 1.0714\n",
      "Epoch 1/63, Train Loss: 1.1504, Test Loss: 1.0218\n",
      "Epoch 2/63, Train Loss: 1.0751, Test Loss: 1.0234\n",
      "Epoch 3/63, Train Loss: 1.0561, Test Loss: 1.0257\n",
      "Epoch 4/63, Train Loss: 1.0751, Test Loss: 1.0284\n",
      "Epoch 5/63, Train Loss: 1.0526, Test Loss: 1.0323\n",
      "Epoch 6/63, Train Loss: 1.0636, Test Loss: 1.0371\n",
      "Epoch 7/63, Train Loss: 1.0088, Test Loss: 1.0397\n",
      "Epoch 8/63, Train Loss: 1.0073, Test Loss: 1.0421\n",
      "Epoch 9/63, Train Loss: 1.0392, Test Loss: 1.0465\n",
      "Epoch 10/63, Train Loss: 0.9667, Test Loss: 1.0480\n",
      "Epoch 11/63, Train Loss: 0.9598, Test Loss: 1.0564\n",
      "Epoch 12/63, Train Loss: 0.9785, Test Loss: 1.0659\n",
      "Epoch 13/63, Train Loss: 0.9648, Test Loss: 1.0772\n",
      "Epoch 14/63, Train Loss: 0.9462, Test Loss: 1.0771\n",
      "Epoch 15/63, Train Loss: 0.9428, Test Loss: 1.0801\n",
      "Epoch 16/63, Train Loss: 0.8786, Test Loss: 1.0797\n",
      "Epoch 17/63, Train Loss: 0.8997, Test Loss: 1.0848\n",
      "Epoch 18/63, Train Loss: 0.8791, Test Loss: 1.0887\n",
      "Epoch 19/63, Train Loss: 0.8338, Test Loss: 1.0892\n",
      "Epoch 20/63, Train Loss: 0.8594, Test Loss: 1.1000\n",
      "Epoch 21/63, Train Loss: 0.7955, Test Loss: 1.1216\n",
      "Epoch 22/63, Train Loss: 0.8164, Test Loss: 1.1573\n",
      "Epoch 23/63, Train Loss: 0.7831, Test Loss: 1.1843\n",
      "Epoch 24/63, Train Loss: 0.7417, Test Loss: 1.2061\n",
      "Epoch 25/63, Train Loss: 0.7497, Test Loss: 1.2229\n",
      "Epoch 26/63, Train Loss: 0.7341, Test Loss: 1.2226\n",
      "Epoch 27/63, Train Loss: 0.6956, Test Loss: 1.2179\n",
      "Epoch 28/63, Train Loss: 0.6783, Test Loss: 1.2196\n",
      "Epoch 29/63, Train Loss: 0.6744, Test Loss: 1.2289\n",
      "Epoch 30/63, Train Loss: 0.6525, Test Loss: 1.2462\n",
      "Epoch 31/63, Train Loss: 0.6193, Test Loss: 1.2640\n",
      "Epoch 32/63, Train Loss: 0.6011, Test Loss: 1.2835\n",
      "Epoch 33/63, Train Loss: 0.6022, Test Loss: 1.2826\n",
      "Epoch 34/63, Train Loss: 0.5648, Test Loss: 1.2620\n",
      "Epoch 35/63, Train Loss: 0.5736, Test Loss: 1.2631\n",
      "Epoch 36/63, Train Loss: 0.5749, Test Loss: 1.2687\n",
      "Epoch 37/63, Train Loss: 0.5558, Test Loss: 1.2815\n",
      "Epoch 38/63, Train Loss: 0.5003, Test Loss: 1.3184\n",
      "Epoch 39/63, Train Loss: 0.5031, Test Loss: 1.3658\n",
      "Epoch 40/63, Train Loss: 0.5258, Test Loss: 1.3682\n",
      "Epoch 41/63, Train Loss: 0.4824, Test Loss: 1.3581\n",
      "Epoch 42/63, Train Loss: 0.4874, Test Loss: 1.3463\n",
      "Epoch 43/63, Train Loss: 0.4523, Test Loss: 1.3503\n",
      "Epoch 44/63, Train Loss: 0.4258, Test Loss: 1.3617\n",
      "Epoch 45/63, Train Loss: 0.4179, Test Loss: 1.3908\n",
      "Epoch 46/63, Train Loss: 0.4222, Test Loss: 1.3976\n",
      "Epoch 47/63, Train Loss: 0.4546, Test Loss: 1.3863\n",
      "Epoch 48/63, Train Loss: 0.4303, Test Loss: 1.3670\n",
      "Epoch 49/63, Train Loss: 0.3899, Test Loss: 1.3722\n",
      "Epoch 50/63, Train Loss: 0.4082, Test Loss: 1.3717\n",
      "Epoch 51/63, Train Loss: 0.3773, Test Loss: 1.3794\n",
      "Epoch 52/63, Train Loss: 0.3926, Test Loss: 1.3892\n",
      "Epoch 53/63, Train Loss: 0.3749, Test Loss: 1.3843\n",
      "Epoch 54/63, Train Loss: 0.3581, Test Loss: 1.4191\n",
      "Epoch 55/63, Train Loss: 0.3667, Test Loss: 1.4283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:54:49,431] Trial 80 finished with value: 1.4081567525863647 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 126, 'layer_1_size': 247, 'layer_2_size': 96, 'layer_3_size': 61, 'layer_4_size': 147, 'layer_5_size': 99, 'dropout_rate': 0.23790744086786625, 'learning_rate': 0.0016220335657933802, 'batch_size': 256, 'epochs': 63}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/63, Train Loss: 0.3615, Test Loss: 1.4141\n",
      "Epoch 57/63, Train Loss: 0.3677, Test Loss: 1.4091\n",
      "Epoch 58/63, Train Loss: 0.3549, Test Loss: 1.4031\n",
      "Epoch 59/63, Train Loss: 0.4002, Test Loss: 1.3830\n",
      "Epoch 60/63, Train Loss: 0.3393, Test Loss: 1.3741\n",
      "Epoch 61/63, Train Loss: 0.3407, Test Loss: 1.4066\n",
      "Epoch 62/63, Train Loss: 0.3301, Test Loss: 1.4290\n",
      "Epoch 63/63, Train Loss: 0.3150, Test Loss: 1.4082\n",
      "Epoch 1/70, Train Loss: 1.1002, Test Loss: 0.9584\n",
      "Epoch 2/70, Train Loss: 1.1150, Test Loss: 0.9482\n",
      "Epoch 3/70, Train Loss: 1.1236, Test Loss: 0.9566\n",
      "Epoch 4/70, Train Loss: 1.0462, Test Loss: 0.9493\n",
      "Epoch 5/70, Train Loss: 1.1169, Test Loss: 0.9405\n",
      "Epoch 6/70, Train Loss: 1.1146, Test Loss: 0.9449\n",
      "Epoch 7/70, Train Loss: 1.1321, Test Loss: 0.9480\n",
      "Epoch 8/70, Train Loss: 1.1011, Test Loss: 0.9501\n",
      "Epoch 9/70, Train Loss: 1.0870, Test Loss: 0.9445\n",
      "Epoch 10/70, Train Loss: 1.0842, Test Loss: 0.9501\n",
      "Epoch 11/70, Train Loss: 1.1085, Test Loss: 0.9570\n",
      "Epoch 12/70, Train Loss: 1.0894, Test Loss: 0.9447\n",
      "Epoch 13/70, Train Loss: 1.0475, Test Loss: 0.9545\n",
      "Epoch 14/70, Train Loss: 1.0482, Test Loss: 0.9502\n",
      "Epoch 15/70, Train Loss: 1.0978, Test Loss: 0.9439\n",
      "Epoch 16/70, Train Loss: 1.0693, Test Loss: 0.9495\n",
      "Epoch 17/70, Train Loss: 1.0837, Test Loss: 0.9454\n",
      "Epoch 18/70, Train Loss: 1.0141, Test Loss: 0.9492\n",
      "Epoch 19/70, Train Loss: 1.0652, Test Loss: 0.9443\n",
      "Epoch 20/70, Train Loss: 1.0739, Test Loss: 0.9502\n",
      "Epoch 21/70, Train Loss: 1.0733, Test Loss: 0.9498\n",
      "Epoch 22/70, Train Loss: 1.0360, Test Loss: 0.9459\n",
      "Epoch 23/70, Train Loss: 1.0798, Test Loss: 0.9414\n",
      "Epoch 24/70, Train Loss: 1.0618, Test Loss: 0.9439\n",
      "Epoch 25/70, Train Loss: 1.0248, Test Loss: 0.9455\n",
      "Epoch 26/70, Train Loss: 1.0447, Test Loss: 0.9507\n",
      "Epoch 27/70, Train Loss: 1.0406, Test Loss: 0.9491\n",
      "Epoch 28/70, Train Loss: 1.0316, Test Loss: 0.9487\n",
      "Epoch 29/70, Train Loss: 1.0408, Test Loss: 0.9529\n",
      "Epoch 30/70, Train Loss: 0.9956, Test Loss: 0.9508\n",
      "Epoch 31/70, Train Loss: 1.0107, Test Loss: 0.9467\n",
      "Epoch 32/70, Train Loss: 1.0456, Test Loss: 0.9491\n",
      "Epoch 33/70, Train Loss: 1.0118, Test Loss: 0.9498\n",
      "Epoch 34/70, Train Loss: 1.0230, Test Loss: 0.9495\n",
      "Epoch 35/70, Train Loss: 1.0440, Test Loss: 0.9513\n",
      "Epoch 36/70, Train Loss: 0.9960, Test Loss: 0.9474\n",
      "Epoch 37/70, Train Loss: 1.0277, Test Loss: 0.9484\n",
      "Epoch 38/70, Train Loss: 1.0475, Test Loss: 0.9480\n",
      "Epoch 39/70, Train Loss: 1.0084, Test Loss: 0.9474\n",
      "Epoch 40/70, Train Loss: 1.0255, Test Loss: 0.9547\n",
      "Epoch 41/70, Train Loss: 1.0040, Test Loss: 0.9498\n",
      "Epoch 42/70, Train Loss: 1.0525, Test Loss: 0.9485\n",
      "Epoch 43/70, Train Loss: 1.0356, Test Loss: 0.9511\n",
      "Epoch 44/70, Train Loss: 1.0082, Test Loss: 0.9510\n",
      "Epoch 45/70, Train Loss: 1.0482, Test Loss: 0.9525\n",
      "Epoch 46/70, Train Loss: 1.0137, Test Loss: 0.9556\n",
      "Epoch 47/70, Train Loss: 1.0131, Test Loss: 0.9543\n",
      "Epoch 48/70, Train Loss: 1.0663, Test Loss: 0.9519\n",
      "Epoch 49/70, Train Loss: 1.0521, Test Loss: 0.9533\n",
      "Epoch 50/70, Train Loss: 1.0372, Test Loss: 0.9509\n",
      "Epoch 51/70, Train Loss: 1.0190, Test Loss: 0.9492\n",
      "Epoch 52/70, Train Loss: 1.0299, Test Loss: 0.9476\n",
      "Epoch 53/70, Train Loss: 1.0477, Test Loss: 0.9530\n",
      "Epoch 54/70, Train Loss: 1.0440, Test Loss: 0.9491\n",
      "Epoch 55/70, Train Loss: 1.0069, Test Loss: 0.9644\n",
      "Epoch 56/70, Train Loss: 1.0324, Test Loss: 0.9615\n",
      "Epoch 57/70, Train Loss: 1.0445, Test Loss: 0.9529\n",
      "Epoch 58/70, Train Loss: 1.0165, Test Loss: 0.9656\n",
      "Epoch 59/70, Train Loss: 1.0311, Test Loss: 0.9651\n",
      "Epoch 60/70, Train Loss: 1.0413, Test Loss: 0.9574\n",
      "Epoch 61/70, Train Loss: 1.0085, Test Loss: 0.9600\n",
      "Epoch 62/70, Train Loss: 1.0217, Test Loss: 0.9501\n",
      "Epoch 63/70, Train Loss: 1.0053, Test Loss: 0.9590\n",
      "Epoch 64/70, Train Loss: 1.0273, Test Loss: 0.9468\n",
      "Epoch 65/70, Train Loss: 1.0094, Test Loss: 0.9539\n",
      "Epoch 66/70, Train Loss: 1.0127, Test Loss: 0.9613\n",
      "Epoch 67/70, Train Loss: 1.0184, Test Loss: 0.9531\n",
      "Epoch 68/70, Train Loss: 1.0096, Test Loss: 0.9530\n",
      "Epoch 69/70, Train Loss: 1.0233, Test Loss: 0.9565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:55:13,826] Trial 81 finished with value: 0.9545241892337799 and parameters: {'num_hidden_layers': 20, 'layer_0_size': 108, 'layer_1_size': 163, 'layer_2_size': 151, 'layer_3_size': 135, 'layer_4_size': 218, 'layer_5_size': 216, 'layer_6_size': 58, 'layer_7_size': 83, 'layer_8_size': 213, 'layer_9_size': 244, 'layer_10_size': 36, 'layer_11_size': 169, 'layer_12_size': 237, 'layer_13_size': 76, 'layer_14_size': 222, 'layer_15_size': 144, 'layer_16_size': 200, 'layer_17_size': 156, 'layer_18_size': 41, 'layer_19_size': 91, 'dropout_rate': 0.22439390225471076, 'learning_rate': 4.55260826405838e-05, 'batch_size': 32, 'epochs': 70}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/70, Train Loss: 1.0325, Test Loss: 0.9545\n",
      "Epoch 1/78, Train Loss: 1.1305, Test Loss: 1.0600\n",
      "Epoch 2/78, Train Loss: 1.1063, Test Loss: 1.0651\n",
      "Epoch 3/78, Train Loss: 1.1617, Test Loss: 1.0772\n",
      "Epoch 4/78, Train Loss: 1.1104, Test Loss: 1.0832\n",
      "Epoch 5/78, Train Loss: 1.0862, Test Loss: 1.0803\n",
      "Epoch 6/78, Train Loss: 1.1267, Test Loss: 1.0859\n",
      "Epoch 7/78, Train Loss: 1.1039, Test Loss: 1.0881\n",
      "Epoch 8/78, Train Loss: 1.1267, Test Loss: 1.0839\n",
      "Epoch 9/78, Train Loss: 1.0945, Test Loss: 1.0743\n",
      "Epoch 10/78, Train Loss: 1.0913, Test Loss: 1.0716\n",
      "Epoch 11/78, Train Loss: 1.0790, Test Loss: 1.0723\n",
      "Epoch 12/78, Train Loss: 1.0915, Test Loss: 1.0722\n",
      "Epoch 13/78, Train Loss: 1.1099, Test Loss: 1.0780\n",
      "Epoch 14/78, Train Loss: 1.1096, Test Loss: 1.0760\n",
      "Epoch 15/78, Train Loss: 1.0771, Test Loss: 1.0698\n",
      "Epoch 16/78, Train Loss: 1.0658, Test Loss: 1.0711\n",
      "Epoch 17/78, Train Loss: 1.0870, Test Loss: 1.0663\n",
      "Epoch 18/78, Train Loss: 1.0616, Test Loss: 1.0647\n",
      "Epoch 19/78, Train Loss: 1.0655, Test Loss: 1.0672\n",
      "Epoch 20/78, Train Loss: 1.0583, Test Loss: 1.0671\n",
      "Epoch 21/78, Train Loss: 1.1473, Test Loss: 1.0660\n",
      "Epoch 22/78, Train Loss: 1.0967, Test Loss: 1.0698\n",
      "Epoch 23/78, Train Loss: 1.1058, Test Loss: 1.0726\n",
      "Epoch 24/78, Train Loss: 1.1122, Test Loss: 1.0701\n",
      "Epoch 25/78, Train Loss: 1.0725, Test Loss: 1.0677\n",
      "Epoch 26/78, Train Loss: 1.1206, Test Loss: 1.0684\n",
      "Epoch 27/78, Train Loss: 1.0721, Test Loss: 1.0692\n",
      "Epoch 28/78, Train Loss: 1.1068, Test Loss: 1.0617\n",
      "Epoch 29/78, Train Loss: 1.0622, Test Loss: 1.0580\n",
      "Epoch 30/78, Train Loss: 1.0783, Test Loss: 1.0647\n",
      "Epoch 31/78, Train Loss: 1.0972, Test Loss: 1.0654\n",
      "Epoch 32/78, Train Loss: 1.0777, Test Loss: 1.0588\n",
      "Epoch 33/78, Train Loss: 1.0880, Test Loss: 1.0601\n",
      "Epoch 34/78, Train Loss: 1.0865, Test Loss: 1.0628\n",
      "Epoch 35/78, Train Loss: 1.0660, Test Loss: 1.0617\n",
      "Epoch 36/78, Train Loss: 1.0689, Test Loss: 1.0614\n",
      "Epoch 37/78, Train Loss: 1.0576, Test Loss: 1.0621\n",
      "Epoch 38/78, Train Loss: 1.1074, Test Loss: 1.0621\n",
      "Epoch 39/78, Train Loss: 1.0573, Test Loss: 1.0644\n",
      "Epoch 40/78, Train Loss: 1.0812, Test Loss: 1.0623\n",
      "Epoch 41/78, Train Loss: 1.0527, Test Loss: 1.0618\n",
      "Epoch 42/78, Train Loss: 1.0575, Test Loss: 1.0630\n",
      "Epoch 43/78, Train Loss: 1.0846, Test Loss: 1.0640\n",
      "Epoch 44/78, Train Loss: 1.0904, Test Loss: 1.0611\n",
      "Epoch 45/78, Train Loss: 1.0614, Test Loss: 1.0616\n",
      "Epoch 46/78, Train Loss: 1.0721, Test Loss: 1.0630\n",
      "Epoch 47/78, Train Loss: 1.0497, Test Loss: 1.0628\n",
      "Epoch 48/78, Train Loss: 1.0586, Test Loss: 1.0628\n",
      "Epoch 49/78, Train Loss: 1.1066, Test Loss: 1.0634\n",
      "Epoch 50/78, Train Loss: 1.0820, Test Loss: 1.0608\n",
      "Epoch 51/78, Train Loss: 1.0553, Test Loss: 1.0620\n",
      "Epoch 52/78, Train Loss: 1.0632, Test Loss: 1.0643\n",
      "Epoch 53/78, Train Loss: 1.0797, Test Loss: 1.0623\n",
      "Epoch 54/78, Train Loss: 1.0790, Test Loss: 1.0608\n",
      "Epoch 55/78, Train Loss: 1.0710, Test Loss: 1.0614\n",
      "Epoch 56/78, Train Loss: 1.0638, Test Loss: 1.0617\n",
      "Epoch 57/78, Train Loss: 1.0401, Test Loss: 1.0612\n",
      "Epoch 58/78, Train Loss: 1.0763, Test Loss: 1.0620\n",
      "Epoch 59/78, Train Loss: 1.0647, Test Loss: 1.0602\n",
      "Epoch 60/78, Train Loss: 1.0931, Test Loss: 1.0584\n",
      "Epoch 61/78, Train Loss: 1.0646, Test Loss: 1.0584\n",
      "Epoch 62/78, Train Loss: 1.0740, Test Loss: 1.0602\n",
      "Epoch 63/78, Train Loss: 1.0468, Test Loss: 1.0584\n",
      "Epoch 64/78, Train Loss: 1.0661, Test Loss: 1.0610\n",
      "Epoch 65/78, Train Loss: 1.0606, Test Loss: 1.0583\n",
      "Epoch 66/78, Train Loss: 1.0452, Test Loss: 1.0604\n",
      "Epoch 67/78, Train Loss: 1.0425, Test Loss: 1.0608\n",
      "Epoch 68/78, Train Loss: 1.0604, Test Loss: 1.0609\n",
      "Epoch 69/78, Train Loss: 1.0561, Test Loss: 1.0598\n",
      "Epoch 70/78, Train Loss: 1.0801, Test Loss: 1.0594\n",
      "Epoch 71/78, Train Loss: 1.0620, Test Loss: 1.0594\n",
      "Epoch 72/78, Train Loss: 1.0652, Test Loss: 1.0612\n",
      "Epoch 73/78, Train Loss: 1.0556, Test Loss: 1.0603\n",
      "Epoch 74/78, Train Loss: 1.0619, Test Loss: 1.0605\n",
      "Epoch 75/78, Train Loss: 1.0383, Test Loss: 1.0589\n",
      "Epoch 76/78, Train Loss: 1.0629, Test Loss: 1.0577\n",
      "Epoch 77/78, Train Loss: 1.0574, Test Loss: 1.0605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:55:22,637] Trial 82 finished with value: 1.0625247359275818 and parameters: {'num_hidden_layers': 18, 'layer_0_size': 92, 'layer_1_size': 171, 'layer_2_size': 159, 'layer_3_size': 122, 'layer_4_size': 188, 'layer_5_size': 212, 'layer_6_size': 62, 'layer_7_size': 47, 'layer_8_size': 190, 'layer_9_size': 251, 'layer_10_size': 72, 'layer_11_size': 176, 'layer_12_size': 249, 'layer_13_size': 68, 'layer_14_size': 255, 'layer_15_size': 146, 'layer_16_size': 197, 'layer_17_size': 149, 'dropout_rate': 0.19644335595489587, 'learning_rate': 8.026488970184681e-05, 'batch_size': 128, 'epochs': 78}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/78, Train Loss: 1.0717, Test Loss: 1.0625\n",
      "Epoch 1/74, Train Loss: 1.0889, Test Loss: 0.9616\n",
      "Epoch 2/74, Train Loss: 1.1508, Test Loss: 0.9612\n",
      "Epoch 3/74, Train Loss: 1.0713, Test Loss: 0.9573\n",
      "Epoch 4/74, Train Loss: 1.1422, Test Loss: 0.9626\n",
      "Epoch 5/74, Train Loss: 1.0980, Test Loss: 0.9640\n",
      "Epoch 6/74, Train Loss: 1.0950, Test Loss: 0.9706\n",
      "Epoch 7/74, Train Loss: 1.1175, Test Loss: 0.9678\n",
      "Epoch 8/74, Train Loss: 1.0919, Test Loss: 0.9723\n",
      "Epoch 9/74, Train Loss: 1.0632, Test Loss: 0.9719\n",
      "Epoch 10/74, Train Loss: 1.0679, Test Loss: 0.9719\n",
      "Epoch 11/74, Train Loss: 1.1060, Test Loss: 0.9736\n",
      "Epoch 12/74, Train Loss: 1.1075, Test Loss: 0.9749\n",
      "Epoch 13/74, Train Loss: 1.0778, Test Loss: 0.9738\n",
      "Epoch 14/74, Train Loss: 1.0441, Test Loss: 0.9667\n",
      "Epoch 15/74, Train Loss: 1.0718, Test Loss: 0.9682\n",
      "Epoch 16/74, Train Loss: 1.0545, Test Loss: 0.9645\n",
      "Epoch 17/74, Train Loss: 1.0284, Test Loss: 0.9666\n",
      "Epoch 18/74, Train Loss: 1.0821, Test Loss: 0.9650\n",
      "Epoch 19/74, Train Loss: 1.0219, Test Loss: 0.9637\n",
      "Epoch 20/74, Train Loss: 1.0457, Test Loss: 0.9651\n",
      "Epoch 21/74, Train Loss: 1.0580, Test Loss: 0.9701\n",
      "Epoch 22/74, Train Loss: 1.0622, Test Loss: 0.9672\n",
      "Epoch 23/74, Train Loss: 1.0387, Test Loss: 0.9698\n",
      "Epoch 24/74, Train Loss: 1.0746, Test Loss: 0.9691\n",
      "Epoch 25/74, Train Loss: 1.0568, Test Loss: 0.9676\n",
      "Epoch 26/74, Train Loss: 1.0617, Test Loss: 0.9650\n",
      "Epoch 27/74, Train Loss: 1.0504, Test Loss: 0.9643\n",
      "Epoch 28/74, Train Loss: 1.0333, Test Loss: 0.9673\n",
      "Epoch 29/74, Train Loss: 1.0888, Test Loss: 0.9668\n",
      "Epoch 30/74, Train Loss: 1.0480, Test Loss: 0.9705\n",
      "Epoch 31/74, Train Loss: 1.0567, Test Loss: 0.9695\n",
      "Epoch 32/74, Train Loss: 1.0863, Test Loss: 0.9690\n",
      "Epoch 33/74, Train Loss: 1.0642, Test Loss: 0.9680\n",
      "Epoch 34/74, Train Loss: 1.0608, Test Loss: 0.9648\n",
      "Epoch 35/74, Train Loss: 1.0736, Test Loss: 0.9664\n",
      "Epoch 36/74, Train Loss: 1.0252, Test Loss: 0.9629\n",
      "Epoch 37/74, Train Loss: 1.0213, Test Loss: 0.9628\n",
      "Epoch 38/74, Train Loss: 1.0239, Test Loss: 0.9599\n",
      "Epoch 39/74, Train Loss: 1.0539, Test Loss: 0.9638\n",
      "Epoch 40/74, Train Loss: 1.0515, Test Loss: 0.9613\n",
      "Epoch 41/74, Train Loss: 1.0712, Test Loss: 0.9610\n",
      "Epoch 42/74, Train Loss: 1.0301, Test Loss: 0.9621\n",
      "Epoch 43/74, Train Loss: 1.0232, Test Loss: 0.9619\n",
      "Epoch 44/74, Train Loss: 1.0397, Test Loss: 0.9591\n",
      "Epoch 45/74, Train Loss: 1.0439, Test Loss: 0.9615\n",
      "Epoch 46/74, Train Loss: 1.0494, Test Loss: 0.9584\n",
      "Epoch 47/74, Train Loss: 1.0385, Test Loss: 0.9587\n",
      "Epoch 48/74, Train Loss: 1.0468, Test Loss: 0.9563\n",
      "Epoch 49/74, Train Loss: 1.0496, Test Loss: 0.9560\n",
      "Epoch 50/74, Train Loss: 1.0387, Test Loss: 0.9565\n",
      "Epoch 51/74, Train Loss: 1.0567, Test Loss: 0.9587\n",
      "Epoch 52/74, Train Loss: 1.0770, Test Loss: 0.9630\n",
      "Epoch 53/74, Train Loss: 1.0478, Test Loss: 0.9640\n",
      "Epoch 54/74, Train Loss: 1.0266, Test Loss: 0.9653\n",
      "Epoch 55/74, Train Loss: 1.0692, Test Loss: 0.9618\n",
      "Epoch 56/74, Train Loss: 1.0670, Test Loss: 0.9615\n",
      "Epoch 57/74, Train Loss: 1.0552, Test Loss: 0.9631\n",
      "Epoch 58/74, Train Loss: 1.0518, Test Loss: 0.9613\n",
      "Epoch 59/74, Train Loss: 1.0286, Test Loss: 0.9612\n",
      "Epoch 60/74, Train Loss: 1.0469, Test Loss: 0.9617\n",
      "Epoch 61/74, Train Loss: 1.0420, Test Loss: 0.9623\n",
      "Epoch 62/74, Train Loss: 1.0383, Test Loss: 0.9613\n",
      "Epoch 63/74, Train Loss: 1.0354, Test Loss: 0.9660\n",
      "Epoch 64/74, Train Loss: 1.0363, Test Loss: 0.9651\n",
      "Epoch 65/74, Train Loss: 1.0330, Test Loss: 0.9647\n",
      "Epoch 66/74, Train Loss: 1.0706, Test Loss: 0.9650\n",
      "Epoch 67/74, Train Loss: 1.0506, Test Loss: 0.9699\n",
      "Epoch 68/74, Train Loss: 1.0362, Test Loss: 0.9697\n",
      "Epoch 69/74, Train Loss: 1.0422, Test Loss: 0.9678\n",
      "Epoch 70/74, Train Loss: 1.0218, Test Loss: 0.9658\n",
      "Epoch 71/74, Train Loss: 1.0412, Test Loss: 0.9667\n",
      "Epoch 72/74, Train Loss: 1.0624, Test Loss: 0.9634\n",
      "Epoch 73/74, Train Loss: 1.0498, Test Loss: 0.9607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:55:29,757] Trial 83 finished with value: 0.9620230197906494 and parameters: {'num_hidden_layers': 15, 'layer_0_size': 100, 'layer_1_size': 163, 'layer_2_size': 189, 'layer_3_size': 127, 'layer_4_size': 206, 'layer_5_size': 201, 'layer_6_size': 52, 'layer_7_size': 61, 'layer_8_size': 180, 'layer_9_size': 239, 'layer_10_size': 171, 'layer_11_size': 160, 'layer_12_size': 238, 'layer_13_size': 54, 'layer_14_size': 210, 'dropout_rate': 0.18133361823691774, 'learning_rate': 9.343800325446555e-05, 'batch_size': 128, 'epochs': 74}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/74, Train Loss: 1.0387, Test Loss: 0.9620\n",
      "Epoch 1/56, Train Loss: 1.1848, Test Loss: 0.9417\n",
      "Epoch 2/56, Train Loss: 1.1918, Test Loss: 0.9770\n",
      "Epoch 3/56, Train Loss: 1.1782, Test Loss: 1.0229\n",
      "Epoch 4/56, Train Loss: 1.2259, Test Loss: 1.0503\n",
      "Epoch 5/56, Train Loss: 1.2191, Test Loss: 1.0679\n",
      "Epoch 6/56, Train Loss: 1.1386, Test Loss: 1.0517\n",
      "Epoch 7/56, Train Loss: 1.1438, Test Loss: 1.0331\n",
      "Epoch 8/56, Train Loss: 1.1089, Test Loss: 1.0348\n",
      "Epoch 9/56, Train Loss: 1.1747, Test Loss: 1.0177\n",
      "Epoch 10/56, Train Loss: 1.1104, Test Loss: 1.0141\n",
      "Epoch 11/56, Train Loss: 1.1578, Test Loss: 1.0183\n",
      "Epoch 12/56, Train Loss: 1.1720, Test Loss: 1.0134\n",
      "Epoch 13/56, Train Loss: 1.0800, Test Loss: 0.9981\n",
      "Epoch 14/56, Train Loss: 1.1119, Test Loss: 0.9961\n",
      "Epoch 15/56, Train Loss: 1.1201, Test Loss: 0.9964\n",
      "Epoch 16/56, Train Loss: 1.1374, Test Loss: 0.9924\n",
      "Epoch 17/56, Train Loss: 1.1287, Test Loss: 0.9787\n",
      "Epoch 18/56, Train Loss: 1.1429, Test Loss: 0.9797\n",
      "Epoch 19/56, Train Loss: 1.1436, Test Loss: 0.9728\n",
      "Epoch 20/56, Train Loss: 1.1376, Test Loss: 0.9782\n",
      "Epoch 21/56, Train Loss: 1.1197, Test Loss: 0.9762\n",
      "Epoch 22/56, Train Loss: 1.1253, Test Loss: 0.9647\n",
      "Epoch 23/56, Train Loss: 1.1329, Test Loss: 0.9679\n",
      "Epoch 24/56, Train Loss: 1.1226, Test Loss: 0.9681\n",
      "Epoch 25/56, Train Loss: 1.1079, Test Loss: 0.9692\n",
      "Epoch 26/56, Train Loss: 1.1210, Test Loss: 0.9628\n",
      "Epoch 27/56, Train Loss: 1.1175, Test Loss: 0.9639\n",
      "Epoch 28/56, Train Loss: 1.1206, Test Loss: 0.9650\n",
      "Epoch 29/56, Train Loss: 1.1291, Test Loss: 0.9636\n",
      "Epoch 30/56, Train Loss: 1.1096, Test Loss: 0.9601\n",
      "Epoch 31/56, Train Loss: 1.1568, Test Loss: 0.9636\n",
      "Epoch 32/56, Train Loss: 1.1285, Test Loss: 0.9552\n",
      "Epoch 33/56, Train Loss: 1.1031, Test Loss: 0.9549\n",
      "Epoch 34/56, Train Loss: 1.1211, Test Loss: 0.9580\n",
      "Epoch 35/56, Train Loss: 1.0837, Test Loss: 0.9557\n",
      "Epoch 36/56, Train Loss: 1.0917, Test Loss: 0.9510\n",
      "Epoch 37/56, Train Loss: 1.1092, Test Loss: 0.9465\n",
      "Epoch 38/56, Train Loss: 1.0827, Test Loss: 0.9511\n",
      "Epoch 39/56, Train Loss: 1.1118, Test Loss: 0.9416\n",
      "Epoch 40/56, Train Loss: 1.0896, Test Loss: 0.9468\n",
      "Epoch 41/56, Train Loss: 1.1101, Test Loss: 0.9426\n",
      "Epoch 42/56, Train Loss: 1.1038, Test Loss: 0.9463\n",
      "Epoch 43/56, Train Loss: 1.1379, Test Loss: 0.9493\n",
      "Epoch 44/56, Train Loss: 1.1627, Test Loss: 0.9432\n",
      "Epoch 45/56, Train Loss: 1.1241, Test Loss: 0.9388\n",
      "Epoch 46/56, Train Loss: 1.1222, Test Loss: 0.9444\n",
      "Epoch 47/56, Train Loss: 1.1343, Test Loss: 0.9424\n",
      "Epoch 48/56, Train Loss: 1.1071, Test Loss: 0.9479\n",
      "Epoch 49/56, Train Loss: 1.1571, Test Loss: 0.9468\n",
      "Epoch 50/56, Train Loss: 1.0810, Test Loss: 0.9465\n",
      "Epoch 51/56, Train Loss: 1.1545, Test Loss: 0.9442\n",
      "Epoch 52/56, Train Loss: 1.0864, Test Loss: 0.9418\n",
      "Epoch 53/56, Train Loss: 1.1096, Test Loss: 0.9401\n",
      "Epoch 54/56, Train Loss: 1.1028, Test Loss: 0.9386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:55:35,956] Trial 84 finished with value: 0.9417665004730225 and parameters: {'num_hidden_layers': 18, 'layer_0_size': 111, 'layer_1_size': 191, 'layer_2_size': 139, 'layer_3_size': 138, 'layer_4_size': 241, 'layer_5_size': 207, 'layer_6_size': 70, 'layer_7_size': 35, 'layer_8_size': 164, 'layer_9_size': 228, 'layer_10_size': 35, 'layer_11_size': 132, 'layer_12_size': 232, 'layer_13_size': 88, 'layer_14_size': 186, 'layer_15_size': 87, 'layer_16_size': 192, 'layer_17_size': 109, 'dropout_rate': 0.21163624318177385, 'learning_rate': 1.8398716279702684e-05, 'batch_size': 128, 'epochs': 56}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/56, Train Loss: 1.0978, Test Loss: 0.9393\n",
      "Epoch 56/56, Train Loss: 1.0776, Test Loss: 0.9418\n",
      "Epoch 1/59, Train Loss: 1.1835, Test Loss: 1.1615\n",
      "Epoch 2/59, Train Loss: 1.1063, Test Loss: 1.1424\n",
      "Epoch 3/59, Train Loss: 1.1169, Test Loss: 1.1384\n",
      "Epoch 4/59, Train Loss: 1.1179, Test Loss: 1.1430\n",
      "Epoch 5/59, Train Loss: 1.1416, Test Loss: 1.1436\n",
      "Epoch 6/59, Train Loss: 1.1351, Test Loss: 1.1504\n",
      "Epoch 7/59, Train Loss: 1.1111, Test Loss: 1.1439\n",
      "Epoch 8/59, Train Loss: 1.0981, Test Loss: 1.1457\n",
      "Epoch 9/59, Train Loss: 1.1460, Test Loss: 1.1496\n",
      "Epoch 10/59, Train Loss: 1.0973, Test Loss: 1.1533\n",
      "Epoch 11/59, Train Loss: 1.1410, Test Loss: 1.1562\n",
      "Epoch 12/59, Train Loss: 1.1220, Test Loss: 1.1526\n",
      "Epoch 13/59, Train Loss: 1.0976, Test Loss: 1.1586\n",
      "Epoch 14/59, Train Loss: 1.1036, Test Loss: 1.1526\n",
      "Epoch 15/59, Train Loss: 1.1375, Test Loss: 1.1585\n",
      "Epoch 16/59, Train Loss: 1.1300, Test Loss: 1.1673\n",
      "Epoch 17/59, Train Loss: 1.1081, Test Loss: 1.1587\n",
      "Epoch 18/59, Train Loss: 1.0798, Test Loss: 1.1639\n",
      "Epoch 19/59, Train Loss: 1.1349, Test Loss: 1.1646\n",
      "Epoch 20/59, Train Loss: 1.1705, Test Loss: 1.1610\n",
      "Epoch 21/59, Train Loss: 1.1072, Test Loss: 1.1651\n",
      "Epoch 22/59, Train Loss: 1.0730, Test Loss: 1.1662\n",
      "Epoch 23/59, Train Loss: 1.1609, Test Loss: 1.1650\n",
      "Epoch 24/59, Train Loss: 1.0995, Test Loss: 1.1672\n",
      "Epoch 25/59, Train Loss: 1.1290, Test Loss: 1.1653\n",
      "Epoch 26/59, Train Loss: 1.1198, Test Loss: 1.1661\n",
      "Epoch 27/59, Train Loss: 1.1188, Test Loss: 1.1682\n",
      "Epoch 28/59, Train Loss: 1.0462, Test Loss: 1.1610\n",
      "Epoch 29/59, Train Loss: 1.1339, Test Loss: 1.1613\n",
      "Epoch 30/59, Train Loss: 1.1200, Test Loss: 1.1603\n",
      "Epoch 31/59, Train Loss: 1.1382, Test Loss: 1.1602\n",
      "Epoch 32/59, Train Loss: 1.1006, Test Loss: 1.1634\n",
      "Epoch 33/59, Train Loss: 1.0941, Test Loss: 1.1645\n",
      "Epoch 34/59, Train Loss: 1.1401, Test Loss: 1.1614\n",
      "Epoch 35/59, Train Loss: 1.0637, Test Loss: 1.1565\n",
      "Epoch 36/59, Train Loss: 1.1595, Test Loss: 1.1622\n",
      "Epoch 37/59, Train Loss: 1.1010, Test Loss: 1.1592\n",
      "Epoch 38/59, Train Loss: 1.1137, Test Loss: 1.1633\n",
      "Epoch 39/59, Train Loss: 1.0931, Test Loss: 1.1633\n",
      "Epoch 40/59, Train Loss: 1.1203, Test Loss: 1.1632\n",
      "Epoch 41/59, Train Loss: 1.1145, Test Loss: 1.1708\n",
      "Epoch 42/59, Train Loss: 1.1209, Test Loss: 1.1691\n",
      "Epoch 43/59, Train Loss: 1.0891, Test Loss: 1.1689\n",
      "Epoch 44/59, Train Loss: 1.0895, Test Loss: 1.1648\n",
      "Epoch 45/59, Train Loss: 1.1047, Test Loss: 1.1629\n",
      "Epoch 46/59, Train Loss: 1.1088, Test Loss: 1.1619\n",
      "Epoch 47/59, Train Loss: 1.0917, Test Loss: 1.1657\n",
      "Epoch 48/59, Train Loss: 1.1019, Test Loss: 1.1676\n",
      "Epoch 49/59, Train Loss: 1.0875, Test Loss: 1.1741\n",
      "Epoch 50/59, Train Loss: 1.1231, Test Loss: 1.1682\n",
      "Epoch 51/59, Train Loss: 1.0782, Test Loss: 1.1665\n",
      "Epoch 52/59, Train Loss: 1.0586, Test Loss: 1.1796\n",
      "Epoch 53/59, Train Loss: 1.1291, Test Loss: 1.1760\n",
      "Epoch 54/59, Train Loss: 1.1140, Test Loss: 1.1704\n",
      "Epoch 55/59, Train Loss: 1.1462, Test Loss: 1.1584\n",
      "Epoch 56/59, Train Loss: 1.0674, Test Loss: 1.1598\n",
      "Epoch 57/59, Train Loss: 1.0866, Test Loss: 1.1648\n",
      "Epoch 58/59, Train Loss: 1.0905, Test Loss: 1.1619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:55:42,514] Trial 85 finished with value: 1.165757954120636 and parameters: {'num_hidden_layers': 19, 'layer_0_size': 120, 'layer_1_size': 178, 'layer_2_size': 150, 'layer_3_size': 164, 'layer_4_size': 102, 'layer_5_size': 169, 'layer_6_size': 35, 'layer_7_size': 83, 'layer_8_size': 67, 'layer_9_size': 216, 'layer_10_size': 57, 'layer_11_size': 142, 'layer_12_size': 210, 'layer_13_size': 117, 'layer_14_size': 155, 'layer_15_size': 123, 'layer_16_size': 255, 'layer_17_size': 210, 'layer_18_size': 35, 'dropout_rate': 0.2259655623219603, 'learning_rate': 3.2043995302191763e-05, 'batch_size': 128, 'epochs': 59}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/59, Train Loss: 1.0796, Test Loss: 1.1658\n",
      "Epoch 1/10, Train Loss: 1.2248, Test Loss: 1.0770\n",
      "Epoch 2/10, Train Loss: 1.1691, Test Loss: 1.0782\n",
      "Epoch 3/10, Train Loss: 1.1789, Test Loss: 1.0767\n",
      "Epoch 4/10, Train Loss: 1.1294, Test Loss: 1.0759\n",
      "Epoch 5/10, Train Loss: 1.1504, Test Loss: 1.0776\n",
      "Epoch 6/10, Train Loss: 1.1426, Test Loss: 1.0814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:55:43,140] Trial 86 finished with value: 1.08174529671669 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 131, 'layer_1_size': 202, 'layer_2_size': 127, 'layer_3_size': 183, 'layer_4_size': 218, 'layer_5_size': 146, 'layer_6_size': 44, 'layer_7_size': 223, 'dropout_rate': 0.27318025040546334, 'learning_rate': 0.00012795023605594844, 'batch_size': 128, 'epochs': 10}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Loss: 1.1414, Test Loss: 1.0837\n",
      "Epoch 8/10, Train Loss: 1.1461, Test Loss: 1.0817\n",
      "Epoch 9/10, Train Loss: 1.1413, Test Loss: 1.0850\n",
      "Epoch 10/10, Train Loss: 1.1010, Test Loss: 1.0817\n",
      "Epoch 1/83, Train Loss: 1.1662, Test Loss: 1.0097\n",
      "Epoch 2/83, Train Loss: 1.2214, Test Loss: 1.0283\n",
      "Epoch 3/83, Train Loss: 1.0956, Test Loss: 1.0474\n",
      "Epoch 4/83, Train Loss: 1.1732, Test Loss: 1.0617\n",
      "Epoch 5/83, Train Loss: 1.1747, Test Loss: 1.0737\n",
      "Epoch 6/83, Train Loss: 1.1563, Test Loss: 1.0872\n",
      "Epoch 7/83, Train Loss: 1.1371, Test Loss: 1.1005\n",
      "Epoch 8/83, Train Loss: 1.1352, Test Loss: 1.1141\n",
      "Epoch 9/83, Train Loss: 1.1501, Test Loss: 1.1180\n",
      "Epoch 10/83, Train Loss: 1.1885, Test Loss: 1.1186\n",
      "Epoch 11/83, Train Loss: 1.1259, Test Loss: 1.1190\n",
      "Epoch 12/83, Train Loss: 1.1882, Test Loss: 1.1255\n",
      "Epoch 13/83, Train Loss: 1.1327, Test Loss: 1.1276\n",
      "Epoch 14/83, Train Loss: 1.1620, Test Loss: 1.1290\n",
      "Epoch 15/83, Train Loss: 1.1179, Test Loss: 1.1369\n",
      "Epoch 16/83, Train Loss: 1.1736, Test Loss: 1.1321\n",
      "Epoch 17/83, Train Loss: 1.1283, Test Loss: 1.1371\n",
      "Epoch 18/83, Train Loss: 1.1613, Test Loss: 1.1309\n",
      "Epoch 19/83, Train Loss: 1.1075, Test Loss: 1.1225\n",
      "Epoch 20/83, Train Loss: 1.1436, Test Loss: 1.1232\n",
      "Epoch 21/83, Train Loss: 1.1289, Test Loss: 1.1335\n",
      "Epoch 22/83, Train Loss: 1.1531, Test Loss: 1.1278\n",
      "Epoch 23/83, Train Loss: 1.1296, Test Loss: 1.1283\n",
      "Epoch 24/83, Train Loss: 1.1270, Test Loss: 1.1244\n",
      "Epoch 25/83, Train Loss: 1.1403, Test Loss: 1.1233\n",
      "Epoch 26/83, Train Loss: 1.1120, Test Loss: 1.1251\n",
      "Epoch 27/83, Train Loss: 1.0868, Test Loss: 1.1230\n",
      "Epoch 28/83, Train Loss: 1.1154, Test Loss: 1.1316\n",
      "Epoch 29/83, Train Loss: 1.1069, Test Loss: 1.1289\n",
      "Epoch 30/83, Train Loss: 1.1216, Test Loss: 1.1225\n",
      "Epoch 31/83, Train Loss: 1.1636, Test Loss: 1.1323\n",
      "Epoch 32/83, Train Loss: 1.1458, Test Loss: 1.1333\n",
      "Epoch 33/83, Train Loss: 1.1127, Test Loss: 1.1323\n",
      "Epoch 34/83, Train Loss: 1.1026, Test Loss: 1.1293\n",
      "Epoch 35/83, Train Loss: 1.1517, Test Loss: 1.1398\n",
      "Epoch 36/83, Train Loss: 1.1130, Test Loss: 1.1371\n",
      "Epoch 37/83, Train Loss: 1.1346, Test Loss: 1.1304\n",
      "Epoch 38/83, Train Loss: 1.0798, Test Loss: 1.1301\n",
      "Epoch 39/83, Train Loss: 1.0941, Test Loss: 1.1205\n",
      "Epoch 40/83, Train Loss: 1.1677, Test Loss: 1.1213\n",
      "Epoch 41/83, Train Loss: 1.1102, Test Loss: 1.1245\n",
      "Epoch 42/83, Train Loss: 1.1035, Test Loss: 1.1208\n",
      "Epoch 43/83, Train Loss: 1.0916, Test Loss: 1.1145\n",
      "Epoch 44/83, Train Loss: 1.0735, Test Loss: 1.1205\n",
      "Epoch 45/83, Train Loss: 1.0772, Test Loss: 1.1216\n",
      "Epoch 46/83, Train Loss: 1.1210, Test Loss: 1.1232\n",
      "Epoch 47/83, Train Loss: 1.0608, Test Loss: 1.1191\n",
      "Epoch 48/83, Train Loss: 1.0638, Test Loss: 1.1093\n",
      "Epoch 49/83, Train Loss: 1.1304, Test Loss: 1.1103\n",
      "Epoch 50/83, Train Loss: 1.0831, Test Loss: 1.1132\n",
      "Epoch 51/83, Train Loss: 1.1568, Test Loss: 1.1113\n",
      "Epoch 52/83, Train Loss: 1.0699, Test Loss: 1.1123\n",
      "Epoch 53/83, Train Loss: 1.0813, Test Loss: 1.1120\n",
      "Epoch 54/83, Train Loss: 1.1216, Test Loss: 1.1048\n",
      "Epoch 55/83, Train Loss: 1.0406, Test Loss: 1.1101\n",
      "Epoch 56/83, Train Loss: 1.0478, Test Loss: 1.1098\n",
      "Epoch 57/83, Train Loss: 1.1100, Test Loss: 1.1136\n",
      "Epoch 58/83, Train Loss: 1.0902, Test Loss: 1.1099\n",
      "Epoch 59/83, Train Loss: 1.0579, Test Loss: 1.1061\n",
      "Epoch 60/83, Train Loss: 1.1327, Test Loss: 1.0992\n",
      "Epoch 61/83, Train Loss: 1.0932, Test Loss: 1.1034\n",
      "Epoch 62/83, Train Loss: 1.0769, Test Loss: 1.0992\n",
      "Epoch 63/83, Train Loss: 1.0766, Test Loss: 1.1026\n",
      "Epoch 64/83, Train Loss: 1.0713, Test Loss: 1.0989\n",
      "Epoch 65/83, Train Loss: 1.0859, Test Loss: 1.1016\n",
      "Epoch 66/83, Train Loss: 1.0790, Test Loss: 1.0996\n",
      "Epoch 67/83, Train Loss: 1.0850, Test Loss: 1.0928\n",
      "Epoch 68/83, Train Loss: 1.0978, Test Loss: 1.0910\n",
      "Epoch 69/83, Train Loss: 1.0594, Test Loss: 1.0894\n",
      "Epoch 70/83, Train Loss: 1.0649, Test Loss: 1.0891\n",
      "Epoch 71/83, Train Loss: 1.0967, Test Loss: 1.0873\n",
      "Epoch 72/83, Train Loss: 1.1132, Test Loss: 1.0885\n",
      "Epoch 73/83, Train Loss: 1.1058, Test Loss: 1.0907\n",
      "Epoch 74/83, Train Loss: 1.0914, Test Loss: 1.0911\n",
      "Epoch 75/83, Train Loss: 1.0785, Test Loss: 1.0869\n",
      "Epoch 76/83, Train Loss: 1.1075, Test Loss: 1.0829\n",
      "Epoch 77/83, Train Loss: 1.0866, Test Loss: 1.0828\n",
      "Epoch 78/83, Train Loss: 1.0790, Test Loss: 1.0885\n",
      "Epoch 79/83, Train Loss: 1.1033, Test Loss: 1.0851\n",
      "Epoch 80/83, Train Loss: 1.0617, Test Loss: 1.0778\n",
      "Epoch 81/83, Train Loss: 1.0673, Test Loss: 1.0798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:55:50,381] Trial 87 finished with value: 1.0836728811264038 and parameters: {'num_hidden_layers': 19, 'layer_0_size': 188, 'layer_1_size': 208, 'layer_2_size': 156, 'layer_3_size': 116, 'layer_4_size': 93, 'layer_5_size': 223, 'layer_6_size': 207, 'layer_7_size': 101, 'layer_8_size': 196, 'layer_9_size': 40, 'layer_10_size': 251, 'layer_11_size': 170, 'layer_12_size': 189, 'layer_13_size': 131, 'layer_14_size': 90, 'layer_15_size': 195, 'layer_16_size': 219, 'layer_17_size': 190, 'layer_18_size': 113, 'dropout_rate': 0.2892406834820703, 'learning_rate': 1.3780162555709553e-05, 'batch_size': 256, 'epochs': 83}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/83, Train Loss: 1.0517, Test Loss: 1.0776\n",
      "Epoch 83/83, Train Loss: 1.0695, Test Loss: 1.0837\n",
      "Epoch 1/67, Train Loss: 1.1704, Test Loss: 0.8688\n",
      "Epoch 2/67, Train Loss: 1.2371, Test Loss: 0.8694\n",
      "Epoch 3/67, Train Loss: 1.1545, Test Loss: 0.8741\n",
      "Epoch 4/67, Train Loss: 1.1403, Test Loss: 0.8815\n",
      "Epoch 5/67, Train Loss: 1.1262, Test Loss: 0.8821\n",
      "Epoch 6/67, Train Loss: 1.1031, Test Loss: 0.8864\n",
      "Epoch 7/67, Train Loss: 1.0998, Test Loss: 0.8939\n",
      "Epoch 8/67, Train Loss: 1.0646, Test Loss: 0.9012\n",
      "Epoch 9/67, Train Loss: 1.0946, Test Loss: 0.8999\n",
      "Epoch 10/67, Train Loss: 1.1391, Test Loss: 0.9091\n",
      "Epoch 11/67, Train Loss: 1.1067, Test Loss: 0.9198\n",
      "Epoch 12/67, Train Loss: 1.0517, Test Loss: 0.9123\n",
      "Epoch 13/67, Train Loss: 1.0730, Test Loss: 0.9146\n",
      "Epoch 14/67, Train Loss: 1.0641, Test Loss: 0.9041\n",
      "Epoch 15/67, Train Loss: 1.0974, Test Loss: 0.9072\n",
      "Epoch 16/67, Train Loss: 1.0784, Test Loss: 0.9056\n",
      "Epoch 17/67, Train Loss: 1.0806, Test Loss: 0.9086\n",
      "Epoch 18/67, Train Loss: 1.0732, Test Loss: 0.9045\n",
      "Epoch 19/67, Train Loss: 1.0806, Test Loss: 0.8917\n",
      "Epoch 20/67, Train Loss: 1.0915, Test Loss: 0.9109\n",
      "Epoch 21/67, Train Loss: 1.0736, Test Loss: 0.9123\n",
      "Epoch 22/67, Train Loss: 1.0757, Test Loss: 0.9070\n",
      "Epoch 23/67, Train Loss: 1.0427, Test Loss: 0.9082\n",
      "Epoch 24/67, Train Loss: 1.0579, Test Loss: 0.9293\n",
      "Epoch 25/67, Train Loss: 1.0376, Test Loss: 0.9184\n",
      "Epoch 26/67, Train Loss: 1.0842, Test Loss: 0.9068\n",
      "Epoch 27/67, Train Loss: 1.1008, Test Loss: 0.9021\n",
      "Epoch 28/67, Train Loss: 1.0656, Test Loss: 0.9088\n",
      "Epoch 29/67, Train Loss: 1.0302, Test Loss: 0.8998\n",
      "Epoch 30/67, Train Loss: 1.0830, Test Loss: 0.9023\n",
      "Epoch 31/67, Train Loss: 1.0524, Test Loss: 0.8948\n",
      "Epoch 32/67, Train Loss: 1.0580, Test Loss: 0.8916\n",
      "Epoch 33/67, Train Loss: 1.0545, Test Loss: 0.8958\n",
      "Epoch 34/67, Train Loss: 1.0271, Test Loss: 0.8974\n",
      "Epoch 35/67, Train Loss: 1.0196, Test Loss: 0.8940\n",
      "Epoch 36/67, Train Loss: 0.9943, Test Loss: 0.8921\n",
      "Epoch 37/67, Train Loss: 0.9895, Test Loss: 0.8975\n",
      "Epoch 38/67, Train Loss: 1.0030, Test Loss: 0.8955\n",
      "Epoch 39/67, Train Loss: 1.0376, Test Loss: 0.8949\n",
      "Epoch 40/67, Train Loss: 1.0379, Test Loss: 0.8947\n",
      "Epoch 41/67, Train Loss: 1.0404, Test Loss: 0.8925\n",
      "Epoch 42/67, Train Loss: 1.0164, Test Loss: 0.8947\n",
      "Epoch 43/67, Train Loss: 1.0569, Test Loss: 0.8922\n",
      "Epoch 44/67, Train Loss: 1.0493, Test Loss: 0.8924\n",
      "Epoch 45/67, Train Loss: 1.0532, Test Loss: 0.9006\n",
      "Epoch 46/67, Train Loss: 1.0676, Test Loss: 0.9002\n",
      "Epoch 47/67, Train Loss: 1.0204, Test Loss: 0.8941\n",
      "Epoch 48/67, Train Loss: 1.0164, Test Loss: 0.8993\n",
      "Epoch 49/67, Train Loss: 1.0434, Test Loss: 0.8947\n",
      "Epoch 50/67, Train Loss: 1.0265, Test Loss: 0.8934\n",
      "Epoch 51/67, Train Loss: 1.0286, Test Loss: 0.8984\n",
      "Epoch 52/67, Train Loss: 1.0374, Test Loss: 0.8971\n",
      "Epoch 53/67, Train Loss: 0.9816, Test Loss: 0.8910\n",
      "Epoch 54/67, Train Loss: 1.0031, Test Loss: 0.8963\n",
      "Epoch 55/67, Train Loss: 1.0266, Test Loss: 0.8913\n",
      "Epoch 56/67, Train Loss: 1.0146, Test Loss: 0.8884\n",
      "Epoch 57/67, Train Loss: 1.0265, Test Loss: 0.8927\n",
      "Epoch 58/67, Train Loss: 1.0514, Test Loss: 0.8912\n",
      "Epoch 59/67, Train Loss: 1.0242, Test Loss: 0.8912\n",
      "Epoch 60/67, Train Loss: 1.0292, Test Loss: 0.8970\n",
      "Epoch 61/67, Train Loss: 1.0571, Test Loss: 0.8980\n",
      "Epoch 62/67, Train Loss: 1.0627, Test Loss: 0.8936\n",
      "Epoch 63/67, Train Loss: 1.0347, Test Loss: 0.8982\n",
      "Epoch 64/67, Train Loss: 1.0472, Test Loss: 0.8998\n",
      "Epoch 65/67, Train Loss: 1.0082, Test Loss: 0.8965\n",
      "Epoch 66/67, Train Loss: 1.0288, Test Loss: 0.8980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:56:03,627] Trial 88 finished with value: 0.8920471966266632 and parameters: {'num_hidden_layers': 19, 'layer_0_size': 79, 'layer_1_size': 219, 'layer_2_size': 166, 'layer_3_size': 253, 'layer_4_size': 197, 'layer_5_size': 85, 'layer_6_size': 135, 'layer_7_size': 116, 'layer_8_size': 234, 'layer_9_size': 107, 'layer_10_size': 94, 'layer_11_size': 194, 'layer_12_size': 176, 'layer_13_size': 52, 'layer_14_size': 220, 'layer_15_size': 129, 'layer_16_size': 172, 'layer_17_size': 82, 'layer_18_size': 223, 'dropout_rate': 0.3211235852374953, 'learning_rate': 6.75006200493086e-05, 'batch_size': 64, 'epochs': 67}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/67, Train Loss: 1.0462, Test Loss: 0.8920\n",
      "Epoch 1/89, Train Loss: 1.1941, Test Loss: 1.0650\n",
      "Epoch 2/89, Train Loss: 1.1303, Test Loss: 1.0750\n",
      "Epoch 3/89, Train Loss: 1.1522, Test Loss: 1.0781\n",
      "Epoch 4/89, Train Loss: 1.1050, Test Loss: 1.0768\n",
      "Epoch 5/89, Train Loss: 1.1468, Test Loss: 1.0623\n",
      "Epoch 6/89, Train Loss: 1.1499, Test Loss: 1.0600\n",
      "Epoch 7/89, Train Loss: 1.1303, Test Loss: 1.0648\n",
      "Epoch 8/89, Train Loss: 1.1085, Test Loss: 1.0636\n",
      "Epoch 9/89, Train Loss: 1.0951, Test Loss: 1.0623\n",
      "Epoch 10/89, Train Loss: 1.1134, Test Loss: 1.0644\n",
      "Epoch 11/89, Train Loss: 1.0931, Test Loss: 1.0644\n",
      "Epoch 12/89, Train Loss: 1.0979, Test Loss: 1.0695\n",
      "Epoch 13/89, Train Loss: 1.1461, Test Loss: 1.0660\n",
      "Epoch 14/89, Train Loss: 1.0563, Test Loss: 1.0655\n",
      "Epoch 15/89, Train Loss: 1.1143, Test Loss: 1.0695\n",
      "Epoch 16/89, Train Loss: 1.0410, Test Loss: 1.0648\n",
      "Epoch 17/89, Train Loss: 1.0781, Test Loss: 1.0633\n",
      "Epoch 18/89, Train Loss: 1.0534, Test Loss: 1.0562\n",
      "Epoch 19/89, Train Loss: 1.0603, Test Loss: 1.0715\n",
      "Epoch 20/89, Train Loss: 1.0603, Test Loss: 1.0695\n",
      "Epoch 21/89, Train Loss: 1.0871, Test Loss: 1.0581\n",
      "Epoch 22/89, Train Loss: 1.0809, Test Loss: 1.0610\n",
      "Epoch 23/89, Train Loss: 1.0376, Test Loss: 1.0624\n",
      "Epoch 24/89, Train Loss: 1.0633, Test Loss: 1.0607\n",
      "Epoch 25/89, Train Loss: 1.1061, Test Loss: 1.0589\n",
      "Epoch 26/89, Train Loss: 1.0366, Test Loss: 1.0659\n",
      "Epoch 27/89, Train Loss: 1.0782, Test Loss: 1.0709\n",
      "Epoch 28/89, Train Loss: 1.0346, Test Loss: 1.0581\n",
      "Epoch 29/89, Train Loss: 1.0732, Test Loss: 1.0620\n",
      "Epoch 30/89, Train Loss: 1.0625, Test Loss: 1.0660\n",
      "Epoch 31/89, Train Loss: 1.0667, Test Loss: 1.0595\n",
      "Epoch 32/89, Train Loss: 1.0007, Test Loss: 1.0650\n",
      "Epoch 33/89, Train Loss: 1.0492, Test Loss: 1.0561\n",
      "Epoch 34/89, Train Loss: 1.0670, Test Loss: 1.0561\n",
      "Epoch 35/89, Train Loss: 1.0402, Test Loss: 1.0679\n",
      "Epoch 36/89, Train Loss: 1.0181, Test Loss: 1.0581\n",
      "Epoch 37/89, Train Loss: 1.0212, Test Loss: 1.0563\n",
      "Epoch 38/89, Train Loss: 1.0523, Test Loss: 1.0609\n",
      "Epoch 39/89, Train Loss: 1.0403, Test Loss: 1.0613\n",
      "Epoch 40/89, Train Loss: 1.0271, Test Loss: 1.0555\n",
      "Epoch 41/89, Train Loss: 1.0692, Test Loss: 1.0600\n",
      "Epoch 42/89, Train Loss: 1.0430, Test Loss: 1.0616\n",
      "Epoch 43/89, Train Loss: 1.0743, Test Loss: 1.0639\n",
      "Epoch 44/89, Train Loss: 1.0241, Test Loss: 1.0606\n",
      "Epoch 45/89, Train Loss: 1.0354, Test Loss: 1.0631\n",
      "Epoch 46/89, Train Loss: 1.0669, Test Loss: 1.0595\n",
      "Epoch 47/89, Train Loss: 0.9893, Test Loss: 1.0616\n",
      "Epoch 48/89, Train Loss: 1.0178, Test Loss: 1.0618\n",
      "Epoch 49/89, Train Loss: 1.0375, Test Loss: 1.0636\n",
      "Epoch 50/89, Train Loss: 1.0310, Test Loss: 1.0554\n",
      "Epoch 51/89, Train Loss: 1.0704, Test Loss: 1.0655\n",
      "Epoch 52/89, Train Loss: 1.0321, Test Loss: 1.0607\n",
      "Epoch 53/89, Train Loss: 1.0345, Test Loss: 1.0632\n",
      "Epoch 54/89, Train Loss: 1.0337, Test Loss: 1.0613\n",
      "Epoch 55/89, Train Loss: 1.0612, Test Loss: 1.0605\n",
      "Epoch 56/89, Train Loss: 1.0698, Test Loss: 1.0589\n",
      "Epoch 57/89, Train Loss: 1.0084, Test Loss: 1.0550\n",
      "Epoch 58/89, Train Loss: 1.0349, Test Loss: 1.0600\n",
      "Epoch 59/89, Train Loss: 1.0486, Test Loss: 1.0589\n",
      "Epoch 60/89, Train Loss: 1.0270, Test Loss: 1.0551\n",
      "Epoch 61/89, Train Loss: 1.0146, Test Loss: 1.0567\n",
      "Epoch 62/89, Train Loss: 1.0085, Test Loss: 1.0597\n",
      "Epoch 63/89, Train Loss: 1.0136, Test Loss: 1.0558\n",
      "Epoch 64/89, Train Loss: 1.0444, Test Loss: 1.0590\n",
      "Epoch 65/89, Train Loss: 1.0047, Test Loss: 1.0587\n",
      "Epoch 66/89, Train Loss: 1.0319, Test Loss: 1.0611\n",
      "Epoch 67/89, Train Loss: 1.0025, Test Loss: 1.0627\n",
      "Epoch 68/89, Train Loss: 1.0514, Test Loss: 1.0629\n",
      "Epoch 69/89, Train Loss: 1.0216, Test Loss: 1.0587\n",
      "Epoch 70/89, Train Loss: 1.0318, Test Loss: 1.0549\n",
      "Epoch 71/89, Train Loss: 1.0498, Test Loss: 1.0582\n",
      "Epoch 72/89, Train Loss: 0.9962, Test Loss: 1.0562\n",
      "Epoch 73/89, Train Loss: 1.0012, Test Loss: 1.0571\n",
      "Epoch 74/89, Train Loss: 0.9916, Test Loss: 1.0566\n",
      "Epoch 75/89, Train Loss: 1.0422, Test Loss: 1.0539\n",
      "Epoch 76/89, Train Loss: 0.9979, Test Loss: 1.0554\n",
      "Epoch 77/89, Train Loss: 1.0156, Test Loss: 1.0548\n",
      "Epoch 78/89, Train Loss: 1.0401, Test Loss: 1.0598\n",
      "Epoch 79/89, Train Loss: 0.9849, Test Loss: 1.0574\n",
      "Epoch 80/89, Train Loss: 1.0093, Test Loss: 1.0599\n",
      "Epoch 81/89, Train Loss: 1.0217, Test Loss: 1.0571\n",
      "Epoch 82/89, Train Loss: 1.0126, Test Loss: 1.0554\n",
      "Epoch 83/89, Train Loss: 0.9999, Test Loss: 1.0546\n",
      "Epoch 84/89, Train Loss: 1.0718, Test Loss: 1.0620\n",
      "Epoch 85/89, Train Loss: 1.0234, Test Loss: 1.0570\n",
      "Epoch 86/89, Train Loss: 1.0188, Test Loss: 1.0558\n",
      "Epoch 87/89, Train Loss: 1.0481, Test Loss: 1.0597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:56:23,027] Trial 89 finished with value: 1.0558938894953047 and parameters: {'num_hidden_layers': 11, 'layer_0_size': 151, 'layer_1_size': 225, 'layer_2_size': 121, 'layer_3_size': 152, 'layer_4_size': 228, 'layer_5_size': 179, 'layer_6_size': 102, 'layer_7_size': 245, 'layer_8_size': 154, 'layer_9_size': 57, 'layer_10_size': 237, 'dropout_rate': 0.30765462475719385, 'learning_rate': 5.098500423545435e-05, 'batch_size': 32, 'epochs': 89}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/89, Train Loss: 1.0435, Test Loss: 1.0559\n",
      "Epoch 89/89, Train Loss: 1.0218, Test Loss: 1.0559\n",
      "Epoch 1/77, Train Loss: 1.2162, Test Loss: 1.1232\n",
      "Epoch 2/77, Train Loss: 1.1856, Test Loss: 1.1231\n",
      "Epoch 3/77, Train Loss: 1.2145, Test Loss: 1.1231\n",
      "Epoch 4/77, Train Loss: 1.1564, Test Loss: 1.1233\n",
      "Epoch 5/77, Train Loss: 1.1978, Test Loss: 1.1233\n",
      "Epoch 6/77, Train Loss: 1.1678, Test Loss: 1.1233\n",
      "Epoch 7/77, Train Loss: 1.1630, Test Loss: 1.1244\n",
      "Epoch 8/77, Train Loss: 1.1598, Test Loss: 1.1270\n",
      "Epoch 9/77, Train Loss: 1.2983, Test Loss: 1.1259\n",
      "Epoch 10/77, Train Loss: 1.1795, Test Loss: 1.1237\n",
      "Epoch 11/77, Train Loss: 1.1675, Test Loss: 1.1222\n",
      "Epoch 12/77, Train Loss: 1.1890, Test Loss: 1.1226\n",
      "Epoch 13/77, Train Loss: 1.1883, Test Loss: 1.1236\n",
      "Epoch 14/77, Train Loss: 1.2055, Test Loss: 1.1252\n",
      "Epoch 15/77, Train Loss: 1.1731, Test Loss: 1.1241\n",
      "Epoch 16/77, Train Loss: 1.1908, Test Loss: 1.1262\n",
      "Epoch 17/77, Train Loss: 1.1878, Test Loss: 1.1249\n",
      "Epoch 18/77, Train Loss: 1.1700, Test Loss: 1.1255\n",
      "Epoch 19/77, Train Loss: 1.2253, Test Loss: 1.1255\n",
      "Epoch 20/77, Train Loss: 1.1524, Test Loss: 1.1241\n",
      "Epoch 21/77, Train Loss: 1.2183, Test Loss: 1.1234\n",
      "Epoch 22/77, Train Loss: 1.2091, Test Loss: 1.1240\n",
      "Epoch 23/77, Train Loss: 1.1710, Test Loss: 1.1240\n",
      "Epoch 24/77, Train Loss: 1.1833, Test Loss: 1.1274\n",
      "Epoch 25/77, Train Loss: 1.1742, Test Loss: 1.1246\n",
      "Epoch 26/77, Train Loss: 1.1810, Test Loss: 1.1259\n",
      "Epoch 27/77, Train Loss: 1.2061, Test Loss: 1.1268\n",
      "Epoch 28/77, Train Loss: 1.1850, Test Loss: 1.1244\n",
      "Epoch 29/77, Train Loss: 1.1727, Test Loss: 1.1232\n",
      "Epoch 30/77, Train Loss: 1.1598, Test Loss: 1.1259\n",
      "Epoch 31/77, Train Loss: 1.1827, Test Loss: 1.1254\n",
      "Epoch 32/77, Train Loss: 1.1891, Test Loss: 1.1241\n",
      "Epoch 33/77, Train Loss: 1.1797, Test Loss: 1.1261\n",
      "Epoch 34/77, Train Loss: 1.1776, Test Loss: 1.1251\n",
      "Epoch 35/77, Train Loss: 1.1921, Test Loss: 1.1278\n",
      "Epoch 36/77, Train Loss: 1.2141, Test Loss: 1.1265\n",
      "Epoch 37/77, Train Loss: 1.1962, Test Loss: 1.1272\n",
      "Epoch 38/77, Train Loss: 1.1783, Test Loss: 1.1271\n",
      "Epoch 39/77, Train Loss: 1.2042, Test Loss: 1.1281\n",
      "Epoch 40/77, Train Loss: 1.1593, Test Loss: 1.1279\n",
      "Epoch 41/77, Train Loss: 1.1663, Test Loss: 1.1300\n",
      "Epoch 42/77, Train Loss: 1.1735, Test Loss: 1.1295\n",
      "Epoch 43/77, Train Loss: 1.1401, Test Loss: 1.1274\n",
      "Epoch 44/77, Train Loss: 1.1503, Test Loss: 1.1275\n",
      "Epoch 45/77, Train Loss: 1.1621, Test Loss: 1.1267\n",
      "Epoch 46/77, Train Loss: 1.1895, Test Loss: 1.1264\n",
      "Epoch 47/77, Train Loss: 1.1829, Test Loss: 1.1262\n",
      "Epoch 48/77, Train Loss: 1.1256, Test Loss: 1.1257\n",
      "Epoch 49/77, Train Loss: 1.1682, Test Loss: 1.1261\n",
      "Epoch 50/77, Train Loss: 1.1428, Test Loss: 1.1275\n",
      "Epoch 51/77, Train Loss: 1.1694, Test Loss: 1.1260\n",
      "Epoch 52/77, Train Loss: 1.1033, Test Loss: 1.1272\n",
      "Epoch 53/77, Train Loss: 1.1737, Test Loss: 1.1274\n",
      "Epoch 54/77, Train Loss: 1.1308, Test Loss: 1.1250\n",
      "Epoch 55/77, Train Loss: 1.1432, Test Loss: 1.1266\n",
      "Epoch 56/77, Train Loss: 1.1257, Test Loss: 1.1236\n",
      "Epoch 57/77, Train Loss: 1.1879, Test Loss: 1.1234\n",
      "Epoch 58/77, Train Loss: 1.1703, Test Loss: 1.1227\n",
      "Epoch 59/77, Train Loss: 1.2159, Test Loss: 1.1227\n",
      "Epoch 60/77, Train Loss: 1.1986, Test Loss: 1.1232\n",
      "Epoch 61/77, Train Loss: 1.1865, Test Loss: 1.1232\n",
      "Epoch 62/77, Train Loss: 1.1617, Test Loss: 1.1233\n",
      "Epoch 63/77, Train Loss: 1.1494, Test Loss: 1.1238\n",
      "Epoch 64/77, Train Loss: 1.1325, Test Loss: 1.1247\n",
      "Epoch 65/77, Train Loss: 1.1588, Test Loss: 1.1242\n",
      "Epoch 66/77, Train Loss: 1.1776, Test Loss: 1.1238\n",
      "Epoch 67/77, Train Loss: 1.1582, Test Loss: 1.1221\n",
      "Epoch 68/77, Train Loss: 1.1142, Test Loss: 1.1233\n",
      "Epoch 69/77, Train Loss: 1.1296, Test Loss: 1.1247\n",
      "Epoch 70/77, Train Loss: 1.1667, Test Loss: 1.1234\n",
      "Epoch 71/77, Train Loss: 1.1458, Test Loss: 1.1252\n",
      "Epoch 72/77, Train Loss: 1.1582, Test Loss: 1.1260\n",
      "Epoch 73/77, Train Loss: 1.1884, Test Loss: 1.1267\n",
      "Epoch 74/77, Train Loss: 1.1652, Test Loss: 1.1267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:56:28,323] Trial 90 finished with value: 1.1234363317489624 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 103, 'layer_1_size': 57, 'layer_2_size': 205, 'layer_3_size': 131, 'layer_4_size': 79, 'layer_5_size': 242, 'layer_6_size': 59, 'layer_7_size': 239, 'layer_8_size': 180, 'layer_9_size': 201, 'layer_10_size': 227, 'layer_11_size': 148, 'layer_12_size': 226, 'dropout_rate': 0.23654556924786496, 'learning_rate': 1.0709081360753965e-05, 'batch_size': 256, 'epochs': 77}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/77, Train Loss: 1.1419, Test Loss: 1.1245\n",
      "Epoch 76/77, Train Loss: 1.1643, Test Loss: 1.1236\n",
      "Epoch 77/77, Train Loss: 1.1493, Test Loss: 1.1234\n",
      "Epoch 1/45, Train Loss: 1.3092, Test Loss: 1.0504\n",
      "Epoch 2/45, Train Loss: 1.1681, Test Loss: 1.0518\n",
      "Epoch 3/45, Train Loss: 1.1433, Test Loss: 1.0555\n",
      "Epoch 4/45, Train Loss: 1.1472, Test Loss: 1.0816\n",
      "Epoch 5/45, Train Loss: 1.1414, Test Loss: 1.0785\n",
      "Epoch 6/45, Train Loss: 1.1234, Test Loss: 1.0855\n",
      "Epoch 7/45, Train Loss: 1.1402, Test Loss: 1.0412\n",
      "Epoch 8/45, Train Loss: 1.0787, Test Loss: 1.0945\n",
      "Epoch 9/45, Train Loss: 1.1002, Test Loss: 1.0586\n",
      "Epoch 10/45, Train Loss: 1.1126, Test Loss: 1.0750\n",
      "Epoch 11/45, Train Loss: 1.1039, Test Loss: 1.0585\n",
      "Epoch 12/45, Train Loss: 1.0844, Test Loss: 1.1082\n",
      "Epoch 13/45, Train Loss: 1.1021, Test Loss: 1.0494\n",
      "Epoch 14/45, Train Loss: 1.0806, Test Loss: 1.0476\n",
      "Epoch 15/45, Train Loss: 1.0636, Test Loss: 1.0356\n",
      "Epoch 16/45, Train Loss: 1.0884, Test Loss: 1.0289\n",
      "Epoch 17/45, Train Loss: 1.0874, Test Loss: 1.0518\n",
      "Epoch 18/45, Train Loss: 1.0792, Test Loss: 1.0752\n",
      "Epoch 19/45, Train Loss: 1.0560, Test Loss: 1.0563\n",
      "Epoch 20/45, Train Loss: 1.0870, Test Loss: 1.0251\n",
      "Epoch 21/45, Train Loss: 1.0958, Test Loss: 1.0382\n",
      "Epoch 22/45, Train Loss: 1.0709, Test Loss: 1.0597\n",
      "Epoch 23/45, Train Loss: 1.0661, Test Loss: 1.0619\n",
      "Epoch 24/45, Train Loss: 1.0678, Test Loss: 1.0481\n",
      "Epoch 25/45, Train Loss: 1.0484, Test Loss: 1.0512\n",
      "Epoch 26/45, Train Loss: 1.0444, Test Loss: 1.0055\n",
      "Epoch 27/45, Train Loss: 0.9925, Test Loss: 1.0621\n",
      "Epoch 28/45, Train Loss: 1.0226, Test Loss: 1.0500\n",
      "Epoch 29/45, Train Loss: 1.0274, Test Loss: 1.1105\n",
      "Epoch 30/45, Train Loss: 1.0368, Test Loss: 1.1492\n",
      "Epoch 31/45, Train Loss: 1.0110, Test Loss: 1.0937\n",
      "Epoch 32/45, Train Loss: 1.0205, Test Loss: 1.1537\n",
      "Epoch 33/45, Train Loss: 1.0432, Test Loss: 1.2261\n",
      "Epoch 34/45, Train Loss: 0.9769, Test Loss: 1.2242\n",
      "Epoch 35/45, Train Loss: 0.9741, Test Loss: 1.1145\n",
      "Epoch 36/45, Train Loss: 0.9301, Test Loss: 1.2683\n",
      "Epoch 37/45, Train Loss: 0.9504, Test Loss: 1.0952\n",
      "Epoch 38/45, Train Loss: 0.9249, Test Loss: 1.2300\n",
      "Epoch 39/45, Train Loss: 0.9622, Test Loss: 1.0563\n",
      "Epoch 40/45, Train Loss: 0.9472, Test Loss: 1.0742\n",
      "Epoch 41/45, Train Loss: 0.9342, Test Loss: 1.0585\n",
      "Epoch 42/45, Train Loss: 0.9093, Test Loss: 1.1193\n",
      "Epoch 43/45, Train Loss: 0.8776, Test Loss: 1.1452\n",
      "Epoch 44/45, Train Loss: 0.8837, Test Loss: 1.1492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:56:34,784] Trial 91 finished with value: 1.0796502828598022 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 83, 'layer_1_size': 147, 'layer_2_size': 142, 'layer_3_size': 142, 'layer_4_size': 53, 'layer_5_size': 102, 'layer_6_size': 172, 'layer_7_size': 90, 'layer_8_size': 162, 'layer_9_size': 87, 'layer_10_size': 218, 'layer_11_size': 197, 'layer_12_size': 122, 'layer_13_size': 189, 'dropout_rate': 0.3385087707461599, 'learning_rate': 0.005102259152345172, 'batch_size': 64, 'epochs': 45}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/45, Train Loss: 0.8802, Test Loss: 1.0797\n",
      "Epoch 1/56, Train Loss: 1.1583, Test Loss: 0.9654\n",
      "Epoch 2/56, Train Loss: 1.1677, Test Loss: 0.9661\n",
      "Epoch 3/56, Train Loss: 1.0999, Test Loss: 0.9646\n",
      "Epoch 4/56, Train Loss: 1.0994, Test Loss: 0.9697\n",
      "Epoch 5/56, Train Loss: 1.0917, Test Loss: 0.9642\n",
      "Epoch 6/56, Train Loss: 1.0914, Test Loss: 0.9650\n",
      "Epoch 7/56, Train Loss: 1.0911, Test Loss: 0.9636\n",
      "Epoch 8/56, Train Loss: 1.0970, Test Loss: 0.9660\n",
      "Epoch 9/56, Train Loss: 1.0572, Test Loss: 0.9661\n",
      "Epoch 10/56, Train Loss: 1.0733, Test Loss: 0.9650\n",
      "Epoch 11/56, Train Loss: 1.0438, Test Loss: 0.9651\n",
      "Epoch 12/56, Train Loss: 1.0624, Test Loss: 0.9662\n",
      "Epoch 13/56, Train Loss: 1.0813, Test Loss: 0.9642\n",
      "Epoch 14/56, Train Loss: 1.0879, Test Loss: 0.9653\n",
      "Epoch 15/56, Train Loss: 1.0592, Test Loss: 0.9655\n",
      "Epoch 16/56, Train Loss: 1.0756, Test Loss: 0.9646\n",
      "Epoch 17/56, Train Loss: 1.0359, Test Loss: 0.9619\n",
      "Epoch 18/56, Train Loss: 1.0899, Test Loss: 0.9657\n",
      "Epoch 19/56, Train Loss: 1.0649, Test Loss: 0.9655\n",
      "Epoch 20/56, Train Loss: 1.0305, Test Loss: 0.9651\n",
      "Epoch 21/56, Train Loss: 1.0926, Test Loss: 0.9652\n",
      "Epoch 22/56, Train Loss: 1.0416, Test Loss: 0.9658\n",
      "Epoch 23/56, Train Loss: 1.0372, Test Loss: 0.9660\n",
      "Epoch 24/56, Train Loss: 1.0370, Test Loss: 0.9693\n",
      "Epoch 25/56, Train Loss: 1.0652, Test Loss: 0.9674\n",
      "Epoch 26/56, Train Loss: 1.0399, Test Loss: 0.9648\n",
      "Epoch 27/56, Train Loss: 1.0212, Test Loss: 0.9645\n",
      "Epoch 28/56, Train Loss: 1.0555, Test Loss: 0.9633\n",
      "Epoch 29/56, Train Loss: 1.0463, Test Loss: 0.9643\n",
      "Epoch 30/56, Train Loss: 1.0784, Test Loss: 0.9660\n",
      "Epoch 31/56, Train Loss: 1.0594, Test Loss: 0.9651\n",
      "Epoch 32/56, Train Loss: 1.0703, Test Loss: 0.9657\n",
      "Epoch 33/56, Train Loss: 0.9948, Test Loss: 0.9690\n",
      "Epoch 34/56, Train Loss: 1.0405, Test Loss: 0.9697\n",
      "Epoch 35/56, Train Loss: 1.0537, Test Loss: 0.9700\n",
      "Epoch 36/56, Train Loss: 1.0306, Test Loss: 0.9660\n",
      "Epoch 37/56, Train Loss: 1.0469, Test Loss: 0.9688\n",
      "Epoch 38/56, Train Loss: 1.0190, Test Loss: 0.9694\n",
      "Epoch 39/56, Train Loss: 1.0237, Test Loss: 0.9704\n",
      "Epoch 40/56, Train Loss: 1.0115, Test Loss: 0.9735\n",
      "Epoch 41/56, Train Loss: 1.0296, Test Loss: 0.9730\n",
      "Epoch 42/56, Train Loss: 1.0272, Test Loss: 0.9704\n",
      "Epoch 43/56, Train Loss: 1.0421, Test Loss: 0.9704\n",
      "Epoch 44/56, Train Loss: 1.0541, Test Loss: 0.9706\n",
      "Epoch 45/56, Train Loss: 1.0470, Test Loss: 0.9716\n",
      "Epoch 46/56, Train Loss: 1.0421, Test Loss: 0.9729\n",
      "Epoch 47/56, Train Loss: 1.0237, Test Loss: 0.9675\n",
      "Epoch 48/56, Train Loss: 1.0521, Test Loss: 0.9680\n",
      "Epoch 49/56, Train Loss: 1.0276, Test Loss: 0.9677\n",
      "Epoch 50/56, Train Loss: 1.0384, Test Loss: 0.9679\n",
      "Epoch 51/56, Train Loss: 1.0701, Test Loss: 0.9671\n",
      "Epoch 52/56, Train Loss: 1.0472, Test Loss: 0.9670\n",
      "Epoch 53/56, Train Loss: 1.0387, Test Loss: 0.9684\n",
      "Epoch 54/56, Train Loss: 1.0162, Test Loss: 0.9709\n",
      "Epoch 55/56, Train Loss: 1.0293, Test Loss: 0.9668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:56:42,741] Trial 92 finished with value: 0.9647307246923447 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 96, 'layer_1_size': 136, 'layer_2_size': 157, 'layer_3_size': 96, 'layer_4_size': 67, 'layer_5_size': 162, 'layer_6_size': 182, 'layer_7_size': 93, 'layer_8_size': 173, 'layer_9_size': 76, 'layer_10_size': 208, 'layer_11_size': 177, 'layer_12_size': 53, 'layer_13_size': 245, 'dropout_rate': 0.26579577589490466, 'learning_rate': 0.00021439344096578822, 'batch_size': 64, 'epochs': 56}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/56, Train Loss: 1.0311, Test Loss: 0.9647\n",
      "Epoch 1/41, Train Loss: 1.3800, Test Loss: 0.9476\n",
      "Epoch 2/41, Train Loss: 1.1548, Test Loss: 0.9589\n",
      "Epoch 3/41, Train Loss: 1.1181, Test Loss: 0.9605\n",
      "Epoch 4/41, Train Loss: 1.1160, Test Loss: 0.9268\n",
      "Epoch 5/41, Train Loss: 1.1175, Test Loss: 0.9352\n",
      "Epoch 6/41, Train Loss: 1.1382, Test Loss: 0.9553\n",
      "Epoch 7/41, Train Loss: 1.1318, Test Loss: 0.9408\n",
      "Epoch 8/41, Train Loss: 1.1189, Test Loss: 0.9429\n",
      "Epoch 9/41, Train Loss: 1.1177, Test Loss: 0.9877\n",
      "Epoch 10/41, Train Loss: 1.1007, Test Loss: 0.9411\n",
      "Epoch 11/41, Train Loss: 1.0697, Test Loss: 0.9580\n",
      "Epoch 12/41, Train Loss: 1.0974, Test Loss: 1.0141\n",
      "Epoch 13/41, Train Loss: 1.0929, Test Loss: 0.9693\n",
      "Epoch 14/41, Train Loss: 1.0719, Test Loss: 1.0011\n",
      "Epoch 15/41, Train Loss: 1.0746, Test Loss: 1.0290\n",
      "Epoch 16/41, Train Loss: 1.0239, Test Loss: 1.0900\n",
      "Epoch 17/41, Train Loss: 1.0774, Test Loss: 0.9519\n",
      "Epoch 18/41, Train Loss: 1.0407, Test Loss: 1.0672\n",
      "Epoch 19/41, Train Loss: 1.0102, Test Loss: 1.0625\n",
      "Epoch 20/41, Train Loss: 0.9792, Test Loss: 1.0944\n",
      "Epoch 21/41, Train Loss: 0.9484, Test Loss: 1.1432\n",
      "Epoch 22/41, Train Loss: 0.9434, Test Loss: 1.0931\n",
      "Epoch 23/41, Train Loss: 0.9250, Test Loss: 1.1539\n",
      "Epoch 24/41, Train Loss: 0.9005, Test Loss: 1.1164\n",
      "Epoch 25/41, Train Loss: 0.8586, Test Loss: 1.3108\n",
      "Epoch 26/41, Train Loss: 0.8593, Test Loss: 1.2710\n",
      "Epoch 27/41, Train Loss: 0.8675, Test Loss: 1.1646\n",
      "Epoch 28/41, Train Loss: 0.8189, Test Loss: 1.2041\n",
      "Epoch 29/41, Train Loss: 0.8259, Test Loss: 1.3190\n",
      "Epoch 30/41, Train Loss: 0.8051, Test Loss: 1.3363\n",
      "Epoch 31/41, Train Loss: 0.8170, Test Loss: 1.2410\n",
      "Epoch 32/41, Train Loss: 0.7758, Test Loss: 1.3551\n",
      "Epoch 33/41, Train Loss: 0.7035, Test Loss: 1.3639\n",
      "Epoch 34/41, Train Loss: 0.7267, Test Loss: 1.2070\n",
      "Epoch 35/41, Train Loss: 0.7062, Test Loss: 1.2996\n",
      "Epoch 36/41, Train Loss: 0.7107, Test Loss: 1.1930\n",
      "Epoch 37/41, Train Loss: 0.6493, Test Loss: 1.1938\n",
      "Epoch 38/41, Train Loss: 0.6555, Test Loss: 1.3889\n",
      "Epoch 39/41, Train Loss: 0.6336, Test Loss: 1.3451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:56:48,140] Trial 93 finished with value: 1.265590250492096 and parameters: {'num_hidden_layers': 12, 'layer_0_size': 66, 'layer_1_size': 121, 'layer_2_size': 148, 'layer_3_size': 157, 'layer_4_size': 84, 'layer_5_size': 94, 'layer_6_size': 200, 'layer_7_size': 108, 'layer_8_size': 172, 'layer_9_size': 256, 'layer_10_size': 207, 'layer_11_size': 182, 'dropout_rate': 0.25016974403253567, 'learning_rate': 0.0055560521939107994, 'batch_size': 64, 'epochs': 41}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/41, Train Loss: 0.6677, Test Loss: 1.3601\n",
      "Epoch 41/41, Train Loss: 0.6533, Test Loss: 1.2656\n",
      "Epoch 1/64, Train Loss: 1.0808, Test Loss: 1.2642\n",
      "Epoch 2/64, Train Loss: 1.0132, Test Loss: 1.2692\n",
      "Epoch 3/64, Train Loss: 1.0342, Test Loss: 1.2700\n",
      "Epoch 4/64, Train Loss: 0.9820, Test Loss: 1.2677\n",
      "Epoch 5/64, Train Loss: 0.9781, Test Loss: 1.2607\n",
      "Epoch 6/64, Train Loss: 0.9787, Test Loss: 1.2638\n",
      "Epoch 7/64, Train Loss: 0.9594, Test Loss: 1.2746\n",
      "Epoch 8/64, Train Loss: 0.9740, Test Loss: 1.2689\n",
      "Epoch 9/64, Train Loss: 0.9701, Test Loss: 1.2717\n",
      "Epoch 10/64, Train Loss: 0.9764, Test Loss: 1.2696\n",
      "Epoch 11/64, Train Loss: 0.9525, Test Loss: 1.2625\n",
      "Epoch 12/64, Train Loss: 0.9577, Test Loss: 1.2654\n",
      "Epoch 13/64, Train Loss: 0.9627, Test Loss: 1.2678\n",
      "Epoch 14/64, Train Loss: 0.9772, Test Loss: 1.2655\n",
      "Epoch 15/64, Train Loss: 0.9582, Test Loss: 1.2734\n",
      "Epoch 16/64, Train Loss: 0.9638, Test Loss: 1.2711\n",
      "Epoch 17/64, Train Loss: 0.9472, Test Loss: 1.2719\n",
      "Epoch 18/64, Train Loss: 0.9528, Test Loss: 1.2589\n",
      "Epoch 19/64, Train Loss: 0.9575, Test Loss: 1.2666\n",
      "Epoch 20/64, Train Loss: 0.9410, Test Loss: 1.2644\n",
      "Epoch 21/64, Train Loss: 0.9435, Test Loss: 1.2850\n",
      "Epoch 22/64, Train Loss: 0.9478, Test Loss: 1.2557\n",
      "Epoch 23/64, Train Loss: 0.9533, Test Loss: 1.2539\n",
      "Epoch 24/64, Train Loss: 0.9536, Test Loss: 1.2640\n",
      "Epoch 25/64, Train Loss: 0.9343, Test Loss: 1.2803\n",
      "Epoch 26/64, Train Loss: 0.9519, Test Loss: 1.2999\n",
      "Epoch 27/64, Train Loss: 0.9308, Test Loss: 1.2843\n",
      "Epoch 28/64, Train Loss: 0.9303, Test Loss: 1.2694\n",
      "Epoch 29/64, Train Loss: 0.9321, Test Loss: 1.2908\n",
      "Epoch 30/64, Train Loss: 0.9184, Test Loss: 1.3179\n",
      "Epoch 31/64, Train Loss: 0.9266, Test Loss: 1.2831\n",
      "Epoch 32/64, Train Loss: 0.9006, Test Loss: 1.2873\n",
      "Epoch 33/64, Train Loss: 0.9153, Test Loss: 1.2912\n",
      "Epoch 34/64, Train Loss: 0.8873, Test Loss: 1.3199\n",
      "Epoch 35/64, Train Loss: 0.8713, Test Loss: 1.3344\n",
      "Epoch 36/64, Train Loss: 0.8652, Test Loss: 1.3153\n",
      "Epoch 37/64, Train Loss: 0.8665, Test Loss: 1.3146\n",
      "Epoch 38/64, Train Loss: 0.8761, Test Loss: 1.3061\n",
      "Epoch 39/64, Train Loss: 0.8142, Test Loss: 1.3279\n",
      "Epoch 40/64, Train Loss: 0.8384, Test Loss: 1.3086\n",
      "Epoch 41/64, Train Loss: 0.8042, Test Loss: 1.3616\n",
      "Epoch 42/64, Train Loss: 0.8379, Test Loss: 1.3347\n",
      "Epoch 43/64, Train Loss: 0.8296, Test Loss: 1.3079\n",
      "Epoch 44/64, Train Loss: 0.7754, Test Loss: 1.4381\n",
      "Epoch 45/64, Train Loss: 0.7836, Test Loss: 1.3367\n",
      "Epoch 46/64, Train Loss: 0.7407, Test Loss: 1.3606\n",
      "Epoch 47/64, Train Loss: 0.7646, Test Loss: 1.3829\n",
      "Epoch 48/64, Train Loss: 0.7314, Test Loss: 1.4053\n",
      "Epoch 49/64, Train Loss: 0.7315, Test Loss: 1.3395\n",
      "Epoch 50/64, Train Loss: 0.7195, Test Loss: 1.3018\n",
      "Epoch 51/64, Train Loss: 0.7007, Test Loss: 1.3154\n",
      "Epoch 52/64, Train Loss: 0.6943, Test Loss: 1.3051\n",
      "Epoch 53/64, Train Loss: 0.6761, Test Loss: 1.4433\n",
      "Epoch 54/64, Train Loss: 0.6832, Test Loss: 1.3555\n",
      "Epoch 55/64, Train Loss: 0.6298, Test Loss: 1.4262\n",
      "Epoch 56/64, Train Loss: 0.6401, Test Loss: 1.3903\n",
      "Epoch 57/64, Train Loss: 0.6587, Test Loss: 1.3102\n",
      "Epoch 58/64, Train Loss: 0.6486, Test Loss: 1.3941\n",
      "Epoch 59/64, Train Loss: 0.6325, Test Loss: 1.3369\n",
      "Epoch 60/64, Train Loss: 0.6289, Test Loss: 1.4077\n",
      "Epoch 61/64, Train Loss: 0.6011, Test Loss: 1.3924\n",
      "Epoch 62/64, Train Loss: 0.5577, Test Loss: 1.3050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:57:00,223] Trial 94 finished with value: 1.41886705160141 and parameters: {'num_hidden_layers': 17, 'layer_0_size': 256, 'layer_1_size': 194, 'layer_2_size': 134, 'layer_3_size': 89, 'layer_4_size': 65, 'layer_5_size': 192, 'layer_6_size': 158, 'layer_7_size': 74, 'layer_8_size': 199, 'layer_9_size': 243, 'layer_10_size': 163, 'layer_11_size': 210, 'layer_12_size': 83, 'layer_13_size': 196, 'layer_14_size': 133, 'layer_15_size': 49, 'layer_16_size': 124, 'dropout_rate': 0.29472136219072415, 'learning_rate': 0.0035111646992771963, 'batch_size': 64, 'epochs': 64}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/64, Train Loss: 0.5885, Test Loss: 1.3262\n",
      "Epoch 64/64, Train Loss: 0.5781, Test Loss: 1.4189\n",
      "Epoch 1/36, Train Loss: 1.2853, Test Loss: 0.9522\n",
      "Epoch 2/36, Train Loss: 1.2522, Test Loss: 0.9444\n",
      "Epoch 3/36, Train Loss: 1.1784, Test Loss: 0.9472\n",
      "Epoch 4/36, Train Loss: 1.1190, Test Loss: 0.9461\n",
      "Epoch 5/36, Train Loss: 1.0967, Test Loss: 0.9376\n",
      "Epoch 6/36, Train Loss: 1.1394, Test Loss: 0.9423\n",
      "Epoch 7/36, Train Loss: 1.1387, Test Loss: 0.9454\n",
      "Epoch 8/36, Train Loss: 1.0894, Test Loss: 0.9512\n",
      "Epoch 9/36, Train Loss: 1.0864, Test Loss: 0.9603\n",
      "Epoch 10/36, Train Loss: 1.0854, Test Loss: 0.9571\n",
      "Epoch 11/36, Train Loss: 1.0812, Test Loss: 0.9414\n",
      "Epoch 12/36, Train Loss: 1.0965, Test Loss: 0.9430\n",
      "Epoch 13/36, Train Loss: 1.0815, Test Loss: 0.9409\n",
      "Epoch 14/36, Train Loss: 1.0751, Test Loss: 0.9420\n",
      "Epoch 15/36, Train Loss: 1.0649, Test Loss: 0.9439\n",
      "Epoch 16/36, Train Loss: 1.0803, Test Loss: 0.9466\n",
      "Epoch 17/36, Train Loss: 1.0715, Test Loss: 0.9443\n",
      "Epoch 18/36, Train Loss: 1.0733, Test Loss: 0.9595\n",
      "Epoch 19/36, Train Loss: 1.0809, Test Loss: 0.9432\n",
      "Epoch 20/36, Train Loss: 1.0674, Test Loss: 0.9463\n",
      "Epoch 21/36, Train Loss: 1.0572, Test Loss: 0.9590\n",
      "Epoch 22/36, Train Loss: 1.0792, Test Loss: 0.9467\n",
      "Epoch 23/36, Train Loss: 1.0639, Test Loss: 0.9441\n",
      "Epoch 24/36, Train Loss: 1.0668, Test Loss: 0.9509\n",
      "Epoch 25/36, Train Loss: 1.0722, Test Loss: 0.9499\n",
      "Epoch 26/36, Train Loss: 1.0665, Test Loss: 0.9462\n",
      "Epoch 27/36, Train Loss: 1.0673, Test Loss: 0.9418\n",
      "Epoch 28/36, Train Loss: 1.0556, Test Loss: 0.9520\n",
      "Epoch 29/36, Train Loss: 1.0662, Test Loss: 0.9415\n",
      "Epoch 30/36, Train Loss: 1.0315, Test Loss: 0.9482\n",
      "Epoch 31/36, Train Loss: 1.0494, Test Loss: 0.9357\n",
      "Epoch 32/36, Train Loss: 1.0535, Test Loss: 0.9324\n",
      "Epoch 33/36, Train Loss: 1.0277, Test Loss: 0.9383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:57:03,385] Trial 95 finished with value: 0.9344200491905212 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 60, 'layer_1_size': 168, 'layer_2_size': 187, 'layer_3_size': 223, 'layer_4_size': 185, 'layer_5_size': 75, 'layer_6_size': 169, 'layer_7_size': 66, 'layer_8_size': 184, 'layer_9_size': 50, 'layer_10_size': 194, 'layer_11_size': 198, 'layer_12_size': 246, 'dropout_rate': 0.2836205757047816, 'learning_rate': 0.0024725156288033802, 'batch_size': 128, 'epochs': 36}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/36, Train Loss: 1.0309, Test Loss: 0.9361\n",
      "Epoch 35/36, Train Loss: 1.0137, Test Loss: 0.9312\n",
      "Epoch 36/36, Train Loss: 1.0405, Test Loss: 0.9344\n",
      "Epoch 1/48, Train Loss: 1.1715, Test Loss: 0.9113\n",
      "Epoch 2/48, Train Loss: 0.9991, Test Loss: 1.1440\n",
      "Epoch 3/48, Train Loss: 1.0105, Test Loss: 0.9065\n",
      "Epoch 4/48, Train Loss: 0.9826, Test Loss: 0.9367\n",
      "Epoch 5/48, Train Loss: 0.9943, Test Loss: 0.9595\n",
      "Epoch 6/48, Train Loss: 0.9848, Test Loss: 0.9455\n",
      "Epoch 7/48, Train Loss: 0.9794, Test Loss: 0.9199\n",
      "Epoch 8/48, Train Loss: 0.9520, Test Loss: 0.9718\n",
      "Epoch 9/48, Train Loss: 0.9706, Test Loss: 0.8960\n",
      "Epoch 10/48, Train Loss: 0.9549, Test Loss: 0.9078\n",
      "Epoch 11/48, Train Loss: 0.9340, Test Loss: 0.9077\n",
      "Epoch 12/48, Train Loss: 0.9207, Test Loss: 0.9180\n",
      "Epoch 13/48, Train Loss: 0.8982, Test Loss: 0.8950\n",
      "Epoch 14/48, Train Loss: 0.8727, Test Loss: 0.9054\n",
      "Epoch 15/48, Train Loss: 0.8735, Test Loss: 0.8761\n",
      "Epoch 16/48, Train Loss: 0.8481, Test Loss: 0.9120\n",
      "Epoch 17/48, Train Loss: 0.8378, Test Loss: 0.9450\n",
      "Epoch 18/48, Train Loss: 0.8292, Test Loss: 0.9164\n",
      "Epoch 19/48, Train Loss: 0.8201, Test Loss: 0.9390\n",
      "Epoch 20/48, Train Loss: 0.8411, Test Loss: 1.0100\n",
      "Epoch 21/48, Train Loss: 0.7772, Test Loss: 1.0524\n",
      "Epoch 22/48, Train Loss: 0.7931, Test Loss: 1.0052\n",
      "Epoch 23/48, Train Loss: 0.7818, Test Loss: 1.0048\n",
      "Epoch 24/48, Train Loss: 0.7546, Test Loss: 0.9974\n",
      "Epoch 25/48, Train Loss: 0.6951, Test Loss: 1.0561\n",
      "Epoch 26/48, Train Loss: 0.7156, Test Loss: 1.0829\n",
      "Epoch 27/48, Train Loss: 0.6851, Test Loss: 1.0446\n",
      "Epoch 28/48, Train Loss: 0.6672, Test Loss: 1.0546\n",
      "Epoch 29/48, Train Loss: 0.6892, Test Loss: 0.9906\n",
      "Epoch 30/48, Train Loss: 0.6726, Test Loss: 1.0041\n",
      "Epoch 31/48, Train Loss: 0.6553, Test Loss: 1.0380\n",
      "Epoch 32/48, Train Loss: 0.6567, Test Loss: 0.9954\n",
      "Epoch 33/48, Train Loss: 0.6515, Test Loss: 1.0578\n",
      "Epoch 34/48, Train Loss: 0.7140, Test Loss: 0.9517\n",
      "Epoch 35/48, Train Loss: 0.6374, Test Loss: 1.0506\n",
      "Epoch 36/48, Train Loss: 0.6211, Test Loss: 1.0760\n",
      "Epoch 37/48, Train Loss: 0.5916, Test Loss: 1.1244\n",
      "Epoch 38/48, Train Loss: 0.6164, Test Loss: 1.0863\n",
      "Epoch 39/48, Train Loss: 0.5904, Test Loss: 0.9702\n",
      "Epoch 40/48, Train Loss: 0.6398, Test Loss: 0.9969\n",
      "Epoch 41/48, Train Loss: 0.5941, Test Loss: 0.9476\n",
      "Epoch 42/48, Train Loss: 0.6057, Test Loss: 1.1000\n",
      "Epoch 43/48, Train Loss: 0.5854, Test Loss: 1.0737\n",
      "Epoch 44/48, Train Loss: 0.5791, Test Loss: 1.0534\n",
      "Epoch 45/48, Train Loss: 0.5643, Test Loss: 1.0280\n",
      "Epoch 46/48, Train Loss: 0.5304, Test Loss: 1.0364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:57:09,150] Trial 96 finished with value: 1.0220442754881722 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 72, 'layer_1_size': 154, 'layer_2_size': 220, 'layer_3_size': 104, 'layer_4_size': 43, 'layer_5_size': 86, 'layer_6_size': 179, 'dropout_rate': 0.31398380350099914, 'learning_rate': 0.006836520716110794, 'batch_size': 32, 'epochs': 48}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/48, Train Loss: 0.4772, Test Loss: 1.0338\n",
      "Epoch 48/48, Train Loss: 0.5211, Test Loss: 1.0220\n",
      "Epoch 1/52, Train Loss: 1.1743, Test Loss: 1.7317\n",
      "Epoch 2/52, Train Loss: 1.0256, Test Loss: 1.1959\n",
      "Epoch 3/52, Train Loss: 0.9947, Test Loss: 1.0371\n",
      "Epoch 4/52, Train Loss: 0.9889, Test Loss: 1.0163\n",
      "Epoch 5/52, Train Loss: 1.0013, Test Loss: 0.9987\n",
      "Epoch 6/52, Train Loss: 0.9869, Test Loss: 0.9964\n",
      "Epoch 7/52, Train Loss: 1.0007, Test Loss: 0.9993\n",
      "Epoch 8/52, Train Loss: 1.0074, Test Loss: 0.9949\n",
      "Epoch 9/52, Train Loss: 1.0051, Test Loss: 0.9984\n",
      "Epoch 10/52, Train Loss: 0.9970, Test Loss: 1.0151\n",
      "Epoch 11/52, Train Loss: 0.9835, Test Loss: 1.0368\n",
      "Epoch 12/52, Train Loss: 0.9719, Test Loss: 0.9951\n",
      "Epoch 13/52, Train Loss: 0.9694, Test Loss: 1.0084\n",
      "Epoch 14/52, Train Loss: 0.9887, Test Loss: 0.9970\n",
      "Epoch 15/52, Train Loss: 1.0249, Test Loss: 0.9642\n",
      "Epoch 16/52, Train Loss: 0.9931, Test Loss: 0.9952\n",
      "Epoch 17/52, Train Loss: 0.9512, Test Loss: 1.0218\n",
      "Epoch 18/52, Train Loss: 0.9466, Test Loss: 1.0354\n",
      "Epoch 19/52, Train Loss: 0.9421, Test Loss: 1.0391\n",
      "Epoch 20/52, Train Loss: 0.9231, Test Loss: 0.9961\n",
      "Epoch 21/52, Train Loss: 0.9088, Test Loss: 0.9985\n",
      "Epoch 22/52, Train Loss: 0.9087, Test Loss: 1.0583\n",
      "Epoch 23/52, Train Loss: 0.9339, Test Loss: 0.9536\n",
      "Epoch 24/52, Train Loss: 0.8487, Test Loss: 1.0024\n",
      "Epoch 25/52, Train Loss: 0.8875, Test Loss: 1.0649\n",
      "Epoch 26/52, Train Loss: 0.8449, Test Loss: 1.0824\n",
      "Epoch 27/52, Train Loss: 0.7824, Test Loss: 1.2892\n",
      "Epoch 28/52, Train Loss: 0.8099, Test Loss: 1.2167\n",
      "Epoch 29/52, Train Loss: 0.7978, Test Loss: 1.0780\n",
      "Epoch 30/52, Train Loss: 0.7817, Test Loss: 1.1094\n",
      "Epoch 31/52, Train Loss: 0.7275, Test Loss: 1.2641\n",
      "Epoch 32/52, Train Loss: 0.7299, Test Loss: 1.1424\n",
      "Epoch 33/52, Train Loss: 0.6755, Test Loss: 1.2151\n",
      "Epoch 34/52, Train Loss: 0.6592, Test Loss: 1.3093\n",
      "Epoch 35/52, Train Loss: 0.6431, Test Loss: 1.2411\n",
      "Epoch 36/52, Train Loss: 0.6493, Test Loss: 1.1636\n",
      "Epoch 37/52, Train Loss: 0.6204, Test Loss: 1.2395\n",
      "Epoch 38/52, Train Loss: 0.6258, Test Loss: 1.1259\n",
      "Epoch 39/52, Train Loss: 0.5761, Test Loss: 1.0751\n",
      "Epoch 40/52, Train Loss: 0.6073, Test Loss: 1.1233\n",
      "Epoch 41/52, Train Loss: 0.6149, Test Loss: 1.1383\n",
      "Epoch 42/52, Train Loss: 0.5336, Test Loss: 1.1297\n",
      "Epoch 43/52, Train Loss: 0.5340, Test Loss: 1.1349\n",
      "Epoch 44/52, Train Loss: 0.4854, Test Loss: 1.1161\n",
      "Epoch 45/52, Train Loss: 0.5006, Test Loss: 1.1727\n",
      "Epoch 46/52, Train Loss: 0.4609, Test Loss: 1.1679\n",
      "Epoch 47/52, Train Loss: 0.4723, Test Loss: 1.1562\n",
      "Epoch 48/52, Train Loss: 0.4608, Test Loss: 1.1430\n",
      "Epoch 49/52, Train Loss: 0.4500, Test Loss: 1.0949\n",
      "Epoch 50/52, Train Loss: 0.4359, Test Loss: 1.1161\n",
      "Epoch 51/52, Train Loss: 0.4211, Test Loss: 1.1782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:57:17,596] Trial 97 finished with value: 1.1830628961324692 and parameters: {'num_hidden_layers': 15, 'layer_0_size': 90, 'layer_1_size': 182, 'layer_2_size': 246, 'layer_3_size': 243, 'layer_4_size': 74, 'layer_5_size': 64, 'layer_6_size': 94, 'layer_7_size': 133, 'layer_8_size': 207, 'layer_9_size': 139, 'layer_10_size': 229, 'layer_11_size': 217, 'layer_12_size': 125, 'layer_13_size': 156, 'layer_14_size': 101, 'dropout_rate': 0.2216588161914082, 'learning_rate': 0.00995669502421252, 'batch_size': 64, 'epochs': 52}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/52, Train Loss: 0.4096, Test Loss: 1.1831\n",
      "Epoch 1/80, Train Loss: 1.2142, Test Loss: 0.9939\n",
      "Epoch 2/80, Train Loss: 1.1629, Test Loss: 1.0115\n",
      "Epoch 3/80, Train Loss: 1.1718, Test Loss: 1.0638\n",
      "Epoch 4/80, Train Loss: 1.1376, Test Loss: 1.0060\n",
      "Epoch 5/80, Train Loss: 1.1341, Test Loss: 1.0002\n",
      "Epoch 6/80, Train Loss: 1.1030, Test Loss: 1.0274\n",
      "Epoch 7/80, Train Loss: 1.1024, Test Loss: 1.0203\n",
      "Epoch 8/80, Train Loss: 1.0924, Test Loss: 1.0158\n",
      "Epoch 9/80, Train Loss: 1.0900, Test Loss: 1.0139\n",
      "Epoch 10/80, Train Loss: 1.0953, Test Loss: 1.0413\n",
      "Epoch 11/80, Train Loss: 1.0910, Test Loss: 1.0046\n",
      "Epoch 12/80, Train Loss: 1.0875, Test Loss: 1.0053\n",
      "Epoch 13/80, Train Loss: 1.0553, Test Loss: 1.0086\n",
      "Epoch 14/80, Train Loss: 1.0380, Test Loss: 0.9929\n",
      "Epoch 15/80, Train Loss: 1.0310, Test Loss: 1.0059\n",
      "Epoch 16/80, Train Loss: 1.0443, Test Loss: 0.9931\n",
      "Epoch 17/80, Train Loss: 1.0350, Test Loss: 0.9943\n",
      "Epoch 18/80, Train Loss: 1.0116, Test Loss: 0.9755\n",
      "Epoch 19/80, Train Loss: 1.0896, Test Loss: 1.0029\n",
      "Epoch 20/80, Train Loss: 1.0288, Test Loss: 0.9816\n",
      "Epoch 21/80, Train Loss: 0.9781, Test Loss: 0.9871\n",
      "Epoch 22/80, Train Loss: 1.0644, Test Loss: 0.9927\n",
      "Epoch 23/80, Train Loss: 1.0121, Test Loss: 0.9893\n",
      "Epoch 24/80, Train Loss: 1.0051, Test Loss: 0.9941\n",
      "Epoch 25/80, Train Loss: 0.9915, Test Loss: 0.9905\n",
      "Epoch 26/80, Train Loss: 1.0364, Test Loss: 0.9865\n",
      "Epoch 27/80, Train Loss: 1.0215, Test Loss: 0.9785\n",
      "Epoch 28/80, Train Loss: 1.0311, Test Loss: 0.9843\n",
      "Epoch 29/80, Train Loss: 1.0625, Test Loss: 0.9881\n",
      "Epoch 30/80, Train Loss: 1.0073, Test Loss: 0.9883\n",
      "Epoch 31/80, Train Loss: 1.0083, Test Loss: 0.9799\n",
      "Epoch 32/80, Train Loss: 1.0595, Test Loss: 0.9808\n",
      "Epoch 33/80, Train Loss: 1.0244, Test Loss: 0.9815\n",
      "Epoch 34/80, Train Loss: 1.0068, Test Loss: 0.9922\n",
      "Epoch 35/80, Train Loss: 1.0301, Test Loss: 0.9845\n",
      "Epoch 36/80, Train Loss: 1.0625, Test Loss: 0.9811\n",
      "Epoch 37/80, Train Loss: 0.9889, Test Loss: 0.9817\n",
      "Epoch 38/80, Train Loss: 0.9980, Test Loss: 0.9759\n",
      "Epoch 39/80, Train Loss: 1.0212, Test Loss: 0.9765\n",
      "Epoch 40/80, Train Loss: 1.0066, Test Loss: 0.9806\n",
      "Epoch 41/80, Train Loss: 1.0098, Test Loss: 0.9717\n",
      "Epoch 42/80, Train Loss: 1.0069, Test Loss: 0.9729\n",
      "Epoch 43/80, Train Loss: 1.0066, Test Loss: 0.9736\n",
      "Epoch 44/80, Train Loss: 0.9859, Test Loss: 0.9680\n",
      "Epoch 45/80, Train Loss: 1.0234, Test Loss: 0.9787\n",
      "Epoch 46/80, Train Loss: 1.0067, Test Loss: 0.9736\n",
      "Epoch 47/80, Train Loss: 1.0215, Test Loss: 0.9711\n",
      "Epoch 48/80, Train Loss: 1.0117, Test Loss: 0.9707\n",
      "Epoch 49/80, Train Loss: 0.9678, Test Loss: 0.9782\n",
      "Epoch 50/80, Train Loss: 1.0103, Test Loss: 0.9782\n",
      "Epoch 51/80, Train Loss: 1.0080, Test Loss: 0.9693\n",
      "Epoch 52/80, Train Loss: 1.0093, Test Loss: 0.9802\n",
      "Epoch 53/80, Train Loss: 0.9935, Test Loss: 0.9764\n",
      "Epoch 54/80, Train Loss: 1.0140, Test Loss: 0.9719\n",
      "Epoch 55/80, Train Loss: 0.9979, Test Loss: 0.9698\n",
      "Epoch 56/80, Train Loss: 1.0433, Test Loss: 0.9751\n",
      "Epoch 57/80, Train Loss: 0.9929, Test Loss: 0.9730\n",
      "Epoch 58/80, Train Loss: 1.0094, Test Loss: 0.9673\n",
      "Epoch 59/80, Train Loss: 1.0201, Test Loss: 0.9736\n",
      "Epoch 60/80, Train Loss: 0.9943, Test Loss: 0.9760\n",
      "Epoch 61/80, Train Loss: 1.0158, Test Loss: 0.9689\n",
      "Epoch 62/80, Train Loss: 1.0279, Test Loss: 0.9706\n",
      "Epoch 63/80, Train Loss: 1.0052, Test Loss: 0.9682\n",
      "Epoch 64/80, Train Loss: 1.0086, Test Loss: 0.9632\n",
      "Epoch 65/80, Train Loss: 0.9952, Test Loss: 0.9712\n",
      "Epoch 66/80, Train Loss: 0.9990, Test Loss: 0.9721\n",
      "Epoch 67/80, Train Loss: 0.9841, Test Loss: 0.9701\n",
      "Epoch 68/80, Train Loss: 0.9789, Test Loss: 0.9675\n",
      "Epoch 69/80, Train Loss: 1.0472, Test Loss: 0.9726\n",
      "Epoch 70/80, Train Loss: 0.9951, Test Loss: 0.9738\n",
      "Epoch 71/80, Train Loss: 1.0121, Test Loss: 0.9739\n",
      "Epoch 72/80, Train Loss: 1.0139, Test Loss: 0.9668\n",
      "Epoch 73/80, Train Loss: 0.9996, Test Loss: 0.9751\n",
      "Epoch 74/80, Train Loss: 0.9878, Test Loss: 0.9750\n",
      "Epoch 75/80, Train Loss: 0.9975, Test Loss: 0.9702\n",
      "Epoch 76/80, Train Loss: 1.0347, Test Loss: 0.9767\n",
      "Epoch 77/80, Train Loss: 1.0258, Test Loss: 0.9741\n",
      "Epoch 78/80, Train Loss: 0.9748, Test Loss: 0.9705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:57:33,157] Trial 98 finished with value: 0.9777453201157706 and parameters: {'num_hidden_layers': 11, 'layer_0_size': 115, 'layer_1_size': 145, 'layer_2_size': 71, 'layer_3_size': 122, 'layer_4_size': 88, 'layer_5_size': 155, 'layer_6_size': 255, 'layer_7_size': 84, 'layer_8_size': 147, 'layer_9_size': 62, 'layer_10_size': 249, 'dropout_rate': 0.21019617532552856, 'learning_rate': 1.6739573961888037e-05, 'batch_size': 32, 'epochs': 80}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/80, Train Loss: 1.0059, Test Loss: 0.9663\n",
      "Epoch 80/80, Train Loss: 1.0138, Test Loss: 0.9777\n",
      "Epoch 1/60, Train Loss: 1.2197, Test Loss: 0.9218\n",
      "Epoch 2/60, Train Loss: 1.2201, Test Loss: 0.9215\n",
      "Epoch 3/60, Train Loss: 1.1909, Test Loss: 0.9185\n",
      "Epoch 4/60, Train Loss: 1.1619, Test Loss: 0.9097\n",
      "Epoch 5/60, Train Loss: 1.1518, Test Loss: 0.9029\n",
      "Epoch 6/60, Train Loss: 1.1335, Test Loss: 0.8991\n",
      "Epoch 7/60, Train Loss: 1.1282, Test Loss: 0.8957\n",
      "Epoch 8/60, Train Loss: 1.1235, Test Loss: 0.8964\n",
      "Epoch 9/60, Train Loss: 1.1758, Test Loss: 0.9013\n",
      "Epoch 10/60, Train Loss: 1.1389, Test Loss: 0.9027\n",
      "Epoch 11/60, Train Loss: 1.1265, Test Loss: 0.8962\n",
      "Epoch 12/60, Train Loss: 1.1108, Test Loss: 0.8946\n",
      "Epoch 13/60, Train Loss: 1.1350, Test Loss: 0.8918\n",
      "Epoch 14/60, Train Loss: 1.1234, Test Loss: 0.8930\n",
      "Epoch 15/60, Train Loss: 1.0971, Test Loss: 0.8925\n",
      "Epoch 16/60, Train Loss: 1.0977, Test Loss: 0.8894\n",
      "Epoch 17/60, Train Loss: 1.0859, Test Loss: 0.8838\n",
      "Epoch 18/60, Train Loss: 1.1059, Test Loss: 0.8818\n",
      "Epoch 19/60, Train Loss: 1.0855, Test Loss: 0.8805\n",
      "Epoch 20/60, Train Loss: 1.1249, Test Loss: 0.8785\n",
      "Epoch 21/60, Train Loss: 1.0878, Test Loss: 0.8749\n",
      "Epoch 22/60, Train Loss: 1.0835, Test Loss: 0.8724\n",
      "Epoch 23/60, Train Loss: 1.0899, Test Loss: 0.8747\n",
      "Epoch 24/60, Train Loss: 1.0851, Test Loss: 0.8719\n",
      "Epoch 25/60, Train Loss: 1.0738, Test Loss: 0.8689\n",
      "Epoch 26/60, Train Loss: 1.1076, Test Loss: 0.8636\n",
      "Epoch 27/60, Train Loss: 1.1062, Test Loss: 0.8638\n",
      "Epoch 28/60, Train Loss: 1.1252, Test Loss: 0.8656\n",
      "Epoch 29/60, Train Loss: 1.0989, Test Loss: 0.8658\n",
      "Epoch 30/60, Train Loss: 1.0915, Test Loss: 0.8642\n",
      "Epoch 31/60, Train Loss: 1.0773, Test Loss: 0.8602\n",
      "Epoch 32/60, Train Loss: 1.0682, Test Loss: 0.8587\n",
      "Epoch 33/60, Train Loss: 1.0752, Test Loss: 0.8598\n",
      "Epoch 34/60, Train Loss: 1.0888, Test Loss: 0.8611\n",
      "Epoch 35/60, Train Loss: 1.0841, Test Loss: 0.8604\n",
      "Epoch 36/60, Train Loss: 1.0735, Test Loss: 0.8600\n",
      "Epoch 37/60, Train Loss: 1.0888, Test Loss: 0.8628\n",
      "Epoch 38/60, Train Loss: 1.0888, Test Loss: 0.8639\n",
      "Epoch 39/60, Train Loss: 1.0857, Test Loss: 0.8642\n",
      "Epoch 40/60, Train Loss: 1.0854, Test Loss: 0.8640\n",
      "Epoch 41/60, Train Loss: 1.0804, Test Loss: 0.8617\n",
      "Epoch 42/60, Train Loss: 1.0854, Test Loss: 0.8649\n",
      "Epoch 43/60, Train Loss: 1.0818, Test Loss: 0.8661\n",
      "Epoch 44/60, Train Loss: 1.0643, Test Loss: 0.8647\n",
      "Epoch 45/60, Train Loss: 1.0905, Test Loss: 0.8614\n",
      "Epoch 46/60, Train Loss: 1.0617, Test Loss: 0.8586\n",
      "Epoch 47/60, Train Loss: 1.0524, Test Loss: 0.8581\n",
      "Epoch 48/60, Train Loss: 1.0319, Test Loss: 0.8579\n",
      "Epoch 49/60, Train Loss: 1.0701, Test Loss: 0.8590\n",
      "Epoch 50/60, Train Loss: 1.0094, Test Loss: 0.8610\n",
      "Epoch 51/60, Train Loss: 1.0295, Test Loss: 0.8606\n",
      "Epoch 52/60, Train Loss: 1.0275, Test Loss: 0.8615\n",
      "Epoch 53/60, Train Loss: 1.0508, Test Loss: 0.8627\n",
      "Epoch 54/60, Train Loss: 1.0122, Test Loss: 0.8614\n",
      "Epoch 55/60, Train Loss: 1.0149, Test Loss: 0.8599\n",
      "Epoch 56/60, Train Loss: 1.0656, Test Loss: 0.8608\n",
      "Epoch 57/60, Train Loss: 1.0067, Test Loss: 0.8594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:57:36,515] Trial 99 finished with value: 0.8644423484802246 and parameters: {'num_hidden_layers': 10, 'layer_0_size': 241, 'layer_1_size': 234, 'layer_2_size': 108, 'layer_3_size': 112, 'layer_4_size': 103, 'layer_5_size': 115, 'layer_6_size': 222, 'layer_7_size': 199, 'layer_8_size': 223, 'layer_9_size': 234, 'dropout_rate': 0.1744613510642068, 'learning_rate': 0.00033152957554564554, 'batch_size': 256, 'epochs': 60}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/60, Train Loss: 1.0155, Test Loss: 0.8608\n",
      "Epoch 59/60, Train Loss: 1.0023, Test Loss: 0.8607\n",
      "Epoch 60/60, Train Loss: 1.0102, Test Loss: 0.8644\n",
      "Epoch 1/60, Train Loss: 1.2491, Test Loss: 0.8215\n",
      "Epoch 2/60, Train Loss: 1.0864, Test Loss: 0.8228\n",
      "Epoch 3/60, Train Loss: 1.1043, Test Loss: 0.8251\n",
      "Epoch 4/60, Train Loss: 1.1179, Test Loss: 0.8274\n",
      "Epoch 5/60, Train Loss: 1.1009, Test Loss: 0.8270\n",
      "Epoch 6/60, Train Loss: 1.0533, Test Loss: 0.8264\n",
      "Epoch 7/60, Train Loss: 1.0527, Test Loss: 0.8263\n",
      "Epoch 8/60, Train Loss: 1.0574, Test Loss: 0.8246\n",
      "Epoch 9/60, Train Loss: 1.0432, Test Loss: 0.8260\n",
      "Epoch 10/60, Train Loss: 1.0501, Test Loss: 0.8356\n",
      "Epoch 11/60, Train Loss: 1.0422, Test Loss: 0.8419\n",
      "Epoch 12/60, Train Loss: 1.0258, Test Loss: 0.8344\n",
      "Epoch 13/60, Train Loss: 0.9998, Test Loss: 0.8313\n",
      "Epoch 14/60, Train Loss: 1.0277, Test Loss: 0.8334\n",
      "Epoch 15/60, Train Loss: 0.9782, Test Loss: 0.8351\n",
      "Epoch 16/60, Train Loss: 1.0061, Test Loss: 0.8412\n",
      "Epoch 17/60, Train Loss: 0.9702, Test Loss: 0.8453\n",
      "Epoch 18/60, Train Loss: 0.9737, Test Loss: 0.8497\n",
      "Epoch 19/60, Train Loss: 0.9698, Test Loss: 0.8527\n",
      "Epoch 20/60, Train Loss: 0.9594, Test Loss: 0.8751\n",
      "Epoch 21/60, Train Loss: 0.9483, Test Loss: 0.9000\n",
      "Epoch 22/60, Train Loss: 0.9443, Test Loss: 0.8948\n",
      "Epoch 23/60, Train Loss: 0.9804, Test Loss: 0.8903\n",
      "Epoch 24/60, Train Loss: 0.9518, Test Loss: 0.8942\n",
      "Epoch 25/60, Train Loss: 0.9074, Test Loss: 0.8910\n",
      "Epoch 26/60, Train Loss: 0.8476, Test Loss: 0.9083\n",
      "Epoch 27/60, Train Loss: 0.8809, Test Loss: 0.9333\n",
      "Epoch 28/60, Train Loss: 0.8434, Test Loss: 0.9391\n",
      "Epoch 29/60, Train Loss: 0.8339, Test Loss: 0.9578\n",
      "Epoch 30/60, Train Loss: 0.8374, Test Loss: 0.9639\n",
      "Epoch 31/60, Train Loss: 0.8197, Test Loss: 0.9917\n",
      "Epoch 32/60, Train Loss: 0.7738, Test Loss: 1.0090\n",
      "Epoch 33/60, Train Loss: 0.7929, Test Loss: 1.0183\n",
      "Epoch 34/60, Train Loss: 0.7345, Test Loss: 1.0249\n",
      "Epoch 35/60, Train Loss: 0.7634, Test Loss: 1.0684\n",
      "Epoch 36/60, Train Loss: 0.7257, Test Loss: 1.0825\n",
      "Epoch 37/60, Train Loss: 0.6859, Test Loss: 1.1278\n",
      "Epoch 38/60, Train Loss: 0.6868, Test Loss: 1.1780\n",
      "Epoch 39/60, Train Loss: 0.6659, Test Loss: 1.2264\n",
      "Epoch 40/60, Train Loss: 0.6260, Test Loss: 1.2781\n",
      "Epoch 41/60, Train Loss: 0.6514, Test Loss: 1.2911\n",
      "Epoch 42/60, Train Loss: 0.5847, Test Loss: 1.3326\n",
      "Epoch 43/60, Train Loss: 0.5794, Test Loss: 1.3372\n",
      "Epoch 44/60, Train Loss: 0.5768, Test Loss: 1.3274\n",
      "Epoch 45/60, Train Loss: 0.5887, Test Loss: 1.3286\n",
      "Epoch 46/60, Train Loss: 0.5451, Test Loss: 1.2965\n",
      "Epoch 47/60, Train Loss: 0.5404, Test Loss: 1.2766\n",
      "Epoch 48/60, Train Loss: 0.5218, Test Loss: 1.2820\n",
      "Epoch 49/60, Train Loss: 0.5141, Test Loss: 1.3046\n",
      "Epoch 50/60, Train Loss: 0.4843, Test Loss: 1.3634\n",
      "Epoch 51/60, Train Loss: 0.5080, Test Loss: 1.3657\n",
      "Epoch 52/60, Train Loss: 0.4435, Test Loss: 1.3741\n",
      "Epoch 53/60, Train Loss: 0.4578, Test Loss: 1.3424\n",
      "Epoch 54/60, Train Loss: 0.4815, Test Loss: 1.3062\n",
      "Epoch 55/60, Train Loss: 0.4332, Test Loss: 1.2989\n",
      "Epoch 56/60, Train Loss: 0.4269, Test Loss: 1.3101\n",
      "Epoch 57/60, Train Loss: 0.4136, Test Loss: 1.3585\n",
      "Epoch 58/60, Train Loss: 0.4125, Test Loss: 1.4327\n",
      "Epoch 59/60, Train Loss: 0.4078, Test Loss: 1.4903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:57:39,706] Trial 100 finished with value: 1.448755145072937 and parameters: {'num_hidden_layers': 10, 'layer_0_size': 229, 'layer_1_size': 236, 'layer_2_size': 101, 'layer_3_size': 111, 'layer_4_size': 124, 'layer_5_size': 117, 'layer_6_size': 247, 'layer_7_size': 198, 'layer_8_size': 242, 'layer_9_size': 220, 'dropout_rate': 0.13647691931305628, 'learning_rate': 0.000642083218072311, 'batch_size': 256, 'epochs': 60}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/60, Train Loss: 0.4032, Test Loss: 1.4488\n",
      "Epoch 1/55, Train Loss: 1.2139, Test Loss: 0.9525\n",
      "Epoch 2/55, Train Loss: 1.1264, Test Loss: 0.9554\n",
      "Epoch 3/55, Train Loss: 1.0957, Test Loss: 0.9606\n",
      "Epoch 4/55, Train Loss: 1.1082, Test Loss: 0.9644\n",
      "Epoch 5/55, Train Loss: 1.0784, Test Loss: 0.9648\n",
      "Epoch 6/55, Train Loss: 1.0658, Test Loss: 0.9646\n",
      "Epoch 7/55, Train Loss: 1.0720, Test Loss: 0.9662\n",
      "Epoch 8/55, Train Loss: 1.0672, Test Loss: 0.9683\n",
      "Epoch 9/55, Train Loss: 1.0860, Test Loss: 0.9709\n",
      "Epoch 10/55, Train Loss: 1.0916, Test Loss: 0.9749\n",
      "Epoch 11/55, Train Loss: 1.0486, Test Loss: 0.9790\n",
      "Epoch 12/55, Train Loss: 1.0719, Test Loss: 0.9814\n",
      "Epoch 13/55, Train Loss: 1.0957, Test Loss: 0.9845\n",
      "Epoch 14/55, Train Loss: 1.0813, Test Loss: 0.9826\n",
      "Epoch 15/55, Train Loss: 1.0477, Test Loss: 0.9763\n",
      "Epoch 16/55, Train Loss: 1.0577, Test Loss: 0.9708\n",
      "Epoch 17/55, Train Loss: 1.0484, Test Loss: 0.9693\n",
      "Epoch 18/55, Train Loss: 1.0583, Test Loss: 0.9704\n",
      "Epoch 19/55, Train Loss: 1.0574, Test Loss: 0.9733\n",
      "Epoch 20/55, Train Loss: 1.0302, Test Loss: 0.9813\n",
      "Epoch 21/55, Train Loss: 1.0545, Test Loss: 0.9893\n",
      "Epoch 22/55, Train Loss: 1.0382, Test Loss: 0.9967\n",
      "Epoch 23/55, Train Loss: 1.0144, Test Loss: 0.9983\n",
      "Epoch 24/55, Train Loss: 1.0332, Test Loss: 0.9987\n",
      "Epoch 25/55, Train Loss: 1.0437, Test Loss: 0.9995\n",
      "Epoch 26/55, Train Loss: 1.0207, Test Loss: 0.9947\n",
      "Epoch 27/55, Train Loss: 1.0173, Test Loss: 0.9905\n",
      "Epoch 28/55, Train Loss: 1.0266, Test Loss: 0.9860\n",
      "Epoch 29/55, Train Loss: 1.0504, Test Loss: 0.9877\n",
      "Epoch 30/55, Train Loss: 1.0121, Test Loss: 0.9932\n",
      "Epoch 31/55, Train Loss: 1.0170, Test Loss: 1.0005\n",
      "Epoch 32/55, Train Loss: 0.9884, Test Loss: 1.0043\n",
      "Epoch 33/55, Train Loss: 0.9893, Test Loss: 1.0073\n",
      "Epoch 34/55, Train Loss: 0.9898, Test Loss: 1.0145\n",
      "Epoch 35/55, Train Loss: 1.0029, Test Loss: 1.0178\n",
      "Epoch 36/55, Train Loss: 1.0015, Test Loss: 1.0167\n",
      "Epoch 37/55, Train Loss: 1.0004, Test Loss: 1.0135\n",
      "Epoch 38/55, Train Loss: 0.9771, Test Loss: 1.0110\n",
      "Epoch 39/55, Train Loss: 0.9726, Test Loss: 1.0093\n",
      "Epoch 40/55, Train Loss: 0.9698, Test Loss: 1.0073\n",
      "Epoch 41/55, Train Loss: 0.9736, Test Loss: 1.0051\n",
      "Epoch 42/55, Train Loss: 0.9614, Test Loss: 1.0070\n",
      "Epoch 43/55, Train Loss: 0.9274, Test Loss: 1.0071\n",
      "Epoch 44/55, Train Loss: 0.9354, Test Loss: 1.0067\n",
      "Epoch 45/55, Train Loss: 0.8998, Test Loss: 1.0100\n",
      "Epoch 46/55, Train Loss: 0.9209, Test Loss: 1.0155\n",
      "Epoch 47/55, Train Loss: 0.9227, Test Loss: 1.0244\n",
      "Epoch 48/55, Train Loss: 0.9517, Test Loss: 1.0286\n",
      "Epoch 49/55, Train Loss: 0.8704, Test Loss: 1.0308\n",
      "Epoch 50/55, Train Loss: 0.9180, Test Loss: 1.0335\n",
      "Epoch 51/55, Train Loss: 0.8966, Test Loss: 1.0334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:57:42,484] Trial 101 finished with value: 1.0383812189102173 and parameters: {'num_hidden_layers': 9, 'layer_0_size': 246, 'layer_1_size': 229, 'layer_2_size': 139, 'layer_3_size': 136, 'layer_4_size': 102, 'layer_5_size': 104, 'layer_6_size': 230, 'layer_7_size': 209, 'layer_8_size': 255, 'dropout_rate': 0.15363568415861256, 'learning_rate': 0.00031468496225138667, 'batch_size': 256, 'epochs': 55}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/55, Train Loss: 0.8519, Test Loss: 1.0371\n",
      "Epoch 53/55, Train Loss: 0.9164, Test Loss: 1.0367\n",
      "Epoch 54/55, Train Loss: 0.8711, Test Loss: 1.0408\n",
      "Epoch 55/55, Train Loss: 0.8714, Test Loss: 1.0384\n",
      "Epoch 1/68, Train Loss: 1.2102, Test Loss: 1.2000\n",
      "Epoch 2/68, Train Loss: 1.1638, Test Loss: 1.2004\n",
      "Epoch 3/68, Train Loss: 1.1684, Test Loss: 1.2012\n",
      "Epoch 4/68, Train Loss: 1.1631, Test Loss: 1.2014\n",
      "Epoch 5/68, Train Loss: 1.1803, Test Loss: 1.2005\n",
      "Epoch 6/68, Train Loss: 1.1735, Test Loss: 1.1997\n",
      "Epoch 7/68, Train Loss: 1.1626, Test Loss: 1.1963\n",
      "Epoch 8/68, Train Loss: 1.1669, Test Loss: 1.1924\n",
      "Epoch 9/68, Train Loss: 1.1474, Test Loss: 1.1925\n",
      "Epoch 10/68, Train Loss: 1.1296, Test Loss: 1.1940\n",
      "Epoch 11/68, Train Loss: 1.1288, Test Loss: 1.2004\n",
      "Epoch 12/68, Train Loss: 1.1269, Test Loss: 1.2062\n",
      "Epoch 13/68, Train Loss: 1.1370, Test Loss: 1.2081\n",
      "Epoch 14/68, Train Loss: 1.1027, Test Loss: 1.2092\n",
      "Epoch 15/68, Train Loss: 1.1344, Test Loss: 1.2072\n",
      "Epoch 16/68, Train Loss: 1.1006, Test Loss: 1.2056\n",
      "Epoch 17/68, Train Loss: 1.1407, Test Loss: 1.2047\n",
      "Epoch 18/68, Train Loss: 1.1335, Test Loss: 1.2034\n",
      "Epoch 19/68, Train Loss: 1.0851, Test Loss: 1.2057\n",
      "Epoch 20/68, Train Loss: 1.1235, Test Loss: 1.2126\n",
      "Epoch 21/68, Train Loss: 1.1144, Test Loss: 1.2161\n",
      "Epoch 22/68, Train Loss: 1.1101, Test Loss: 1.2127\n",
      "Epoch 23/68, Train Loss: 1.1188, Test Loss: 1.2121\n",
      "Epoch 24/68, Train Loss: 1.1033, Test Loss: 1.2125\n",
      "Epoch 25/68, Train Loss: 1.1149, Test Loss: 1.2162\n",
      "Epoch 26/68, Train Loss: 1.1355, Test Loss: 1.2153\n",
      "Epoch 27/68, Train Loss: 1.0985, Test Loss: 1.2148\n",
      "Epoch 28/68, Train Loss: 1.1096, Test Loss: 1.2205\n",
      "Epoch 29/68, Train Loss: 1.0921, Test Loss: 1.2236\n",
      "Epoch 30/68, Train Loss: 1.0861, Test Loss: 1.2243\n",
      "Epoch 31/68, Train Loss: 1.0832, Test Loss: 1.2236\n",
      "Epoch 32/68, Train Loss: 1.0534, Test Loss: 1.2269\n",
      "Epoch 33/68, Train Loss: 1.0629, Test Loss: 1.2321\n",
      "Epoch 34/68, Train Loss: 1.0669, Test Loss: 1.2374\n",
      "Epoch 35/68, Train Loss: 1.0809, Test Loss: 1.2439\n",
      "Epoch 36/68, Train Loss: 1.1063, Test Loss: 1.2448\n",
      "Epoch 37/68, Train Loss: 1.0617, Test Loss: 1.2443\n",
      "Epoch 38/68, Train Loss: 1.0469, Test Loss: 1.2429\n",
      "Epoch 39/68, Train Loss: 1.0378, Test Loss: 1.2462\n",
      "Epoch 40/68, Train Loss: 1.0705, Test Loss: 1.2472\n",
      "Epoch 41/68, Train Loss: 1.0545, Test Loss: 1.2449\n",
      "Epoch 42/68, Train Loss: 1.0481, Test Loss: 1.2440\n",
      "Epoch 43/68, Train Loss: 1.0293, Test Loss: 1.2437\n",
      "Epoch 44/68, Train Loss: 1.0480, Test Loss: 1.2419\n",
      "Epoch 45/68, Train Loss: 1.0391, Test Loss: 1.2425\n",
      "Epoch 46/68, Train Loss: 1.0266, Test Loss: 1.2417\n",
      "Epoch 47/68, Train Loss: 0.9795, Test Loss: 1.2492\n",
      "Epoch 48/68, Train Loss: 0.9771, Test Loss: 1.2617\n",
      "Epoch 49/68, Train Loss: 0.9994, Test Loss: 1.2745\n",
      "Epoch 50/68, Train Loss: 0.9501, Test Loss: 1.2800\n",
      "Epoch 51/68, Train Loss: 0.9555, Test Loss: 1.2885\n",
      "Epoch 52/68, Train Loss: 0.9813, Test Loss: 1.3028\n",
      "Epoch 53/68, Train Loss: 0.8938, Test Loss: 1.3186\n",
      "Epoch 54/68, Train Loss: 0.9254, Test Loss: 1.3197\n",
      "Epoch 55/68, Train Loss: 0.9334, Test Loss: 1.3213\n",
      "Epoch 56/68, Train Loss: 0.9286, Test Loss: 1.3286\n",
      "Epoch 57/68, Train Loss: 0.9473, Test Loss: 1.3358\n",
      "Epoch 58/68, Train Loss: 0.9055, Test Loss: 1.3330\n",
      "Epoch 59/68, Train Loss: 0.9017, Test Loss: 1.3313\n",
      "Epoch 60/68, Train Loss: 0.8871, Test Loss: 1.3369\n",
      "Epoch 61/68, Train Loss: 0.8225, Test Loss: 1.3484\n",
      "Epoch 62/68, Train Loss: 0.8666, Test Loss: 1.3646\n",
      "Epoch 63/68, Train Loss: 0.8113, Test Loss: 1.3823\n",
      "Epoch 64/68, Train Loss: 0.8499, Test Loss: 1.3891\n",
      "Epoch 65/68, Train Loss: 0.7883, Test Loss: 1.4080\n",
      "Epoch 66/68, Train Loss: 0.7846, Test Loss: 1.4207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:57:45,249] Trial 102 finished with value: 1.4281532764434814 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 241, 'layer_1_size': 201, 'layer_2_size': 107, 'layer_3_size': 41, 'layer_4_size': 115, 'layer_5_size': 113, 'layer_6_size': 219, 'layer_7_size': 189, 'dropout_rate': 0.18724207849431146, 'learning_rate': 0.00046418979945414096, 'batch_size': 256, 'epochs': 68}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/68, Train Loss: 0.8155, Test Loss: 1.4262\n",
      "Epoch 68/68, Train Loss: 0.8022, Test Loss: 1.4282\n",
      "Epoch 1/65, Train Loss: 1.3565, Test Loss: 1.1019\n",
      "Epoch 2/65, Train Loss: 1.3053, Test Loss: 1.0977\n",
      "Epoch 3/65, Train Loss: 1.2648, Test Loss: 1.0945\n",
      "Epoch 4/65, Train Loss: 1.2011, Test Loss: 1.0895\n",
      "Epoch 5/65, Train Loss: 1.1991, Test Loss: 1.0852\n",
      "Epoch 6/65, Train Loss: 1.2021, Test Loss: 1.0833\n",
      "Epoch 7/65, Train Loss: 1.1532, Test Loss: 1.0835\n",
      "Epoch 8/65, Train Loss: 1.1442, Test Loss: 1.0822\n",
      "Epoch 9/65, Train Loss: 1.1690, Test Loss: 1.0850\n",
      "Epoch 10/65, Train Loss: 1.1271, Test Loss: 1.0895\n",
      "Epoch 11/65, Train Loss: 1.1194, Test Loss: 1.0922\n",
      "Epoch 12/65, Train Loss: 1.1616, Test Loss: 1.0973\n",
      "Epoch 13/65, Train Loss: 1.1232, Test Loss: 1.1011\n",
      "Epoch 14/65, Train Loss: 1.0657, Test Loss: 1.1042\n",
      "Epoch 15/65, Train Loss: 1.1265, Test Loss: 1.1071\n",
      "Epoch 16/65, Train Loss: 1.1057, Test Loss: 1.1087\n",
      "Epoch 17/65, Train Loss: 1.1326, Test Loss: 1.1086\n",
      "Epoch 18/65, Train Loss: 1.1012, Test Loss: 1.1053\n",
      "Epoch 19/65, Train Loss: 1.0856, Test Loss: 1.1108\n",
      "Epoch 20/65, Train Loss: 1.1008, Test Loss: 1.1140\n",
      "Epoch 21/65, Train Loss: 1.0951, Test Loss: 1.1139\n",
      "Epoch 22/65, Train Loss: 1.1051, Test Loss: 1.1172\n",
      "Epoch 23/65, Train Loss: 1.1010, Test Loss: 1.1146\n",
      "Epoch 24/65, Train Loss: 1.1284, Test Loss: 1.1165\n",
      "Epoch 25/65, Train Loss: 1.1594, Test Loss: 1.1156\n",
      "Epoch 26/65, Train Loss: 1.0938, Test Loss: 1.1180\n",
      "Epoch 27/65, Train Loss: 1.1180, Test Loss: 1.1166\n",
      "Epoch 28/65, Train Loss: 1.1089, Test Loss: 1.1135\n",
      "Epoch 29/65, Train Loss: 1.0899, Test Loss: 1.1097\n",
      "Epoch 30/65, Train Loss: 1.1193, Test Loss: 1.1066\n",
      "Epoch 31/65, Train Loss: 1.0720, Test Loss: 1.1073\n",
      "Epoch 32/65, Train Loss: 1.0885, Test Loss: 1.1071\n",
      "Epoch 33/65, Train Loss: 1.0765, Test Loss: 1.1041\n",
      "Epoch 34/65, Train Loss: 1.1244, Test Loss: 1.1022\n",
      "Epoch 35/65, Train Loss: 1.0718, Test Loss: 1.1039\n",
      "Epoch 36/65, Train Loss: 1.0943, Test Loss: 1.1066\n",
      "Epoch 37/65, Train Loss: 1.1006, Test Loss: 1.1080\n",
      "Epoch 38/65, Train Loss: 1.0885, Test Loss: 1.1064\n",
      "Epoch 39/65, Train Loss: 1.0805, Test Loss: 1.1055\n",
      "Epoch 40/65, Train Loss: 1.0852, Test Loss: 1.1014\n",
      "Epoch 41/65, Train Loss: 1.0629, Test Loss: 1.0991\n",
      "Epoch 42/65, Train Loss: 1.0988, Test Loss: 1.0976\n",
      "Epoch 43/65, Train Loss: 1.0777, Test Loss: 1.0956\n",
      "Epoch 44/65, Train Loss: 1.0805, Test Loss: 1.0937\n",
      "Epoch 45/65, Train Loss: 1.0418, Test Loss: 1.0957\n",
      "Epoch 46/65, Train Loss: 1.1012, Test Loss: 1.0946\n",
      "Epoch 47/65, Train Loss: 1.0605, Test Loss: 1.0927\n",
      "Epoch 48/65, Train Loss: 1.0891, Test Loss: 1.0923\n",
      "Epoch 49/65, Train Loss: 1.0893, Test Loss: 1.0927\n",
      "Epoch 50/65, Train Loss: 1.0715, Test Loss: 1.0930\n",
      "Epoch 51/65, Train Loss: 1.0860, Test Loss: 1.0956\n",
      "Epoch 52/65, Train Loss: 1.0618, Test Loss: 1.0983\n",
      "Epoch 53/65, Train Loss: 1.0491, Test Loss: 1.0995\n",
      "Epoch 54/65, Train Loss: 1.0793, Test Loss: 1.1011\n",
      "Epoch 55/65, Train Loss: 1.0485, Test Loss: 1.1013\n",
      "Epoch 56/65, Train Loss: 1.0848, Test Loss: 1.0996\n",
      "Epoch 57/65, Train Loss: 1.0924, Test Loss: 1.0980\n",
      "Epoch 58/65, Train Loss: 1.0763, Test Loss: 1.0966\n",
      "Epoch 59/65, Train Loss: 1.0664, Test Loss: 1.0968\n",
      "Epoch 60/65, Train Loss: 1.0628, Test Loss: 1.0980\n",
      "Epoch 61/65, Train Loss: 1.0648, Test Loss: 1.0985\n",
      "Epoch 62/65, Train Loss: 1.0536, Test Loss: 1.0987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:57:50,104] Trial 103 finished with value: 1.099324107170105 and parameters: {'num_hidden_layers': 16, 'layer_0_size': 176, 'layer_1_size': 175, 'layer_2_size': 116, 'layer_3_size': 116, 'layer_4_size': 97, 'layer_5_size': 96, 'layer_6_size': 188, 'layer_7_size': 256, 'layer_8_size': 228, 'layer_9_size': 209, 'layer_10_size': 256, 'layer_11_size': 124, 'layer_12_size': 155, 'layer_13_size': 109, 'layer_14_size': 69, 'layer_15_size': 88, 'dropout_rate': 0.2768902293570761, 'learning_rate': 0.0003341633174175847, 'batch_size': 256, 'epochs': 65}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/65, Train Loss: 1.0676, Test Loss: 1.1000\n",
      "Epoch 64/65, Train Loss: 1.0407, Test Loss: 1.0990\n",
      "Epoch 65/65, Train Loss: 1.0716, Test Loss: 1.0993\n",
      "Epoch 1/63, Train Loss: 1.3357, Test Loss: 0.8325\n",
      "Epoch 2/63, Train Loss: 1.2618, Test Loss: 0.8372\n",
      "Epoch 3/63, Train Loss: 1.2493, Test Loss: 0.8424\n",
      "Epoch 4/63, Train Loss: 1.1734, Test Loss: 0.8480\n",
      "Epoch 5/63, Train Loss: 1.2492, Test Loss: 0.8532\n",
      "Epoch 6/63, Train Loss: 1.1825, Test Loss: 0.8598\n",
      "Epoch 7/63, Train Loss: 1.1800, Test Loss: 0.8678\n",
      "Epoch 8/63, Train Loss: 1.2099, Test Loss: 0.8697\n",
      "Epoch 9/63, Train Loss: 1.2522, Test Loss: 0.8696\n",
      "Epoch 10/63, Train Loss: 1.1921, Test Loss: 0.8735\n",
      "Epoch 11/63, Train Loss: 1.1610, Test Loss: 0.8699\n",
      "Epoch 12/63, Train Loss: 1.1866, Test Loss: 0.8626\n",
      "Epoch 13/63, Train Loss: 1.1843, Test Loss: 0.8569\n",
      "Epoch 14/63, Train Loss: 1.2239, Test Loss: 0.8524\n",
      "Epoch 15/63, Train Loss: 1.1755, Test Loss: 0.8496\n",
      "Epoch 16/63, Train Loss: 1.1707, Test Loss: 0.8477\n",
      "Epoch 17/63, Train Loss: 1.2013, Test Loss: 0.8458\n",
      "Epoch 18/63, Train Loss: 1.1619, Test Loss: 0.8444\n",
      "Epoch 19/63, Train Loss: 1.1647, Test Loss: 0.8426\n",
      "Epoch 20/63, Train Loss: 1.1715, Test Loss: 0.8431\n",
      "Epoch 21/63, Train Loss: 1.1080, Test Loss: 0.8433\n",
      "Epoch 22/63, Train Loss: 1.1384, Test Loss: 0.8415\n",
      "Epoch 23/63, Train Loss: 1.1264, Test Loss: 0.8413\n",
      "Epoch 24/63, Train Loss: 1.1316, Test Loss: 0.8412\n",
      "Epoch 25/63, Train Loss: 1.1261, Test Loss: 0.8405\n",
      "Epoch 26/63, Train Loss: 1.1278, Test Loss: 0.8417\n",
      "Epoch 27/63, Train Loss: 1.1546, Test Loss: 0.8422\n",
      "Epoch 28/63, Train Loss: 1.1439, Test Loss: 0.8442\n",
      "Epoch 29/63, Train Loss: 1.1302, Test Loss: 0.8459\n",
      "Epoch 30/63, Train Loss: 1.1264, Test Loss: 0.8447\n",
      "Epoch 31/63, Train Loss: 1.1872, Test Loss: 0.8447\n",
      "Epoch 32/63, Train Loss: 1.1192, Test Loss: 0.8441\n",
      "Epoch 33/63, Train Loss: 1.1223, Test Loss: 0.8426\n",
      "Epoch 34/63, Train Loss: 1.1607, Test Loss: 0.8434\n",
      "Epoch 35/63, Train Loss: 1.1773, Test Loss: 0.8423\n",
      "Epoch 36/63, Train Loss: 1.1570, Test Loss: 0.8398\n",
      "Epoch 37/63, Train Loss: 1.1155, Test Loss: 0.8361\n",
      "Epoch 38/63, Train Loss: 1.1299, Test Loss: 0.8348\n",
      "Epoch 39/63, Train Loss: 1.1382, Test Loss: 0.8349\n",
      "Epoch 40/63, Train Loss: 1.1073, Test Loss: 0.8340\n",
      "Epoch 41/63, Train Loss: 1.1167, Test Loss: 0.8348\n",
      "Epoch 42/63, Train Loss: 1.1454, Test Loss: 0.8345\n",
      "Epoch 43/63, Train Loss: 1.1298, Test Loss: 0.8344\n",
      "Epoch 44/63, Train Loss: 1.0965, Test Loss: 0.8340\n",
      "Epoch 45/63, Train Loss: 1.1375, Test Loss: 0.8354\n",
      "Epoch 46/63, Train Loss: 1.1166, Test Loss: 0.8359\n",
      "Epoch 47/63, Train Loss: 1.1392, Test Loss: 0.8372\n",
      "Epoch 48/63, Train Loss: 1.1149, Test Loss: 0.8386\n",
      "Epoch 49/63, Train Loss: 1.1242, Test Loss: 0.8401\n",
      "Epoch 50/63, Train Loss: 1.1296, Test Loss: 0.8406\n",
      "Epoch 51/63, Train Loss: 1.1245, Test Loss: 0.8416\n",
      "Epoch 52/63, Train Loss: 1.1403, Test Loss: 0.8433\n",
      "Epoch 53/63, Train Loss: 1.1364, Test Loss: 0.8433\n",
      "Epoch 54/63, Train Loss: 1.1713, Test Loss: 0.8420\n",
      "Epoch 55/63, Train Loss: 1.1114, Test Loss: 0.8415\n",
      "Epoch 56/63, Train Loss: 1.1220, Test Loss: 0.8414\n",
      "Epoch 57/63, Train Loss: 1.1439, Test Loss: 0.8413\n",
      "Epoch 58/63, Train Loss: 1.1331, Test Loss: 0.8391\n",
      "Epoch 59/63, Train Loss: 1.1156, Test Loss: 0.8383\n",
      "Epoch 60/63, Train Loss: 1.1470, Test Loss: 0.8372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:57:54,049] Trial 104 finished with value: 0.835590124130249 and parameters: {'num_hidden_layers': 12, 'layer_0_size': 220, 'layer_1_size': 34, 'layer_2_size': 234, 'layer_3_size': 146, 'layer_4_size': 170, 'layer_5_size': 125, 'layer_6_size': 240, 'layer_7_size': 149, 'layer_8_size': 133, 'layer_9_size': 97, 'layer_10_size': 237, 'layer_11_size': 136, 'dropout_rate': 0.3035088165003762, 'learning_rate': 0.00015654119072195418, 'batch_size': 256, 'epochs': 63}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/63, Train Loss: 1.0969, Test Loss: 0.8364\n",
      "Epoch 62/63, Train Loss: 1.1011, Test Loss: 0.8359\n",
      "Epoch 63/63, Train Loss: 1.0988, Test Loss: 0.8356\n",
      "Epoch 1/73, Train Loss: 1.5122, Test Loss: 0.9677\n",
      "Epoch 2/73, Train Loss: 1.4187, Test Loss: 0.9675\n",
      "Epoch 3/73, Train Loss: 1.4002, Test Loss: 0.9706\n",
      "Epoch 4/73, Train Loss: 1.4197, Test Loss: 0.9745\n",
      "Epoch 5/73, Train Loss: 1.3488, Test Loss: 0.9819\n",
      "Epoch 6/73, Train Loss: 1.3088, Test Loss: 0.9929\n",
      "Epoch 7/73, Train Loss: 1.2952, Test Loss: 0.9985\n",
      "Epoch 8/73, Train Loss: 1.2571, Test Loss: 1.0063\n",
      "Epoch 9/73, Train Loss: 1.3146, Test Loss: 1.0081\n",
      "Epoch 10/73, Train Loss: 1.2852, Test Loss: 1.0058\n",
      "Epoch 11/73, Train Loss: 1.2437, Test Loss: 1.0024\n",
      "Epoch 12/73, Train Loss: 1.2108, Test Loss: 0.9958\n",
      "Epoch 13/73, Train Loss: 1.1860, Test Loss: 0.9903\n",
      "Epoch 14/73, Train Loss: 1.1987, Test Loss: 0.9821\n",
      "Epoch 15/73, Train Loss: 1.1695, Test Loss: 0.9766\n",
      "Epoch 16/73, Train Loss: 1.1626, Test Loss: 0.9735\n",
      "Epoch 17/73, Train Loss: 1.1609, Test Loss: 0.9714\n",
      "Epoch 18/73, Train Loss: 1.1484, Test Loss: 0.9700\n",
      "Epoch 19/73, Train Loss: 1.1634, Test Loss: 0.9703\n",
      "Epoch 20/73, Train Loss: 1.1449, Test Loss: 0.9696\n",
      "Epoch 21/73, Train Loss: 1.1071, Test Loss: 0.9694\n",
      "Epoch 22/73, Train Loss: 1.1041, Test Loss: 0.9731\n",
      "Epoch 23/73, Train Loss: 1.1122, Test Loss: 0.9731\n",
      "Epoch 24/73, Train Loss: 1.0837, Test Loss: 0.9732\n",
      "Epoch 25/73, Train Loss: 1.1325, Test Loss: 0.9737\n",
      "Epoch 26/73, Train Loss: 1.1067, Test Loss: 0.9749\n",
      "Epoch 27/73, Train Loss: 1.1086, Test Loss: 0.9742\n",
      "Epoch 28/73, Train Loss: 1.1073, Test Loss: 0.9741\n",
      "Epoch 29/73, Train Loss: 1.1124, Test Loss: 0.9758\n",
      "Epoch 30/73, Train Loss: 1.1202, Test Loss: 0.9751\n",
      "Epoch 31/73, Train Loss: 1.0856, Test Loss: 0.9761\n",
      "Epoch 32/73, Train Loss: 1.1016, Test Loss: 0.9760\n",
      "Epoch 33/73, Train Loss: 1.1211, Test Loss: 0.9782\n",
      "Epoch 34/73, Train Loss: 1.0950, Test Loss: 0.9779\n",
      "Epoch 35/73, Train Loss: 1.1072, Test Loss: 0.9797\n",
      "Epoch 36/73, Train Loss: 1.0989, Test Loss: 0.9811\n",
      "Epoch 37/73, Train Loss: 1.0852, Test Loss: 0.9830\n",
      "Epoch 38/73, Train Loss: 1.0921, Test Loss: 0.9823\n",
      "Epoch 39/73, Train Loss: 1.1236, Test Loss: 0.9826\n",
      "Epoch 40/73, Train Loss: 1.0715, Test Loss: 0.9809\n",
      "Epoch 41/73, Train Loss: 1.1145, Test Loss: 0.9820\n",
      "Epoch 42/73, Train Loss: 1.1067, Test Loss: 0.9841\n",
      "Epoch 43/73, Train Loss: 1.0987, Test Loss: 0.9870\n",
      "Epoch 44/73, Train Loss: 1.0942, Test Loss: 0.9890\n",
      "Epoch 45/73, Train Loss: 1.1029, Test Loss: 0.9916\n",
      "Epoch 46/73, Train Loss: 1.1138, Test Loss: 0.9949\n",
      "Epoch 47/73, Train Loss: 1.0725, Test Loss: 0.9975\n",
      "Epoch 48/73, Train Loss: 1.0985, Test Loss: 0.9992\n",
      "Epoch 49/73, Train Loss: 1.0769, Test Loss: 0.9979\n",
      "Epoch 50/73, Train Loss: 1.0910, Test Loss: 1.0015\n",
      "Epoch 51/73, Train Loss: 1.0965, Test Loss: 1.0008\n",
      "Epoch 52/73, Train Loss: 1.1062, Test Loss: 1.0000\n",
      "Epoch 53/73, Train Loss: 1.1088, Test Loss: 0.9995\n",
      "Epoch 54/73, Train Loss: 1.0740, Test Loss: 0.9975\n",
      "Epoch 55/73, Train Loss: 1.0808, Test Loss: 0.9983\n",
      "Epoch 56/73, Train Loss: 1.1283, Test Loss: 0.9998\n",
      "Epoch 57/73, Train Loss: 1.0884, Test Loss: 1.0014\n",
      "Epoch 58/73, Train Loss: 1.1072, Test Loss: 1.0030\n",
      "Epoch 59/73, Train Loss: 1.1047, Test Loss: 1.0033\n",
      "Epoch 60/73, Train Loss: 1.0746, Test Loss: 1.0036\n",
      "Epoch 61/73, Train Loss: 1.0826, Test Loss: 1.0040\n",
      "Epoch 62/73, Train Loss: 1.0701, Test Loss: 1.0034\n",
      "Epoch 63/73, Train Loss: 1.0859, Test Loss: 1.0046\n",
      "Epoch 64/73, Train Loss: 1.0756, Test Loss: 1.0075\n",
      "Epoch 65/73, Train Loss: 1.0995, Test Loss: 1.0072\n",
      "Epoch 66/73, Train Loss: 1.0947, Test Loss: 1.0081\n",
      "Epoch 67/73, Train Loss: 1.0810, Test Loss: 1.0081\n",
      "Epoch 68/73, Train Loss: 1.0919, Test Loss: 1.0078\n",
      "Epoch 69/73, Train Loss: 1.0857, Test Loss: 1.0084\n",
      "Epoch 70/73, Train Loss: 1.0681, Test Loss: 1.0069\n",
      "Epoch 71/73, Train Loss: 1.0768, Test Loss: 1.0091\n",
      "Epoch 72/73, Train Loss: 1.0823, Test Loss: 1.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:57:58,156] Trial 105 finished with value: 1.0094408988952637 and parameters: {'num_hidden_layers': 12, 'layer_0_size': 225, 'layer_1_size': 47, 'layer_2_size': 124, 'layer_3_size': 81, 'layer_4_size': 147, 'layer_5_size': 126, 'layer_6_size': 236, 'layer_7_size': 180, 'layer_8_size': 123, 'layer_9_size': 234, 'layer_10_size': 45, 'layer_11_size': 107, 'dropout_rate': 0.1735802881947425, 'learning_rate': 0.00016399848170727262, 'batch_size': 256, 'epochs': 73}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/73, Train Loss: 1.0858, Test Loss: 1.0094\n",
      "Epoch 1/62, Train Loss: 1.1403, Test Loss: 0.9212\n",
      "Epoch 2/62, Train Loss: 1.1092, Test Loss: 0.9206\n",
      "Epoch 3/62, Train Loss: 1.1113, Test Loss: 0.9191\n",
      "Epoch 4/62, Train Loss: 1.1042, Test Loss: 0.9183\n",
      "Epoch 5/62, Train Loss: 1.0930, Test Loss: 0.9202\n",
      "Epoch 6/62, Train Loss: 1.0701, Test Loss: 0.9249\n",
      "Epoch 7/62, Train Loss: 1.0474, Test Loss: 0.9325\n",
      "Epoch 8/62, Train Loss: 1.0671, Test Loss: 0.9368\n",
      "Epoch 9/62, Train Loss: 1.0631, Test Loss: 0.9317\n",
      "Epoch 10/62, Train Loss: 1.0451, Test Loss: 0.9237\n",
      "Epoch 11/62, Train Loss: 1.0596, Test Loss: 0.9192\n",
      "Epoch 12/62, Train Loss: 1.0305, Test Loss: 0.9182\n",
      "Epoch 13/62, Train Loss: 1.0762, Test Loss: 0.9216\n",
      "Epoch 14/62, Train Loss: 1.0123, Test Loss: 0.9283\n",
      "Epoch 15/62, Train Loss: 1.0420, Test Loss: 0.9435\n",
      "Epoch 16/62, Train Loss: 1.0404, Test Loss: 0.9548\n",
      "Epoch 17/62, Train Loss: 1.0478, Test Loss: 0.9625\n",
      "Epoch 18/62, Train Loss: 1.0490, Test Loss: 0.9602\n",
      "Epoch 19/62, Train Loss: 1.0376, Test Loss: 0.9617\n",
      "Epoch 20/62, Train Loss: 1.0205, Test Loss: 0.9665\n",
      "Epoch 21/62, Train Loss: 1.0035, Test Loss: 0.9723\n",
      "Epoch 22/62, Train Loss: 1.0436, Test Loss: 0.9865\n",
      "Epoch 23/62, Train Loss: 1.0089, Test Loss: 0.9894\n",
      "Epoch 24/62, Train Loss: 0.9889, Test Loss: 1.0075\n",
      "Epoch 25/62, Train Loss: 1.0164, Test Loss: 1.0154\n",
      "Epoch 26/62, Train Loss: 1.0044, Test Loss: 1.0172\n",
      "Epoch 27/62, Train Loss: 1.0042, Test Loss: 1.0206\n",
      "Epoch 28/62, Train Loss: 0.9839, Test Loss: 1.0222\n",
      "Epoch 29/62, Train Loss: 0.9711, Test Loss: 1.0230\n",
      "Epoch 30/62, Train Loss: 0.9560, Test Loss: 1.0396\n",
      "Epoch 31/62, Train Loss: 0.9855, Test Loss: 1.0535\n",
      "Epoch 32/62, Train Loss: 0.9535, Test Loss: 1.0769\n",
      "Epoch 33/62, Train Loss: 0.9706, Test Loss: 1.1018\n",
      "Epoch 34/62, Train Loss: 0.9229, Test Loss: 1.1159\n",
      "Epoch 35/62, Train Loss: 0.9156, Test Loss: 1.1421\n",
      "Epoch 36/62, Train Loss: 0.9172, Test Loss: 1.1710\n",
      "Epoch 37/62, Train Loss: 0.8973, Test Loss: 1.2016\n",
      "Epoch 38/62, Train Loss: 0.8895, Test Loss: 1.2048\n",
      "Epoch 39/62, Train Loss: 0.8635, Test Loss: 1.2369\n",
      "Epoch 40/62, Train Loss: 0.8594, Test Loss: 1.2730\n",
      "Epoch 41/62, Train Loss: 0.8367, Test Loss: 1.2946\n",
      "Epoch 42/62, Train Loss: 0.8438, Test Loss: 1.3058\n",
      "Epoch 43/62, Train Loss: 0.8277, Test Loss: 1.3039\n",
      "Epoch 44/62, Train Loss: 0.7743, Test Loss: 1.3195\n",
      "Epoch 45/62, Train Loss: 0.8021, Test Loss: 1.3554\n",
      "Epoch 46/62, Train Loss: 0.7227, Test Loss: 1.3978\n",
      "Epoch 47/62, Train Loss: 0.7275, Test Loss: 1.4548\n",
      "Epoch 48/62, Train Loss: 0.7471, Test Loss: 1.5068\n",
      "Epoch 49/62, Train Loss: 0.6897, Test Loss: 1.5337\n",
      "Epoch 50/62, Train Loss: 0.7032, Test Loss: 1.5398\n",
      "Epoch 51/62, Train Loss: 0.7250, Test Loss: 1.5416\n",
      "Epoch 52/62, Train Loss: 0.6682, Test Loss: 1.5277\n",
      "Epoch 53/62, Train Loss: 0.6258, Test Loss: 1.5481\n",
      "Epoch 54/62, Train Loss: 0.6688, Test Loss: 1.5700\n",
      "Epoch 55/62, Train Loss: 0.6554, Test Loss: 1.5778\n",
      "Epoch 56/62, Train Loss: 0.6107, Test Loss: 1.6328\n",
      "Epoch 57/62, Train Loss: 0.6530, Test Loss: 1.6174\n",
      "Epoch 58/62, Train Loss: 0.6129, Test Loss: 1.5505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:58:01,115] Trial 106 finished with value: 1.829015851020813 and parameters: {'num_hidden_layers': 9, 'layer_0_size': 198, 'layer_1_size': 51, 'layer_2_size': 231, 'layer_3_size': 100, 'layer_4_size': 158, 'layer_5_size': 121, 'layer_6_size': 246, 'layer_7_size': 148, 'layer_8_size': 136, 'dropout_rate': 0.2430737672602841, 'learning_rate': 0.001068022476049145, 'batch_size': 256, 'epochs': 62}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/62, Train Loss: 0.5962, Test Loss: 1.5853\n",
      "Epoch 60/62, Train Loss: 0.5584, Test Loss: 1.6575\n",
      "Epoch 61/62, Train Loss: 0.5884, Test Loss: 1.7703\n",
      "Epoch 62/62, Train Loss: 0.5552, Test Loss: 1.8290\n",
      "Epoch 1/71, Train Loss: 1.1760, Test Loss: 1.1926\n",
      "Epoch 2/71, Train Loss: 1.1419, Test Loss: 1.1924\n",
      "Epoch 3/71, Train Loss: 1.1181, Test Loss: 1.1923\n",
      "Epoch 4/71, Train Loss: 1.1030, Test Loss: 1.1927\n",
      "Epoch 5/71, Train Loss: 1.1325, Test Loss: 1.1941\n",
      "Epoch 6/71, Train Loss: 1.1233, Test Loss: 1.1949\n",
      "Epoch 7/71, Train Loss: 1.1230, Test Loss: 1.1945\n",
      "Epoch 8/71, Train Loss: 1.0970, Test Loss: 1.1944\n",
      "Epoch 9/71, Train Loss: 1.1010, Test Loss: 1.1933\n",
      "Epoch 10/71, Train Loss: 1.0709, Test Loss: 1.1935\n",
      "Epoch 11/71, Train Loss: 1.1041, Test Loss: 1.1957\n",
      "Epoch 12/71, Train Loss: 1.0828, Test Loss: 1.1954\n",
      "Epoch 13/71, Train Loss: 1.1170, Test Loss: 1.1997\n",
      "Epoch 14/71, Train Loss: 1.1056, Test Loss: 1.2024\n",
      "Epoch 15/71, Train Loss: 1.0960, Test Loss: 1.2064\n",
      "Epoch 16/71, Train Loss: 1.0636, Test Loss: 1.2168\n",
      "Epoch 17/71, Train Loss: 1.0359, Test Loss: 1.2281\n",
      "Epoch 18/71, Train Loss: 1.0909, Test Loss: 1.2267\n",
      "Epoch 19/71, Train Loss: 1.0592, Test Loss: 1.2197\n",
      "Epoch 20/71, Train Loss: 1.0433, Test Loss: 1.2185\n",
      "Epoch 21/71, Train Loss: 1.0656, Test Loss: 1.2236\n",
      "Epoch 22/71, Train Loss: 1.0515, Test Loss: 1.2264\n",
      "Epoch 23/71, Train Loss: 1.0674, Test Loss: 1.2244\n",
      "Epoch 24/71, Train Loss: 1.0805, Test Loss: 1.2198\n",
      "Epoch 25/71, Train Loss: 1.0569, Test Loss: 1.2103\n",
      "Epoch 26/71, Train Loss: 1.0513, Test Loss: 1.2060\n",
      "Epoch 27/71, Train Loss: 1.0430, Test Loss: 1.2020\n",
      "Epoch 28/71, Train Loss: 1.0733, Test Loss: 1.2013\n",
      "Epoch 29/71, Train Loss: 1.0648, Test Loss: 1.2053\n",
      "Epoch 30/71, Train Loss: 1.0666, Test Loss: 1.2135\n",
      "Epoch 31/71, Train Loss: 1.0677, Test Loss: 1.2169\n",
      "Epoch 32/71, Train Loss: 1.0391, Test Loss: 1.2209\n",
      "Epoch 33/71, Train Loss: 1.0641, Test Loss: 1.2179\n",
      "Epoch 34/71, Train Loss: 1.0634, Test Loss: 1.2083\n",
      "Epoch 35/71, Train Loss: 1.0508, Test Loss: 1.2022\n",
      "Epoch 36/71, Train Loss: 1.0577, Test Loss: 1.1978\n",
      "Epoch 37/71, Train Loss: 1.0387, Test Loss: 1.1947\n",
      "Epoch 38/71, Train Loss: 1.0462, Test Loss: 1.1962\n",
      "Epoch 39/71, Train Loss: 1.0344, Test Loss: 1.2001\n",
      "Epoch 40/71, Train Loss: 1.0310, Test Loss: 1.2057\n",
      "Epoch 41/71, Train Loss: 1.0387, Test Loss: 1.2122\n",
      "Epoch 42/71, Train Loss: 1.0368, Test Loss: 1.2121\n",
      "Epoch 43/71, Train Loss: 1.0634, Test Loss: 1.2141\n",
      "Epoch 44/71, Train Loss: 1.0522, Test Loss: 1.2123\n",
      "Epoch 45/71, Train Loss: 1.0220, Test Loss: 1.2136\n",
      "Epoch 46/71, Train Loss: 1.0604, Test Loss: 1.2139\n",
      "Epoch 47/71, Train Loss: 1.0242, Test Loss: 1.2209\n",
      "Epoch 48/71, Train Loss: 1.0526, Test Loss: 1.2242\n",
      "Epoch 49/71, Train Loss: 1.0363, Test Loss: 1.2286\n",
      "Epoch 50/71, Train Loss: 1.0302, Test Loss: 1.2268\n",
      "Epoch 51/71, Train Loss: 1.0782, Test Loss: 1.2211\n",
      "Epoch 52/71, Train Loss: 1.0337, Test Loss: 1.2193\n",
      "Epoch 53/71, Train Loss: 1.0464, Test Loss: 1.2273\n",
      "Epoch 54/71, Train Loss: 1.0390, Test Loss: 1.2334\n",
      "Epoch 55/71, Train Loss: 1.0288, Test Loss: 1.2387\n",
      "Epoch 56/71, Train Loss: 1.0370, Test Loss: 1.2386\n",
      "Epoch 57/71, Train Loss: 1.0464, Test Loss: 1.2356\n",
      "Epoch 58/71, Train Loss: 1.0186, Test Loss: 1.2313\n",
      "Epoch 59/71, Train Loss: 1.0212, Test Loss: 1.2255\n",
      "Epoch 60/71, Train Loss: 1.0405, Test Loss: 1.2224\n",
      "Epoch 61/71, Train Loss: 1.0354, Test Loss: 1.2243\n",
      "Epoch 62/71, Train Loss: 1.0382, Test Loss: 1.2242\n",
      "Epoch 63/71, Train Loss: 1.0235, Test Loss: 1.2269\n",
      "Epoch 64/71, Train Loss: 1.0271, Test Loss: 1.2328\n",
      "Epoch 65/71, Train Loss: 1.0258, Test Loss: 1.2364\n",
      "Epoch 66/71, Train Loss: 1.0042, Test Loss: 1.2427\n",
      "Epoch 67/71, Train Loss: 1.0241, Test Loss: 1.2507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:58:05,250] Trial 107 finished with value: 1.280476689338684 and parameters: {'num_hidden_layers': 11, 'layer_0_size': 209, 'layer_1_size': 38, 'layer_2_size': 196, 'layer_3_size': 145, 'layer_4_size': 137, 'layer_5_size': 134, 'layer_6_size': 222, 'layer_7_size': 156, 'layer_8_size': 105, 'layer_9_size': 244, 'layer_10_size': 240, 'dropout_rate': 0.26257611549014054, 'learning_rate': 0.0005443448190433747, 'batch_size': 256, 'epochs': 71}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/71, Train Loss: 0.9983, Test Loss: 1.2631\n",
      "Epoch 69/71, Train Loss: 1.0038, Test Loss: 1.2716\n",
      "Epoch 70/71, Train Loss: 1.0163, Test Loss: 1.2800\n",
      "Epoch 71/71, Train Loss: 1.0178, Test Loss: 1.2805\n",
      "Epoch 1/57, Train Loss: 1.0947, Test Loss: 0.9961\n",
      "Epoch 2/57, Train Loss: 1.0978, Test Loss: 0.9928\n",
      "Epoch 3/57, Train Loss: 1.0956, Test Loss: 0.9879\n",
      "Epoch 4/57, Train Loss: 1.0719, Test Loss: 0.9856\n",
      "Epoch 5/57, Train Loss: 1.0231, Test Loss: 0.9860\n",
      "Epoch 6/57, Train Loss: 0.9970, Test Loss: 0.9891\n",
      "Epoch 7/57, Train Loss: 1.0284, Test Loss: 0.9958\n",
      "Epoch 8/57, Train Loss: 1.0353, Test Loss: 1.0066\n",
      "Epoch 9/57, Train Loss: 1.0317, Test Loss: 1.0117\n",
      "Epoch 10/57, Train Loss: 1.0510, Test Loss: 1.0137\n",
      "Epoch 11/57, Train Loss: 1.0154, Test Loss: 1.0206\n",
      "Epoch 12/57, Train Loss: 1.0388, Test Loss: 1.0218\n",
      "Epoch 13/57, Train Loss: 1.0531, Test Loss: 1.0182\n",
      "Epoch 14/57, Train Loss: 1.0519, Test Loss: 1.0150\n",
      "Epoch 15/57, Train Loss: 1.0276, Test Loss: 1.0115\n",
      "Epoch 16/57, Train Loss: 1.0341, Test Loss: 1.0048\n",
      "Epoch 17/57, Train Loss: 1.0295, Test Loss: 1.0053\n",
      "Epoch 18/57, Train Loss: 1.0309, Test Loss: 1.0065\n",
      "Epoch 19/57, Train Loss: 1.0138, Test Loss: 1.0079\n",
      "Epoch 20/57, Train Loss: 1.0091, Test Loss: 1.0140\n",
      "Epoch 21/57, Train Loss: 0.9949, Test Loss: 1.0154\n",
      "Epoch 22/57, Train Loss: 1.0147, Test Loss: 1.0153\n",
      "Epoch 23/57, Train Loss: 1.0070, Test Loss: 1.0183\n",
      "Epoch 24/57, Train Loss: 1.0186, Test Loss: 1.0203\n",
      "Epoch 25/57, Train Loss: 1.0119, Test Loss: 1.0166\n",
      "Epoch 26/57, Train Loss: 0.9881, Test Loss: 1.0132\n",
      "Epoch 27/57, Train Loss: 1.0195, Test Loss: 1.0106\n",
      "Epoch 28/57, Train Loss: 0.9951, Test Loss: 1.0141\n",
      "Epoch 29/57, Train Loss: 0.9853, Test Loss: 1.0195\n",
      "Epoch 30/57, Train Loss: 0.9910, Test Loss: 1.0212\n",
      "Epoch 31/57, Train Loss: 1.0209, Test Loss: 1.0190\n",
      "Epoch 32/57, Train Loss: 1.0139, Test Loss: 1.0173\n",
      "Epoch 33/57, Train Loss: 1.0127, Test Loss: 1.0186\n",
      "Epoch 34/57, Train Loss: 1.0005, Test Loss: 1.0226\n",
      "Epoch 35/57, Train Loss: 0.9535, Test Loss: 1.0264\n",
      "Epoch 36/57, Train Loss: 1.0018, Test Loss: 1.0267\n",
      "Epoch 37/57, Train Loss: 0.9828, Test Loss: 1.0228\n",
      "Epoch 38/57, Train Loss: 0.9924, Test Loss: 1.0199\n",
      "Epoch 39/57, Train Loss: 0.9966, Test Loss: 1.0168\n",
      "Epoch 40/57, Train Loss: 1.0065, Test Loss: 1.0119\n",
      "Epoch 41/57, Train Loss: 0.9917, Test Loss: 1.0112\n",
      "Epoch 42/57, Train Loss: 1.0130, Test Loss: 1.0139\n",
      "Epoch 43/57, Train Loss: 0.9843, Test Loss: 1.0219\n",
      "Epoch 44/57, Train Loss: 0.9892, Test Loss: 1.0270\n",
      "Epoch 45/57, Train Loss: 0.9906, Test Loss: 1.0266\n",
      "Epoch 46/57, Train Loss: 0.9920, Test Loss: 1.0265\n",
      "Epoch 47/57, Train Loss: 0.9764, Test Loss: 1.0287\n",
      "Epoch 48/57, Train Loss: 0.9959, Test Loss: 1.0297\n",
      "Epoch 49/57, Train Loss: 1.0070, Test Loss: 1.0341\n",
      "Epoch 50/57, Train Loss: 0.9733, Test Loss: 1.0383\n",
      "Epoch 51/57, Train Loss: 0.9909, Test Loss: 1.0374\n",
      "Epoch 52/57, Train Loss: 0.9801, Test Loss: 1.0369\n",
      "Epoch 53/57, Train Loss: 0.9924, Test Loss: 1.0451\n",
      "Epoch 54/57, Train Loss: 0.9553, Test Loss: 1.0462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:58:08,754] Trial 108 finished with value: 1.0427632331848145 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 239, 'layer_1_size': 42, 'layer_2_size': 240, 'layer_3_size': 128, 'layer_4_size': 166, 'layer_5_size': 108, 'layer_6_size': 111, 'layer_7_size': 171, 'layer_8_size': 113, 'layer_9_size': 114, 'layer_10_size': 146, 'layer_11_size': 137, 'layer_12_size': 209, 'dropout_rate': 0.11448467529780729, 'learning_rate': 0.00038926461367105636, 'batch_size': 256, 'epochs': 57}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/57, Train Loss: 0.9727, Test Loss: 1.0532\n",
      "Epoch 56/57, Train Loss: 0.9681, Test Loss: 1.0477\n",
      "Epoch 57/57, Train Loss: 0.9810, Test Loss: 1.0428\n",
      "Epoch 1/61, Train Loss: 1.0321, Test Loss: 0.9134\n",
      "Epoch 2/61, Train Loss: 1.0213, Test Loss: 0.8947\n",
      "Epoch 3/61, Train Loss: 1.0173, Test Loss: 0.8975\n",
      "Epoch 4/61, Train Loss: 0.9843, Test Loss: 0.9014\n",
      "Epoch 5/61, Train Loss: 0.9836, Test Loss: 0.8979\n",
      "Epoch 6/61, Train Loss: 0.9831, Test Loss: 0.8991\n",
      "Epoch 7/61, Train Loss: 0.9733, Test Loss: 0.8989\n",
      "Epoch 8/61, Train Loss: 0.9352, Test Loss: 0.8983\n",
      "Epoch 9/61, Train Loss: 0.9744, Test Loss: 0.9044\n",
      "Epoch 10/61, Train Loss: 0.9490, Test Loss: 0.9101\n",
      "Epoch 11/61, Train Loss: 0.9636, Test Loss: 0.9075\n",
      "Epoch 12/61, Train Loss: 0.9282, Test Loss: 0.9097\n",
      "Epoch 13/61, Train Loss: 0.9094, Test Loss: 0.9077\n",
      "Epoch 14/61, Train Loss: 0.9566, Test Loss: 0.9071\n",
      "Epoch 15/61, Train Loss: 0.9879, Test Loss: 0.9079\n",
      "Epoch 16/61, Train Loss: 0.9439, Test Loss: 0.9056\n",
      "Epoch 17/61, Train Loss: 0.9453, Test Loss: 0.9082\n",
      "Epoch 18/61, Train Loss: 0.9418, Test Loss: 0.9080\n",
      "Epoch 19/61, Train Loss: 0.9611, Test Loss: 0.9038\n",
      "Epoch 20/61, Train Loss: 0.9334, Test Loss: 0.9041\n",
      "Epoch 21/61, Train Loss: 0.9539, Test Loss: 0.9067\n",
      "Epoch 22/61, Train Loss: 0.9413, Test Loss: 0.9124\n",
      "Epoch 23/61, Train Loss: 0.9444, Test Loss: 0.9140\n",
      "Epoch 24/61, Train Loss: 0.9353, Test Loss: 0.9163\n",
      "Epoch 25/61, Train Loss: 0.9239, Test Loss: 0.9037\n",
      "Epoch 26/61, Train Loss: 0.8954, Test Loss: 0.9052\n",
      "Epoch 27/61, Train Loss: 0.9216, Test Loss: 0.9036\n",
      "Epoch 28/61, Train Loss: 0.9277, Test Loss: 0.9000\n",
      "Epoch 29/61, Train Loss: 0.9164, Test Loss: 0.9191\n",
      "Epoch 30/61, Train Loss: 0.9216, Test Loss: 0.9269\n",
      "Epoch 31/61, Train Loss: 0.9354, Test Loss: 0.9287\n",
      "Epoch 32/61, Train Loss: 0.9390, Test Loss: 0.9296\n",
      "Epoch 33/61, Train Loss: 0.9321, Test Loss: 0.9146\n",
      "Epoch 34/61, Train Loss: 0.9278, Test Loss: 0.9164\n",
      "Epoch 35/61, Train Loss: 0.9186, Test Loss: 0.9040\n",
      "Epoch 36/61, Train Loss: 0.9766, Test Loss: 0.9202\n",
      "Epoch 37/61, Train Loss: 0.9145, Test Loss: 0.9145\n",
      "Epoch 38/61, Train Loss: 0.9054, Test Loss: 0.9079\n",
      "Epoch 39/61, Train Loss: 0.9343, Test Loss: 0.9151\n",
      "Epoch 40/61, Train Loss: 0.9149, Test Loss: 0.9199\n",
      "Epoch 41/61, Train Loss: 0.9328, Test Loss: 0.9106\n",
      "Epoch 42/61, Train Loss: 0.9408, Test Loss: 0.9175\n",
      "Epoch 43/61, Train Loss: 0.9173, Test Loss: 0.9188\n",
      "Epoch 44/61, Train Loss: 0.9265, Test Loss: 0.9132\n",
      "Epoch 45/61, Train Loss: 0.9139, Test Loss: 0.9093\n",
      "Epoch 46/61, Train Loss: 0.9430, Test Loss: 0.9070\n",
      "Epoch 47/61, Train Loss: 0.8906, Test Loss: 0.9105\n",
      "Epoch 48/61, Train Loss: 0.9296, Test Loss: 0.9040\n",
      "Epoch 49/61, Train Loss: 0.9124, Test Loss: 0.9066\n",
      "Epoch 50/61, Train Loss: 0.9078, Test Loss: 0.9072\n",
      "Epoch 51/61, Train Loss: 0.9019, Test Loss: 0.9082\n",
      "Epoch 52/61, Train Loss: 0.9206, Test Loss: 0.9066\n",
      "Epoch 53/61, Train Loss: 0.8935, Test Loss: 0.9032\n",
      "Epoch 54/61, Train Loss: 0.9017, Test Loss: 0.9066\n",
      "Epoch 55/61, Train Loss: 0.9207, Test Loss: 0.9122\n",
      "Epoch 56/61, Train Loss: 0.9102, Test Loss: 0.9088\n",
      "Epoch 57/61, Train Loss: 0.8941, Test Loss: 0.9137\n",
      "Epoch 58/61, Train Loss: 0.9291, Test Loss: 0.9092\n",
      "Epoch 59/61, Train Loss: 0.9161, Test Loss: 0.9098\n",
      "Epoch 60/61, Train Loss: 0.9032, Test Loss: 0.9128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:58:22,750] Trial 109 finished with value: 0.907401408467974 and parameters: {'num_hidden_layers': 12, 'layer_0_size': 133, 'layer_1_size': 243, 'layer_2_size': 255, 'layer_3_size': 106, 'layer_4_size': 106, 'layer_5_size': 233, 'layer_6_size': 237, 'layer_7_size': 193, 'layer_8_size': 97, 'layer_9_size': 223, 'layer_10_size': 122, 'layer_11_size': 155, 'dropout_rate': 0.2323288354056061, 'learning_rate': 0.00010679411294502411, 'batch_size': 32, 'epochs': 61}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/61, Train Loss: 0.9070, Test Loss: 0.9074\n",
      "Epoch 1/53, Train Loss: 1.2603, Test Loss: 0.9812\n",
      "Epoch 2/53, Train Loss: 1.2822, Test Loss: 0.9769\n",
      "Epoch 3/53, Train Loss: 1.1944, Test Loss: 0.9725\n",
      "Epoch 4/53, Train Loss: 1.2693, Test Loss: 0.9706\n",
      "Epoch 5/53, Train Loss: 1.1769, Test Loss: 0.9700\n",
      "Epoch 6/53, Train Loss: 1.3055, Test Loss: 0.9710\n",
      "Epoch 7/53, Train Loss: 1.1454, Test Loss: 0.9720\n",
      "Epoch 8/53, Train Loss: 1.2227, Test Loss: 0.9743\n",
      "Epoch 9/53, Train Loss: 1.1821, Test Loss: 0.9766\n",
      "Epoch 10/53, Train Loss: 1.2199, Test Loss: 0.9771\n",
      "Epoch 11/53, Train Loss: 1.2047, Test Loss: 0.9805\n",
      "Epoch 12/53, Train Loss: 1.2828, Test Loss: 0.9842\n",
      "Epoch 13/53, Train Loss: 1.2162, Test Loss: 0.9840\n",
      "Epoch 14/53, Train Loss: 1.2288, Test Loss: 0.9864\n",
      "Epoch 15/53, Train Loss: 1.2189, Test Loss: 0.9865\n",
      "Epoch 16/53, Train Loss: 1.1882, Test Loss: 0.9872\n",
      "Epoch 17/53, Train Loss: 1.2258, Test Loss: 0.9901\n",
      "Epoch 18/53, Train Loss: 1.2001, Test Loss: 0.9937\n",
      "Epoch 19/53, Train Loss: 1.1596, Test Loss: 0.9950\n",
      "Epoch 20/53, Train Loss: 1.1869, Test Loss: 0.9953\n",
      "Epoch 21/53, Train Loss: 1.1849, Test Loss: 0.9978\n",
      "Epoch 22/53, Train Loss: 1.1929, Test Loss: 1.0000\n",
      "Epoch 23/53, Train Loss: 1.1980, Test Loss: 1.0006\n",
      "Epoch 24/53, Train Loss: 1.2125, Test Loss: 1.0033\n",
      "Epoch 25/53, Train Loss: 1.2168, Test Loss: 1.0087\n",
      "Epoch 26/53, Train Loss: 1.1915, Test Loss: 1.0077\n",
      "Epoch 27/53, Train Loss: 1.2252, Test Loss: 1.0089\n",
      "Epoch 28/53, Train Loss: 1.2132, Test Loss: 1.0097\n",
      "Epoch 29/53, Train Loss: 1.1727, Test Loss: 1.0089\n",
      "Epoch 30/53, Train Loss: 1.2042, Test Loss: 1.0101\n",
      "Epoch 31/53, Train Loss: 1.1456, Test Loss: 1.0092\n",
      "Epoch 32/53, Train Loss: 1.1145, Test Loss: 1.0123\n",
      "Epoch 33/53, Train Loss: 1.1311, Test Loss: 1.0131\n",
      "Epoch 34/53, Train Loss: 1.1687, Test Loss: 1.0146\n",
      "Epoch 35/53, Train Loss: 1.1802, Test Loss: 1.0133\n",
      "Epoch 36/53, Train Loss: 1.1184, Test Loss: 1.0112\n",
      "Epoch 37/53, Train Loss: 1.1775, Test Loss: 1.0087\n",
      "Epoch 38/53, Train Loss: 1.1681, Test Loss: 1.0101\n",
      "Epoch 39/53, Train Loss: 1.1835, Test Loss: 1.0110\n",
      "Epoch 40/53, Train Loss: 1.1676, Test Loss: 1.0124\n",
      "Epoch 41/53, Train Loss: 1.1870, Test Loss: 1.0105\n",
      "Epoch 42/53, Train Loss: 1.1794, Test Loss: 1.0102\n",
      "Epoch 43/53, Train Loss: 1.1479, Test Loss: 1.0117\n",
      "Epoch 44/53, Train Loss: 1.1554, Test Loss: 1.0112\n",
      "Epoch 45/53, Train Loss: 1.1153, Test Loss: 1.0122\n",
      "Epoch 46/53, Train Loss: 1.1903, Test Loss: 1.0132\n",
      "Epoch 47/53, Train Loss: 1.1250, Test Loss: 1.0131\n",
      "Epoch 48/53, Train Loss: 1.0859, Test Loss: 1.0162\n",
      "Epoch 49/53, Train Loss: 1.1645, Test Loss: 1.0148\n",
      "Epoch 50/53, Train Loss: 1.1265, Test Loss: 1.0138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:58:25,370] Trial 110 finished with value: 1.0145251750946045 and parameters: {'num_hidden_layers': 10, 'layer_0_size': 235, 'layer_1_size': 32, 'layer_2_size': 132, 'layer_3_size': 122, 'layer_4_size': 112, 'layer_5_size': 141, 'layer_6_size': 241, 'layer_7_size': 220, 'layer_8_size': 225, 'layer_9_size': 99, 'dropout_rate': 0.3260685953279448, 'learning_rate': 2.107710552955516e-05, 'batch_size': 256, 'epochs': 53}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/53, Train Loss: 1.1570, Test Loss: 1.0127\n",
      "Epoch 52/53, Train Loss: 1.1650, Test Loss: 1.0131\n",
      "Epoch 53/53, Train Loss: 1.1684, Test Loss: 1.0145\n",
      "Epoch 1/46, Train Loss: 1.3105, Test Loss: 1.3051\n",
      "Epoch 2/46, Train Loss: 1.3353, Test Loss: 1.3141\n",
      "Epoch 3/46, Train Loss: 1.2134, Test Loss: 1.2836\n",
      "Epoch 4/46, Train Loss: 1.2494, Test Loss: 1.2992\n",
      "Epoch 5/46, Train Loss: 1.2020, Test Loss: 1.2879\n",
      "Epoch 6/46, Train Loss: 1.1990, Test Loss: 1.2810\n",
      "Epoch 7/46, Train Loss: 1.2318, Test Loss: 1.2881\n",
      "Epoch 8/46, Train Loss: 1.2539, Test Loss: 1.3019\n",
      "Epoch 9/46, Train Loss: 1.1936, Test Loss: 1.2988\n",
      "Epoch 10/46, Train Loss: 1.1972, Test Loss: 1.3224\n",
      "Epoch 11/46, Train Loss: 1.2055, Test Loss: 1.3154\n",
      "Epoch 12/46, Train Loss: 1.2330, Test Loss: 1.3080\n",
      "Epoch 13/46, Train Loss: 1.2379, Test Loss: 1.3124\n",
      "Epoch 14/46, Train Loss: 1.2090, Test Loss: 1.3005\n",
      "Epoch 15/46, Train Loss: 1.2031, Test Loss: 1.3163\n",
      "Epoch 16/46, Train Loss: 1.1684, Test Loss: 1.3048\n",
      "Epoch 17/46, Train Loss: 1.2456, Test Loss: 1.3033\n",
      "Epoch 18/46, Train Loss: 1.1914, Test Loss: 1.3006\n",
      "Epoch 19/46, Train Loss: 1.2054, Test Loss: 1.3148\n",
      "Epoch 20/46, Train Loss: 1.1703, Test Loss: 1.2938\n",
      "Epoch 21/46, Train Loss: 1.1756, Test Loss: 1.3072\n",
      "Epoch 22/46, Train Loss: 1.2084, Test Loss: 1.3011\n",
      "Epoch 23/46, Train Loss: 1.2007, Test Loss: 1.2971\n",
      "Epoch 24/46, Train Loss: 1.1482, Test Loss: 1.2937\n",
      "Epoch 25/46, Train Loss: 1.1353, Test Loss: 1.2955\n",
      "Epoch 26/46, Train Loss: 1.1555, Test Loss: 1.2920\n",
      "Epoch 27/46, Train Loss: 1.2210, Test Loss: 1.3065\n",
      "Epoch 28/46, Train Loss: 1.1633, Test Loss: 1.3084\n",
      "Epoch 29/46, Train Loss: 1.1753, Test Loss: 1.2999\n",
      "Epoch 30/46, Train Loss: 1.1838, Test Loss: 1.3043\n",
      "Epoch 31/46, Train Loss: 1.1625, Test Loss: 1.3012\n",
      "Epoch 32/46, Train Loss: 1.1825, Test Loss: 1.3059\n",
      "Epoch 33/46, Train Loss: 1.1574, Test Loss: 1.3182\n",
      "Epoch 34/46, Train Loss: 1.1440, Test Loss: 1.3061\n",
      "Epoch 35/46, Train Loss: 1.1297, Test Loss: 1.3034\n",
      "Epoch 36/46, Train Loss: 1.1969, Test Loss: 1.3083\n",
      "Epoch 37/46, Train Loss: 1.1615, Test Loss: 1.2932\n",
      "Epoch 38/46, Train Loss: 1.2251, Test Loss: 1.2998\n",
      "Epoch 39/46, Train Loss: 1.1164, Test Loss: 1.3045\n",
      "Epoch 40/46, Train Loss: 1.1795, Test Loss: 1.2964\n",
      "Epoch 41/46, Train Loss: 1.1768, Test Loss: 1.2963\n",
      "Epoch 42/46, Train Loss: 1.1506, Test Loss: 1.2849\n",
      "Epoch 43/46, Train Loss: 1.1212, Test Loss: 1.2836\n",
      "Epoch 44/46, Train Loss: 1.1784, Test Loss: 1.2776\n",
      "Epoch 45/46, Train Loss: 1.1313, Test Loss: 1.2769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:58:37,679] Trial 111 finished with value: 1.2808205315044947 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 157, 'layer_1_size': 213, 'layer_2_size': 161, 'layer_3_size': 133, 'layer_4_size': 172, 'layer_5_size': 109, 'layer_6_size': 194, 'layer_7_size': 200, 'layer_8_size': 89, 'layer_9_size': 87, 'layer_10_size': 231, 'layer_11_size': 162, 'layer_12_size': 102, 'layer_13_size': 91, 'dropout_rate': 0.33972820350416283, 'learning_rate': 0.0001365987397329539, 'batch_size': 32, 'epochs': 46}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/46, Train Loss: 1.1295, Test Loss: 1.2808\n",
      "Epoch 1/65, Train Loss: 1.3373, Test Loss: 0.8785\n",
      "Epoch 2/65, Train Loss: 1.2568, Test Loss: 0.8946\n",
      "Epoch 3/65, Train Loss: 1.2592, Test Loss: 0.8738\n",
      "Epoch 4/65, Train Loss: 1.2204, Test Loss: 0.8669\n",
      "Epoch 5/65, Train Loss: 1.2438, Test Loss: 0.8773\n",
      "Epoch 6/65, Train Loss: 1.1438, Test Loss: 0.8698\n",
      "Epoch 7/65, Train Loss: 1.1762, Test Loss: 0.8700\n",
      "Epoch 8/65, Train Loss: 1.2224, Test Loss: 0.8602\n",
      "Epoch 9/65, Train Loss: 1.1665, Test Loss: 0.8508\n",
      "Epoch 10/65, Train Loss: 1.1415, Test Loss: 0.8625\n",
      "Epoch 11/65, Train Loss: 1.1699, Test Loss: 0.8551\n",
      "Epoch 12/65, Train Loss: 1.1818, Test Loss: 0.8421\n",
      "Epoch 13/65, Train Loss: 1.1296, Test Loss: 0.8440\n",
      "Epoch 14/65, Train Loss: 1.1593, Test Loss: 0.8477\n",
      "Epoch 15/65, Train Loss: 1.2030, Test Loss: 0.8445\n",
      "Epoch 16/65, Train Loss: 1.1250, Test Loss: 0.8440\n",
      "Epoch 17/65, Train Loss: 1.1857, Test Loss: 0.8436\n",
      "Epoch 18/65, Train Loss: 1.1886, Test Loss: 0.8405\n",
      "Epoch 19/65, Train Loss: 1.1854, Test Loss: 0.8433\n",
      "Epoch 20/65, Train Loss: 1.1521, Test Loss: 0.8420\n",
      "Epoch 21/65, Train Loss: 1.1235, Test Loss: 0.8410\n",
      "Epoch 22/65, Train Loss: 1.0932, Test Loss: 0.8373\n",
      "Epoch 23/65, Train Loss: 1.1705, Test Loss: 0.8400\n",
      "Epoch 24/65, Train Loss: 1.1298, Test Loss: 0.8421\n",
      "Epoch 25/65, Train Loss: 1.1060, Test Loss: 0.8371\n",
      "Epoch 26/65, Train Loss: 1.1133, Test Loss: 0.8383\n",
      "Epoch 27/65, Train Loss: 1.1514, Test Loss: 0.8431\n",
      "Epoch 28/65, Train Loss: 1.1103, Test Loss: 0.8383\n",
      "Epoch 29/65, Train Loss: 1.1122, Test Loss: 0.8423\n",
      "Epoch 30/65, Train Loss: 1.1117, Test Loss: 0.8380\n",
      "Epoch 31/65, Train Loss: 1.1380, Test Loss: 0.8402\n",
      "Epoch 32/65, Train Loss: 1.1308, Test Loss: 0.8368\n",
      "Epoch 33/65, Train Loss: 1.1514, Test Loss: 0.8357\n",
      "Epoch 34/65, Train Loss: 1.1095, Test Loss: 0.8383\n",
      "Epoch 35/65, Train Loss: 1.1437, Test Loss: 0.8402\n",
      "Epoch 36/65, Train Loss: 1.0997, Test Loss: 0.8401\n",
      "Epoch 37/65, Train Loss: 1.1123, Test Loss: 0.8384\n",
      "Epoch 38/65, Train Loss: 1.1415, Test Loss: 0.8416\n",
      "Epoch 39/65, Train Loss: 1.0739, Test Loss: 0.8433\n",
      "Epoch 40/65, Train Loss: 1.1197, Test Loss: 0.8423\n",
      "Epoch 41/65, Train Loss: 1.1061, Test Loss: 0.8399\n",
      "Epoch 42/65, Train Loss: 1.0485, Test Loss: 0.8377\n",
      "Epoch 43/65, Train Loss: 1.0920, Test Loss: 0.8429\n",
      "Epoch 44/65, Train Loss: 1.1083, Test Loss: 0.8427\n",
      "Epoch 45/65, Train Loss: 1.1039, Test Loss: 0.8446\n",
      "Epoch 46/65, Train Loss: 1.0719, Test Loss: 0.8442\n",
      "Epoch 47/65, Train Loss: 1.0905, Test Loss: 0.8464\n",
      "Epoch 48/65, Train Loss: 1.1359, Test Loss: 0.8409\n",
      "Epoch 49/65, Train Loss: 1.0833, Test Loss: 0.8454\n",
      "Epoch 50/65, Train Loss: 1.0575, Test Loss: 0.8395\n",
      "Epoch 51/65, Train Loss: 1.0943, Test Loss: 0.8420\n",
      "Epoch 52/65, Train Loss: 1.1038, Test Loss: 0.8447\n",
      "Epoch 53/65, Train Loss: 1.0758, Test Loss: 0.8461\n",
      "Epoch 54/65, Train Loss: 1.0689, Test Loss: 0.8416\n",
      "Epoch 55/65, Train Loss: 1.0848, Test Loss: 0.8442\n",
      "Epoch 56/65, Train Loss: 1.0964, Test Loss: 0.8415\n",
      "Epoch 57/65, Train Loss: 1.0733, Test Loss: 0.8415\n",
      "Epoch 58/65, Train Loss: 1.1071, Test Loss: 0.8420\n",
      "Epoch 59/65, Train Loss: 1.0946, Test Loss: 0.8454\n",
      "Epoch 60/65, Train Loss: 1.0913, Test Loss: 0.8428\n",
      "Epoch 61/65, Train Loss: 1.1181, Test Loss: 0.8404\n",
      "Epoch 62/65, Train Loss: 1.0647, Test Loss: 0.8431\n",
      "Epoch 63/65, Train Loss: 1.0722, Test Loss: 0.8440\n",
      "Epoch 64/65, Train Loss: 1.0841, Test Loss: 0.8432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:58:54,935] Trial 112 finished with value: 0.8409656371389117 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 220, 'layer_1_size': 198, 'layer_2_size': 234, 'layer_3_size': 150, 'layer_4_size': 256, 'layer_5_size': 250, 'layer_6_size': 207, 'layer_7_size': 141, 'layer_8_size': 156, 'layer_9_size': 38, 'layer_10_size': 189, 'layer_11_size': 173, 'layer_12_size': 75, 'dropout_rate': 0.30756277005332144, 'learning_rate': 6.571303922151983e-05, 'batch_size': 32, 'epochs': 65}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/65, Train Loss: 1.0828, Test Loss: 0.8410\n",
      "Epoch 1/69, Train Loss: 1.1267, Test Loss: 1.2823\n",
      "Epoch 2/69, Train Loss: 1.1530, Test Loss: 1.2892\n",
      "Epoch 3/69, Train Loss: 1.1356, Test Loss: 1.2634\n",
      "Epoch 4/69, Train Loss: 1.1859, Test Loss: 1.2500\n",
      "Epoch 5/69, Train Loss: 1.1866, Test Loss: 1.2611\n",
      "Epoch 6/69, Train Loss: 1.1383, Test Loss: 1.2518\n",
      "Epoch 7/69, Train Loss: 1.1821, Test Loss: 1.2538\n",
      "Epoch 8/69, Train Loss: 1.1922, Test Loss: 1.2596\n",
      "Epoch 9/69, Train Loss: 1.1184, Test Loss: 1.2389\n",
      "Epoch 10/69, Train Loss: 1.1518, Test Loss: 1.2488\n",
      "Epoch 11/69, Train Loss: 1.1385, Test Loss: 1.2507\n",
      "Epoch 12/69, Train Loss: 1.1352, Test Loss: 1.2350\n",
      "Epoch 13/69, Train Loss: 1.0922, Test Loss: 1.2317\n",
      "Epoch 14/69, Train Loss: 1.1500, Test Loss: 1.2520\n",
      "Epoch 15/69, Train Loss: 1.1465, Test Loss: 1.2492\n",
      "Epoch 16/69, Train Loss: 1.1690, Test Loss: 1.2349\n",
      "Epoch 17/69, Train Loss: 1.1417, Test Loss: 1.2280\n",
      "Epoch 18/69, Train Loss: 1.1842, Test Loss: 1.2347\n",
      "Epoch 19/69, Train Loss: 1.1199, Test Loss: 1.2330\n",
      "Epoch 20/69, Train Loss: 1.1462, Test Loss: 1.2362\n",
      "Epoch 21/69, Train Loss: 1.1446, Test Loss: 1.2279\n",
      "Epoch 22/69, Train Loss: 1.1918, Test Loss: 1.2270\n",
      "Epoch 23/69, Train Loss: 1.0807, Test Loss: 1.2240\n",
      "Epoch 24/69, Train Loss: 1.1197, Test Loss: 1.2332\n",
      "Epoch 25/69, Train Loss: 1.1166, Test Loss: 1.2390\n",
      "Epoch 26/69, Train Loss: 1.0976, Test Loss: 1.2328\n",
      "Epoch 27/69, Train Loss: 1.1157, Test Loss: 1.2307\n",
      "Epoch 28/69, Train Loss: 1.1072, Test Loss: 1.2249\n",
      "Epoch 29/69, Train Loss: 1.1155, Test Loss: 1.2244\n",
      "Epoch 30/69, Train Loss: 1.1187, Test Loss: 1.2261\n",
      "Epoch 31/69, Train Loss: 1.1479, Test Loss: 1.2299\n",
      "Epoch 32/69, Train Loss: 1.0946, Test Loss: 1.2213\n",
      "Epoch 33/69, Train Loss: 1.1288, Test Loss: 1.2172\n",
      "Epoch 34/69, Train Loss: 1.0851, Test Loss: 1.2210\n",
      "Epoch 35/69, Train Loss: 1.1300, Test Loss: 1.2320\n",
      "Epoch 36/69, Train Loss: 1.1274, Test Loss: 1.2265\n",
      "Epoch 37/69, Train Loss: 1.1589, Test Loss: 1.2236\n",
      "Epoch 38/69, Train Loss: 1.1021, Test Loss: 1.2236\n",
      "Epoch 39/69, Train Loss: 1.1772, Test Loss: 1.2257\n",
      "Epoch 40/69, Train Loss: 1.1278, Test Loss: 1.2207\n",
      "Epoch 41/69, Train Loss: 1.1057, Test Loss: 1.2197\n",
      "Epoch 42/69, Train Loss: 1.1014, Test Loss: 1.2232\n",
      "Epoch 43/69, Train Loss: 1.1245, Test Loss: 1.2266\n",
      "Epoch 44/69, Train Loss: 1.1529, Test Loss: 1.2272\n",
      "Epoch 45/69, Train Loss: 1.1126, Test Loss: 1.2254\n",
      "Epoch 46/69, Train Loss: 1.1553, Test Loss: 1.2241\n",
      "Epoch 47/69, Train Loss: 1.1235, Test Loss: 1.2307\n",
      "Epoch 48/69, Train Loss: 1.1159, Test Loss: 1.2210\n",
      "Epoch 49/69, Train Loss: 1.1434, Test Loss: 1.2283\n",
      "Epoch 50/69, Train Loss: 1.1129, Test Loss: 1.2316\n",
      "Epoch 51/69, Train Loss: 1.0918, Test Loss: 1.2375\n",
      "Epoch 52/69, Train Loss: 1.1345, Test Loss: 1.2260\n",
      "Epoch 53/69, Train Loss: 1.0760, Test Loss: 1.2327\n",
      "Epoch 54/69, Train Loss: 1.0873, Test Loss: 1.2158\n",
      "Epoch 55/69, Train Loss: 1.1178, Test Loss: 1.2342\n",
      "Epoch 56/69, Train Loss: 1.1195, Test Loss: 1.2238\n",
      "Epoch 57/69, Train Loss: 1.0837, Test Loss: 1.2316\n",
      "Epoch 58/69, Train Loss: 1.0834, Test Loss: 1.2258\n",
      "Epoch 59/69, Train Loss: 1.0907, Test Loss: 1.2228\n",
      "Epoch 60/69, Train Loss: 1.0914, Test Loss: 1.2191\n",
      "Epoch 61/69, Train Loss: 1.1339, Test Loss: 1.2267\n",
      "Epoch 62/69, Train Loss: 1.1001, Test Loss: 1.2341\n",
      "Epoch 63/69, Train Loss: 1.0810, Test Loss: 1.2313\n",
      "Epoch 64/69, Train Loss: 1.0992, Test Loss: 1.2313\n",
      "Epoch 65/69, Train Loss: 1.0861, Test Loss: 1.2193\n",
      "Epoch 66/69, Train Loss: 1.1212, Test Loss: 1.2193\n",
      "Epoch 67/69, Train Loss: 1.0965, Test Loss: 1.2287\n",
      "Epoch 68/69, Train Loss: 1.1139, Test Loss: 1.2217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:59:13,258] Trial 113 finished with value: 1.2184252100331443 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 222, 'layer_1_size': 196, 'layer_2_size': 244, 'layer_3_size': 172, 'layer_4_size': 232, 'layer_5_size': 256, 'layer_6_size': 228, 'layer_7_size': 139, 'layer_8_size': 143, 'layer_9_size': 51, 'layer_10_size': 188, 'layer_11_size': 172, 'layer_12_size': 33, 'dropout_rate': 0.29978205836509275, 'learning_rate': 2.4375245618111817e-05, 'batch_size': 32, 'epochs': 69}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/69, Train Loss: 1.1150, Test Loss: 1.2184\n",
      "Epoch 1/63, Train Loss: 1.2552, Test Loss: 0.8929\n",
      "Epoch 2/63, Train Loss: 1.2464, Test Loss: 0.8962\n",
      "Epoch 3/63, Train Loss: 1.2644, Test Loss: 0.8975\n",
      "Epoch 4/63, Train Loss: 1.2875, Test Loss: 0.9052\n",
      "Epoch 5/63, Train Loss: 1.1982, Test Loss: 0.8955\n",
      "Epoch 6/63, Train Loss: 1.2483, Test Loss: 0.8889\n",
      "Epoch 7/63, Train Loss: 1.2270, Test Loss: 0.8896\n",
      "Epoch 8/63, Train Loss: 1.2183, Test Loss: 0.8871\n",
      "Epoch 9/63, Train Loss: 1.2189, Test Loss: 0.8872\n",
      "Epoch 10/63, Train Loss: 1.2396, Test Loss: 0.8932\n",
      "Epoch 11/63, Train Loss: 1.2087, Test Loss: 0.8902\n",
      "Epoch 12/63, Train Loss: 1.1956, Test Loss: 0.8875\n",
      "Epoch 13/63, Train Loss: 1.2370, Test Loss: 0.8875\n",
      "Epoch 14/63, Train Loss: 1.1770, Test Loss: 0.8875\n",
      "Epoch 15/63, Train Loss: 1.1698, Test Loss: 0.8902\n",
      "Epoch 16/63, Train Loss: 1.2215, Test Loss: 0.8892\n",
      "Epoch 17/63, Train Loss: 1.2119, Test Loss: 0.8911\n",
      "Epoch 18/63, Train Loss: 1.2394, Test Loss: 0.8886\n",
      "Epoch 19/63, Train Loss: 1.1684, Test Loss: 0.8914\n",
      "Epoch 20/63, Train Loss: 1.1974, Test Loss: 0.8915\n",
      "Epoch 21/63, Train Loss: 1.1817, Test Loss: 0.8853\n",
      "Epoch 22/63, Train Loss: 1.1901, Test Loss: 0.8808\n",
      "Epoch 23/63, Train Loss: 1.1811, Test Loss: 0.8853\n",
      "Epoch 24/63, Train Loss: 1.2189, Test Loss: 0.8867\n",
      "Epoch 25/63, Train Loss: 1.1104, Test Loss: 0.8835\n",
      "Epoch 26/63, Train Loss: 1.2232, Test Loss: 0.9006\n",
      "Epoch 27/63, Train Loss: 1.1799, Test Loss: 0.9011\n",
      "Epoch 28/63, Train Loss: 1.1484, Test Loss: 0.8917\n",
      "Epoch 29/63, Train Loss: 1.1959, Test Loss: 0.8978\n",
      "Epoch 30/63, Train Loss: 1.2193, Test Loss: 0.8824\n",
      "Epoch 31/63, Train Loss: 1.1595, Test Loss: 0.8850\n",
      "Epoch 32/63, Train Loss: 1.1485, Test Loss: 0.8856\n",
      "Epoch 33/63, Train Loss: 1.1613, Test Loss: 0.8812\n",
      "Epoch 34/63, Train Loss: 1.1731, Test Loss: 0.8829\n",
      "Epoch 35/63, Train Loss: 1.1850, Test Loss: 0.8814\n",
      "Epoch 36/63, Train Loss: 1.1737, Test Loss: 0.8846\n",
      "Epoch 37/63, Train Loss: 1.1885, Test Loss: 0.8836\n",
      "Epoch 38/63, Train Loss: 1.1592, Test Loss: 0.8826\n",
      "Epoch 39/63, Train Loss: 1.1938, Test Loss: 0.8811\n",
      "Epoch 40/63, Train Loss: 1.1812, Test Loss: 0.8856\n",
      "Epoch 41/63, Train Loss: 1.1789, Test Loss: 0.8867\n",
      "Epoch 42/63, Train Loss: 1.1498, Test Loss: 0.8792\n",
      "Epoch 43/63, Train Loss: 1.1379, Test Loss: 0.8867\n",
      "Epoch 44/63, Train Loss: 1.2051, Test Loss: 0.8840\n",
      "Epoch 45/63, Train Loss: 1.1837, Test Loss: 0.8848\n",
      "Epoch 46/63, Train Loss: 1.2308, Test Loss: 0.8902\n",
      "Epoch 47/63, Train Loss: 1.1401, Test Loss: 0.8875\n",
      "Epoch 48/63, Train Loss: 1.1644, Test Loss: 0.8879\n",
      "Epoch 49/63, Train Loss: 1.1264, Test Loss: 0.8900\n",
      "Epoch 50/63, Train Loss: 1.1335, Test Loss: 0.8823\n",
      "Epoch 51/63, Train Loss: 1.1527, Test Loss: 0.8845\n",
      "Epoch 52/63, Train Loss: 1.1332, Test Loss: 0.8824\n",
      "Epoch 53/63, Train Loss: 1.1475, Test Loss: 0.8868\n",
      "Epoch 54/63, Train Loss: 1.1299, Test Loss: 0.8853\n",
      "Epoch 55/63, Train Loss: 1.1332, Test Loss: 0.8828\n",
      "Epoch 56/63, Train Loss: 1.1504, Test Loss: 0.8809\n",
      "Epoch 57/63, Train Loss: 1.1337, Test Loss: 0.8789\n",
      "Epoch 58/63, Train Loss: 1.1462, Test Loss: 0.8892\n",
      "Epoch 59/63, Train Loss: 1.1411, Test Loss: 0.8813\n",
      "Epoch 60/63, Train Loss: 1.1595, Test Loss: 0.8825\n",
      "Epoch 61/63, Train Loss: 1.1809, Test Loss: 0.8829\n",
      "Epoch 62/63, Train Loss: 1.1669, Test Loss: 0.8863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:59:28,817] Trial 114 finished with value: 0.8856164557593209 and parameters: {'num_hidden_layers': 12, 'layer_0_size': 211, 'layer_1_size': 205, 'layer_2_size': 234, 'layer_3_size': 152, 'layer_4_size': 244, 'layer_5_size': 248, 'layer_6_size': 207, 'layer_7_size': 130, 'layer_8_size': 131, 'layer_9_size': 42, 'layer_10_size': 178, 'layer_11_size': 148, 'dropout_rate': 0.3563024961180814, 'learning_rate': 5.8093347213275236e-05, 'batch_size': 32, 'epochs': 63}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/63, Train Loss: 1.1589, Test Loss: 0.8856\n",
      "Epoch 1/66, Train Loss: 1.1964, Test Loss: 1.1717\n",
      "Epoch 2/66, Train Loss: 1.1608, Test Loss: 1.1574\n",
      "Epoch 3/66, Train Loss: 1.1697, Test Loss: 1.1679\n",
      "Epoch 4/66, Train Loss: 1.1879, Test Loss: 1.1696\n",
      "Epoch 5/66, Train Loss: 1.1585, Test Loss: 1.1573\n",
      "Epoch 6/66, Train Loss: 1.0967, Test Loss: 1.1614\n",
      "Epoch 7/66, Train Loss: 1.1238, Test Loss: 1.1631\n",
      "Epoch 8/66, Train Loss: 1.1665, Test Loss: 1.1574\n",
      "Epoch 9/66, Train Loss: 1.1737, Test Loss: 1.1646\n",
      "Epoch 10/66, Train Loss: 1.1620, Test Loss: 1.1617\n",
      "Epoch 11/66, Train Loss: 1.1290, Test Loss: 1.1613\n",
      "Epoch 12/66, Train Loss: 1.1645, Test Loss: 1.1767\n",
      "Epoch 13/66, Train Loss: 1.0783, Test Loss: 1.1584\n",
      "Epoch 14/66, Train Loss: 1.1570, Test Loss: 1.1534\n",
      "Epoch 15/66, Train Loss: 1.1383, Test Loss: 1.1649\n",
      "Epoch 16/66, Train Loss: 1.1061, Test Loss: 1.1681\n",
      "Epoch 17/66, Train Loss: 1.1531, Test Loss: 1.1625\n",
      "Epoch 18/66, Train Loss: 1.1701, Test Loss: 1.1689\n",
      "Epoch 19/66, Train Loss: 1.1315, Test Loss: 1.1681\n",
      "Epoch 20/66, Train Loss: 1.0936, Test Loss: 1.1710\n",
      "Epoch 21/66, Train Loss: 1.1656, Test Loss: 1.1591\n",
      "Epoch 22/66, Train Loss: 1.1033, Test Loss: 1.1649\n",
      "Epoch 23/66, Train Loss: 1.1151, Test Loss: 1.1648\n",
      "Epoch 24/66, Train Loss: 1.1002, Test Loss: 1.1631\n",
      "Epoch 25/66, Train Loss: 1.0932, Test Loss: 1.1741\n",
      "Epoch 26/66, Train Loss: 1.0919, Test Loss: 1.1652\n",
      "Epoch 27/66, Train Loss: 1.0578, Test Loss: 1.1719\n",
      "Epoch 28/66, Train Loss: 1.1123, Test Loss: 1.1651\n",
      "Epoch 29/66, Train Loss: 1.1231, Test Loss: 1.1669\n",
      "Epoch 30/66, Train Loss: 1.1611, Test Loss: 1.1635\n",
      "Epoch 31/66, Train Loss: 1.1101, Test Loss: 1.1667\n",
      "Epoch 32/66, Train Loss: 1.1319, Test Loss: 1.1674\n",
      "Epoch 33/66, Train Loss: 1.0889, Test Loss: 1.1714\n",
      "Epoch 34/66, Train Loss: 1.1295, Test Loss: 1.1700\n",
      "Epoch 35/66, Train Loss: 1.0899, Test Loss: 1.1709\n",
      "Epoch 36/66, Train Loss: 1.1289, Test Loss: 1.1737\n",
      "Epoch 37/66, Train Loss: 1.1189, Test Loss: 1.1694\n",
      "Epoch 38/66, Train Loss: 1.0997, Test Loss: 1.1714\n",
      "Epoch 39/66, Train Loss: 1.1307, Test Loss: 1.1701\n",
      "Epoch 40/66, Train Loss: 1.0968, Test Loss: 1.1683\n",
      "Epoch 41/66, Train Loss: 1.0916, Test Loss: 1.1761\n",
      "Epoch 42/66, Train Loss: 1.1107, Test Loss: 1.1702\n",
      "Epoch 43/66, Train Loss: 1.0806, Test Loss: 1.1657\n",
      "Epoch 44/66, Train Loss: 1.0950, Test Loss: 1.1662\n",
      "Epoch 45/66, Train Loss: 1.0778, Test Loss: 1.1720\n",
      "Epoch 46/66, Train Loss: 1.0708, Test Loss: 1.1711\n",
      "Epoch 47/66, Train Loss: 1.1147, Test Loss: 1.1626\n",
      "Epoch 48/66, Train Loss: 1.1148, Test Loss: 1.1608\n",
      "Epoch 49/66, Train Loss: 1.1299, Test Loss: 1.1625\n",
      "Epoch 50/66, Train Loss: 1.0762, Test Loss: 1.1746\n",
      "Epoch 51/66, Train Loss: 1.0629, Test Loss: 1.1724\n",
      "Epoch 52/66, Train Loss: 1.0692, Test Loss: 1.1697\n",
      "Epoch 53/66, Train Loss: 1.1059, Test Loss: 1.1657\n",
      "Epoch 54/66, Train Loss: 1.1269, Test Loss: 1.1718\n",
      "Epoch 55/66, Train Loss: 1.1072, Test Loss: 1.1723\n",
      "Epoch 56/66, Train Loss: 1.0827, Test Loss: 1.1635\n",
      "Epoch 57/66, Train Loss: 1.1115, Test Loss: 1.1723\n",
      "Epoch 58/66, Train Loss: 1.1088, Test Loss: 1.1706\n",
      "Epoch 59/66, Train Loss: 1.1052, Test Loss: 1.1711\n",
      "Epoch 60/66, Train Loss: 1.0734, Test Loss: 1.1729\n",
      "Epoch 61/66, Train Loss: 1.0888, Test Loss: 1.1711\n",
      "Epoch 62/66, Train Loss: 1.0747, Test Loss: 1.1724\n",
      "Epoch 63/66, Train Loss: 1.1170, Test Loss: 1.1712\n",
      "Epoch 64/66, Train Loss: 1.0723, Test Loss: 1.1735\n",
      "Epoch 65/66, Train Loss: 1.0809, Test Loss: 1.1722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:59:43,247] Trial 115 finished with value: 1.1696681209972926 and parameters: {'num_hidden_layers': 11, 'layer_0_size': 217, 'layer_1_size': 191, 'layer_2_size': 222, 'layer_3_size': 148, 'layer_4_size': 255, 'layer_5_size': 151, 'layer_6_size': 250, 'layer_7_size': 120, 'layer_8_size': 83, 'layer_9_size': 37, 'layer_10_size': 156, 'dropout_rate': 0.31401632891271564, 'learning_rate': 3.7558031442971174e-05, 'batch_size': 32, 'epochs': 66}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/66, Train Loss: 1.1135, Test Loss: 1.1697\n",
      "Epoch 1/59, Train Loss: 1.2352, Test Loss: 1.0078\n",
      "Epoch 2/59, Train Loss: 1.2752, Test Loss: 1.0118\n",
      "Epoch 3/59, Train Loss: 1.2301, Test Loss: 1.0093\n",
      "Epoch 4/59, Train Loss: 1.2229, Test Loss: 1.0064\n",
      "Epoch 5/59, Train Loss: 1.1842, Test Loss: 1.0094\n",
      "Epoch 6/59, Train Loss: 1.1365, Test Loss: 1.0080\n",
      "Epoch 7/59, Train Loss: 1.1384, Test Loss: 0.9986\n",
      "Epoch 8/59, Train Loss: 1.1746, Test Loss: 0.9998\n",
      "Epoch 9/59, Train Loss: 1.1028, Test Loss: 0.9972\n",
      "Epoch 10/59, Train Loss: 1.1198, Test Loss: 0.9988\n",
      "Epoch 11/59, Train Loss: 1.1129, Test Loss: 1.0001\n",
      "Epoch 12/59, Train Loss: 1.0769, Test Loss: 0.9964\n",
      "Epoch 13/59, Train Loss: 1.0967, Test Loss: 0.9960\n",
      "Epoch 14/59, Train Loss: 1.1399, Test Loss: 0.9980\n",
      "Epoch 15/59, Train Loss: 1.1309, Test Loss: 0.9896\n",
      "Epoch 16/59, Train Loss: 1.1222, Test Loss: 0.9951\n",
      "Epoch 17/59, Train Loss: 1.1102, Test Loss: 0.9977\n",
      "Epoch 18/59, Train Loss: 1.1276, Test Loss: 0.9898\n",
      "Epoch 19/59, Train Loss: 1.1411, Test Loss: 0.9883\n",
      "Epoch 20/59, Train Loss: 1.1342, Test Loss: 0.9886\n",
      "Epoch 21/59, Train Loss: 1.0913, Test Loss: 0.9864\n",
      "Epoch 22/59, Train Loss: 1.1214, Test Loss: 0.9944\n",
      "Epoch 23/59, Train Loss: 1.0927, Test Loss: 0.9892\n",
      "Epoch 24/59, Train Loss: 1.0922, Test Loss: 0.9851\n",
      "Epoch 25/59, Train Loss: 1.0941, Test Loss: 0.9935\n",
      "Epoch 26/59, Train Loss: 1.1292, Test Loss: 0.9895\n",
      "Epoch 27/59, Train Loss: 1.1128, Test Loss: 0.9906\n",
      "Epoch 28/59, Train Loss: 1.1249, Test Loss: 0.9931\n",
      "Epoch 29/59, Train Loss: 1.1208, Test Loss: 0.9915\n",
      "Epoch 30/59, Train Loss: 1.0674, Test Loss: 0.9885\n",
      "Epoch 31/59, Train Loss: 1.1240, Test Loss: 0.9894\n",
      "Epoch 32/59, Train Loss: 1.0803, Test Loss: 0.9903\n",
      "Epoch 33/59, Train Loss: 1.0715, Test Loss: 0.9907\n",
      "Epoch 34/59, Train Loss: 1.1061, Test Loss: 0.9883\n",
      "Epoch 35/59, Train Loss: 1.1000, Test Loss: 0.9935\n",
      "Epoch 36/59, Train Loss: 1.0747, Test Loss: 0.9884\n",
      "Epoch 37/59, Train Loss: 1.0970, Test Loss: 0.9898\n",
      "Epoch 38/59, Train Loss: 1.0736, Test Loss: 0.9888\n",
      "Epoch 39/59, Train Loss: 1.0948, Test Loss: 0.9896\n",
      "Epoch 40/59, Train Loss: 1.1189, Test Loss: 0.9912\n",
      "Epoch 41/59, Train Loss: 1.0664, Test Loss: 0.9929\n",
      "Epoch 42/59, Train Loss: 1.0596, Test Loss: 0.9917\n",
      "Epoch 43/59, Train Loss: 1.1035, Test Loss: 0.9869\n",
      "Epoch 44/59, Train Loss: 1.0806, Test Loss: 0.9848\n",
      "Epoch 45/59, Train Loss: 1.0498, Test Loss: 0.9822\n",
      "Epoch 46/59, Train Loss: 1.0863, Test Loss: 0.9837\n",
      "Epoch 47/59, Train Loss: 1.0492, Test Loss: 0.9861\n",
      "Epoch 48/59, Train Loss: 1.0913, Test Loss: 0.9811\n",
      "Epoch 49/59, Train Loss: 1.0627, Test Loss: 0.9839\n",
      "Epoch 50/59, Train Loss: 1.0954, Test Loss: 0.9828\n",
      "Epoch 51/59, Train Loss: 1.1138, Test Loss: 0.9867\n",
      "Epoch 52/59, Train Loss: 1.0308, Test Loss: 0.9812\n",
      "Epoch 53/59, Train Loss: 1.0327, Test Loss: 0.9830\n",
      "Epoch 54/59, Train Loss: 1.0514, Test Loss: 0.9830\n",
      "Epoch 55/59, Train Loss: 1.0684, Test Loss: 0.9823\n",
      "Epoch 56/59, Train Loss: 1.0825, Test Loss: 0.9777\n",
      "Epoch 57/59, Train Loss: 1.0466, Test Loss: 0.9818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:59:52,817] Trial 116 finished with value: 0.9854991606303624 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 141, 'layer_1_size': 218, 'layer_2_size': 208, 'layer_3_size': 237, 'layer_4_size': 249, 'layer_5_size': 179, 'layer_6_size': 212, 'dropout_rate': 0.30879254813020046, 'learning_rate': 7.013404875363844e-05, 'batch_size': 32, 'epochs': 59}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/59, Train Loss: 1.0665, Test Loss: 0.9831\n",
      "Epoch 59/59, Train Loss: 1.0991, Test Loss: 0.9855\n",
      "Epoch 1/76, Train Loss: 1.1812, Test Loss: 1.2815\n",
      "Epoch 2/76, Train Loss: 1.1358, Test Loss: 1.2761\n",
      "Epoch 3/76, Train Loss: 1.1156, Test Loss: 1.2708\n",
      "Epoch 4/76, Train Loss: 1.0593, Test Loss: 1.2707\n",
      "Epoch 5/76, Train Loss: 1.1706, Test Loss: 1.2663\n",
      "Epoch 6/76, Train Loss: 1.1050, Test Loss: 1.2727\n",
      "Epoch 7/76, Train Loss: 1.1066, Test Loss: 1.2707\n",
      "Epoch 8/76, Train Loss: 1.1046, Test Loss: 1.2658\n",
      "Epoch 9/76, Train Loss: 1.0648, Test Loss: 1.2700\n",
      "Epoch 10/76, Train Loss: 1.0976, Test Loss: 1.2671\n",
      "Epoch 11/76, Train Loss: 1.0421, Test Loss: 1.2696\n",
      "Epoch 12/76, Train Loss: 1.0382, Test Loss: 1.2663\n",
      "Epoch 13/76, Train Loss: 1.0646, Test Loss: 1.2738\n",
      "Epoch 14/76, Train Loss: 1.0624, Test Loss: 1.2669\n",
      "Epoch 15/76, Train Loss: 1.1058, Test Loss: 1.2708\n",
      "Epoch 16/76, Train Loss: 1.0685, Test Loss: 1.2699\n",
      "Epoch 17/76, Train Loss: 1.0463, Test Loss: 1.2778\n",
      "Epoch 18/76, Train Loss: 1.0605, Test Loss: 1.2681\n",
      "Epoch 19/76, Train Loss: 1.0356, Test Loss: 1.2651\n",
      "Epoch 20/76, Train Loss: 1.0357, Test Loss: 1.2708\n",
      "Epoch 21/76, Train Loss: 1.0253, Test Loss: 1.2694\n",
      "Epoch 22/76, Train Loss: 1.0349, Test Loss: 1.2725\n",
      "Epoch 23/76, Train Loss: 1.0024, Test Loss: 1.2716\n",
      "Epoch 24/76, Train Loss: 1.0484, Test Loss: 1.2704\n",
      "Epoch 25/76, Train Loss: 1.0205, Test Loss: 1.2666\n",
      "Epoch 26/76, Train Loss: 1.0390, Test Loss: 1.2706\n",
      "Epoch 27/76, Train Loss: 0.9900, Test Loss: 1.2726\n",
      "Epoch 28/76, Train Loss: 1.0364, Test Loss: 1.2707\n",
      "Epoch 29/76, Train Loss: 1.0162, Test Loss: 1.2733\n",
      "Epoch 30/76, Train Loss: 0.9922, Test Loss: 1.2745\n",
      "Epoch 31/76, Train Loss: 1.0319, Test Loss: 1.2711\n",
      "Epoch 32/76, Train Loss: 1.0093, Test Loss: 1.2728\n",
      "Epoch 33/76, Train Loss: 1.0469, Test Loss: 1.2747\n",
      "Epoch 34/76, Train Loss: 0.9789, Test Loss: 1.2707\n",
      "Epoch 35/76, Train Loss: 1.0139, Test Loss: 1.2732\n",
      "Epoch 36/76, Train Loss: 0.9862, Test Loss: 1.2761\n",
      "Epoch 37/76, Train Loss: 1.0041, Test Loss: 1.2735\n",
      "Epoch 38/76, Train Loss: 0.9846, Test Loss: 1.2767\n",
      "Epoch 39/76, Train Loss: 0.9833, Test Loss: 1.2757\n",
      "Epoch 40/76, Train Loss: 0.9580, Test Loss: 1.2809\n",
      "Epoch 41/76, Train Loss: 0.9631, Test Loss: 1.2793\n",
      "Epoch 42/76, Train Loss: 0.9883, Test Loss: 1.2783\n",
      "Epoch 43/76, Train Loss: 1.0047, Test Loss: 1.2781\n",
      "Epoch 44/76, Train Loss: 0.9888, Test Loss: 1.2773\n",
      "Epoch 45/76, Train Loss: 0.9921, Test Loss: 1.2797\n",
      "Epoch 46/76, Train Loss: 1.0026, Test Loss: 1.2746\n",
      "Epoch 47/76, Train Loss: 0.9675, Test Loss: 1.2786\n",
      "Epoch 48/76, Train Loss: 0.9569, Test Loss: 1.2790\n",
      "Epoch 49/76, Train Loss: 0.9815, Test Loss: 1.2800\n",
      "Epoch 50/76, Train Loss: 0.9617, Test Loss: 1.2838\n",
      "Epoch 51/76, Train Loss: 0.9751, Test Loss: 1.2881\n",
      "Epoch 52/76, Train Loss: 0.9852, Test Loss: 1.2821\n",
      "Epoch 53/76, Train Loss: 0.9576, Test Loss: 1.2879\n",
      "Epoch 54/76, Train Loss: 0.9813, Test Loss: 1.2837\n",
      "Epoch 55/76, Train Loss: 0.9505, Test Loss: 1.2943\n",
      "Epoch 56/76, Train Loss: 0.9568, Test Loss: 1.2887\n",
      "Epoch 57/76, Train Loss: 0.9779, Test Loss: 1.2879\n",
      "Epoch 58/76, Train Loss: 0.9676, Test Loss: 1.2832\n",
      "Epoch 59/76, Train Loss: 0.9325, Test Loss: 1.2881\n",
      "Epoch 60/76, Train Loss: 0.9345, Test Loss: 1.2879\n",
      "Epoch 61/76, Train Loss: 0.9652, Test Loss: 1.2898\n",
      "Epoch 62/76, Train Loss: 0.9436, Test Loss: 1.2887\n",
      "Epoch 63/76, Train Loss: 0.9370, Test Loss: 1.2981\n",
      "Epoch 64/76, Train Loss: 0.9520, Test Loss: 1.2934\n",
      "Epoch 65/76, Train Loss: 0.9511, Test Loss: 1.2911\n",
      "Epoch 66/76, Train Loss: 0.9191, Test Loss: 1.2925\n",
      "Epoch 67/76, Train Loss: 0.9317, Test Loss: 1.2899\n",
      "Epoch 68/76, Train Loss: 0.9305, Test Loss: 1.2946\n",
      "Epoch 69/76, Train Loss: 0.9211, Test Loss: 1.2914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:59:54,834] Trial 117 finished with value: 1.297646905694689 and parameters: {'num_hidden_layers': 1, 'layer_0_size': 206, 'dropout_rate': 0.3752556007718164, 'learning_rate': 2.752401425411436e-05, 'batch_size': 32, 'epochs': 76}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/76, Train Loss: 0.9532, Test Loss: 1.2906\n",
      "Epoch 71/76, Train Loss: 0.9131, Test Loss: 1.2967\n",
      "Epoch 72/76, Train Loss: 0.8916, Test Loss: 1.2909\n",
      "Epoch 73/76, Train Loss: 0.9230, Test Loss: 1.2969\n",
      "Epoch 74/76, Train Loss: 0.9305, Test Loss: 1.2945\n",
      "Epoch 75/76, Train Loss: 0.9156, Test Loss: 1.3006\n",
      "Epoch 76/76, Train Loss: 0.9230, Test Loss: 1.2976\n",
      "Epoch 1/72, Train Loss: 1.2177, Test Loss: 1.0004\n",
      "Epoch 2/72, Train Loss: 1.1830, Test Loss: 0.9961\n",
      "Epoch 3/72, Train Loss: 1.2192, Test Loss: 0.9925\n",
      "Epoch 4/72, Train Loss: 1.2143, Test Loss: 0.9913\n",
      "Epoch 5/72, Train Loss: 1.2450, Test Loss: 0.9917\n",
      "Epoch 6/72, Train Loss: 1.1398, Test Loss: 0.9946\n",
      "Epoch 7/72, Train Loss: 1.1657, Test Loss: 0.9975\n",
      "Epoch 8/72, Train Loss: 1.2176, Test Loss: 1.0025\n",
      "Epoch 9/72, Train Loss: 1.1447, Test Loss: 1.0069\n",
      "Epoch 10/72, Train Loss: 1.1627, Test Loss: 1.0090\n",
      "Epoch 11/72, Train Loss: 1.1562, Test Loss: 1.0118\n",
      "Epoch 12/72, Train Loss: 1.1248, Test Loss: 1.0140\n",
      "Epoch 13/72, Train Loss: 1.1132, Test Loss: 1.0135\n",
      "Epoch 14/72, Train Loss: 1.1383, Test Loss: 1.0162\n",
      "Epoch 15/72, Train Loss: 1.0826, Test Loss: 1.0160\n",
      "Epoch 16/72, Train Loss: 1.1121, Test Loss: 1.0163\n",
      "Epoch 17/72, Train Loss: 1.1551, Test Loss: 1.0159\n",
      "Epoch 18/72, Train Loss: 1.0997, Test Loss: 1.0175\n",
      "Epoch 19/72, Train Loss: 1.1462, Test Loss: 1.0193\n",
      "Epoch 20/72, Train Loss: 1.1124, Test Loss: 1.0215\n",
      "Epoch 21/72, Train Loss: 1.1200, Test Loss: 1.0221\n",
      "Epoch 22/72, Train Loss: 1.1459, Test Loss: 1.0240\n",
      "Epoch 23/72, Train Loss: 1.1410, Test Loss: 1.0223\n",
      "Epoch 24/72, Train Loss: 1.1023, Test Loss: 1.0193\n",
      "Epoch 25/72, Train Loss: 1.1049, Test Loss: 1.0194\n",
      "Epoch 26/72, Train Loss: 1.1084, Test Loss: 1.0201\n",
      "Epoch 27/72, Train Loss: 1.1226, Test Loss: 1.0208\n",
      "Epoch 28/72, Train Loss: 1.1237, Test Loss: 1.0190\n",
      "Epoch 29/72, Train Loss: 1.1348, Test Loss: 1.0194\n",
      "Epoch 30/72, Train Loss: 1.1183, Test Loss: 1.0176\n",
      "Epoch 31/72, Train Loss: 1.1273, Test Loss: 1.0181\n",
      "Epoch 32/72, Train Loss: 1.0983, Test Loss: 1.0185\n",
      "Epoch 33/72, Train Loss: 1.1215, Test Loss: 1.0193\n",
      "Epoch 34/72, Train Loss: 1.0918, Test Loss: 1.0187\n",
      "Epoch 35/72, Train Loss: 1.1139, Test Loss: 1.0205\n",
      "Epoch 36/72, Train Loss: 1.1110, Test Loss: 1.0171\n",
      "Epoch 37/72, Train Loss: 1.0831, Test Loss: 1.0138\n",
      "Epoch 38/72, Train Loss: 1.1362, Test Loss: 1.0159\n",
      "Epoch 39/72, Train Loss: 1.0956, Test Loss: 1.0182\n",
      "Epoch 40/72, Train Loss: 1.1423, Test Loss: 1.0195\n",
      "Epoch 41/72, Train Loss: 1.1135, Test Loss: 1.0188\n",
      "Epoch 42/72, Train Loss: 1.0848, Test Loss: 1.0192\n",
      "Epoch 43/72, Train Loss: 1.0869, Test Loss: 1.0186\n",
      "Epoch 44/72, Train Loss: 1.0650, Test Loss: 1.0160\n",
      "Epoch 45/72, Train Loss: 1.1177, Test Loss: 1.0183\n",
      "Epoch 46/72, Train Loss: 1.1173, Test Loss: 1.0149\n",
      "Epoch 47/72, Train Loss: 1.0988, Test Loss: 1.0133\n",
      "Epoch 48/72, Train Loss: 1.0854, Test Loss: 1.0157\n",
      "Epoch 49/72, Train Loss: 1.0618, Test Loss: 1.0149\n",
      "Epoch 50/72, Train Loss: 1.0870, Test Loss: 1.0141\n",
      "Epoch 51/72, Train Loss: 1.1099, Test Loss: 1.0130\n",
      "Epoch 52/72, Train Loss: 1.0717, Test Loss: 1.0148\n",
      "Epoch 53/72, Train Loss: 1.0867, Test Loss: 1.0126\n",
      "Epoch 54/72, Train Loss: 1.0786, Test Loss: 1.0145\n",
      "Epoch 55/72, Train Loss: 1.1178, Test Loss: 1.0149\n",
      "Epoch 56/72, Train Loss: 1.0828, Test Loss: 1.0162\n",
      "Epoch 57/72, Train Loss: 1.1232, Test Loss: 1.0165\n",
      "Epoch 58/72, Train Loss: 1.0936, Test Loss: 1.0146\n",
      "Epoch 59/72, Train Loss: 1.0640, Test Loss: 1.0128\n",
      "Epoch 60/72, Train Loss: 1.0822, Test Loss: 1.0122\n",
      "Epoch 61/72, Train Loss: 1.0621, Test Loss: 1.0128\n",
      "Epoch 62/72, Train Loss: 1.1029, Test Loss: 1.0118\n",
      "Epoch 63/72, Train Loss: 1.1039, Test Loss: 1.0137\n",
      "Epoch 64/72, Train Loss: 1.0919, Test Loss: 1.0135\n",
      "Epoch 65/72, Train Loss: 1.0461, Test Loss: 1.0137\n",
      "Epoch 66/72, Train Loss: 1.1108, Test Loss: 1.0127\n",
      "Epoch 67/72, Train Loss: 1.0463, Test Loss: 1.0140\n",
      "Epoch 68/72, Train Loss: 1.0997, Test Loss: 1.0126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 11:59:59,335] Trial 118 finished with value: 1.0090546607971191 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 168, 'layer_1_size': 234, 'layer_2_size': 229, 'layer_3_size': 185, 'layer_4_size': 237, 'layer_5_size': 236, 'layer_6_size': 51, 'layer_7_size': 144, 'layer_8_size': 160, 'layer_9_size': 37, 'layer_10_size': 137, 'layer_11_size': 112, 'layer_12_size': 61, 'dropout_rate': 0.2832724290549699, 'learning_rate': 5.0241491428646836e-05, 'batch_size': 256, 'epochs': 72}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/72, Train Loss: 1.0664, Test Loss: 1.0136\n",
      "Epoch 70/72, Train Loss: 1.0888, Test Loss: 1.0101\n",
      "Epoch 71/72, Train Loss: 1.0824, Test Loss: 1.0104\n",
      "Epoch 72/72, Train Loss: 1.0694, Test Loss: 1.0091\n",
      "Epoch 1/19, Train Loss: 1.1497, Test Loss: 0.9205\n",
      "Epoch 2/19, Train Loss: 1.1156, Test Loss: 0.9178\n",
      "Epoch 3/19, Train Loss: 1.1013, Test Loss: 0.9133\n",
      "Epoch 4/19, Train Loss: 1.1158, Test Loss: 0.9125\n",
      "Epoch 5/19, Train Loss: 1.0576, Test Loss: 0.9169\n",
      "Epoch 6/19, Train Loss: 1.0739, Test Loss: 0.9168\n",
      "Epoch 7/19, Train Loss: 1.0518, Test Loss: 0.9177\n",
      "Epoch 8/19, Train Loss: 1.0339, Test Loss: 0.9182\n",
      "Epoch 9/19, Train Loss: 1.0405, Test Loss: 0.9214\n",
      "Epoch 10/19, Train Loss: 1.0195, Test Loss: 0.9221\n",
      "Epoch 11/19, Train Loss: 1.0322, Test Loss: 0.9240\n",
      "Epoch 12/19, Train Loss: 1.0172, Test Loss: 0.9207\n",
      "Epoch 13/19, Train Loss: 1.0580, Test Loss: 0.9184\n",
      "Epoch 14/19, Train Loss: 1.0332, Test Loss: 0.9162\n",
      "Epoch 15/19, Train Loss: 1.0367, Test Loss: 0.9177\n",
      "Epoch 16/19, Train Loss: 1.0283, Test Loss: 0.9181\n",
      "Epoch 17/19, Train Loss: 1.0223, Test Loss: 0.9191\n",
      "Epoch 18/19, Train Loss: 1.0064, Test Loss: 0.9183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:00:00,809] Trial 119 finished with value: 0.9156080186367035 and parameters: {'num_hidden_layers': 10, 'layer_0_size': 248, 'layer_1_size': 185, 'layer_2_size': 218, 'layer_3_size': 113, 'layer_4_size': 128, 'layer_5_size': 248, 'layer_6_size': 232, 'layer_7_size': 207, 'layer_8_size': 117, 'layer_9_size': 236, 'dropout_rate': 0.25533113008028735, 'learning_rate': 0.00021031328035454676, 'batch_size': 128, 'epochs': 19}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/19, Train Loss: 1.0194, Test Loss: 0.9156\n",
      "Epoch 1/64, Train Loss: 1.2903, Test Loss: 1.1485\n",
      "Epoch 2/64, Train Loss: 1.2412, Test Loss: 1.1368\n",
      "Epoch 3/64, Train Loss: 1.2614, Test Loss: 1.1016\n",
      "Epoch 4/64, Train Loss: 1.2162, Test Loss: 1.1128\n",
      "Epoch 5/64, Train Loss: 1.2322, Test Loss: 1.0986\n",
      "Epoch 6/64, Train Loss: 1.2469, Test Loss: 1.1080\n",
      "Epoch 7/64, Train Loss: 1.1750, Test Loss: 1.1294\n",
      "Epoch 8/64, Train Loss: 1.2312, Test Loss: 1.1227\n",
      "Epoch 9/64, Train Loss: 1.1660, Test Loss: 1.1011\n",
      "Epoch 10/64, Train Loss: 1.1712, Test Loss: 1.1184\n",
      "Epoch 11/64, Train Loss: 1.1640, Test Loss: 1.1076\n",
      "Epoch 12/64, Train Loss: 1.2334, Test Loss: 1.0948\n",
      "Epoch 13/64, Train Loss: 1.1617, Test Loss: 1.0860\n",
      "Epoch 14/64, Train Loss: 1.1631, Test Loss: 1.0957\n",
      "Epoch 15/64, Train Loss: 1.2066, Test Loss: 1.1046\n",
      "Epoch 16/64, Train Loss: 1.2009, Test Loss: 1.1129\n",
      "Epoch 17/64, Train Loss: 1.1963, Test Loss: 1.0934\n",
      "Epoch 18/64, Train Loss: 1.1677, Test Loss: 1.0915\n",
      "Epoch 19/64, Train Loss: 1.1536, Test Loss: 1.0943\n",
      "Epoch 20/64, Train Loss: 1.1761, Test Loss: 1.0935\n",
      "Epoch 21/64, Train Loss: 1.1591, Test Loss: 1.0989\n",
      "Epoch 22/64, Train Loss: 1.2067, Test Loss: 1.1003\n",
      "Epoch 23/64, Train Loss: 1.1820, Test Loss: 1.1083\n",
      "Epoch 24/64, Train Loss: 1.1718, Test Loss: 1.0939\n",
      "Epoch 25/64, Train Loss: 1.1805, Test Loss: 1.0955\n",
      "Epoch 26/64, Train Loss: 1.1790, Test Loss: 1.1065\n",
      "Epoch 27/64, Train Loss: 1.1386, Test Loss: 1.1104\n",
      "Epoch 28/64, Train Loss: 1.1781, Test Loss: 1.1246\n",
      "Epoch 29/64, Train Loss: 1.1470, Test Loss: 1.1033\n",
      "Epoch 30/64, Train Loss: 1.1757, Test Loss: 1.1034\n",
      "Epoch 31/64, Train Loss: 1.1731, Test Loss: 1.0950\n",
      "Epoch 32/64, Train Loss: 1.1617, Test Loss: 1.0997\n",
      "Epoch 33/64, Train Loss: 1.1654, Test Loss: 1.0995\n",
      "Epoch 34/64, Train Loss: 1.2011, Test Loss: 1.0908\n",
      "Epoch 35/64, Train Loss: 1.1437, Test Loss: 1.0884\n",
      "Epoch 36/64, Train Loss: 1.1645, Test Loss: 1.0890\n",
      "Epoch 37/64, Train Loss: 1.1498, Test Loss: 1.0864\n",
      "Epoch 38/64, Train Loss: 1.1170, Test Loss: 1.0925\n",
      "Epoch 39/64, Train Loss: 1.1497, Test Loss: 1.0874\n",
      "Epoch 40/64, Train Loss: 1.1751, Test Loss: 1.0838\n",
      "Epoch 41/64, Train Loss: 1.1472, Test Loss: 1.0863\n",
      "Epoch 42/64, Train Loss: 1.1541, Test Loss: 1.0855\n",
      "Epoch 43/64, Train Loss: 1.1870, Test Loss: 1.0973\n",
      "Epoch 44/64, Train Loss: 1.1432, Test Loss: 1.1008\n",
      "Epoch 45/64, Train Loss: 1.1604, Test Loss: 1.1068\n",
      "Epoch 46/64, Train Loss: 1.1726, Test Loss: 1.0924\n",
      "Epoch 47/64, Train Loss: 1.1747, Test Loss: 1.0859\n",
      "Epoch 48/64, Train Loss: 1.1622, Test Loss: 1.0930\n",
      "Epoch 49/64, Train Loss: 1.1403, Test Loss: 1.0844\n",
      "Epoch 50/64, Train Loss: 1.1237, Test Loss: 1.0736\n",
      "Epoch 51/64, Train Loss: 1.1621, Test Loss: 1.0791\n",
      "Epoch 52/64, Train Loss: 1.1792, Test Loss: 1.0851\n",
      "Epoch 53/64, Train Loss: 1.1680, Test Loss: 1.0971\n",
      "Epoch 54/64, Train Loss: 1.1613, Test Loss: 1.0947\n",
      "Epoch 55/64, Train Loss: 1.1370, Test Loss: 1.0892\n",
      "Epoch 56/64, Train Loss: 1.1674, Test Loss: 1.0883\n",
      "Epoch 57/64, Train Loss: 1.1665, Test Loss: 1.0783\n",
      "Epoch 58/64, Train Loss: 1.1410, Test Loss: 1.0852\n",
      "Epoch 59/64, Train Loss: 1.1231, Test Loss: 1.1002\n",
      "Epoch 60/64, Train Loss: 1.1501, Test Loss: 1.0832\n",
      "Epoch 61/64, Train Loss: 1.1578, Test Loss: 1.0899\n",
      "Epoch 62/64, Train Loss: 1.1476, Test Loss: 1.0849\n",
      "Epoch 63/64, Train Loss: 1.1622, Test Loss: 1.0808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:00:13,465] Trial 120 finished with value: 1.0830854432923454 and parameters: {'num_hidden_layers': 12, 'layer_0_size': 218, 'layer_1_size': 75, 'layer_2_size': 109, 'layer_3_size': 75, 'layer_4_size': 221, 'layer_5_size': 166, 'layer_6_size': 36, 'layer_7_size': 229, 'layer_8_size': 59, 'layer_9_size': 69, 'layer_10_size': 170, 'layer_11_size': 188, 'dropout_rate': 0.19972372085137058, 'learning_rate': 8.915437703552782e-05, 'batch_size': 32, 'epochs': 64}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/64, Train Loss: 1.1630, Test Loss: 1.0831\n",
      "Epoch 1/54, Train Loss: 1.1934, Test Loss: 0.8661\n",
      "Epoch 2/54, Train Loss: 1.1319, Test Loss: 0.8717\n",
      "Epoch 3/54, Train Loss: 1.1733, Test Loss: 0.8708\n",
      "Epoch 4/54, Train Loss: 1.0946, Test Loss: 0.8759\n",
      "Epoch 5/54, Train Loss: 1.1267, Test Loss: 0.8759\n",
      "Epoch 6/54, Train Loss: 1.1003, Test Loss: 0.8775\n",
      "Epoch 7/54, Train Loss: 1.0773, Test Loss: 0.8753\n",
      "Epoch 8/54, Train Loss: 1.1491, Test Loss: 0.8747\n",
      "Epoch 9/54, Train Loss: 1.0976, Test Loss: 0.8748\n",
      "Epoch 10/54, Train Loss: 1.1025, Test Loss: 0.8788\n",
      "Epoch 11/54, Train Loss: 1.0985, Test Loss: 0.8725\n",
      "Epoch 12/54, Train Loss: 1.1179, Test Loss: 0.8715\n",
      "Epoch 13/54, Train Loss: 1.1188, Test Loss: 0.8683\n",
      "Epoch 14/54, Train Loss: 1.1153, Test Loss: 0.8685\n",
      "Epoch 15/54, Train Loss: 1.0640, Test Loss: 0.8686\n",
      "Epoch 16/54, Train Loss: 1.0691, Test Loss: 0.8701\n",
      "Epoch 17/54, Train Loss: 1.0660, Test Loss: 0.8684\n",
      "Epoch 18/54, Train Loss: 1.0618, Test Loss: 0.8637\n",
      "Epoch 19/54, Train Loss: 1.0644, Test Loss: 0.8636\n",
      "Epoch 20/54, Train Loss: 1.0355, Test Loss: 0.8626\n",
      "Epoch 21/54, Train Loss: 1.0760, Test Loss: 0.8635\n",
      "Epoch 22/54, Train Loss: 1.0771, Test Loss: 0.8671\n",
      "Epoch 23/54, Train Loss: 1.0876, Test Loss: 0.8656\n",
      "Epoch 24/54, Train Loss: 1.0593, Test Loss: 0.8678\n",
      "Epoch 25/54, Train Loss: 1.0512, Test Loss: 0.8673\n",
      "Epoch 26/54, Train Loss: 1.0708, Test Loss: 0.8607\n",
      "Epoch 27/54, Train Loss: 1.0432, Test Loss: 0.8651\n",
      "Epoch 28/54, Train Loss: 1.0653, Test Loss: 0.8657\n",
      "Epoch 29/54, Train Loss: 1.0609, Test Loss: 0.8671\n",
      "Epoch 30/54, Train Loss: 1.0462, Test Loss: 0.8633\n",
      "Epoch 31/54, Train Loss: 1.0691, Test Loss: 0.8608\n",
      "Epoch 32/54, Train Loss: 1.0730, Test Loss: 0.8620\n",
      "Epoch 33/54, Train Loss: 1.0576, Test Loss: 0.8592\n",
      "Epoch 34/54, Train Loss: 1.0526, Test Loss: 0.8637\n",
      "Epoch 35/54, Train Loss: 1.0333, Test Loss: 0.8607\n",
      "Epoch 36/54, Train Loss: 1.0442, Test Loss: 0.8598\n",
      "Epoch 37/54, Train Loss: 1.0543, Test Loss: 0.8613\n",
      "Epoch 38/54, Train Loss: 1.0283, Test Loss: 0.8621\n",
      "Epoch 39/54, Train Loss: 1.0440, Test Loss: 0.8634\n",
      "Epoch 40/54, Train Loss: 1.0272, Test Loss: 0.8625\n",
      "Epoch 41/54, Train Loss: 1.0720, Test Loss: 0.8626\n",
      "Epoch 42/54, Train Loss: 1.0571, Test Loss: 0.8633\n",
      "Epoch 43/54, Train Loss: 1.0411, Test Loss: 0.8636\n",
      "Epoch 44/54, Train Loss: 1.0572, Test Loss: 0.8634\n",
      "Epoch 45/54, Train Loss: 1.0463, Test Loss: 0.8631\n",
      "Epoch 46/54, Train Loss: 1.0194, Test Loss: 0.8665\n",
      "Epoch 47/54, Train Loss: 1.0359, Test Loss: 0.8659\n",
      "Epoch 48/54, Train Loss: 1.0499, Test Loss: 0.8667\n",
      "Epoch 49/54, Train Loss: 1.0544, Test Loss: 0.8660\n",
      "Epoch 50/54, Train Loss: 1.0231, Test Loss: 0.8679\n",
      "Epoch 51/54, Train Loss: 1.0284, Test Loss: 0.8684\n",
      "Epoch 52/54, Train Loss: 1.0466, Test Loss: 0.8687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:00:20,993] Trial 121 finished with value: 0.8669761344790459 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 79, 'layer_1_size': 141, 'layer_2_size': 153, 'layer_3_size': 159, 'layer_4_size': 57, 'layer_5_size': 224, 'layer_6_size': 204, 'layer_7_size': 96, 'layer_8_size': 167, 'layer_9_size': 82, 'layer_10_size': 213, 'layer_11_size': 182, 'layer_12_size': 75, 'layer_13_size': 170, 'dropout_rate': 0.3238968338344068, 'learning_rate': 0.00024476999302506814, 'batch_size': 64, 'epochs': 54}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/54, Train Loss: 1.0229, Test Loss: 0.8688\n",
      "Epoch 54/54, Train Loss: 1.0412, Test Loss: 0.8670\n",
      "Epoch 1/61, Train Loss: 1.2388, Test Loss: 1.0881\n",
      "Epoch 2/61, Train Loss: 1.1847, Test Loss: 1.0956\n",
      "Epoch 3/61, Train Loss: 1.1537, Test Loss: 1.1016\n",
      "Epoch 4/61, Train Loss: 1.1333, Test Loss: 1.1006\n",
      "Epoch 5/61, Train Loss: 1.1256, Test Loss: 1.0975\n",
      "Epoch 6/61, Train Loss: 1.1094, Test Loss: 1.0963\n",
      "Epoch 7/61, Train Loss: 1.1115, Test Loss: 1.0952\n",
      "Epoch 8/61, Train Loss: 1.1221, Test Loss: 1.0886\n",
      "Epoch 9/61, Train Loss: 1.0972, Test Loss: 1.0885\n",
      "Epoch 10/61, Train Loss: 1.0849, Test Loss: 1.0939\n",
      "Epoch 11/61, Train Loss: 1.1019, Test Loss: 1.0962\n",
      "Epoch 12/61, Train Loss: 1.0864, Test Loss: 1.0928\n",
      "Epoch 13/61, Train Loss: 1.0989, Test Loss: 1.0911\n",
      "Epoch 14/61, Train Loss: 1.0619, Test Loss: 1.0937\n",
      "Epoch 15/61, Train Loss: 1.0677, Test Loss: 1.0907\n",
      "Epoch 16/61, Train Loss: 1.0736, Test Loss: 1.0978\n",
      "Epoch 17/61, Train Loss: 1.0492, Test Loss: 1.0953\n",
      "Epoch 18/61, Train Loss: 1.0432, Test Loss: 1.0988\n",
      "Epoch 19/61, Train Loss: 1.0606, Test Loss: 1.0950\n",
      "Epoch 20/61, Train Loss: 1.0765, Test Loss: 1.0931\n",
      "Epoch 21/61, Train Loss: 1.0709, Test Loss: 1.0921\n",
      "Epoch 22/61, Train Loss: 1.0550, Test Loss: 1.0923\n",
      "Epoch 23/61, Train Loss: 1.0820, Test Loss: 1.0914\n",
      "Epoch 24/61, Train Loss: 1.0551, Test Loss: 1.0876\n",
      "Epoch 25/61, Train Loss: 1.0688, Test Loss: 1.0836\n",
      "Epoch 26/61, Train Loss: 1.0885, Test Loss: 1.0912\n",
      "Epoch 27/61, Train Loss: 1.0654, Test Loss: 1.0942\n",
      "Epoch 28/61, Train Loss: 1.0202, Test Loss: 1.0947\n",
      "Epoch 29/61, Train Loss: 1.0478, Test Loss: 1.0988\n",
      "Epoch 30/61, Train Loss: 1.0466, Test Loss: 1.1043\n",
      "Epoch 31/61, Train Loss: 1.0310, Test Loss: 1.1141\n",
      "Epoch 32/61, Train Loss: 1.0580, Test Loss: 1.1068\n",
      "Epoch 33/61, Train Loss: 1.0525, Test Loss: 1.1109\n",
      "Epoch 34/61, Train Loss: 1.0386, Test Loss: 1.1084\n",
      "Epoch 35/61, Train Loss: 1.0416, Test Loss: 1.1088\n",
      "Epoch 36/61, Train Loss: 1.0198, Test Loss: 1.1042\n",
      "Epoch 37/61, Train Loss: 1.0417, Test Loss: 1.1028\n",
      "Epoch 38/61, Train Loss: 1.0424, Test Loss: 1.1043\n",
      "Epoch 39/61, Train Loss: 1.0565, Test Loss: 1.1013\n",
      "Epoch 40/61, Train Loss: 1.0252, Test Loss: 1.1030\n",
      "Epoch 41/61, Train Loss: 1.0284, Test Loss: 1.1011\n",
      "Epoch 42/61, Train Loss: 1.0300, Test Loss: 1.1032\n",
      "Epoch 43/61, Train Loss: 1.0284, Test Loss: 1.1069\n",
      "Epoch 44/61, Train Loss: 1.0287, Test Loss: 1.1093\n",
      "Epoch 45/61, Train Loss: 1.0417, Test Loss: 1.1065\n",
      "Epoch 46/61, Train Loss: 1.0263, Test Loss: 1.1052\n",
      "Epoch 47/61, Train Loss: 1.0182, Test Loss: 1.1013\n",
      "Epoch 48/61, Train Loss: 1.0079, Test Loss: 1.0972\n",
      "Epoch 49/61, Train Loss: 1.0152, Test Loss: 1.0977\n",
      "Epoch 50/61, Train Loss: 1.0332, Test Loss: 1.0999\n",
      "Epoch 51/61, Train Loss: 1.0374, Test Loss: 1.1045\n",
      "Epoch 52/61, Train Loss: 1.0382, Test Loss: 1.1121\n",
      "Epoch 53/61, Train Loss: 1.0333, Test Loss: 1.1122\n",
      "Epoch 54/61, Train Loss: 1.0353, Test Loss: 1.1077\n",
      "Epoch 55/61, Train Loss: 1.0372, Test Loss: 1.1042\n",
      "Epoch 56/61, Train Loss: 1.0208, Test Loss: 1.1053\n",
      "Epoch 57/61, Train Loss: 1.0185, Test Loss: 1.1037\n",
      "Epoch 58/61, Train Loss: 1.0442, Test Loss: 1.1076\n",
      "Epoch 59/61, Train Loss: 1.0019, Test Loss: 1.1107\n",
      "Epoch 60/61, Train Loss: 1.0028, Test Loss: 1.1049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:00:29,375] Trial 122 finished with value: 1.0999368727207184 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 122, 'layer_1_size': 126, 'layer_2_size': 146, 'layer_3_size': 163, 'layer_4_size': 201, 'layer_5_size': 221, 'layer_6_size': 223, 'layer_7_size': 105, 'layer_8_size': 168, 'layer_9_size': 80, 'layer_10_size': 215, 'layer_11_size': 182, 'layer_12_size': 69, 'dropout_rate': 0.3240488828324261, 'learning_rate': 0.00028480498528578904, 'batch_size': 64, 'epochs': 61}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/61, Train Loss: 1.0235, Test Loss: 1.0999\n",
      "Epoch 1/57, Train Loss: 1.1946, Test Loss: 0.8633\n",
      "Epoch 2/57, Train Loss: 1.1283, Test Loss: 0.8687\n",
      "Epoch 3/57, Train Loss: 1.0989, Test Loss: 0.8744\n",
      "Epoch 4/57, Train Loss: 1.0752, Test Loss: 0.8773\n",
      "Epoch 5/57, Train Loss: 1.0992, Test Loss: 0.8737\n",
      "Epoch 6/57, Train Loss: 1.0515, Test Loss: 0.8707\n",
      "Epoch 7/57, Train Loss: 1.0377, Test Loss: 0.8671\n",
      "Epoch 8/57, Train Loss: 1.0721, Test Loss: 0.8652\n",
      "Epoch 9/57, Train Loss: 1.0537, Test Loss: 0.8642\n",
      "Epoch 10/57, Train Loss: 1.0431, Test Loss: 0.8615\n",
      "Epoch 11/57, Train Loss: 1.0341, Test Loss: 0.8605\n",
      "Epoch 12/57, Train Loss: 1.0546, Test Loss: 0.8591\n",
      "Epoch 13/57, Train Loss: 1.0077, Test Loss: 0.8592\n",
      "Epoch 14/57, Train Loss: 1.0279, Test Loss: 0.8580\n",
      "Epoch 15/57, Train Loss: 1.0040, Test Loss: 0.8577\n",
      "Epoch 16/57, Train Loss: 1.0507, Test Loss: 0.8591\n",
      "Epoch 17/57, Train Loss: 1.0023, Test Loss: 0.8595\n",
      "Epoch 18/57, Train Loss: 1.0416, Test Loss: 0.8614\n",
      "Epoch 19/57, Train Loss: 1.0843, Test Loss: 0.8604\n",
      "Epoch 20/57, Train Loss: 1.0530, Test Loss: 0.8605\n",
      "Epoch 21/57, Train Loss: 1.0139, Test Loss: 0.8597\n",
      "Epoch 22/57, Train Loss: 1.0181, Test Loss: 0.8593\n",
      "Epoch 23/57, Train Loss: 1.0408, Test Loss: 0.8607\n",
      "Epoch 24/57, Train Loss: 1.0194, Test Loss: 0.8622\n",
      "Epoch 25/57, Train Loss: 1.0091, Test Loss: 0.8635\n",
      "Epoch 26/57, Train Loss: 1.0332, Test Loss: 0.8636\n",
      "Epoch 27/57, Train Loss: 1.0103, Test Loss: 0.8625\n",
      "Epoch 28/57, Train Loss: 1.0357, Test Loss: 0.8615\n",
      "Epoch 29/57, Train Loss: 1.0287, Test Loss: 0.8618\n",
      "Epoch 30/57, Train Loss: 1.0223, Test Loss: 0.8605\n",
      "Epoch 31/57, Train Loss: 0.9815, Test Loss: 0.8616\n",
      "Epoch 32/57, Train Loss: 0.9989, Test Loss: 0.8608\n",
      "Epoch 33/57, Train Loss: 1.0334, Test Loss: 0.8621\n",
      "Epoch 34/57, Train Loss: 0.9847, Test Loss: 0.8638\n",
      "Epoch 35/57, Train Loss: 1.0223, Test Loss: 0.8625\n",
      "Epoch 36/57, Train Loss: 1.0122, Test Loss: 0.8630\n",
      "Epoch 37/57, Train Loss: 0.9801, Test Loss: 0.8624\n",
      "Epoch 38/57, Train Loss: 1.0225, Test Loss: 0.8634\n",
      "Epoch 39/57, Train Loss: 1.0055, Test Loss: 0.8629\n",
      "Epoch 40/57, Train Loss: 0.9834, Test Loss: 0.8632\n",
      "Epoch 41/57, Train Loss: 1.0281, Test Loss: 0.8634\n",
      "Epoch 42/57, Train Loss: 1.0040, Test Loss: 0.8640\n",
      "Epoch 43/57, Train Loss: 0.9867, Test Loss: 0.8646\n",
      "Epoch 44/57, Train Loss: 1.0052, Test Loss: 0.8650\n",
      "Epoch 45/57, Train Loss: 1.0142, Test Loss: 0.8644\n",
      "Epoch 46/57, Train Loss: 0.9990, Test Loss: 0.8656\n",
      "Epoch 47/57, Train Loss: 1.0091, Test Loss: 0.8662\n",
      "Epoch 48/57, Train Loss: 0.9887, Test Loss: 0.8667\n",
      "Epoch 49/57, Train Loss: 1.0045, Test Loss: 0.8665\n",
      "Epoch 50/57, Train Loss: 0.9986, Test Loss: 0.8673\n",
      "Epoch 51/57, Train Loss: 1.0004, Test Loss: 0.8670\n",
      "Epoch 52/57, Train Loss: 0.9809, Test Loss: 0.8663\n",
      "Epoch 53/57, Train Loss: 0.9851, Test Loss: 0.8658\n",
      "Epoch 54/57, Train Loss: 0.9623, Test Loss: 0.8657\n",
      "Epoch 55/57, Train Loss: 0.9895, Test Loss: 0.8665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:00:33,311] Trial 123 finished with value: 0.8684841990470886 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 148, 'layer_1_size': 135, 'layer_2_size': 249, 'layer_3_size': 139, 'layer_4_size': 60, 'layer_5_size': 205, 'layer_6_size': 196, 'layer_7_size': 97, 'layer_8_size': 157, 'layer_9_size': 92, 'layer_10_size': 234, 'layer_11_size': 132, 'layer_12_size': 78, 'layer_13_size': 178, 'dropout_rate': 0.2935593857168057, 'learning_rate': 0.00024065100611487218, 'batch_size': 256, 'epochs': 57}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/57, Train Loss: 0.9984, Test Loss: 0.8678\n",
      "Epoch 57/57, Train Loss: 0.9887, Test Loss: 0.8685\n",
      "Epoch 1/54, Train Loss: 1.2160, Test Loss: 1.0619\n",
      "Epoch 2/54, Train Loss: 1.2244, Test Loss: 1.0605\n",
      "Epoch 3/54, Train Loss: 1.2771, Test Loss: 1.0603\n",
      "Epoch 4/54, Train Loss: 1.1999, Test Loss: 1.0609\n",
      "Epoch 5/54, Train Loss: 1.1537, Test Loss: 1.0623\n",
      "Epoch 6/54, Train Loss: 1.1810, Test Loss: 1.0630\n",
      "Epoch 7/54, Train Loss: 1.1566, Test Loss: 1.0625\n",
      "Epoch 8/54, Train Loss: 1.1634, Test Loss: 1.0621\n",
      "Epoch 9/54, Train Loss: 1.1224, Test Loss: 1.0625\n",
      "Epoch 10/54, Train Loss: 1.1582, Test Loss: 1.0623\n",
      "Epoch 11/54, Train Loss: 1.1732, Test Loss: 1.0617\n",
      "Epoch 12/54, Train Loss: 1.1578, Test Loss: 1.0612\n",
      "Epoch 13/54, Train Loss: 1.1210, Test Loss: 1.0604\n",
      "Epoch 14/54, Train Loss: 1.1500, Test Loss: 1.0618\n",
      "Epoch 15/54, Train Loss: 1.1507, Test Loss: 1.0617\n",
      "Epoch 16/54, Train Loss: 1.0909, Test Loss: 1.0594\n",
      "Epoch 17/54, Train Loss: 1.1559, Test Loss: 1.0608\n",
      "Epoch 18/54, Train Loss: 1.1592, Test Loss: 1.0605\n",
      "Epoch 19/54, Train Loss: 1.1161, Test Loss: 1.0597\n",
      "Epoch 20/54, Train Loss: 1.0731, Test Loss: 1.0599\n",
      "Epoch 21/54, Train Loss: 1.1392, Test Loss: 1.0610\n",
      "Epoch 22/54, Train Loss: 1.0931, Test Loss: 1.0603\n",
      "Epoch 23/54, Train Loss: 1.1369, Test Loss: 1.0595\n",
      "Epoch 24/54, Train Loss: 1.1291, Test Loss: 1.0598\n",
      "Epoch 25/54, Train Loss: 1.0974, Test Loss: 1.0587\n",
      "Epoch 26/54, Train Loss: 1.0844, Test Loss: 1.0601\n",
      "Epoch 27/54, Train Loss: 1.0876, Test Loss: 1.0609\n",
      "Epoch 28/54, Train Loss: 1.1317, Test Loss: 1.0636\n",
      "Epoch 29/54, Train Loss: 1.1071, Test Loss: 1.0643\n",
      "Epoch 30/54, Train Loss: 1.0988, Test Loss: 1.0647\n",
      "Epoch 31/54, Train Loss: 1.1335, Test Loss: 1.0657\n",
      "Epoch 32/54, Train Loss: 1.1270, Test Loss: 1.0659\n",
      "Epoch 33/54, Train Loss: 1.0897, Test Loss: 1.0664\n",
      "Epoch 34/54, Train Loss: 1.0821, Test Loss: 1.0681\n",
      "Epoch 35/54, Train Loss: 1.1161, Test Loss: 1.0661\n",
      "Epoch 36/54, Train Loss: 1.1157, Test Loss: 1.0662\n",
      "Epoch 37/54, Train Loss: 1.0875, Test Loss: 1.0654\n",
      "Epoch 38/54, Train Loss: 1.0756, Test Loss: 1.0650\n",
      "Epoch 39/54, Train Loss: 1.0884, Test Loss: 1.0644\n",
      "Epoch 40/54, Train Loss: 1.0886, Test Loss: 1.0651\n",
      "Epoch 41/54, Train Loss: 1.0998, Test Loss: 1.0656\n",
      "Epoch 42/54, Train Loss: 1.0828, Test Loss: 1.0662\n",
      "Epoch 43/54, Train Loss: 1.0920, Test Loss: 1.0669\n",
      "Epoch 44/54, Train Loss: 1.0926, Test Loss: 1.0662\n",
      "Epoch 45/54, Train Loss: 1.1036, Test Loss: 1.0656\n",
      "Epoch 46/54, Train Loss: 1.0868, Test Loss: 1.0651\n",
      "Epoch 47/54, Train Loss: 1.0954, Test Loss: 1.0638\n",
      "Epoch 48/54, Train Loss: 1.0925, Test Loss: 1.0625\n",
      "Epoch 49/54, Train Loss: 1.0974, Test Loss: 1.0617\n",
      "Epoch 50/54, Train Loss: 1.0747, Test Loss: 1.0615\n",
      "Epoch 51/54, Train Loss: 1.1039, Test Loss: 1.0605\n",
      "Epoch 52/54, Train Loss: 1.0930, Test Loss: 1.0596\n",
      "Epoch 53/54, Train Loss: 1.0694, Test Loss: 1.0603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:00:37,162] Trial 124 finished with value: 1.061042308807373 and parameters: {'num_hidden_layers': 15, 'layer_0_size': 146, 'layer_1_size': 140, 'layer_2_size': 245, 'layer_3_size': 143, 'layer_4_size': 56, 'layer_5_size': 230, 'layer_6_size': 203, 'layer_7_size': 99, 'layer_8_size': 153, 'layer_9_size': 83, 'layer_10_size': 248, 'layer_11_size': 133, 'layer_12_size': 74, 'layer_13_size': 178, 'layer_14_size': 60, 'dropout_rate': 0.2965114868169438, 'learning_rate': 0.0002421024513701155, 'batch_size': 256, 'epochs': 54}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/54, Train Loss: 1.0778, Test Loss: 1.0610\n",
      "Epoch 1/57, Train Loss: 1.1898, Test Loss: 1.0542\n",
      "Epoch 2/57, Train Loss: 1.1902, Test Loss: 1.0531\n",
      "Epoch 3/57, Train Loss: 1.1520, Test Loss: 1.0532\n",
      "Epoch 4/57, Train Loss: 1.1005, Test Loss: 1.0552\n",
      "Epoch 5/57, Train Loss: 1.1276, Test Loss: 1.0575\n",
      "Epoch 6/57, Train Loss: 1.0777, Test Loss: 1.0606\n",
      "Epoch 7/57, Train Loss: 1.1480, Test Loss: 1.0642\n",
      "Epoch 8/57, Train Loss: 1.0414, Test Loss: 1.0654\n",
      "Epoch 9/57, Train Loss: 1.0542, Test Loss: 1.0645\n",
      "Epoch 10/57, Train Loss: 1.0263, Test Loss: 1.0625\n",
      "Epoch 11/57, Train Loss: 1.0415, Test Loss: 1.0590\n",
      "Epoch 12/57, Train Loss: 1.0484, Test Loss: 1.0572\n",
      "Epoch 13/57, Train Loss: 1.0553, Test Loss: 1.0576\n",
      "Epoch 14/57, Train Loss: 0.9902, Test Loss: 1.0559\n",
      "Epoch 15/57, Train Loss: 1.0102, Test Loss: 1.0574\n",
      "Epoch 16/57, Train Loss: 1.0304, Test Loss: 1.0560\n",
      "Epoch 17/57, Train Loss: 0.9905, Test Loss: 1.0560\n",
      "Epoch 18/57, Train Loss: 1.0249, Test Loss: 1.0536\n",
      "Epoch 19/57, Train Loss: 0.9892, Test Loss: 1.0522\n",
      "Epoch 20/57, Train Loss: 1.0080, Test Loss: 1.0528\n",
      "Epoch 21/57, Train Loss: 1.0131, Test Loss: 1.0535\n",
      "Epoch 22/57, Train Loss: 1.0121, Test Loss: 1.0550\n",
      "Epoch 23/57, Train Loss: 1.0032, Test Loss: 1.0544\n",
      "Epoch 24/57, Train Loss: 1.0169, Test Loss: 1.0552\n",
      "Epoch 25/57, Train Loss: 1.0046, Test Loss: 1.0550\n",
      "Epoch 26/57, Train Loss: 1.0295, Test Loss: 1.0577\n",
      "Epoch 27/57, Train Loss: 0.9916, Test Loss: 1.0581\n",
      "Epoch 28/57, Train Loss: 0.9897, Test Loss: 1.0586\n",
      "Epoch 29/57, Train Loss: 0.9656, Test Loss: 1.0558\n",
      "Epoch 30/57, Train Loss: 0.9642, Test Loss: 1.0572\n",
      "Epoch 31/57, Train Loss: 1.0083, Test Loss: 1.0580\n",
      "Epoch 32/57, Train Loss: 0.9768, Test Loss: 1.0578\n",
      "Epoch 33/57, Train Loss: 0.9876, Test Loss: 1.0577\n",
      "Epoch 34/57, Train Loss: 0.9974, Test Loss: 1.0568\n",
      "Epoch 35/57, Train Loss: 0.9891, Test Loss: 1.0565\n",
      "Epoch 36/57, Train Loss: 0.9861, Test Loss: 1.0554\n",
      "Epoch 37/57, Train Loss: 0.9947, Test Loss: 1.0549\n",
      "Epoch 38/57, Train Loss: 0.9832, Test Loss: 1.0544\n",
      "Epoch 39/57, Train Loss: 0.9898, Test Loss: 1.0546\n",
      "Epoch 40/57, Train Loss: 1.0071, Test Loss: 1.0540\n",
      "Epoch 41/57, Train Loss: 0.9813, Test Loss: 1.0539\n",
      "Epoch 42/57, Train Loss: 0.9700, Test Loss: 1.0549\n",
      "Epoch 43/57, Train Loss: 0.9892, Test Loss: 1.0550\n",
      "Epoch 44/57, Train Loss: 0.9861, Test Loss: 1.0547\n",
      "Epoch 45/57, Train Loss: 0.9921, Test Loss: 1.0538\n",
      "Epoch 46/57, Train Loss: 1.0074, Test Loss: 1.0533\n",
      "Epoch 47/57, Train Loss: 0.9506, Test Loss: 1.0531\n",
      "Epoch 48/57, Train Loss: 0.9908, Test Loss: 1.0528\n",
      "Epoch 49/57, Train Loss: 0.9851, Test Loss: 1.0528\n",
      "Epoch 50/57, Train Loss: 0.9843, Test Loss: 1.0527\n",
      "Epoch 51/57, Train Loss: 0.9922, Test Loss: 1.0521\n",
      "Epoch 52/57, Train Loss: 0.9721, Test Loss: 1.0517\n",
      "Epoch 53/57, Train Loss: 0.9609, Test Loss: 1.0518\n",
      "Epoch 54/57, Train Loss: 0.9780, Test Loss: 1.0517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:00:41,234] Trial 125 finished with value: 1.0520524978637695 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 150, 'layer_1_size': 134, 'layer_2_size': 239, 'layer_3_size': 157, 'layer_4_size': 61, 'layer_5_size': 239, 'layer_6_size': 194, 'layer_7_size': 250, 'layer_8_size': 159, 'layer_9_size': 100, 'layer_10_size': 234, 'layer_11_size': 157, 'layer_12_size': 50, 'layer_13_size': 164, 'dropout_rate': 0.30255607216467995, 'learning_rate': 0.0002519260692411787, 'batch_size': 256, 'epochs': 57}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/57, Train Loss: 0.9564, Test Loss: 1.0511\n",
      "Epoch 56/57, Train Loss: 0.9876, Test Loss: 1.0513\n",
      "Epoch 57/57, Train Loss: 0.9714, Test Loss: 1.0521\n",
      "Epoch 1/50, Train Loss: 1.4269, Test Loss: 1.1482\n",
      "Epoch 2/50, Train Loss: 1.3848, Test Loss: 1.1540\n",
      "Epoch 3/50, Train Loss: 1.2998, Test Loss: 1.1586\n",
      "Epoch 4/50, Train Loss: 1.3298, Test Loss: 1.1619\n",
      "Epoch 5/50, Train Loss: 1.2972, Test Loss: 1.1620\n",
      "Epoch 6/50, Train Loss: 1.2669, Test Loss: 1.1607\n",
      "Epoch 7/50, Train Loss: 1.2156, Test Loss: 1.1589\n",
      "Epoch 8/50, Train Loss: 1.2448, Test Loss: 1.1570\n",
      "Epoch 9/50, Train Loss: 1.2025, Test Loss: 1.1586\n",
      "Epoch 10/50, Train Loss: 1.2514, Test Loss: 1.1597\n",
      "Epoch 11/50, Train Loss: 1.2200, Test Loss: 1.1596\n",
      "Epoch 12/50, Train Loss: 1.2606, Test Loss: 1.1559\n",
      "Epoch 13/50, Train Loss: 1.1990, Test Loss: 1.1556\n",
      "Epoch 14/50, Train Loss: 1.2157, Test Loss: 1.1552\n",
      "Epoch 15/50, Train Loss: 1.1903, Test Loss: 1.1532\n",
      "Epoch 16/50, Train Loss: 1.2133, Test Loss: 1.1498\n",
      "Epoch 17/50, Train Loss: 1.2267, Test Loss: 1.1501\n",
      "Epoch 18/50, Train Loss: 1.1953, Test Loss: 1.1487\n",
      "Epoch 19/50, Train Loss: 1.1424, Test Loss: 1.1490\n",
      "Epoch 20/50, Train Loss: 1.1955, Test Loss: 1.1490\n",
      "Epoch 21/50, Train Loss: 1.1999, Test Loss: 1.1484\n",
      "Epoch 22/50, Train Loss: 1.1952, Test Loss: 1.1501\n",
      "Epoch 23/50, Train Loss: 1.1863, Test Loss: 1.1501\n",
      "Epoch 24/50, Train Loss: 1.1664, Test Loss: 1.1492\n",
      "Epoch 25/50, Train Loss: 1.1501, Test Loss: 1.1482\n",
      "Epoch 26/50, Train Loss: 1.1853, Test Loss: 1.1472\n",
      "Epoch 27/50, Train Loss: 1.1603, Test Loss: 1.1476\n",
      "Epoch 28/50, Train Loss: 1.1807, Test Loss: 1.1458\n",
      "Epoch 29/50, Train Loss: 1.1720, Test Loss: 1.1446\n",
      "Epoch 30/50, Train Loss: 1.1817, Test Loss: 1.1443\n",
      "Epoch 31/50, Train Loss: 1.1840, Test Loss: 1.1435\n",
      "Epoch 32/50, Train Loss: 1.1805, Test Loss: 1.1423\n",
      "Epoch 33/50, Train Loss: 1.1308, Test Loss: 1.1429\n",
      "Epoch 34/50, Train Loss: 1.1624, Test Loss: 1.1425\n",
      "Epoch 35/50, Train Loss: 1.1416, Test Loss: 1.1429\n",
      "Epoch 36/50, Train Loss: 1.1558, Test Loss: 1.1413\n",
      "Epoch 37/50, Train Loss: 1.2029, Test Loss: 1.1408\n",
      "Epoch 38/50, Train Loss: 1.1824, Test Loss: 1.1408\n",
      "Epoch 39/50, Train Loss: 1.1539, Test Loss: 1.1397\n",
      "Epoch 40/50, Train Loss: 1.1680, Test Loss: 1.1388\n",
      "Epoch 41/50, Train Loss: 1.1900, Test Loss: 1.1389\n",
      "Epoch 42/50, Train Loss: 1.1169, Test Loss: 1.1377\n",
      "Epoch 43/50, Train Loss: 1.1561, Test Loss: 1.1381\n",
      "Epoch 44/50, Train Loss: 1.2106, Test Loss: 1.1384\n",
      "Epoch 45/50, Train Loss: 1.1441, Test Loss: 1.1384\n",
      "Epoch 46/50, Train Loss: 1.1795, Test Loss: 1.1398\n",
      "Epoch 47/50, Train Loss: 1.1499, Test Loss: 1.1392\n",
      "Epoch 48/50, Train Loss: 1.1431, Test Loss: 1.1381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:00:44,538] Trial 126 finished with value: 1.1377453804016113 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 54, 'layer_1_size': 100, 'layer_2_size': 255, 'layer_3_size': 138, 'layer_4_size': 46, 'layer_5_size': 186, 'layer_6_size': 216, 'layer_7_size': 167, 'layer_8_size': 147, 'layer_9_size': 62, 'layer_10_size': 222, 'layer_11_size': 144, 'layer_12_size': 91, 'layer_13_size': 129, 'dropout_rate': 0.33264650525414774, 'learning_rate': 0.00015270269314528726, 'batch_size': 256, 'epochs': 50}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50, Train Loss: 1.1761, Test Loss: 1.1380\n",
      "Epoch 50/50, Train Loss: 1.1344, Test Loss: 1.1377\n",
      "Epoch 1/27, Train Loss: 1.4509, Test Loss: 1.0388\n",
      "Epoch 2/27, Train Loss: 1.3478, Test Loss: 1.0528\n",
      "Epoch 3/27, Train Loss: 1.2913, Test Loss: 1.0642\n",
      "Epoch 4/27, Train Loss: 1.2952, Test Loss: 1.0694\n",
      "Epoch 5/27, Train Loss: 1.2098, Test Loss: 1.0650\n",
      "Epoch 6/27, Train Loss: 1.1867, Test Loss: 1.0535\n",
      "Epoch 7/27, Train Loss: 1.1744, Test Loss: 1.0458\n",
      "Epoch 8/27, Train Loss: 1.1693, Test Loss: 1.0396\n",
      "Epoch 9/27, Train Loss: 1.1743, Test Loss: 1.0335\n",
      "Epoch 10/27, Train Loss: 1.1385, Test Loss: 1.0302\n",
      "Epoch 11/27, Train Loss: 1.1587, Test Loss: 1.0284\n",
      "Epoch 12/27, Train Loss: 1.1823, Test Loss: 1.0281\n",
      "Epoch 13/27, Train Loss: 1.1168, Test Loss: 1.0293\n",
      "Epoch 14/27, Train Loss: 1.1641, Test Loss: 1.0300\n",
      "Epoch 15/27, Train Loss: 1.1374, Test Loss: 1.0302\n",
      "Epoch 16/27, Train Loss: 1.1566, Test Loss: 1.0309\n",
      "Epoch 17/27, Train Loss: 1.1216, Test Loss: 1.0309\n",
      "Epoch 18/27, Train Loss: 1.1476, Test Loss: 1.0323\n",
      "Epoch 19/27, Train Loss: 1.1050, Test Loss: 1.0318\n",
      "Epoch 20/27, Train Loss: 1.1676, Test Loss: 1.0324\n",
      "Epoch 21/27, Train Loss: 1.1206, Test Loss: 1.0334\n",
      "Epoch 22/27, Train Loss: 1.1492, Test Loss: 1.0328\n",
      "Epoch 23/27, Train Loss: 1.1062, Test Loss: 1.0327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:00:45,767] Trial 127 finished with value: 1.0312495231628418 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 162, 'layer_1_size': 150, 'layer_2_size': 248, 'layer_3_size': 127, 'layer_4_size': 70, 'layer_5_size': 225, 'layer_6_size': 187, 'layer_7_size': 122, 'dropout_rate': 0.31920901901029364, 'learning_rate': 0.0004251331033645537, 'batch_size': 256, 'epochs': 27}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/27, Train Loss: 1.1210, Test Loss: 1.0330\n",
      "Epoch 25/27, Train Loss: 1.1327, Test Loss: 1.0331\n",
      "Epoch 26/27, Train Loss: 1.1274, Test Loss: 1.0329\n",
      "Epoch 27/27, Train Loss: 1.1492, Test Loss: 1.0312\n",
      "Epoch 1/60, Train Loss: 1.2344, Test Loss: 1.0854\n",
      "Epoch 2/60, Train Loss: 1.0783, Test Loss: 1.0919\n",
      "Epoch 3/60, Train Loss: 1.1483, Test Loss: 1.0945\n",
      "Epoch 4/60, Train Loss: 1.1364, Test Loss: 1.0809\n",
      "Epoch 5/60, Train Loss: 1.1678, Test Loss: 1.0804\n",
      "Epoch 6/60, Train Loss: 1.1042, Test Loss: 1.0826\n",
      "Epoch 7/60, Train Loss: 1.1152, Test Loss: 1.0940\n",
      "Epoch 8/60, Train Loss: 1.1177, Test Loss: 1.1021\n",
      "Epoch 9/60, Train Loss: 1.0807, Test Loss: 1.1028\n",
      "Epoch 10/60, Train Loss: 1.1149, Test Loss: 1.1045\n",
      "Epoch 11/60, Train Loss: 1.0853, Test Loss: 1.1066\n",
      "Epoch 12/60, Train Loss: 1.1035, Test Loss: 1.1058\n",
      "Epoch 13/60, Train Loss: 1.1236, Test Loss: 1.0928\n",
      "Epoch 14/60, Train Loss: 1.0705, Test Loss: 1.0932\n",
      "Epoch 15/60, Train Loss: 1.1025, Test Loss: 1.0902\n",
      "Epoch 16/60, Train Loss: 1.0726, Test Loss: 1.0908\n",
      "Epoch 17/60, Train Loss: 1.0835, Test Loss: 1.1016\n",
      "Epoch 18/60, Train Loss: 1.0797, Test Loss: 1.0875\n",
      "Epoch 19/60, Train Loss: 1.0995, Test Loss: 1.0866\n",
      "Epoch 20/60, Train Loss: 1.0722, Test Loss: 1.0853\n",
      "Epoch 21/60, Train Loss: 1.0877, Test Loss: 1.0783\n",
      "Epoch 22/60, Train Loss: 1.0537, Test Loss: 1.0805\n",
      "Epoch 23/60, Train Loss: 1.0790, Test Loss: 1.0759\n",
      "Epoch 24/60, Train Loss: 1.0838, Test Loss: 1.0779\n",
      "Epoch 25/60, Train Loss: 1.0772, Test Loss: 1.0766\n",
      "Epoch 26/60, Train Loss: 1.0668, Test Loss: 1.0829\n",
      "Epoch 27/60, Train Loss: 1.0767, Test Loss: 1.0758\n",
      "Epoch 28/60, Train Loss: 1.0686, Test Loss: 1.0727\n",
      "Epoch 29/60, Train Loss: 1.0991, Test Loss: 1.0784\n",
      "Epoch 30/60, Train Loss: 1.0689, Test Loss: 1.0798\n",
      "Epoch 31/60, Train Loss: 1.0921, Test Loss: 1.0827\n",
      "Epoch 32/60, Train Loss: 1.0323, Test Loss: 1.0805\n",
      "Epoch 33/60, Train Loss: 1.0764, Test Loss: 1.0810\n",
      "Epoch 34/60, Train Loss: 1.0552, Test Loss: 1.0781\n",
      "Epoch 35/60, Train Loss: 1.0565, Test Loss: 1.0820\n",
      "Epoch 36/60, Train Loss: 1.0528, Test Loss: 1.0867\n",
      "Epoch 37/60, Train Loss: 1.0869, Test Loss: 1.0917\n",
      "Epoch 38/60, Train Loss: 1.0637, Test Loss: 1.0901\n",
      "Epoch 39/60, Train Loss: 1.0274, Test Loss: 1.0863\n",
      "Epoch 40/60, Train Loss: 1.0402, Test Loss: 1.0805\n",
      "Epoch 41/60, Train Loss: 1.0099, Test Loss: 1.0785\n",
      "Epoch 42/60, Train Loss: 1.0286, Test Loss: 1.0795\n",
      "Epoch 43/60, Train Loss: 1.0686, Test Loss: 1.0755\n",
      "Epoch 44/60, Train Loss: 1.0735, Test Loss: 1.0746\n",
      "Epoch 45/60, Train Loss: 1.0379, Test Loss: 1.0752\n",
      "Epoch 46/60, Train Loss: 1.0339, Test Loss: 1.0654\n",
      "Epoch 47/60, Train Loss: 1.0748, Test Loss: 1.0667\n",
      "Epoch 48/60, Train Loss: 1.0602, Test Loss: 1.0683\n",
      "Epoch 49/60, Train Loss: 1.0415, Test Loss: 1.0698\n",
      "Epoch 50/60, Train Loss: 1.0472, Test Loss: 1.0686\n",
      "Epoch 51/60, Train Loss: 1.0515, Test Loss: 1.0737\n",
      "Epoch 52/60, Train Loss: 1.0368, Test Loss: 1.0712\n",
      "Epoch 53/60, Train Loss: 1.0493, Test Loss: 1.0692\n",
      "Epoch 54/60, Train Loss: 1.0603, Test Loss: 1.0633\n",
      "Epoch 55/60, Train Loss: 1.0555, Test Loss: 1.0695\n",
      "Epoch 56/60, Train Loss: 1.0440, Test Loss: 1.0702\n",
      "Epoch 57/60, Train Loss: 1.0719, Test Loss: 1.0734\n",
      "Epoch 58/60, Train Loss: 1.0528, Test Loss: 1.0754\n",
      "Epoch 59/60, Train Loss: 1.0398, Test Loss: 1.0684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:01:01,513] Trial 128 finished with value: 1.0731210538319178 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 79, 'layer_1_size': 253, 'layer_2_size': 251, 'layer_3_size': 149, 'layer_4_size': 47, 'layer_5_size': 91, 'layer_6_size': 201, 'layer_7_size': 182, 'layer_8_size': 139, 'layer_9_size': 93, 'layer_10_size': 240, 'layer_11_size': 126, 'layer_12_size': 76, 'layer_13_size': 179, 'dropout_rate': 0.28918236063585456, 'learning_rate': 0.00018878888206634437, 'batch_size': 32, 'epochs': 60}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/60, Train Loss: 1.0384, Test Loss: 1.0731\n",
      "Epoch 1/53, Train Loss: 1.2253, Test Loss: 1.1352\n",
      "Epoch 2/53, Train Loss: 1.1809, Test Loss: 1.1338\n",
      "Epoch 3/53, Train Loss: 1.2166, Test Loss: 1.1340\n",
      "Epoch 4/53, Train Loss: 1.1986, Test Loss: 1.1366\n",
      "Epoch 5/53, Train Loss: 1.1757, Test Loss: 1.1350\n",
      "Epoch 6/53, Train Loss: 1.1875, Test Loss: 1.1368\n",
      "Epoch 7/53, Train Loss: 1.1837, Test Loss: 1.1411\n",
      "Epoch 8/53, Train Loss: 1.2161, Test Loss: 1.1471\n",
      "Epoch 9/53, Train Loss: 1.1818, Test Loss: 1.1521\n",
      "Epoch 10/53, Train Loss: 1.1889, Test Loss: 1.1531\n",
      "Epoch 11/53, Train Loss: 1.1733, Test Loss: 1.1539\n",
      "Epoch 12/53, Train Loss: 1.1767, Test Loss: 1.1505\n",
      "Epoch 13/53, Train Loss: 1.1732, Test Loss: 1.1476\n",
      "Epoch 14/53, Train Loss: 1.1726, Test Loss: 1.1454\n",
      "Epoch 15/53, Train Loss: 1.1131, Test Loss: 1.1443\n",
      "Epoch 16/53, Train Loss: 1.1166, Test Loss: 1.1422\n",
      "Epoch 17/53, Train Loss: 1.1628, Test Loss: 1.1379\n",
      "Epoch 18/53, Train Loss: 1.1798, Test Loss: 1.1343\n",
      "Epoch 19/53, Train Loss: 1.1778, Test Loss: 1.1314\n",
      "Epoch 20/53, Train Loss: 1.1643, Test Loss: 1.1323\n",
      "Epoch 21/53, Train Loss: 1.1627, Test Loss: 1.1327\n",
      "Epoch 22/53, Train Loss: 1.1386, Test Loss: 1.1306\n",
      "Epoch 23/53, Train Loss: 1.1879, Test Loss: 1.1278\n",
      "Epoch 24/53, Train Loss: 1.1430, Test Loss: 1.1307\n",
      "Epoch 25/53, Train Loss: 1.1672, Test Loss: 1.1294\n",
      "Epoch 26/53, Train Loss: 1.1120, Test Loss: 1.1294\n",
      "Epoch 27/53, Train Loss: 1.1466, Test Loss: 1.1313\n",
      "Epoch 28/53, Train Loss: 1.1461, Test Loss: 1.1312\n",
      "Epoch 29/53, Train Loss: 1.1679, Test Loss: 1.1311\n",
      "Epoch 30/53, Train Loss: 1.1243, Test Loss: 1.1288\n",
      "Epoch 31/53, Train Loss: 1.1474, Test Loss: 1.1249\n",
      "Epoch 32/53, Train Loss: 1.1551, Test Loss: 1.1232\n",
      "Epoch 33/53, Train Loss: 1.1523, Test Loss: 1.1211\n",
      "Epoch 34/53, Train Loss: 1.1093, Test Loss: 1.1239\n",
      "Epoch 35/53, Train Loss: 1.1378, Test Loss: 1.1220\n",
      "Epoch 36/53, Train Loss: 1.0979, Test Loss: 1.1205\n",
      "Epoch 37/53, Train Loss: 1.1392, Test Loss: 1.1205\n",
      "Epoch 38/53, Train Loss: 1.1370, Test Loss: 1.1219\n",
      "Epoch 39/53, Train Loss: 1.1294, Test Loss: 1.1226\n",
      "Epoch 40/53, Train Loss: 1.1477, Test Loss: 1.1214\n",
      "Epoch 41/53, Train Loss: 1.1167, Test Loss: 1.1191\n",
      "Epoch 42/53, Train Loss: 1.0955, Test Loss: 1.1192\n",
      "Epoch 43/53, Train Loss: 1.0746, Test Loss: 1.1207\n",
      "Epoch 44/53, Train Loss: 1.1092, Test Loss: 1.1204\n",
      "Epoch 45/53, Train Loss: 1.1632, Test Loss: 1.1181\n",
      "Epoch 46/53, Train Loss: 1.1702, Test Loss: 1.1177\n",
      "Epoch 47/53, Train Loss: 1.0750, Test Loss: 1.1169\n",
      "Epoch 48/53, Train Loss: 1.1445, Test Loss: 1.1161\n",
      "Epoch 49/53, Train Loss: 1.1284, Test Loss: 1.1177\n",
      "Epoch 50/53, Train Loss: 1.1370, Test Loss: 1.1176\n",
      "Epoch 51/53, Train Loss: 1.1294, Test Loss: 1.1215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:01:04,626] Trial 129 finished with value: 1.1190282106399536 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 140, 'layer_1_size': 198, 'layer_2_size': 228, 'layer_3_size': 217, 'layer_4_size': 142, 'layer_5_size': 250, 'layer_6_size': 195, 'layer_7_size': 96, 'layer_8_size': 78, 'layer_9_size': 108, 'layer_10_size': 125, 'layer_11_size': 86, 'layer_12_size': 53, 'dropout_rate': 0.27222998904294743, 'learning_rate': 0.00011414782487250018, 'batch_size': 256, 'epochs': 53}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/53, Train Loss: 1.0928, Test Loss: 1.1195\n",
      "Epoch 53/53, Train Loss: 1.1019, Test Loss: 1.1190\n",
      "Epoch 1/13, Train Loss: 1.1826, Test Loss: 1.0356\n",
      "Epoch 2/13, Train Loss: 1.1218, Test Loss: 1.0402\n",
      "Epoch 3/13, Train Loss: 1.1096, Test Loss: 1.0341\n",
      "Epoch 4/13, Train Loss: 1.1040, Test Loss: 1.0539\n",
      "Epoch 5/13, Train Loss: 1.0748, Test Loss: 1.0468\n",
      "Epoch 6/13, Train Loss: 1.0940, Test Loss: 1.0303\n",
      "Epoch 7/13, Train Loss: 1.0592, Test Loss: 1.0279\n",
      "Epoch 8/13, Train Loss: 1.0395, Test Loss: 1.0175\n",
      "Epoch 9/13, Train Loss: 1.0322, Test Loss: 1.0139\n",
      "Epoch 10/13, Train Loss: 1.0253, Test Loss: 1.0077\n",
      "Epoch 11/13, Train Loss: 1.0753, Test Loss: 1.0080\n",
      "Epoch 12/13, Train Loss: 1.0370, Test Loss: 1.0111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:01:06,765] Trial 130 finished with value: 1.011828750371933 and parameters: {'num_hidden_layers': 15, 'layer_0_size': 159, 'layer_1_size': 142, 'layer_2_size': 236, 'layer_3_size': 228, 'layer_4_size': 95, 'layer_5_size': 173, 'layer_6_size': 208, 'layer_7_size': 214, 'layer_8_size': 156, 'layer_9_size': 72, 'layer_10_size': 203, 'layer_11_size': 165, 'layer_12_size': 39, 'layer_13_size': 152, 'layer_14_size': 53, 'dropout_rate': 0.30811081113600625, 'learning_rate': 0.0007086867938612937, 'batch_size': 64, 'epochs': 13}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/13, Train Loss: 1.0443, Test Loss: 1.0118\n",
      "Epoch 1/67, Train Loss: 1.2053, Test Loss: 1.1383\n",
      "Epoch 2/67, Train Loss: 1.1111, Test Loss: 1.1454\n",
      "Epoch 3/67, Train Loss: 1.0794, Test Loss: 1.1687\n",
      "Epoch 4/67, Train Loss: 1.0873, Test Loss: 1.1532\n",
      "Epoch 5/67, Train Loss: 1.1064, Test Loss: 1.1352\n",
      "Epoch 6/67, Train Loss: 1.0875, Test Loss: 1.1479\n",
      "Epoch 7/67, Train Loss: 1.0873, Test Loss: 1.1464\n",
      "Epoch 8/67, Train Loss: 1.1114, Test Loss: 1.1445\n",
      "Epoch 9/67, Train Loss: 1.0443, Test Loss: 1.1616\n",
      "Epoch 10/67, Train Loss: 1.0640, Test Loss: 1.1480\n",
      "Epoch 11/67, Train Loss: 1.0454, Test Loss: 1.1521\n",
      "Epoch 12/67, Train Loss: 1.0616, Test Loss: 1.1368\n",
      "Epoch 13/67, Train Loss: 1.0249, Test Loss: 1.1457\n",
      "Epoch 14/67, Train Loss: 1.0959, Test Loss: 1.1483\n",
      "Epoch 15/67, Train Loss: 1.0444, Test Loss: 1.1430\n",
      "Epoch 16/67, Train Loss: 1.0319, Test Loss: 1.1332\n",
      "Epoch 17/67, Train Loss: 1.0552, Test Loss: 1.1329\n",
      "Epoch 18/67, Train Loss: 1.0582, Test Loss: 1.1314\n",
      "Epoch 19/67, Train Loss: 1.0620, Test Loss: 1.1324\n",
      "Epoch 20/67, Train Loss: 1.0388, Test Loss: 1.1356\n",
      "Epoch 21/67, Train Loss: 1.0537, Test Loss: 1.1395\n",
      "Epoch 22/67, Train Loss: 1.0475, Test Loss: 1.1388\n",
      "Epoch 23/67, Train Loss: 1.0367, Test Loss: 1.1461\n",
      "Epoch 24/67, Train Loss: 1.0364, Test Loss: 1.1435\n",
      "Epoch 25/67, Train Loss: 1.0273, Test Loss: 1.1318\n",
      "Epoch 26/67, Train Loss: 1.0300, Test Loss: 1.1380\n",
      "Epoch 27/67, Train Loss: 1.0026, Test Loss: 1.1302\n",
      "Epoch 28/67, Train Loss: 1.0085, Test Loss: 1.1302\n",
      "Epoch 29/67, Train Loss: 1.0101, Test Loss: 1.1343\n",
      "Epoch 30/67, Train Loss: 1.0442, Test Loss: 1.1226\n",
      "Epoch 31/67, Train Loss: 1.0231, Test Loss: 1.1244\n",
      "Epoch 32/67, Train Loss: 1.0486, Test Loss: 1.1252\n",
      "Epoch 33/67, Train Loss: 1.0235, Test Loss: 1.1236\n",
      "Epoch 34/67, Train Loss: 1.0566, Test Loss: 1.1240\n",
      "Epoch 35/67, Train Loss: 1.0072, Test Loss: 1.1258\n",
      "Epoch 36/67, Train Loss: 1.0389, Test Loss: 1.1258\n",
      "Epoch 37/67, Train Loss: 1.0329, Test Loss: 1.1200\n",
      "Epoch 38/67, Train Loss: 1.0247, Test Loss: 1.1323\n",
      "Epoch 39/67, Train Loss: 1.0046, Test Loss: 1.1300\n",
      "Epoch 40/67, Train Loss: 0.9998, Test Loss: 1.1346\n",
      "Epoch 41/67, Train Loss: 1.0087, Test Loss: 1.1285\n",
      "Epoch 42/67, Train Loss: 1.0310, Test Loss: 1.1269\n",
      "Epoch 43/67, Train Loss: 1.0196, Test Loss: 1.1347\n",
      "Epoch 44/67, Train Loss: 1.0350, Test Loss: 1.1289\n",
      "Epoch 45/67, Train Loss: 1.0185, Test Loss: 1.1355\n",
      "Epoch 46/67, Train Loss: 1.0555, Test Loss: 1.1333\n",
      "Epoch 47/67, Train Loss: 1.0302, Test Loss: 1.1326\n",
      "Epoch 48/67, Train Loss: 1.0140, Test Loss: 1.1275\n",
      "Epoch 49/67, Train Loss: 1.0144, Test Loss: 1.1377\n",
      "Epoch 50/67, Train Loss: 1.0055, Test Loss: 1.1356\n",
      "Epoch 51/67, Train Loss: 1.0122, Test Loss: 1.1372\n",
      "Epoch 52/67, Train Loss: 1.0115, Test Loss: 1.1361\n",
      "Epoch 53/67, Train Loss: 1.0132, Test Loss: 1.1369\n",
      "Epoch 54/67, Train Loss: 0.9684, Test Loss: 1.1362\n",
      "Epoch 55/67, Train Loss: 1.0731, Test Loss: 1.1367\n",
      "Epoch 56/67, Train Loss: 0.9985, Test Loss: 1.1348\n",
      "Epoch 57/67, Train Loss: 1.0202, Test Loss: 1.1339\n",
      "Epoch 58/67, Train Loss: 0.9814, Test Loss: 1.1460\n",
      "Epoch 59/67, Train Loss: 1.0002, Test Loss: 1.1372\n",
      "Epoch 60/67, Train Loss: 1.0214, Test Loss: 1.1429\n",
      "Epoch 61/67, Train Loss: 0.9861, Test Loss: 1.1399\n",
      "Epoch 62/67, Train Loss: 1.0042, Test Loss: 1.1362\n",
      "Epoch 63/67, Train Loss: 0.9981, Test Loss: 1.1342\n",
      "Epoch 64/67, Train Loss: 0.9877, Test Loss: 1.1322\n",
      "Epoch 65/67, Train Loss: 0.9958, Test Loss: 1.1382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:01:19,862] Trial 131 finished with value: 1.1390952893665858 and parameters: {'num_hidden_layers': 11, 'layer_0_size': 137, 'layer_1_size': 133, 'layer_2_size': 172, 'layer_3_size': 143, 'layer_4_size': 88, 'layer_5_size': 202, 'layer_6_size': 99, 'layer_7_size': 78, 'layer_8_size': 165, 'layer_9_size': 248, 'layer_10_size': 191, 'dropout_rate': 0.2462623836869503, 'learning_rate': 0.00029826065191874496, 'batch_size': 32, 'epochs': 67}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/67, Train Loss: 0.9889, Test Loss: 1.1392\n",
      "Epoch 67/67, Train Loss: 0.9811, Test Loss: 1.1391\n",
      "Epoch 1/23, Train Loss: 1.1260, Test Loss: 0.8088\n",
      "Epoch 2/23, Train Loss: 1.0882, Test Loss: 0.8148\n",
      "Epoch 3/23, Train Loss: 1.1234, Test Loss: 0.8133\n",
      "Epoch 4/23, Train Loss: 1.1049, Test Loss: 0.8068\n",
      "Epoch 5/23, Train Loss: 1.0910, Test Loss: 0.8084\n",
      "Epoch 6/23, Train Loss: 1.1140, Test Loss: 0.8072\n",
      "Epoch 7/23, Train Loss: 1.0911, Test Loss: 0.8111\n",
      "Epoch 8/23, Train Loss: 1.0803, Test Loss: 0.8136\n",
      "Epoch 9/23, Train Loss: 1.1090, Test Loss: 0.8170\n",
      "Epoch 10/23, Train Loss: 1.0832, Test Loss: 0.8136\n",
      "Epoch 11/23, Train Loss: 1.0793, Test Loss: 0.8168\n",
      "Epoch 12/23, Train Loss: 1.0599, Test Loss: 0.8127\n",
      "Epoch 13/23, Train Loss: 1.0572, Test Loss: 0.8103\n",
      "Epoch 14/23, Train Loss: 1.0604, Test Loss: 0.8106\n",
      "Epoch 15/23, Train Loss: 1.0162, Test Loss: 0.8111\n",
      "Epoch 16/23, Train Loss: 1.0926, Test Loss: 0.8117\n",
      "Epoch 17/23, Train Loss: 1.0609, Test Loss: 0.8104\n",
      "Epoch 18/23, Train Loss: 1.0986, Test Loss: 0.8098\n",
      "Epoch 19/23, Train Loss: 1.0404, Test Loss: 0.8073\n",
      "Epoch 20/23, Train Loss: 1.0659, Test Loss: 0.8080\n",
      "Epoch 21/23, Train Loss: 1.0402, Test Loss: 0.8096\n",
      "Epoch 22/23, Train Loss: 1.0425, Test Loss: 0.8085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:01:21,576] Trial 132 finished with value: 0.8098416030406952 and parameters: {'num_hidden_layers': 12, 'layer_0_size': 128, 'layer_1_size': 205, 'layer_2_size': 154, 'layer_3_size': 131, 'layer_4_size': 79, 'layer_5_size': 195, 'layer_6_size': 47, 'layer_7_size': 113, 'layer_8_size': 151, 'layer_9_size': 57, 'layer_10_size': 180, 'layer_11_size': 175, 'dropout_rate': 0.21566352940618908, 'learning_rate': 1.3123586636921562e-05, 'batch_size': 128, 'epochs': 23}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/23, Train Loss: 1.0082, Test Loss: 0.8098\n",
      "Epoch 1/6, Train Loss: 1.2480, Test Loss: 0.9576\n",
      "Epoch 2/6, Train Loss: 1.3025, Test Loss: 0.9606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:01:21,933] Trial 133 finished with value: 0.9828133583068848 and parameters: {'num_hidden_layers': 12, 'layer_0_size': 127, 'layer_1_size': 207, 'layer_2_size': 119, 'layer_3_size': 120, 'layer_4_size': 78, 'layer_5_size': 207, 'layer_6_size': 44, 'layer_7_size': 116, 'layer_8_size': 149, 'layer_9_size': 47, 'layer_10_size': 145, 'layer_11_size': 191, 'dropout_rate': 0.29203643568231324, 'learning_rate': 1.2562465059125725e-05, 'batch_size': 256, 'epochs': 6}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/6, Train Loss: 1.2943, Test Loss: 0.9641\n",
      "Epoch 4/6, Train Loss: 1.2129, Test Loss: 0.9673\n",
      "Epoch 5/6, Train Loss: 1.2788, Test Loss: 0.9757\n",
      "Epoch 6/6, Train Loss: 1.2815, Test Loss: 0.9828\n",
      "Epoch 1/11, Train Loss: 1.3000, Test Loss: 0.8119\n",
      "Epoch 2/11, Train Loss: 1.2939, Test Loss: 0.8217\n",
      "Epoch 3/11, Train Loss: 1.3034, Test Loss: 0.8216\n",
      "Epoch 4/11, Train Loss: 1.2069, Test Loss: 0.8191\n",
      "Epoch 5/11, Train Loss: 1.1873, Test Loss: 0.8241\n",
      "Epoch 6/11, Train Loss: 1.2692, Test Loss: 0.8299\n",
      "Epoch 7/11, Train Loss: 1.2196, Test Loss: 0.8339\n",
      "Epoch 8/11, Train Loss: 1.1974, Test Loss: 0.8228\n",
      "Epoch 9/11, Train Loss: 1.1907, Test Loss: 0.8315\n",
      "Epoch 10/11, Train Loss: 1.1662, Test Loss: 0.8173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:01:24,271] Trial 134 finished with value: 0.8306127148015159 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 223, 'layer_1_size': 112, 'layer_2_size': 140, 'layer_3_size': 129, 'layer_4_size': 57, 'layer_5_size': 182, 'layer_6_size': 32, 'layer_7_size': 110, 'layer_8_size': 135, 'layer_9_size': 58, 'layer_10_size': 225, 'layer_11_size': 136, 'layer_12_size': 136, 'dropout_rate': 0.2804409567000833, 'learning_rate': 1.4567541326116646e-05, 'batch_size': 32, 'epochs': 11}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/11, Train Loss: 1.1744, Test Loss: 0.8306\n",
      "Epoch 1/10, Train Loss: 1.1702, Test Loss: 1.2041\n",
      "Epoch 2/10, Train Loss: 1.1959, Test Loss: 1.2092\n",
      "Epoch 3/10, Train Loss: 1.1854, Test Loss: 1.1985\n",
      "Epoch 4/10, Train Loss: 1.1859, Test Loss: 1.2208\n",
      "Epoch 5/10, Train Loss: 1.1971, Test Loss: 1.1955\n",
      "Epoch 6/10, Train Loss: 1.1837, Test Loss: 1.1991\n",
      "Epoch 7/10, Train Loss: 1.2283, Test Loss: 1.2086\n",
      "Epoch 8/10, Train Loss: 1.1836, Test Loss: 1.1956\n",
      "Epoch 9/10, Train Loss: 1.1858, Test Loss: 1.1930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:01:26,544] Trial 135 finished with value: 1.1952823655945914 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 235, 'layer_1_size': 114, 'layer_2_size': 144, 'layer_3_size': 130, 'layer_4_size': 81, 'layer_5_size': 196, 'layer_6_size': 32, 'layer_7_size': 125, 'layer_8_size': 122, 'layer_9_size': 54, 'layer_10_size': 213, 'layer_11_size': 181, 'layer_12_size': 134, 'dropout_rate': 0.26894306215479385, 'learning_rate': 1.0156186379805612e-05, 'batch_size': 32, 'epochs': 10}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Loss: 1.1983, Test Loss: 1.1953\n",
      "Epoch 1/15, Train Loss: 1.1252, Test Loss: 0.8846\n",
      "Epoch 2/15, Train Loss: 1.1792, Test Loss: 0.8881\n",
      "Epoch 3/15, Train Loss: 1.2340, Test Loss: 0.8901\n",
      "Epoch 4/15, Train Loss: 1.1704, Test Loss: 0.8838\n",
      "Epoch 5/15, Train Loss: 1.2150, Test Loss: 0.8914\n",
      "Epoch 6/15, Train Loss: 1.2227, Test Loss: 0.8898\n",
      "Epoch 7/15, Train Loss: 1.2067, Test Loss: 0.8847\n",
      "Epoch 8/15, Train Loss: 1.1830, Test Loss: 0.8888\n",
      "Epoch 9/15, Train Loss: 1.2140, Test Loss: 0.8819\n",
      "Epoch 10/15, Train Loss: 1.2067, Test Loss: 0.8826\n",
      "Epoch 11/15, Train Loss: 1.1739, Test Loss: 0.8829\n",
      "Epoch 12/15, Train Loss: 1.1603, Test Loss: 0.8812\n",
      "Epoch 13/15, Train Loss: 1.1911, Test Loss: 0.8825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:01:29,549] Trial 136 finished with value: 0.8827693973268781 and parameters: {'num_hidden_layers': 12, 'layer_0_size': 219, 'layer_1_size': 212, 'layer_2_size': 151, 'layer_3_size': 123, 'layer_4_size': 64, 'layer_5_size': 159, 'layer_6_size': 39, 'layer_7_size': 111, 'layer_8_size': 125, 'layer_9_size': 57, 'layer_10_size': 179, 'layer_11_size': 137, 'dropout_rate': 0.2590104265458035, 'learning_rate': 1.6615918247358544e-05, 'batch_size': 32, 'epochs': 15}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15, Train Loss: 1.1448, Test Loss: 0.8825\n",
      "Epoch 15/15, Train Loss: 1.1752, Test Loss: 0.8828\n",
      "Epoch 1/20, Train Loss: 1.2118, Test Loss: 0.9246\n",
      "Epoch 2/20, Train Loss: 1.1807, Test Loss: 0.9222\n",
      "Epoch 3/20, Train Loss: 1.1647, Test Loss: 0.9281\n",
      "Epoch 4/20, Train Loss: 1.2514, Test Loss: 0.9282\n",
      "Epoch 5/20, Train Loss: 1.2024, Test Loss: 0.9249\n",
      "Epoch 6/20, Train Loss: 1.1888, Test Loss: 0.9305\n",
      "Epoch 7/20, Train Loss: 1.2188, Test Loss: 0.9234\n",
      "Epoch 8/20, Train Loss: 1.1853, Test Loss: 0.9293\n",
      "Epoch 9/20, Train Loss: 1.2448, Test Loss: 0.9289\n",
      "Epoch 10/20, Train Loss: 1.1868, Test Loss: 0.9211\n",
      "Epoch 11/20, Train Loss: 1.2091, Test Loss: 0.9294\n",
      "Epoch 12/20, Train Loss: 1.1926, Test Loss: 0.9252\n",
      "Epoch 13/20, Train Loss: 1.1513, Test Loss: 0.9328\n",
      "Epoch 14/20, Train Loss: 1.1437, Test Loss: 0.9202\n",
      "Epoch 15/20, Train Loss: 1.2078, Test Loss: 0.9206\n",
      "Epoch 16/20, Train Loss: 1.2169, Test Loss: 0.9282\n",
      "Epoch 17/20, Train Loss: 1.1998, Test Loss: 0.9203\n",
      "Epoch 18/20, Train Loss: 1.1960, Test Loss: 0.9155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:01:33,502] Trial 137 finished with value: 0.9222749131066459 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 193, 'layer_1_size': 54, 'layer_2_size': 139, 'layer_3_size': 90, 'layer_4_size': 56, 'layer_5_size': 191, 'layer_6_size': 87, 'layer_7_size': 156, 'layer_8_size': 70, 'layer_9_size': 60, 'layer_10_size': 224, 'layer_11_size': 56, 'layer_12_size': 99, 'dropout_rate': 0.23660529698191152, 'learning_rate': 1.438009096042962e-05, 'batch_size': 32, 'epochs': 20}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Train Loss: 1.0970, Test Loss: 0.9218\n",
      "Epoch 20/20, Train Loss: 1.1774, Test Loss: 0.9223\n",
      "Epoch 1/30, Train Loss: 1.1977, Test Loss: 1.1399\n",
      "Epoch 2/30, Train Loss: 1.1885, Test Loss: 1.1583\n",
      "Epoch 3/30, Train Loss: 1.1613, Test Loss: 1.1587\n",
      "Epoch 4/30, Train Loss: 1.1671, Test Loss: 1.1613\n",
      "Epoch 5/30, Train Loss: 1.1782, Test Loss: 1.1551\n",
      "Epoch 6/30, Train Loss: 1.1858, Test Loss: 1.1543\n",
      "Epoch 7/30, Train Loss: 1.1455, Test Loss: 1.1532\n",
      "Epoch 8/30, Train Loss: 1.1654, Test Loss: 1.1643\n",
      "Epoch 9/30, Train Loss: 1.1836, Test Loss: 1.1590\n",
      "Epoch 10/30, Train Loss: 1.1749, Test Loss: 1.1517\n",
      "Epoch 11/30, Train Loss: 1.2050, Test Loss: 1.1573\n",
      "Epoch 12/30, Train Loss: 1.1648, Test Loss: 1.1565\n",
      "Epoch 13/30, Train Loss: 1.1424, Test Loss: 1.1495\n",
      "Epoch 14/30, Train Loss: 1.1425, Test Loss: 1.1613\n",
      "Epoch 15/30, Train Loss: 1.1676, Test Loss: 1.1636\n",
      "Epoch 16/30, Train Loss: 1.1504, Test Loss: 1.1562\n",
      "Epoch 17/30, Train Loss: 1.1480, Test Loss: 1.1495\n",
      "Epoch 18/30, Train Loss: 1.1334, Test Loss: 1.1555\n",
      "Epoch 19/30, Train Loss: 1.1547, Test Loss: 1.1646\n",
      "Epoch 20/30, Train Loss: 1.1763, Test Loss: 1.1563\n",
      "Epoch 21/30, Train Loss: 1.1391, Test Loss: 1.1609\n",
      "Epoch 22/30, Train Loss: 1.1865, Test Loss: 1.1626\n",
      "Epoch 23/30, Train Loss: 1.1188, Test Loss: 1.1628\n",
      "Epoch 24/30, Train Loss: 1.1468, Test Loss: 1.1564\n",
      "Epoch 25/30, Train Loss: 1.1318, Test Loss: 1.1627\n",
      "Epoch 26/30, Train Loss: 1.0735, Test Loss: 1.1696\n",
      "Epoch 27/30, Train Loss: 1.1295, Test Loss: 1.1577\n",
      "Epoch 28/30, Train Loss: 1.1701, Test Loss: 1.1615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:01:39,855] Trial 138 finished with value: 1.1678857462746757 and parameters: {'num_hidden_layers': 12, 'layer_0_size': 225, 'layer_1_size': 226, 'layer_2_size': 128, 'layer_3_size': 114, 'layer_4_size': 75, 'layer_5_size': 182, 'layer_6_size': 79, 'layer_7_size': 133, 'layer_8_size': 132, 'layer_9_size': 45, 'layer_10_size': 244, 'layer_11_size': 166, 'dropout_rate': 0.28340264954823297, 'learning_rate': 1.1417654723092711e-05, 'batch_size': 32, 'epochs': 30}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30, Train Loss: 1.1674, Test Loss: 1.1738\n",
      "Epoch 30/30, Train Loss: 1.0920, Test Loss: 1.1679\n",
      "Epoch 1/16, Train Loss: 1.4642, Test Loss: 1.0886\n",
      "Epoch 2/16, Train Loss: 1.3921, Test Loss: 1.1443\n",
      "Epoch 3/16, Train Loss: 1.4360, Test Loss: 1.1185\n",
      "Epoch 4/16, Train Loss: 1.3372, Test Loss: 1.1746\n",
      "Epoch 5/16, Train Loss: 1.2811, Test Loss: 1.0992\n",
      "Epoch 6/16, Train Loss: 1.2862, Test Loss: 1.0928\n",
      "Epoch 7/16, Train Loss: 1.3012, Test Loss: 1.0855\n",
      "Epoch 8/16, Train Loss: 1.2654, Test Loss: 1.0924\n",
      "Epoch 9/16, Train Loss: 1.2358, Test Loss: 1.0875\n",
      "Epoch 10/16, Train Loss: 1.2026, Test Loss: 1.0491\n",
      "Epoch 11/16, Train Loss: 1.1950, Test Loss: 1.0398\n",
      "Epoch 12/16, Train Loss: 1.1735, Test Loss: 1.0171\n",
      "Epoch 13/16, Train Loss: 1.1706, Test Loss: 1.0446\n",
      "Epoch 14/16, Train Loss: 1.1298, Test Loss: 1.0278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:01:42,387] Trial 139 finished with value: 1.0251891953604562 and parameters: {'num_hidden_layers': 11, 'layer_0_size': 231, 'layer_1_size': 63, 'layer_2_size': 113, 'layer_3_size': 109, 'layer_4_size': 69, 'layer_5_size': 79, 'layer_6_size': 48, 'layer_7_size': 116, 'layer_8_size': 41, 'layer_9_size': 34, 'layer_10_size': 168, 'dropout_rate': 0.1533468960437271, 'learning_rate': 1.3816673476485703e-05, 'batch_size': 32, 'epochs': 16}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/16, Train Loss: 1.0548, Test Loss: 1.0199\n",
      "Epoch 16/16, Train Loss: 1.0934, Test Loss: 1.0252\n",
      "Epoch 1/12, Train Loss: 1.1432, Test Loss: 1.3697\n",
      "Epoch 2/12, Train Loss: 1.1222, Test Loss: 1.4265\n",
      "Epoch 3/12, Train Loss: 1.1440, Test Loss: 1.4217\n",
      "Epoch 4/12, Train Loss: 1.1404, Test Loss: 1.4211\n",
      "Epoch 5/12, Train Loss: 1.1446, Test Loss: 1.4128\n",
      "Epoch 6/12, Train Loss: 1.1319, Test Loss: 1.4123\n",
      "Epoch 7/12, Train Loss: 1.1318, Test Loss: 1.4164\n",
      "Epoch 8/12, Train Loss: 1.1286, Test Loss: 1.4074\n",
      "Epoch 9/12, Train Loss: 1.1189, Test Loss: 1.4120\n",
      "Epoch 10/12, Train Loss: 1.1255, Test Loss: 1.4023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:01:43,759] Trial 140 finished with value: 1.403190016746521 and parameters: {'num_hidden_layers': 12, 'layer_0_size': 67, 'layer_1_size': 190, 'layer_2_size': 136, 'layer_3_size': 98, 'layer_4_size': 39, 'layer_5_size': 169, 'layer_6_size': 39, 'layer_7_size': 144, 'layer_8_size': 134, 'layer_9_size': 52, 'layer_10_size': 179, 'layer_11_size': 149, 'dropout_rate': 0.21834678567542332, 'learning_rate': 2.0343951823406813e-05, 'batch_size': 64, 'epochs': 12}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/12, Train Loss: 1.1481, Test Loss: 1.4068\n",
      "Epoch 12/12, Train Loss: 1.0808, Test Loss: 1.4032\n",
      "Epoch 1/21, Train Loss: 1.1992, Test Loss: 1.0006\n",
      "Epoch 2/21, Train Loss: 1.1822, Test Loss: 1.0009\n",
      "Epoch 3/21, Train Loss: 1.2395, Test Loss: 0.9995\n",
      "Epoch 4/21, Train Loss: 1.1760, Test Loss: 0.9955\n",
      "Epoch 5/21, Train Loss: 1.1621, Test Loss: 0.9931\n",
      "Epoch 6/21, Train Loss: 1.1561, Test Loss: 0.9945\n",
      "Epoch 7/21, Train Loss: 1.1667, Test Loss: 0.9921\n",
      "Epoch 8/21, Train Loss: 1.1483, Test Loss: 0.9926\n",
      "Epoch 9/21, Train Loss: 1.2153, Test Loss: 0.9921\n",
      "Epoch 10/21, Train Loss: 1.1761, Test Loss: 0.9918\n",
      "Epoch 11/21, Train Loss: 1.1267, Test Loss: 0.9906\n",
      "Epoch 12/21, Train Loss: 1.0998, Test Loss: 0.9915\n",
      "Epoch 13/21, Train Loss: 1.1588, Test Loss: 0.9918\n",
      "Epoch 14/21, Train Loss: 1.1638, Test Loss: 0.9915\n",
      "Epoch 15/21, Train Loss: 1.1693, Test Loss: 0.9897\n",
      "Epoch 16/21, Train Loss: 1.1648, Test Loss: 0.9908\n",
      "Epoch 17/21, Train Loss: 1.1700, Test Loss: 0.9953\n",
      "Epoch 18/21, Train Loss: 1.1186, Test Loss: 0.9905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:01:45,491] Trial 141 finished with value: 0.9918564856052399 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 145, 'layer_1_size': 127, 'layer_2_size': 214, 'layer_3_size': 140, 'layer_4_size': 52, 'layer_5_size': 189, 'layer_6_size': 185, 'layer_7_size': 106, 'layer_8_size': 141, 'layer_9_size': 65, 'layer_10_size': 236, 'layer_11_size': 114, 'layer_12_size': 115, 'layer_13_size': 207, 'dropout_rate': 0.3153227560715294, 'learning_rate': 1.159116746971197e-05, 'batch_size': 128, 'epochs': 21}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/21, Train Loss: 1.1800, Test Loss: 0.9930\n",
      "Epoch 20/21, Train Loss: 1.1893, Test Loss: 0.9960\n",
      "Epoch 21/21, Train Loss: 1.1478, Test Loss: 0.9919\n",
      "Epoch 1/24, Train Loss: 1.1088, Test Loss: 0.9104\n",
      "Epoch 2/24, Train Loss: 1.1256, Test Loss: 0.9100\n",
      "Epoch 3/24, Train Loss: 1.1493, Test Loss: 0.9107\n",
      "Epoch 4/24, Train Loss: 1.1351, Test Loss: 0.9122\n",
      "Epoch 5/24, Train Loss: 1.1014, Test Loss: 0.9146\n",
      "Epoch 6/24, Train Loss: 1.1190, Test Loss: 0.9169\n",
      "Epoch 7/24, Train Loss: 1.1049, Test Loss: 0.9184\n",
      "Epoch 8/24, Train Loss: 1.0976, Test Loss: 0.9184\n",
      "Epoch 9/24, Train Loss: 1.0652, Test Loss: 0.9201\n",
      "Epoch 10/24, Train Loss: 1.0580, Test Loss: 0.9207\n",
      "Epoch 11/24, Train Loss: 1.0824, Test Loss: 0.9228\n",
      "Epoch 12/24, Train Loss: 1.0966, Test Loss: 0.9248\n",
      "Epoch 13/24, Train Loss: 1.0572, Test Loss: 0.9227\n",
      "Epoch 14/24, Train Loss: 1.1208, Test Loss: 0.9214\n",
      "Epoch 15/24, Train Loss: 1.0807, Test Loss: 0.9209\n",
      "Epoch 16/24, Train Loss: 1.0478, Test Loss: 0.9199\n",
      "Epoch 17/24, Train Loss: 1.0680, Test Loss: 0.9175\n",
      "Epoch 18/24, Train Loss: 1.0544, Test Loss: 0.9180\n",
      "Epoch 19/24, Train Loss: 1.0750, Test Loss: 0.9182\n",
      "Epoch 20/24, Train Loss: 1.0629, Test Loss: 0.9176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:01:46,965] Trial 142 finished with value: 0.9138191938400269 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 153, 'layer_1_size': 123, 'layer_2_size': 167, 'layer_3_size': 134, 'layer_4_size': 60, 'layer_5_size': 175, 'layer_6_size': 204, 'layer_7_size': 109, 'layer_8_size': 144, 'layer_9_size': 72, 'layer_10_size': 132, 'layer_11_size': 123, 'layer_12_size': 145, 'dropout_rate': 0.2763166306550685, 'learning_rate': 0.00017900394600045962, 'batch_size': 256, 'epochs': 24}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/24, Train Loss: 1.0477, Test Loss: 0.9153\n",
      "Epoch 22/24, Train Loss: 1.0488, Test Loss: 0.9147\n",
      "Epoch 23/24, Train Loss: 1.0517, Test Loss: 0.9148\n",
      "Epoch 24/24, Train Loss: 1.0585, Test Loss: 0.9138\n",
      "Epoch 1/7, Train Loss: 1.1185, Test Loss: 0.9894\n",
      "Epoch 2/7, Train Loss: 1.0739, Test Loss: 0.9834\n",
      "Epoch 3/7, Train Loss: 1.0993, Test Loss: 0.9888\n",
      "Epoch 4/7, Train Loss: 1.0711, Test Loss: 0.9916\n",
      "Epoch 5/7, Train Loss: 1.0649, Test Loss: 0.9889\n",
      "Epoch 6/7, Train Loss: 1.0444, Test Loss: 0.9875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:01:48,840] Trial 143 finished with value: 0.9907769986561367 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 171, 'layer_1_size': 82, 'layer_2_size': 154, 'layer_3_size': 154, 'layer_4_size': 84, 'layer_5_size': 87, 'layer_6_size': 66, 'layer_7_size': 130, 'layer_8_size': 105, 'layer_9_size': 93, 'layer_10_size': 249, 'layer_11_size': 132, 'layer_12_size': 183, 'layer_13_size': 165, 'dropout_rate': 0.3026003736159943, 'learning_rate': 0.0003477994181670902, 'batch_size': 32, 'epochs': 7}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/7, Train Loss: 1.0635, Test Loss: 0.9908\n",
      "Epoch 1/55, Train Loss: 1.1466, Test Loss: 0.8942\n",
      "Epoch 2/55, Train Loss: 1.1654, Test Loss: 0.8997\n",
      "Epoch 3/55, Train Loss: 1.1373, Test Loss: 0.9314\n",
      "Epoch 4/55, Train Loss: 1.1242, Test Loss: 0.9096\n",
      "Epoch 5/55, Train Loss: 1.1276, Test Loss: 0.9111\n",
      "Epoch 6/55, Train Loss: 1.1404, Test Loss: 0.8987\n",
      "Epoch 7/55, Train Loss: 1.1610, Test Loss: 0.9123\n",
      "Epoch 8/55, Train Loss: 1.1028, Test Loss: 0.9341\n",
      "Epoch 9/55, Train Loss: 1.0776, Test Loss: 0.9421\n",
      "Epoch 10/55, Train Loss: 1.1223, Test Loss: 0.9395\n",
      "Epoch 11/55, Train Loss: 1.1102, Test Loss: 0.9551\n",
      "Epoch 12/55, Train Loss: 1.0646, Test Loss: 0.9405\n",
      "Epoch 13/55, Train Loss: 1.1014, Test Loss: 0.9736\n",
      "Epoch 14/55, Train Loss: 1.0859, Test Loss: 1.0098\n",
      "Epoch 15/55, Train Loss: 1.0776, Test Loss: 0.9642\n",
      "Epoch 16/55, Train Loss: 1.0562, Test Loss: 0.9556\n",
      "Epoch 17/55, Train Loss: 1.0848, Test Loss: 0.9755\n",
      "Epoch 18/55, Train Loss: 1.0941, Test Loss: 0.9468\n",
      "Epoch 19/55, Train Loss: 1.0869, Test Loss: 0.9430\n",
      "Epoch 20/55, Train Loss: 1.0981, Test Loss: 0.9377\n",
      "Epoch 21/55, Train Loss: 1.0809, Test Loss: 0.9118\n",
      "Epoch 22/55, Train Loss: 1.1031, Test Loss: 0.9151\n",
      "Epoch 23/55, Train Loss: 1.0573, Test Loss: 0.9431\n",
      "Epoch 24/55, Train Loss: 1.0676, Test Loss: 0.9554\n",
      "Epoch 25/55, Train Loss: 1.0784, Test Loss: 0.9216\n",
      "Epoch 26/55, Train Loss: 1.0725, Test Loss: 0.9352\n",
      "Epoch 27/55, Train Loss: 1.0653, Test Loss: 0.9513\n",
      "Epoch 28/55, Train Loss: 1.0721, Test Loss: 0.9872\n",
      "Epoch 29/55, Train Loss: 1.0643, Test Loss: 0.9888\n",
      "Epoch 30/55, Train Loss: 1.1163, Test Loss: 1.0132\n",
      "Epoch 31/55, Train Loss: 1.0819, Test Loss: 0.9915\n",
      "Epoch 32/55, Train Loss: 1.0870, Test Loss: 0.9502\n",
      "Epoch 33/55, Train Loss: 1.0612, Test Loss: 0.9606\n",
      "Epoch 34/55, Train Loss: 1.0620, Test Loss: 0.9469\n",
      "Epoch 35/55, Train Loss: 1.0597, Test Loss: 0.9379\n",
      "Epoch 36/55, Train Loss: 1.0540, Test Loss: 0.9225\n",
      "Epoch 37/55, Train Loss: 1.0584, Test Loss: 0.9253\n",
      "Epoch 38/55, Train Loss: 1.0530, Test Loss: 0.9572\n",
      "Epoch 39/55, Train Loss: 1.0742, Test Loss: 0.9376\n",
      "Epoch 40/55, Train Loss: 1.0660, Test Loss: 0.9322\n",
      "Epoch 41/55, Train Loss: 1.0777, Test Loss: 0.9507\n",
      "Epoch 42/55, Train Loss: 1.0488, Test Loss: 0.9326\n",
      "Epoch 43/55, Train Loss: 1.0664, Test Loss: 0.9323\n",
      "Epoch 44/55, Train Loss: 1.0732, Test Loss: 0.9239\n",
      "Epoch 45/55, Train Loss: 1.0713, Test Loss: 0.9334\n",
      "Epoch 46/55, Train Loss: 1.0806, Test Loss: 0.9369\n",
      "Epoch 47/55, Train Loss: 1.0454, Test Loss: 0.9313\n",
      "Epoch 48/55, Train Loss: 1.0574, Test Loss: 0.9408\n",
      "Epoch 49/55, Train Loss: 1.0696, Test Loss: 0.9435\n",
      "Epoch 50/55, Train Loss: 1.0484, Test Loss: 0.9590\n",
      "Epoch 51/55, Train Loss: 1.0786, Test Loss: 0.9301\n",
      "Epoch 52/55, Train Loss: 1.0665, Test Loss: 0.9514\n",
      "Epoch 53/55, Train Loss: 1.0629, Test Loss: 0.9632\n",
      "Epoch 54/55, Train Loss: 1.0691, Test Loss: 0.9539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:02:04,268] Trial 144 finished with value: 0.9599802834647042 and parameters: {'num_hidden_layers': 15, 'layer_0_size': 240, 'layer_1_size': 202, 'layer_2_size': 132, 'layer_3_size': 198, 'layer_4_size': 157, 'layer_5_size': 196, 'layer_6_size': 242, 'layer_7_size': 96, 'layer_8_size': 152, 'layer_9_size': 47, 'layer_10_size': 218, 'layer_11_size': 159, 'layer_12_size': 104, 'layer_13_size': 140, 'layer_14_size': 176, 'dropout_rate': 0.1762329961319832, 'learning_rate': 0.000238735277214035, 'batch_size': 32, 'epochs': 55}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/55, Train Loss: 1.0526, Test Loss: 0.9600\n",
      "Epoch 1/58, Train Loss: 1.2607, Test Loss: 1.0180\n",
      "Epoch 2/58, Train Loss: 1.1919, Test Loss: 1.0182\n",
      "Epoch 3/58, Train Loss: 1.2800, Test Loss: 1.0184\n",
      "Epoch 4/58, Train Loss: 1.2606, Test Loss: 1.0184\n",
      "Epoch 5/58, Train Loss: 1.2169, Test Loss: 1.0194\n",
      "Epoch 6/58, Train Loss: 1.1781, Test Loss: 1.0199\n",
      "Epoch 7/58, Train Loss: 1.2255, Test Loss: 1.0200\n",
      "Epoch 8/58, Train Loss: 1.1583, Test Loss: 1.0190\n",
      "Epoch 9/58, Train Loss: 1.1233, Test Loss: 1.0187\n",
      "Epoch 10/58, Train Loss: 1.2101, Test Loss: 1.0183\n",
      "Epoch 11/58, Train Loss: 1.1592, Test Loss: 1.0186\n",
      "Epoch 12/58, Train Loss: 1.1672, Test Loss: 1.0192\n",
      "Epoch 13/58, Train Loss: 1.1552, Test Loss: 1.0192\n",
      "Epoch 14/58, Train Loss: 1.0808, Test Loss: 1.0189\n",
      "Epoch 15/58, Train Loss: 1.1227, Test Loss: 1.0192\n",
      "Epoch 16/58, Train Loss: 1.1132, Test Loss: 1.0193\n",
      "Epoch 17/58, Train Loss: 1.1427, Test Loss: 1.0202\n",
      "Epoch 18/58, Train Loss: 1.0669, Test Loss: 1.0220\n",
      "Epoch 19/58, Train Loss: 1.0788, Test Loss: 1.0235\n",
      "Epoch 20/58, Train Loss: 1.0894, Test Loss: 1.0229\n",
      "Epoch 21/58, Train Loss: 1.0840, Test Loss: 1.0250\n",
      "Epoch 22/58, Train Loss: 1.0919, Test Loss: 1.0262\n",
      "Epoch 23/58, Train Loss: 1.0885, Test Loss: 1.0249\n",
      "Epoch 24/58, Train Loss: 1.0938, Test Loss: 1.0241\n",
      "Epoch 25/58, Train Loss: 1.0931, Test Loss: 1.0220\n",
      "Epoch 26/58, Train Loss: 1.1156, Test Loss: 1.0206\n",
      "Epoch 27/58, Train Loss: 1.0878, Test Loss: 1.0202\n",
      "Epoch 28/58, Train Loss: 1.1033, Test Loss: 1.0204\n",
      "Epoch 29/58, Train Loss: 1.0841, Test Loss: 1.0212\n",
      "Epoch 30/58, Train Loss: 1.0704, Test Loss: 1.0218\n",
      "Epoch 31/58, Train Loss: 1.0767, Test Loss: 1.0234\n",
      "Epoch 32/58, Train Loss: 1.0813, Test Loss: 1.0249\n",
      "Epoch 33/58, Train Loss: 1.0615, Test Loss: 1.0262\n",
      "Epoch 34/58, Train Loss: 1.0460, Test Loss: 1.0262\n",
      "Epoch 35/58, Train Loss: 1.0624, Test Loss: 1.0246\n",
      "Epoch 36/58, Train Loss: 1.0536, Test Loss: 1.0241\n",
      "Epoch 37/58, Train Loss: 1.0664, Test Loss: 1.0239\n",
      "Epoch 38/58, Train Loss: 1.0411, Test Loss: 1.0226\n",
      "Epoch 39/58, Train Loss: 1.0295, Test Loss: 1.0218\n",
      "Epoch 40/58, Train Loss: 1.0376, Test Loss: 1.0213\n",
      "Epoch 41/58, Train Loss: 1.0526, Test Loss: 1.0215\n",
      "Epoch 42/58, Train Loss: 1.0293, Test Loss: 1.0218\n",
      "Epoch 43/58, Train Loss: 1.0486, Test Loss: 1.0221\n",
      "Epoch 44/58, Train Loss: 1.0380, Test Loss: 1.0229\n",
      "Epoch 45/58, Train Loss: 1.0471, Test Loss: 1.0231\n",
      "Epoch 46/58, Train Loss: 1.0325, Test Loss: 1.0237\n",
      "Epoch 47/58, Train Loss: 1.0405, Test Loss: 1.0248\n",
      "Epoch 48/58, Train Loss: 1.0257, Test Loss: 1.0262\n",
      "Epoch 49/58, Train Loss: 1.0300, Test Loss: 1.0276\n",
      "Epoch 50/58, Train Loss: 1.0550, Test Loss: 1.0286\n",
      "Epoch 51/58, Train Loss: 1.0511, Test Loss: 1.0282\n",
      "Epoch 52/58, Train Loss: 1.0073, Test Loss: 1.0281\n",
      "Epoch 53/58, Train Loss: 1.0518, Test Loss: 1.0279\n",
      "Epoch 54/58, Train Loss: 1.0478, Test Loss: 1.0281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:02:07,615] Trial 145 finished with value: 1.02798593044281 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 133, 'layer_1_size': 107, 'layer_2_size': 178, 'layer_3_size': 148, 'layer_4_size': 99, 'layer_5_size': 210, 'layer_6_size': 44, 'layer_7_size': 151, 'layer_8_size': 171, 'layer_9_size': 83, 'layer_10_size': 226, 'layer_11_size': 97, 'layer_12_size': 171, 'dropout_rate': 0.4869568516454904, 'learning_rate': 0.0005268477147576919, 'batch_size': 256, 'epochs': 58}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/58, Train Loss: 1.0445, Test Loss: 1.0293\n",
      "Epoch 56/58, Train Loss: 1.0164, Test Loss: 1.0295\n",
      "Epoch 57/58, Train Loss: 1.0295, Test Loss: 1.0291\n",
      "Epoch 58/58, Train Loss: 1.0170, Test Loss: 1.0280\n",
      "Epoch 1/23, Train Loss: 1.2040, Test Loss: 0.9936\n",
      "Epoch 2/23, Train Loss: 1.1573, Test Loss: 0.9926\n",
      "Epoch 3/23, Train Loss: 1.1742, Test Loss: 0.9987\n",
      "Epoch 4/23, Train Loss: 1.2070, Test Loss: 1.0036\n",
      "Epoch 5/23, Train Loss: 1.1774, Test Loss: 1.0025\n",
      "Epoch 6/23, Train Loss: 1.1544, Test Loss: 1.0065\n",
      "Epoch 7/23, Train Loss: 1.1766, Test Loss: 1.0090\n",
      "Epoch 8/23, Train Loss: 1.1996, Test Loss: 1.0071\n",
      "Epoch 9/23, Train Loss: 1.1764, Test Loss: 1.0061\n",
      "Epoch 10/23, Train Loss: 1.1262, Test Loss: 1.0037\n",
      "Epoch 11/23, Train Loss: 1.1900, Test Loss: 1.0054\n",
      "Epoch 12/23, Train Loss: 1.1607, Test Loss: 1.0067\n",
      "Epoch 13/23, Train Loss: 1.1340, Test Loss: 1.0038\n",
      "Epoch 14/23, Train Loss: 1.1121, Test Loss: 1.0038\n",
      "Epoch 15/23, Train Loss: 1.1170, Test Loss: 1.0027\n",
      "Epoch 16/23, Train Loss: 1.1942, Test Loss: 1.0086\n",
      "Epoch 17/23, Train Loss: 1.1986, Test Loss: 1.0097\n",
      "Epoch 18/23, Train Loss: 1.1734, Test Loss: 1.0094\n",
      "Epoch 19/23, Train Loss: 1.1923, Test Loss: 1.0080\n",
      "Epoch 20/23, Train Loss: 1.1408, Test Loss: 1.0055\n",
      "Epoch 21/23, Train Loss: 1.1539, Test Loss: 1.0056\n",
      "Epoch 22/23, Train Loss: 1.1329, Test Loss: 1.0079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:02:10,909] Trial 146 finished with value: 1.002723753452301 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 251, 'layer_1_size': 194, 'layer_2_size': 102, 'layer_3_size': 131, 'layer_4_size': 49, 'layer_5_size': 241, 'layer_6_size': 176, 'layer_7_size': 202, 'layer_8_size': 110, 'layer_9_size': 36, 'layer_10_size': 253, 'layer_11_size': 173, 'layer_12_size': 65, 'layer_13_size': 227, 'dropout_rate': 0.2522388888600893, 'learning_rate': 1.603274486687774e-05, 'batch_size': 64, 'epochs': 23}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/23, Train Loss: 1.1566, Test Loss: 1.0027\n",
      "Epoch 1/17, Train Loss: 1.1425, Test Loss: 1.2174\n",
      "Epoch 2/17, Train Loss: 1.1657, Test Loss: 1.2170\n",
      "Epoch 3/17, Train Loss: 1.1318, Test Loss: 1.2312\n",
      "Epoch 4/17, Train Loss: 1.1329, Test Loss: 1.2351\n",
      "Epoch 5/17, Train Loss: 1.1538, Test Loss: 1.2295\n",
      "Epoch 6/17, Train Loss: 1.1732, Test Loss: 1.2323\n",
      "Epoch 7/17, Train Loss: 1.1606, Test Loss: 1.2382\n",
      "Epoch 8/17, Train Loss: 1.0933, Test Loss: 1.2372\n",
      "Epoch 9/17, Train Loss: 1.1094, Test Loss: 1.2376\n",
      "Epoch 10/17, Train Loss: 1.0560, Test Loss: 1.2318\n",
      "Epoch 11/17, Train Loss: 1.0783, Test Loss: 1.2412\n",
      "Epoch 12/17, Train Loss: 1.0769, Test Loss: 1.2417\n",
      "Epoch 13/17, Train Loss: 1.0820, Test Loss: 1.2390\n",
      "Epoch 14/17, Train Loss: 1.0668, Test Loss: 1.2477\n",
      "Epoch 15/17, Train Loss: 1.1047, Test Loss: 1.2384\n",
      "Epoch 16/17, Train Loss: 1.0569, Test Loss: 1.2384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:02:14,362] Trial 147 finished with value: 1.239902709211622 and parameters: {'num_hidden_layers': 11, 'layer_0_size': 40, 'layer_1_size': 219, 'layer_2_size': 142, 'layer_3_size': 140, 'layer_4_size': 151, 'layer_5_size': 116, 'layer_6_size': 214, 'layer_7_size': 236, 'layer_8_size': 114, 'layer_9_size': 42, 'layer_10_size': 232, 'dropout_rate': 0.2943062218286302, 'learning_rate': 0.0001472519441254809, 'batch_size': 32, 'epochs': 17}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/17, Train Loss: 1.0439, Test Loss: 1.2399\n",
      "Epoch 1/12, Train Loss: 1.1696, Test Loss: 0.9189\n",
      "Epoch 2/12, Train Loss: 1.1598, Test Loss: 0.9117\n",
      "Epoch 3/12, Train Loss: 1.1900, Test Loss: 0.9097\n",
      "Epoch 4/12, Train Loss: 1.1380, Test Loss: 0.9086\n",
      "Epoch 5/12, Train Loss: 1.1180, Test Loss: 0.9064\n",
      "Epoch 6/12, Train Loss: 1.1667, Test Loss: 0.9036\n",
      "Epoch 7/12, Train Loss: 1.1422, Test Loss: 0.9021\n",
      "Epoch 8/12, Train Loss: 1.0721, Test Loss: 0.9027\n",
      "Epoch 9/12, Train Loss: 1.1042, Test Loss: 0.9018\n",
      "Epoch 10/12, Train Loss: 1.0990, Test Loss: 0.9041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:02:15,110] Trial 148 finished with value: 0.9120098948478699 and parameters: {'num_hidden_layers': 12, 'layer_0_size': 71, 'layer_1_size': 214, 'layer_2_size': 148, 'layer_3_size': 119, 'layer_4_size': 91, 'layer_5_size': 216, 'layer_6_size': 225, 'layer_7_size': 138, 'layer_8_size': 101, 'layer_9_size': 159, 'layer_10_size': 196, 'layer_11_size': 139, 'dropout_rate': 0.2636724745593768, 'learning_rate': 0.00127297796201602, 'batch_size': 256, 'epochs': 12}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/12, Train Loss: 1.0833, Test Loss: 0.9086\n",
      "Epoch 12/12, Train Loss: 1.0711, Test Loss: 0.9120\n",
      "Epoch 1/35, Train Loss: 1.1362, Test Loss: 1.0402\n",
      "Epoch 2/35, Train Loss: 1.1157, Test Loss: 1.0406\n",
      "Epoch 3/35, Train Loss: 1.1533, Test Loss: 1.0420\n",
      "Epoch 4/35, Train Loss: 1.1649, Test Loss: 1.0436\n",
      "Epoch 5/35, Train Loss: 1.1360, Test Loss: 1.0424\n",
      "Epoch 6/35, Train Loss: 1.0857, Test Loss: 1.0410\n",
      "Epoch 7/35, Train Loss: 1.1426, Test Loss: 1.0404\n",
      "Epoch 8/35, Train Loss: 1.1541, Test Loss: 1.0435\n",
      "Epoch 9/35, Train Loss: 1.1441, Test Loss: 1.0440\n",
      "Epoch 10/35, Train Loss: 1.1890, Test Loss: 1.0437\n",
      "Epoch 11/35, Train Loss: 1.1338, Test Loss: 1.0423\n",
      "Epoch 12/35, Train Loss: 1.1170, Test Loss: 1.0413\n",
      "Epoch 13/35, Train Loss: 1.1360, Test Loss: 1.0400\n",
      "Epoch 14/35, Train Loss: 1.0936, Test Loss: 1.0398\n",
      "Epoch 15/35, Train Loss: 1.1601, Test Loss: 1.0408\n",
      "Epoch 16/35, Train Loss: 1.1622, Test Loss: 1.0424\n",
      "Epoch 17/35, Train Loss: 1.0721, Test Loss: 1.0425\n",
      "Epoch 18/35, Train Loss: 1.1363, Test Loss: 1.0433\n",
      "Epoch 19/35, Train Loss: 1.1346, Test Loss: 1.0399\n",
      "Epoch 20/35, Train Loss: 1.1268, Test Loss: 1.0393\n",
      "Epoch 21/35, Train Loss: 1.0539, Test Loss: 1.0423\n",
      "Epoch 22/35, Train Loss: 1.1115, Test Loss: 1.0409\n",
      "Epoch 23/35, Train Loss: 1.1288, Test Loss: 1.0395\n",
      "Epoch 24/35, Train Loss: 1.0980, Test Loss: 1.0416\n",
      "Epoch 25/35, Train Loss: 1.0964, Test Loss: 1.0418\n",
      "Epoch 26/35, Train Loss: 1.1108, Test Loss: 1.0407\n",
      "Epoch 27/35, Train Loss: 1.1013, Test Loss: 1.0415\n",
      "Epoch 28/35, Train Loss: 1.1438, Test Loss: 1.0427\n",
      "Epoch 29/35, Train Loss: 1.0938, Test Loss: 1.0426\n",
      "Epoch 30/35, Train Loss: 1.1151, Test Loss: 1.0426\n",
      "Epoch 31/35, Train Loss: 1.1320, Test Loss: 1.0447\n",
      "Epoch 32/35, Train Loss: 1.0724, Test Loss: 1.0440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:02:16,450] Trial 149 finished with value: 1.0450173914432526 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 148, 'layer_1_size': 210, 'layer_2_size': 93, 'layer_3_size': 124, 'layer_4_size': 88, 'layer_5_size': 122, 'dropout_rate': 0.3328131311675074, 'learning_rate': 2.966013891104384e-05, 'batch_size': 128, 'epochs': 35}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/35, Train Loss: 1.1121, Test Loss: 1.0448\n",
      "Epoch 34/35, Train Loss: 1.1452, Test Loss: 1.0453\n",
      "Epoch 35/35, Train Loss: 1.0943, Test Loss: 1.0450\n",
      "Epoch 1/51, Train Loss: 1.1558, Test Loss: 0.9230\n",
      "Epoch 2/51, Train Loss: 1.2582, Test Loss: 0.9475\n",
      "Epoch 3/51, Train Loss: 1.2260, Test Loss: 0.9399\n",
      "Epoch 4/51, Train Loss: 1.1808, Test Loss: 0.9472\n",
      "Epoch 5/51, Train Loss: 1.2431, Test Loss: 0.9364\n",
      "Epoch 6/51, Train Loss: 1.1881, Test Loss: 0.9462\n",
      "Epoch 7/51, Train Loss: 1.1879, Test Loss: 0.9357\n",
      "Epoch 8/51, Train Loss: 1.2235, Test Loss: 0.9369\n",
      "Epoch 9/51, Train Loss: 1.1387, Test Loss: 0.9285\n",
      "Epoch 10/51, Train Loss: 1.2035, Test Loss: 0.9226\n",
      "Epoch 11/51, Train Loss: 1.2099, Test Loss: 0.9242\n",
      "Epoch 12/51, Train Loss: 1.1518, Test Loss: 0.9313\n",
      "Epoch 13/51, Train Loss: 1.1982, Test Loss: 0.9261\n",
      "Epoch 14/51, Train Loss: 1.2232, Test Loss: 0.9286\n",
      "Epoch 15/51, Train Loss: 1.2398, Test Loss: 0.9285\n",
      "Epoch 16/51, Train Loss: 1.2132, Test Loss: 0.9294\n",
      "Epoch 17/51, Train Loss: 1.2383, Test Loss: 0.9249\n",
      "Epoch 18/51, Train Loss: 1.2423, Test Loss: 0.9311\n",
      "Epoch 19/51, Train Loss: 1.1842, Test Loss: 0.9341\n",
      "Epoch 20/51, Train Loss: 1.1932, Test Loss: 0.9314\n",
      "Epoch 21/51, Train Loss: 1.1574, Test Loss: 0.9332\n",
      "Epoch 22/51, Train Loss: 1.1896, Test Loss: 0.9265\n",
      "Epoch 23/51, Train Loss: 1.1804, Test Loss: 0.9305\n",
      "Epoch 24/51, Train Loss: 1.1798, Test Loss: 0.9254\n",
      "Epoch 25/51, Train Loss: 1.1887, Test Loss: 0.9286\n",
      "Epoch 26/51, Train Loss: 1.2092, Test Loss: 0.9308\n",
      "Epoch 27/51, Train Loss: 1.1755, Test Loss: 0.9199\n",
      "Epoch 28/51, Train Loss: 1.1791, Test Loss: 0.9215\n",
      "Epoch 29/51, Train Loss: 1.1152, Test Loss: 0.9274\n",
      "Epoch 30/51, Train Loss: 1.1921, Test Loss: 0.9288\n",
      "Epoch 31/51, Train Loss: 1.1959, Test Loss: 0.9324\n",
      "Epoch 32/51, Train Loss: 1.2278, Test Loss: 0.9233\n",
      "Epoch 33/51, Train Loss: 1.1923, Test Loss: 0.9295\n",
      "Epoch 34/51, Train Loss: 1.1959, Test Loss: 0.9311\n",
      "Epoch 35/51, Train Loss: 1.1792, Test Loss: 0.9257\n",
      "Epoch 36/51, Train Loss: 1.1748, Test Loss: 0.9272\n",
      "Epoch 37/51, Train Loss: 1.1757, Test Loss: 0.9269\n",
      "Epoch 38/51, Train Loss: 1.2158, Test Loss: 0.9346\n",
      "Epoch 39/51, Train Loss: 1.1823, Test Loss: 0.9276\n",
      "Epoch 40/51, Train Loss: 1.1578, Test Loss: 0.9318\n",
      "Epoch 41/51, Train Loss: 1.1856, Test Loss: 0.9269\n",
      "Epoch 42/51, Train Loss: 1.2190, Test Loss: 0.9291\n",
      "Epoch 43/51, Train Loss: 1.1965, Test Loss: 0.9165\n",
      "Epoch 44/51, Train Loss: 1.1748, Test Loss: 0.9311\n",
      "Epoch 45/51, Train Loss: 1.1681, Test Loss: 0.9260\n",
      "Epoch 46/51, Train Loss: 1.1186, Test Loss: 0.9271\n",
      "Epoch 47/51, Train Loss: 1.2040, Test Loss: 0.9274\n",
      "Epoch 48/51, Train Loss: 1.1659, Test Loss: 0.9255\n",
      "Epoch 49/51, Train Loss: 1.1869, Test Loss: 0.9211\n",
      "Epoch 50/51, Train Loss: 1.1555, Test Loss: 0.9319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:02:29,043] Trial 150 finished with value: 0.9232414024216788 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 214, 'layer_1_size': 246, 'layer_2_size': 157, 'layer_3_size': 146, 'layer_4_size': 72, 'layer_5_size': 228, 'layer_6_size': 146, 'layer_7_size': 92, 'layer_8_size': 128, 'layer_9_size': 179, 'layer_10_size': 184, 'layer_11_size': 151, 'layer_12_size': 58, 'dropout_rate': 0.30859989521868897, 'learning_rate': 1.0201823771531352e-05, 'batch_size': 32, 'epochs': 51}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/51, Train Loss: 1.1267, Test Loss: 0.9232\n",
      "Epoch 1/63, Train Loss: 1.2766, Test Loss: 1.1298\n",
      "Epoch 2/63, Train Loss: 1.2712, Test Loss: 1.1624\n",
      "Epoch 3/63, Train Loss: 1.2564, Test Loss: 1.1729\n",
      "Epoch 4/63, Train Loss: 1.3234, Test Loss: 1.1573\n",
      "Epoch 5/63, Train Loss: 1.2088, Test Loss: 1.1499\n",
      "Epoch 6/63, Train Loss: 1.2117, Test Loss: 1.1331\n",
      "Epoch 7/63, Train Loss: 1.2655, Test Loss: 1.1285\n",
      "Epoch 8/63, Train Loss: 1.2635, Test Loss: 1.1229\n",
      "Epoch 9/63, Train Loss: 1.2006, Test Loss: 1.1125\n",
      "Epoch 10/63, Train Loss: 1.2050, Test Loss: 1.1061\n",
      "Epoch 11/63, Train Loss: 1.2049, Test Loss: 1.1022\n",
      "Epoch 12/63, Train Loss: 1.2907, Test Loss: 1.1031\n",
      "Epoch 13/63, Train Loss: 1.1489, Test Loss: 1.1050\n",
      "Epoch 14/63, Train Loss: 1.2176, Test Loss: 1.1026\n",
      "Epoch 15/63, Train Loss: 1.2194, Test Loss: 1.0982\n",
      "Epoch 16/63, Train Loss: 1.2280, Test Loss: 1.0970\n",
      "Epoch 17/63, Train Loss: 1.2374, Test Loss: 1.1006\n",
      "Epoch 18/63, Train Loss: 1.2157, Test Loss: 1.1006\n",
      "Epoch 19/63, Train Loss: 1.2108, Test Loss: 1.0978\n",
      "Epoch 20/63, Train Loss: 1.2226, Test Loss: 1.0982\n",
      "Epoch 21/63, Train Loss: 1.2201, Test Loss: 1.0979\n",
      "Epoch 22/63, Train Loss: 1.2126, Test Loss: 1.0964\n",
      "Epoch 23/63, Train Loss: 1.2134, Test Loss: 1.0981\n",
      "Epoch 24/63, Train Loss: 1.1554, Test Loss: 1.0956\n",
      "Epoch 25/63, Train Loss: 1.2053, Test Loss: 1.0974\n",
      "Epoch 26/63, Train Loss: 1.2223, Test Loss: 1.0995\n",
      "Epoch 27/63, Train Loss: 1.2205, Test Loss: 1.0994\n",
      "Epoch 28/63, Train Loss: 1.2031, Test Loss: 1.0971\n",
      "Epoch 29/63, Train Loss: 1.1744, Test Loss: 1.0991\n",
      "Epoch 30/63, Train Loss: 1.1681, Test Loss: 1.0979\n",
      "Epoch 31/63, Train Loss: 1.1663, Test Loss: 1.1010\n",
      "Epoch 32/63, Train Loss: 1.1761, Test Loss: 1.1021\n",
      "Epoch 33/63, Train Loss: 1.1895, Test Loss: 1.1025\n",
      "Epoch 34/63, Train Loss: 1.1758, Test Loss: 1.1033\n",
      "Epoch 35/63, Train Loss: 1.1780, Test Loss: 1.1047\n",
      "Epoch 36/63, Train Loss: 1.2056, Test Loss: 1.1032\n",
      "Epoch 37/63, Train Loss: 1.1512, Test Loss: 1.1007\n",
      "Epoch 38/63, Train Loss: 1.1490, Test Loss: 1.1020\n",
      "Epoch 39/63, Train Loss: 1.1909, Test Loss: 1.1014\n",
      "Epoch 40/63, Train Loss: 1.1419, Test Loss: 1.1016\n",
      "Epoch 41/63, Train Loss: 1.1929, Test Loss: 1.1018\n",
      "Epoch 42/63, Train Loss: 1.1633, Test Loss: 1.1036\n",
      "Epoch 43/63, Train Loss: 1.2169, Test Loss: 1.1056\n",
      "Epoch 44/63, Train Loss: 1.1788, Test Loss: 1.1021\n",
      "Epoch 45/63, Train Loss: 1.2022, Test Loss: 1.1036\n",
      "Epoch 46/63, Train Loss: 1.1730, Test Loss: 1.1034\n",
      "Epoch 47/63, Train Loss: 1.1763, Test Loss: 1.1054\n",
      "Epoch 48/63, Train Loss: 1.1882, Test Loss: 1.1048\n",
      "Epoch 49/63, Train Loss: 1.1909, Test Loss: 1.1028\n",
      "Epoch 50/63, Train Loss: 1.1361, Test Loss: 1.1029\n",
      "Epoch 51/63, Train Loss: 1.1638, Test Loss: 1.1046\n",
      "Epoch 52/63, Train Loss: 1.1834, Test Loss: 1.1013\n",
      "Epoch 53/63, Train Loss: 1.1753, Test Loss: 1.0993\n",
      "Epoch 54/63, Train Loss: 1.1836, Test Loss: 1.1003\n",
      "Epoch 55/63, Train Loss: 1.1717, Test Loss: 1.1011\n",
      "Epoch 56/63, Train Loss: 1.1639, Test Loss: 1.0999\n",
      "Epoch 57/63, Train Loss: 1.1796, Test Loss: 1.1018\n",
      "Epoch 58/63, Train Loss: 1.1371, Test Loss: 1.1027\n",
      "Epoch 59/63, Train Loss: 1.1432, Test Loss: 1.1014\n",
      "Epoch 60/63, Train Loss: 1.1337, Test Loss: 1.1003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:02:33,215] Trial 151 finished with value: 1.1009023189544678 and parameters: {'num_hidden_layers': 10, 'layer_0_size': 109, 'layer_1_size': 158, 'layer_2_size': 153, 'layer_3_size': 135, 'layer_4_size': 179, 'layer_5_size': 203, 'layer_6_size': 197, 'layer_7_size': 99, 'layer_8_size': 157, 'layer_9_size': 249, 'dropout_rate': 0.2247163285456398, 'learning_rate': 3.394852301342244e-05, 'batch_size': 128, 'epochs': 63}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/63, Train Loss: 1.1543, Test Loss: 1.1004\n",
      "Epoch 62/63, Train Loss: 1.1649, Test Loss: 1.1007\n",
      "Epoch 63/63, Train Loss: 1.1588, Test Loss: 1.1009\n",
      "Epoch 1/70, Train Loss: 1.2699, Test Loss: 0.8933\n",
      "Epoch 2/70, Train Loss: 1.2783, Test Loss: 0.8932\n",
      "Epoch 3/70, Train Loss: 1.2999, Test Loss: 0.8899\n",
      "Epoch 4/70, Train Loss: 1.2869, Test Loss: 0.8948\n",
      "Epoch 5/70, Train Loss: 1.2767, Test Loss: 0.8938\n",
      "Epoch 6/70, Train Loss: 1.2414, Test Loss: 0.8885\n",
      "Epoch 7/70, Train Loss: 1.2407, Test Loss: 0.8903\n",
      "Epoch 8/70, Train Loss: 1.2343, Test Loss: 0.8870\n",
      "Epoch 9/70, Train Loss: 1.2610, Test Loss: 0.8835\n",
      "Epoch 10/70, Train Loss: 1.2390, Test Loss: 0.8802\n",
      "Epoch 11/70, Train Loss: 1.2913, Test Loss: 0.8830\n",
      "Epoch 12/70, Train Loss: 1.2765, Test Loss: 0.8824\n",
      "Epoch 13/70, Train Loss: 1.2104, Test Loss: 0.8829\n",
      "Epoch 14/70, Train Loss: 1.2002, Test Loss: 0.8832\n",
      "Epoch 15/70, Train Loss: 1.2563, Test Loss: 0.8851\n",
      "Epoch 16/70, Train Loss: 1.2292, Test Loss: 0.8880\n",
      "Epoch 17/70, Train Loss: 1.2208, Test Loss: 0.8892\n",
      "Epoch 18/70, Train Loss: 1.2395, Test Loss: 0.8857\n",
      "Epoch 19/70, Train Loss: 1.2392, Test Loss: 0.8831\n",
      "Epoch 20/70, Train Loss: 1.2215, Test Loss: 0.8832\n",
      "Epoch 21/70, Train Loss: 1.2287, Test Loss: 0.8815\n",
      "Epoch 22/70, Train Loss: 1.2472, Test Loss: 0.8795\n",
      "Epoch 23/70, Train Loss: 1.1965, Test Loss: 0.8782\n",
      "Epoch 24/70, Train Loss: 1.1920, Test Loss: 0.8801\n",
      "Epoch 25/70, Train Loss: 1.2041, Test Loss: 0.8812\n",
      "Epoch 26/70, Train Loss: 1.2226, Test Loss: 0.8892\n",
      "Epoch 27/70, Train Loss: 1.2381, Test Loss: 0.8843\n",
      "Epoch 28/70, Train Loss: 1.2254, Test Loss: 0.8821\n",
      "Epoch 29/70, Train Loss: 1.1969, Test Loss: 0.8803\n",
      "Epoch 30/70, Train Loss: 1.2336, Test Loss: 0.8822\n",
      "Epoch 31/70, Train Loss: 1.1732, Test Loss: 0.8848\n",
      "Epoch 32/70, Train Loss: 1.1408, Test Loss: 0.8849\n",
      "Epoch 33/70, Train Loss: 1.1714, Test Loss: 0.8848\n",
      "Epoch 34/70, Train Loss: 1.2112, Test Loss: 0.8871\n",
      "Epoch 35/70, Train Loss: 1.2003, Test Loss: 0.8798\n",
      "Epoch 36/70, Train Loss: 1.2096, Test Loss: 0.8815\n",
      "Epoch 37/70, Train Loss: 1.2467, Test Loss: 0.8826\n",
      "Epoch 38/70, Train Loss: 1.2065, Test Loss: 0.8814\n",
      "Epoch 39/70, Train Loss: 1.2070, Test Loss: 0.8836\n",
      "Epoch 40/70, Train Loss: 1.1869, Test Loss: 0.8854\n",
      "Epoch 41/70, Train Loss: 1.2075, Test Loss: 0.8840\n",
      "Epoch 42/70, Train Loss: 1.1929, Test Loss: 0.8820\n",
      "Epoch 43/70, Train Loss: 1.1368, Test Loss: 0.8832\n",
      "Epoch 44/70, Train Loss: 1.1704, Test Loss: 0.8800\n",
      "Epoch 45/70, Train Loss: 1.1933, Test Loss: 0.8797\n",
      "Epoch 46/70, Train Loss: 1.2078, Test Loss: 0.8804\n",
      "Epoch 47/70, Train Loss: 1.2010, Test Loss: 0.8852\n",
      "Epoch 48/70, Train Loss: 1.1512, Test Loss: 0.8842\n",
      "Epoch 49/70, Train Loss: 1.2065, Test Loss: 0.8845\n",
      "Epoch 50/70, Train Loss: 1.1961, Test Loss: 0.8782\n",
      "Epoch 51/70, Train Loss: 1.2080, Test Loss: 0.8758\n",
      "Epoch 52/70, Train Loss: 1.1635, Test Loss: 0.8789\n",
      "Epoch 53/70, Train Loss: 1.1650, Test Loss: 0.8778\n",
      "Epoch 54/70, Train Loss: 1.1793, Test Loss: 0.8754\n",
      "Epoch 55/70, Train Loss: 1.1771, Test Loss: 0.8797\n",
      "Epoch 56/70, Train Loss: 1.1975, Test Loss: 0.8819\n",
      "Epoch 57/70, Train Loss: 1.1712, Test Loss: 0.8790\n",
      "Epoch 58/70, Train Loss: 1.1668, Test Loss: 0.8807\n",
      "Epoch 59/70, Train Loss: 1.1357, Test Loss: 0.8825\n",
      "Epoch 60/70, Train Loss: 1.1496, Test Loss: 0.8811\n",
      "Epoch 61/70, Train Loss: 1.1781, Test Loss: 0.8823\n",
      "Epoch 62/70, Train Loss: 1.1729, Test Loss: 0.8819\n",
      "Epoch 63/70, Train Loss: 1.2040, Test Loss: 0.8817\n",
      "Epoch 64/70, Train Loss: 1.2124, Test Loss: 0.8854\n",
      "Epoch 65/70, Train Loss: 1.1782, Test Loss: 0.8852\n",
      "Epoch 66/70, Train Loss: 1.1383, Test Loss: 0.8843\n",
      "Epoch 67/70, Train Loss: 1.1515, Test Loss: 0.8805\n",
      "Epoch 68/70, Train Loss: 1.2411, Test Loss: 0.8828\n",
      "Epoch 69/70, Train Loss: 1.1973, Test Loss: 0.8819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:02:39,535] Trial 152 finished with value: 0.8837557137012482 and parameters: {'num_hidden_layers': 15, 'layer_0_size': 122, 'layer_1_size': 181, 'layer_2_size': 163, 'layer_3_size': 127, 'layer_4_size': 56, 'layer_5_size': 182, 'layer_6_size': 53, 'layer_7_size': 52, 'layer_8_size': 249, 'layer_9_size': 243, 'layer_10_size': 238, 'layer_11_size': 176, 'layer_12_size': 80, 'layer_13_size': 98, 'layer_14_size': 247, 'dropout_rate': 0.20686477744643864, 'learning_rate': 1.3448329196258351e-05, 'batch_size': 128, 'epochs': 70}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/70, Train Loss: 1.1622, Test Loss: 0.8838\n",
      "Epoch 1/59, Train Loss: 1.1748, Test Loss: 0.9815\n",
      "Epoch 2/59, Train Loss: 1.1058, Test Loss: 0.9621\n",
      "Epoch 3/59, Train Loss: 1.1756, Test Loss: 0.9627\n",
      "Epoch 4/59, Train Loss: 1.1257, Test Loss: 0.9705\n",
      "Epoch 5/59, Train Loss: 1.1068, Test Loss: 0.9806\n",
      "Epoch 6/59, Train Loss: 1.1679, Test Loss: 0.9860\n",
      "Epoch 7/59, Train Loss: 1.1356, Test Loss: 0.9877\n",
      "Epoch 8/59, Train Loss: 1.1360, Test Loss: 0.9923\n",
      "Epoch 9/59, Train Loss: 1.1297, Test Loss: 0.9902\n",
      "Epoch 10/59, Train Loss: 1.1025, Test Loss: 0.9859\n",
      "Epoch 11/59, Train Loss: 1.1043, Test Loss: 0.9885\n",
      "Epoch 12/59, Train Loss: 1.0745, Test Loss: 0.9876\n",
      "Epoch 13/59, Train Loss: 1.1311, Test Loss: 0.9916\n",
      "Epoch 14/59, Train Loss: 1.0821, Test Loss: 0.9836\n",
      "Epoch 15/59, Train Loss: 1.0880, Test Loss: 0.9854\n",
      "Epoch 16/59, Train Loss: 1.1137, Test Loss: 0.9825\n",
      "Epoch 17/59, Train Loss: 1.0439, Test Loss: 0.9818\n",
      "Epoch 18/59, Train Loss: 1.0716, Test Loss: 0.9825\n",
      "Epoch 19/59, Train Loss: 1.0685, Test Loss: 0.9839\n",
      "Epoch 20/59, Train Loss: 1.0835, Test Loss: 0.9825\n",
      "Epoch 21/59, Train Loss: 1.0610, Test Loss: 0.9841\n",
      "Epoch 22/59, Train Loss: 1.0426, Test Loss: 0.9841\n",
      "Epoch 23/59, Train Loss: 1.0549, Test Loss: 0.9836\n",
      "Epoch 24/59, Train Loss: 1.0454, Test Loss: 0.9835\n",
      "Epoch 25/59, Train Loss: 1.0468, Test Loss: 0.9846\n",
      "Epoch 26/59, Train Loss: 1.0527, Test Loss: 0.9883\n",
      "Epoch 27/59, Train Loss: 1.0750, Test Loss: 0.9841\n",
      "Epoch 28/59, Train Loss: 1.0817, Test Loss: 0.9845\n",
      "Epoch 29/59, Train Loss: 1.0767, Test Loss: 0.9824\n",
      "Epoch 30/59, Train Loss: 1.0760, Test Loss: 0.9855\n",
      "Epoch 31/59, Train Loss: 1.0611, Test Loss: 0.9879\n",
      "Epoch 32/59, Train Loss: 1.0382, Test Loss: 0.9881\n",
      "Epoch 33/59, Train Loss: 1.0832, Test Loss: 0.9897\n",
      "Epoch 34/59, Train Loss: 1.0452, Test Loss: 0.9891\n",
      "Epoch 35/59, Train Loss: 1.0338, Test Loss: 0.9878\n",
      "Epoch 36/59, Train Loss: 1.0434, Test Loss: 0.9891\n",
      "Epoch 37/59, Train Loss: 1.0367, Test Loss: 0.9862\n",
      "Epoch 38/59, Train Loss: 1.0646, Test Loss: 0.9864\n",
      "Epoch 39/59, Train Loss: 1.0572, Test Loss: 0.9906\n",
      "Epoch 40/59, Train Loss: 1.0363, Test Loss: 0.9924\n",
      "Epoch 41/59, Train Loss: 1.0500, Test Loss: 0.9932\n",
      "Epoch 42/59, Train Loss: 1.0504, Test Loss: 0.9946\n",
      "Epoch 43/59, Train Loss: 1.0359, Test Loss: 0.9964\n",
      "Epoch 44/59, Train Loss: 1.0323, Test Loss: 0.9917\n",
      "Epoch 45/59, Train Loss: 1.0371, Test Loss: 0.9922\n",
      "Epoch 46/59, Train Loss: 0.9862, Test Loss: 0.9938\n",
      "Epoch 47/59, Train Loss: 1.0207, Test Loss: 0.9949\n",
      "Epoch 48/59, Train Loss: 1.0629, Test Loss: 0.9946\n",
      "Epoch 49/59, Train Loss: 1.0432, Test Loss: 0.9958\n",
      "Epoch 50/59, Train Loss: 1.0197, Test Loss: 0.9943\n",
      "Epoch 51/59, Train Loss: 1.0210, Test Loss: 0.9948\n",
      "Epoch 52/59, Train Loss: 0.9921, Test Loss: 0.9911\n",
      "Epoch 53/59, Train Loss: 0.9992, Test Loss: 0.9947\n",
      "Epoch 54/59, Train Loss: 1.0272, Test Loss: 0.9939\n",
      "Epoch 55/59, Train Loss: 1.0222, Test Loss: 0.9905\n",
      "Epoch 56/59, Train Loss: 1.0275, Test Loss: 0.9927\n",
      "Epoch 57/59, Train Loss: 1.0221, Test Loss: 0.9887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:02:42,035] Trial 153 finished with value: 0.9949600100517273 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 114, 'layer_1_size': 174, 'layer_2_size': 125, 'layer_3_size': 137, 'layer_4_size': 105, 'layer_5_size': 200, 'layer_6_size': 58, 'dropout_rate': 0.1943198444249023, 'learning_rate': 4.091147604364913e-05, 'batch_size': 128, 'epochs': 59}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/59, Train Loss: 1.0434, Test Loss: 0.9930\n",
      "Epoch 59/59, Train Loss: 1.0405, Test Loss: 0.9950\n",
      "Epoch 1/9, Train Loss: 1.2591, Test Loss: 0.9919\n",
      "Epoch 2/9, Train Loss: 1.1713, Test Loss: 0.9901\n",
      "Epoch 3/9, Train Loss: 1.1330, Test Loss: 0.9991\n",
      "Epoch 4/9, Train Loss: 1.1275, Test Loss: 1.0060\n",
      "Epoch 5/9, Train Loss: 1.1425, Test Loss: 1.0117\n",
      "Epoch 6/9, Train Loss: 1.1128, Test Loss: 1.0066\n",
      "Epoch 7/9, Train Loss: 1.1214, Test Loss: 0.9927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:02:42,884] Trial 154 finished with value: 0.9944687187671661 and parameters: {'num_hidden_layers': 12, 'layer_0_size': 127, 'layer_1_size': 151, 'layer_2_size': 160, 'layer_3_size': 166, 'layer_4_size': 210, 'layer_5_size': 218, 'layer_6_size': 49, 'layer_7_size': 193, 'layer_8_size': 220, 'layer_9_size': 232, 'layer_10_size': 158, 'layer_11_size': 186, 'dropout_rate': 0.24114664723586104, 'learning_rate': 0.0008257945122999661, 'batch_size': 128, 'epochs': 9}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/9, Train Loss: 1.1276, Test Loss: 0.9966\n",
      "Epoch 9/9, Train Loss: 1.0840, Test Loss: 0.9945\n",
      "Epoch 1/65, Train Loss: 1.2467, Test Loss: 0.9956\n",
      "Epoch 2/65, Train Loss: 1.2410, Test Loss: 0.9961\n",
      "Epoch 3/65, Train Loss: 1.2420, Test Loss: 0.9958\n",
      "Epoch 4/65, Train Loss: 1.2212, Test Loss: 0.9960\n",
      "Epoch 5/65, Train Loss: 1.3138, Test Loss: 0.9971\n",
      "Epoch 6/65, Train Loss: 1.2956, Test Loss: 0.9983\n",
      "Epoch 7/65, Train Loss: 1.2735, Test Loss: 0.9997\n",
      "Epoch 8/65, Train Loss: 1.2805, Test Loss: 1.0014\n",
      "Epoch 9/65, Train Loss: 1.2456, Test Loss: 1.0035\n",
      "Epoch 10/65, Train Loss: 1.2147, Test Loss: 1.0066\n",
      "Epoch 11/65, Train Loss: 1.2823, Test Loss: 1.0117\n",
      "Epoch 12/65, Train Loss: 1.2568, Test Loss: 1.0124\n",
      "Epoch 13/65, Train Loss: 1.2491, Test Loss: 1.0134\n",
      "Epoch 14/65, Train Loss: 1.2146, Test Loss: 1.0169\n",
      "Epoch 15/65, Train Loss: 1.2102, Test Loss: 1.0174\n",
      "Epoch 16/65, Train Loss: 1.2158, Test Loss: 1.0179\n",
      "Epoch 17/65, Train Loss: 1.2480, Test Loss: 1.0159\n",
      "Epoch 18/65, Train Loss: 1.2104, Test Loss: 1.0158\n",
      "Epoch 19/65, Train Loss: 1.2385, Test Loss: 1.0164\n",
      "Epoch 20/65, Train Loss: 1.1828, Test Loss: 1.0164\n",
      "Epoch 21/65, Train Loss: 1.2117, Test Loss: 1.0174\n",
      "Epoch 22/65, Train Loss: 1.2482, Test Loss: 1.0168\n",
      "Epoch 23/65, Train Loss: 1.2047, Test Loss: 1.0206\n",
      "Epoch 24/65, Train Loss: 1.2592, Test Loss: 1.0197\n",
      "Epoch 25/65, Train Loss: 1.1553, Test Loss: 1.0206\n",
      "Epoch 26/65, Train Loss: 1.2247, Test Loss: 1.0200\n",
      "Epoch 27/65, Train Loss: 1.2424, Test Loss: 1.0221\n",
      "Epoch 28/65, Train Loss: 1.2200, Test Loss: 1.0210\n",
      "Epoch 29/65, Train Loss: 1.2633, Test Loss: 1.0204\n",
      "Epoch 30/65, Train Loss: 1.2360, Test Loss: 1.0226\n",
      "Epoch 31/65, Train Loss: 1.1596, Test Loss: 1.0210\n",
      "Epoch 32/65, Train Loss: 1.2860, Test Loss: 1.0192\n",
      "Epoch 33/65, Train Loss: 1.1912, Test Loss: 1.0187\n",
      "Epoch 34/65, Train Loss: 1.2316, Test Loss: 1.0176\n",
      "Epoch 35/65, Train Loss: 1.2469, Test Loss: 1.0151\n",
      "Epoch 36/65, Train Loss: 1.2345, Test Loss: 1.0168\n",
      "Epoch 37/65, Train Loss: 1.1709, Test Loss: 1.0183\n",
      "Epoch 38/65, Train Loss: 1.1894, Test Loss: 1.0170\n",
      "Epoch 39/65, Train Loss: 1.1835, Test Loss: 1.0154\n",
      "Epoch 40/65, Train Loss: 1.1430, Test Loss: 1.0173\n",
      "Epoch 41/65, Train Loss: 1.2013, Test Loss: 1.0166\n",
      "Epoch 42/65, Train Loss: 1.2316, Test Loss: 1.0142\n",
      "Epoch 43/65, Train Loss: 1.1785, Test Loss: 1.0160\n",
      "Epoch 44/65, Train Loss: 1.2059, Test Loss: 1.0156\n",
      "Epoch 45/65, Train Loss: 1.1872, Test Loss: 1.0144\n",
      "Epoch 46/65, Train Loss: 1.1723, Test Loss: 1.0138\n",
      "Epoch 47/65, Train Loss: 1.1829, Test Loss: 1.0114\n",
      "Epoch 48/65, Train Loss: 1.1540, Test Loss: 1.0144\n",
      "Epoch 49/65, Train Loss: 1.2088, Test Loss: 1.0138\n",
      "Epoch 50/65, Train Loss: 1.2685, Test Loss: 1.0172\n",
      "Epoch 51/65, Train Loss: 1.1758, Test Loss: 1.0154\n",
      "Epoch 52/65, Train Loss: 1.2037, Test Loss: 1.0162\n",
      "Epoch 53/65, Train Loss: 1.1752, Test Loss: 1.0184\n",
      "Epoch 54/65, Train Loss: 1.2108, Test Loss: 1.0162\n",
      "Epoch 55/65, Train Loss: 1.1792, Test Loss: 1.0135\n",
      "Epoch 56/65, Train Loss: 1.1532, Test Loss: 1.0128\n",
      "Epoch 57/65, Train Loss: 1.1711, Test Loss: 1.0129\n",
      "Epoch 58/65, Train Loss: 1.1633, Test Loss: 1.0103\n",
      "Epoch 59/65, Train Loss: 1.1775, Test Loss: 1.0131\n",
      "Epoch 60/65, Train Loss: 1.2424, Test Loss: 1.0131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:02:45,676] Trial 155 finished with value: 1.0121628046035767 and parameters: {'num_hidden_layers': 9, 'layer_0_size': 61, 'layer_1_size': 165, 'layer_2_size': 233, 'layer_3_size': 133, 'layer_4_size': 185, 'layer_5_size': 101, 'layer_6_size': 35, 'layer_7_size': 89, 'layer_8_size': 166, 'dropout_rate': 0.22910089942651066, 'learning_rate': 1.8466173215818786e-05, 'batch_size': 256, 'epochs': 65}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/65, Train Loss: 1.1904, Test Loss: 1.0118\n",
      "Epoch 62/65, Train Loss: 1.2106, Test Loss: 1.0127\n",
      "Epoch 63/65, Train Loss: 1.2145, Test Loss: 1.0110\n",
      "Epoch 64/65, Train Loss: 1.2219, Test Loss: 1.0126\n",
      "Epoch 65/65, Train Loss: 1.1985, Test Loss: 1.0122\n",
      "Epoch 1/79, Train Loss: 1.0797, Test Loss: 0.8688\n",
      "Epoch 2/79, Train Loss: 0.9469, Test Loss: 0.9730\n",
      "Epoch 3/79, Train Loss: 0.9659, Test Loss: 0.8865\n",
      "Epoch 4/79, Train Loss: 0.9262, Test Loss: 0.8875\n",
      "Epoch 5/79, Train Loss: 0.9335, Test Loss: 0.8852\n",
      "Epoch 6/79, Train Loss: 0.9103, Test Loss: 0.8846\n",
      "Epoch 7/79, Train Loss: 0.9160, Test Loss: 0.8746\n",
      "Epoch 8/79, Train Loss: 0.9174, Test Loss: 0.8835\n",
      "Epoch 9/79, Train Loss: 0.8846, Test Loss: 0.9089\n",
      "Epoch 10/79, Train Loss: 0.8870, Test Loss: 0.8893\n",
      "Epoch 11/79, Train Loss: 0.8714, Test Loss: 0.9276\n",
      "Epoch 12/79, Train Loss: 0.8409, Test Loss: 0.9622\n",
      "Epoch 13/79, Train Loss: 0.7850, Test Loss: 1.0406\n",
      "Epoch 14/79, Train Loss: 0.7599, Test Loss: 1.0566\n",
      "Epoch 15/79, Train Loss: 0.7093, Test Loss: 1.1254\n",
      "Epoch 16/79, Train Loss: 0.6906, Test Loss: 1.0052\n",
      "Epoch 17/79, Train Loss: 0.6695, Test Loss: 1.0668\n",
      "Epoch 18/79, Train Loss: 0.6157, Test Loss: 1.1088\n",
      "Epoch 19/79, Train Loss: 0.5627, Test Loss: 1.0677\n",
      "Epoch 20/79, Train Loss: 0.5103, Test Loss: 1.0443\n",
      "Epoch 21/79, Train Loss: 0.4695, Test Loss: 1.2044\n",
      "Epoch 22/79, Train Loss: 0.4238, Test Loss: 1.0213\n",
      "Epoch 23/79, Train Loss: 0.3753, Test Loss: 1.0466\n",
      "Epoch 24/79, Train Loss: 0.3715, Test Loss: 1.1224\n",
      "Epoch 25/79, Train Loss: 0.3268, Test Loss: 1.1186\n",
      "Epoch 26/79, Train Loss: 0.3251, Test Loss: 1.0713\n",
      "Epoch 27/79, Train Loss: 0.3297, Test Loss: 1.1649\n",
      "Epoch 28/79, Train Loss: 0.3251, Test Loss: 1.0803\n",
      "Epoch 29/79, Train Loss: 0.3071, Test Loss: 1.0293\n",
      "Epoch 30/79, Train Loss: 0.2898, Test Loss: 1.0933\n",
      "Epoch 31/79, Train Loss: 0.2609, Test Loss: 1.0344\n",
      "Epoch 32/79, Train Loss: 0.2669, Test Loss: 1.0989\n",
      "Epoch 33/79, Train Loss: 0.2503, Test Loss: 0.9745\n",
      "Epoch 34/79, Train Loss: 0.2625, Test Loss: 1.0763\n",
      "Epoch 35/79, Train Loss: 0.2519, Test Loss: 1.0276\n",
      "Epoch 36/79, Train Loss: 0.2120, Test Loss: 1.0314\n",
      "Epoch 37/79, Train Loss: 0.2426, Test Loss: 0.9798\n",
      "Epoch 38/79, Train Loss: 0.2514, Test Loss: 1.0169\n",
      "Epoch 39/79, Train Loss: 0.2066, Test Loss: 1.0146\n",
      "Epoch 40/79, Train Loss: 0.2229, Test Loss: 1.0632\n",
      "Epoch 41/79, Train Loss: 0.2078, Test Loss: 1.0330\n",
      "Epoch 42/79, Train Loss: 0.1815, Test Loss: 1.0332\n",
      "Epoch 43/79, Train Loss: 0.1949, Test Loss: 0.9904\n",
      "Epoch 44/79, Train Loss: 0.1741, Test Loss: 1.0494\n",
      "Epoch 45/79, Train Loss: 0.2013, Test Loss: 0.9579\n",
      "Epoch 46/79, Train Loss: 0.1829, Test Loss: 0.9468\n",
      "Epoch 47/79, Train Loss: 0.1928, Test Loss: 0.9864\n",
      "Epoch 48/79, Train Loss: 0.1876, Test Loss: 0.9366\n",
      "Epoch 49/79, Train Loss: 0.1809, Test Loss: 0.9047\n",
      "Epoch 50/79, Train Loss: 0.1735, Test Loss: 0.9322\n",
      "Epoch 51/79, Train Loss: 0.1846, Test Loss: 0.9082\n",
      "Epoch 52/79, Train Loss: 0.1770, Test Loss: 1.0017\n",
      "Epoch 53/79, Train Loss: 0.1513, Test Loss: 0.8955\n",
      "Epoch 54/79, Train Loss: 0.1527, Test Loss: 0.9549\n",
      "Epoch 55/79, Train Loss: 0.1693, Test Loss: 0.9432\n",
      "Epoch 56/79, Train Loss: 0.1779, Test Loss: 0.9883\n",
      "Epoch 57/79, Train Loss: 0.1509, Test Loss: 1.0190\n",
      "Epoch 58/79, Train Loss: 0.1542, Test Loss: 0.9946\n",
      "Epoch 59/79, Train Loss: 0.1554, Test Loss: 1.0720\n",
      "Epoch 60/79, Train Loss: 0.1531, Test Loss: 1.0609\n",
      "Epoch 61/79, Train Loss: 0.1354, Test Loss: 1.0211\n",
      "Epoch 62/79, Train Loss: 0.1471, Test Loss: 1.0126\n",
      "Epoch 63/79, Train Loss: 0.1512, Test Loss: 1.0240\n",
      "Epoch 64/79, Train Loss: 0.1553, Test Loss: 1.0484\n",
      "Epoch 65/79, Train Loss: 0.1484, Test Loss: 1.0776\n",
      "Epoch 66/79, Train Loss: 0.1594, Test Loss: 1.0147\n",
      "Epoch 67/79, Train Loss: 0.1389, Test Loss: 0.9756\n",
      "Epoch 68/79, Train Loss: 0.1452, Test Loss: 1.0294\n",
      "Epoch 69/79, Train Loss: 0.1432, Test Loss: 1.0363\n",
      "Epoch 70/79, Train Loss: 0.1323, Test Loss: 1.0547\n",
      "Epoch 71/79, Train Loss: 0.1520, Test Loss: 1.0152\n",
      "Epoch 72/79, Train Loss: 0.1377, Test Loss: 1.0019\n",
      "Epoch 73/79, Train Loss: 0.1216, Test Loss: 1.0207\n",
      "Epoch 74/79, Train Loss: 0.1322, Test Loss: 1.0247\n",
      "Epoch 75/79, Train Loss: 0.1257, Test Loss: 1.0304\n",
      "Epoch 76/79, Train Loss: 0.1258, Test Loss: 1.0405\n",
      "Epoch 77/79, Train Loss: 0.1502, Test Loss: 1.0407\n",
      "Epoch 78/79, Train Loss: 0.1390, Test Loss: 0.9807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:02:56,102] Trial 156 finished with value: 1.0075733810663223 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 223, 'layer_1_size': 205, 'layer_2_size': 151, 'layer_3_size': 160, 'layer_4_size': 65, 'layer_5_size': 74, 'layer_6_size': 122, 'layer_7_size': 84, 'layer_8_size': 55, 'layer_9_size': 195, 'layer_10_size': 207, 'layer_11_size': 203, 'layer_12_size': 91, 'dropout_rate': 0.10239579624464754, 'learning_rate': 0.0016802034812560627, 'batch_size': 64, 'epochs': 79}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/79, Train Loss: 0.1354, Test Loss: 1.0076\n",
      "Epoch 1/86, Train Loss: 1.3308, Test Loss: 1.2439\n",
      "Epoch 2/86, Train Loss: 1.2711, Test Loss: 1.2434\n",
      "Epoch 3/86, Train Loss: 1.2366, Test Loss: 1.2346\n",
      "Epoch 4/86, Train Loss: 1.2204, Test Loss: 1.2225\n",
      "Epoch 5/86, Train Loss: 1.1710, Test Loss: 1.2201\n",
      "Epoch 6/86, Train Loss: 1.2067, Test Loss: 1.2242\n",
      "Epoch 7/86, Train Loss: 1.2182, Test Loss: 1.2186\n",
      "Epoch 8/86, Train Loss: 1.1661, Test Loss: 1.2116\n",
      "Epoch 9/86, Train Loss: 1.1773, Test Loss: 1.2039\n",
      "Epoch 10/86, Train Loss: 1.1544, Test Loss: 1.2114\n",
      "Epoch 11/86, Train Loss: 1.1731, Test Loss: 1.2060\n",
      "Epoch 12/86, Train Loss: 1.1764, Test Loss: 1.2123\n",
      "Epoch 13/86, Train Loss: 1.1663, Test Loss: 1.2057\n",
      "Epoch 14/86, Train Loss: 1.1623, Test Loss: 1.2213\n",
      "Epoch 15/86, Train Loss: 1.2255, Test Loss: 1.1974\n",
      "Epoch 16/86, Train Loss: 1.1267, Test Loss: 1.1925\n",
      "Epoch 17/86, Train Loss: 1.1634, Test Loss: 1.1937\n",
      "Epoch 18/86, Train Loss: 1.1569, Test Loss: 1.2137\n",
      "Epoch 19/86, Train Loss: 1.1416, Test Loss: 1.2083\n",
      "Epoch 20/86, Train Loss: 1.1608, Test Loss: 1.2143\n",
      "Epoch 21/86, Train Loss: 1.1384, Test Loss: 1.2071\n",
      "Epoch 22/86, Train Loss: 1.1005, Test Loss: 1.1996\n",
      "Epoch 23/86, Train Loss: 1.1527, Test Loss: 1.1977\n",
      "Epoch 24/86, Train Loss: 1.1742, Test Loss: 1.2005\n",
      "Epoch 25/86, Train Loss: 1.1855, Test Loss: 1.1946\n",
      "Epoch 26/86, Train Loss: 1.1427, Test Loss: 1.1955\n",
      "Epoch 27/86, Train Loss: 1.1729, Test Loss: 1.1945\n",
      "Epoch 28/86, Train Loss: 1.1369, Test Loss: 1.2026\n",
      "Epoch 29/86, Train Loss: 1.1458, Test Loss: 1.1909\n",
      "Epoch 30/86, Train Loss: 1.1311, Test Loss: 1.1925\n",
      "Epoch 31/86, Train Loss: 1.1109, Test Loss: 1.1927\n",
      "Epoch 32/86, Train Loss: 1.1393, Test Loss: 1.1909\n",
      "Epoch 33/86, Train Loss: 1.0943, Test Loss: 1.1980\n",
      "Epoch 34/86, Train Loss: 1.1132, Test Loss: 1.1979\n",
      "Epoch 35/86, Train Loss: 1.1640, Test Loss: 1.1956\n",
      "Epoch 36/86, Train Loss: 1.1587, Test Loss: 1.1913\n",
      "Epoch 37/86, Train Loss: 1.1433, Test Loss: 1.1921\n",
      "Epoch 38/86, Train Loss: 1.1387, Test Loss: 1.1919\n",
      "Epoch 39/86, Train Loss: 1.1282, Test Loss: 1.1874\n",
      "Epoch 40/86, Train Loss: 1.1314, Test Loss: 1.1902\n",
      "Epoch 41/86, Train Loss: 1.1369, Test Loss: 1.1899\n",
      "Epoch 42/86, Train Loss: 1.1155, Test Loss: 1.1926\n",
      "Epoch 43/86, Train Loss: 1.1303, Test Loss: 1.2054\n",
      "Epoch 44/86, Train Loss: 1.1502, Test Loss: 1.2050\n",
      "Epoch 45/86, Train Loss: 1.1417, Test Loss: 1.1918\n",
      "Epoch 46/86, Train Loss: 1.1276, Test Loss: 1.1926\n",
      "Epoch 47/86, Train Loss: 1.1528, Test Loss: 1.1909\n",
      "Epoch 48/86, Train Loss: 1.1517, Test Loss: 1.1908\n",
      "Epoch 49/86, Train Loss: 1.1449, Test Loss: 1.1959\n",
      "Epoch 50/86, Train Loss: 1.1561, Test Loss: 1.1862\n",
      "Epoch 51/86, Train Loss: 1.1511, Test Loss: 1.1875\n",
      "Epoch 52/86, Train Loss: 1.1521, Test Loss: 1.1946\n",
      "Epoch 53/86, Train Loss: 1.1228, Test Loss: 1.1903\n",
      "Epoch 54/86, Train Loss: 1.1179, Test Loss: 1.1851\n",
      "Epoch 55/86, Train Loss: 1.0706, Test Loss: 1.1895\n",
      "Epoch 56/86, Train Loss: 1.0738, Test Loss: 1.1944\n",
      "Epoch 57/86, Train Loss: 1.0895, Test Loss: 1.1899\n",
      "Epoch 58/86, Train Loss: 1.0604, Test Loss: 1.2001\n",
      "Epoch 59/86, Train Loss: 1.1214, Test Loss: 1.2095\n",
      "Epoch 60/86, Train Loss: 1.1146, Test Loss: 1.1994\n",
      "Epoch 61/86, Train Loss: 1.1183, Test Loss: 1.1950\n",
      "Epoch 62/86, Train Loss: 1.1160, Test Loss: 1.2063\n",
      "Epoch 63/86, Train Loss: 1.1414, Test Loss: 1.1960\n",
      "Epoch 64/86, Train Loss: 1.1035, Test Loss: 1.1921\n",
      "Epoch 65/86, Train Loss: 1.1061, Test Loss: 1.1990\n",
      "Epoch 66/86, Train Loss: 1.1359, Test Loss: 1.1915\n",
      "Epoch 67/86, Train Loss: 1.1112, Test Loss: 1.1963\n",
      "Epoch 68/86, Train Loss: 1.0931, Test Loss: 1.1956\n",
      "Epoch 69/86, Train Loss: 1.1075, Test Loss: 1.1944\n",
      "Epoch 70/86, Train Loss: 1.1281, Test Loss: 1.1917\n",
      "Epoch 71/86, Train Loss: 1.1159, Test Loss: 1.1936\n",
      "Epoch 72/86, Train Loss: 1.1307, Test Loss: 1.1983\n",
      "Epoch 73/86, Train Loss: 1.1012, Test Loss: 1.1910\n",
      "Epoch 74/86, Train Loss: 1.1171, Test Loss: 1.1942\n",
      "Epoch 75/86, Train Loss: 1.0874, Test Loss: 1.1909\n",
      "Epoch 76/86, Train Loss: 1.1116, Test Loss: 1.1924\n",
      "Epoch 77/86, Train Loss: 1.1026, Test Loss: 1.1922\n",
      "Epoch 78/86, Train Loss: 1.0976, Test Loss: 1.1952\n",
      "Epoch 79/86, Train Loss: 1.1067, Test Loss: 1.1961\n",
      "Epoch 80/86, Train Loss: 1.0829, Test Loss: 1.1926\n",
      "Epoch 81/86, Train Loss: 1.0706, Test Loss: 1.1925\n",
      "Epoch 82/86, Train Loss: 1.0943, Test Loss: 1.1909\n",
      "Epoch 83/86, Train Loss: 1.0754, Test Loss: 1.1940\n",
      "Epoch 84/86, Train Loss: 1.0770, Test Loss: 1.1974\n",
      "Epoch 85/86, Train Loss: 1.1040, Test Loss: 1.1984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:03:23,718] Trial 157 finished with value: 1.1998285566057478 and parameters: {'num_hidden_layers': 19, 'layer_0_size': 88, 'layer_1_size': 186, 'layer_2_size': 141, 'layer_3_size': 143, 'layer_4_size': 193, 'layer_5_size': 128, 'layer_6_size': 191, 'layer_7_size': 88, 'layer_8_size': 145, 'layer_9_size': 76, 'layer_10_size': 74, 'layer_11_size': 145, 'layer_12_size': 255, 'layer_13_size': 78, 'layer_14_size': 236, 'layer_15_size': 165, 'layer_16_size': 136, 'layer_17_size': 188, 'layer_18_size': 88, 'dropout_rate': 0.3246934558449613, 'learning_rate': 6.49126048404669e-05, 'batch_size': 32, 'epochs': 86}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/86, Train Loss: 1.0899, Test Loss: 1.1998\n",
      "Epoch 1/48, Train Loss: 1.1607, Test Loss: 0.9635\n",
      "Epoch 2/48, Train Loss: 1.1492, Test Loss: 1.0016\n",
      "Epoch 3/48, Train Loss: 1.1598, Test Loss: 1.0144\n",
      "Epoch 4/48, Train Loss: 1.1382, Test Loss: 1.0296\n",
      "Epoch 5/48, Train Loss: 1.1481, Test Loss: 1.0279\n",
      "Epoch 6/48, Train Loss: 1.1222, Test Loss: 1.0270\n",
      "Epoch 7/48, Train Loss: 1.0859, Test Loss: 1.0099\n",
      "Epoch 8/48, Train Loss: 1.1166, Test Loss: 1.0041\n",
      "Epoch 9/48, Train Loss: 1.1185, Test Loss: 0.9836\n",
      "Epoch 10/48, Train Loss: 1.0986, Test Loss: 0.9719\n",
      "Epoch 11/48, Train Loss: 1.0697, Test Loss: 0.9624\n",
      "Epoch 12/48, Train Loss: 1.1052, Test Loss: 0.9525\n",
      "Epoch 13/48, Train Loss: 1.0874, Test Loss: 0.9496\n",
      "Epoch 14/48, Train Loss: 1.0681, Test Loss: 0.9497\n",
      "Epoch 15/48, Train Loss: 1.0697, Test Loss: 0.9571\n",
      "Epoch 16/48, Train Loss: 1.0915, Test Loss: 0.9558\n",
      "Epoch 17/48, Train Loss: 1.0769, Test Loss: 0.9598\n",
      "Epoch 18/48, Train Loss: 1.0991, Test Loss: 0.9609\n",
      "Epoch 19/48, Train Loss: 1.0970, Test Loss: 0.9533\n",
      "Epoch 20/48, Train Loss: 1.0976, Test Loss: 0.9508\n",
      "Epoch 21/48, Train Loss: 1.0718, Test Loss: 0.9525\n",
      "Epoch 22/48, Train Loss: 1.0962, Test Loss: 0.9519\n",
      "Epoch 23/48, Train Loss: 1.0899, Test Loss: 0.9512\n",
      "Epoch 24/48, Train Loss: 1.0839, Test Loss: 0.9497\n",
      "Epoch 25/48, Train Loss: 1.0651, Test Loss: 0.9492\n",
      "Epoch 26/48, Train Loss: 1.0452, Test Loss: 0.9482\n",
      "Epoch 27/48, Train Loss: 1.0881, Test Loss: 0.9443\n",
      "Epoch 28/48, Train Loss: 1.0674, Test Loss: 0.9441\n",
      "Epoch 29/48, Train Loss: 1.0581, Test Loss: 0.9457\n",
      "Epoch 30/48, Train Loss: 1.0732, Test Loss: 0.9484\n",
      "Epoch 31/48, Train Loss: 1.0426, Test Loss: 0.9481\n",
      "Epoch 32/48, Train Loss: 1.0626, Test Loss: 0.9503\n",
      "Epoch 33/48, Train Loss: 1.0238, Test Loss: 0.9513\n",
      "Epoch 34/48, Train Loss: 1.0506, Test Loss: 0.9508\n",
      "Epoch 35/48, Train Loss: 1.0448, Test Loss: 0.9497\n",
      "Epoch 36/48, Train Loss: 1.0555, Test Loss: 0.9477\n",
      "Epoch 37/48, Train Loss: 1.0413, Test Loss: 0.9458\n",
      "Epoch 38/48, Train Loss: 1.0360, Test Loss: 0.9474\n",
      "Epoch 39/48, Train Loss: 1.0577, Test Loss: 0.9467\n",
      "Epoch 40/48, Train Loss: 1.0475, Test Loss: 0.9428\n",
      "Epoch 41/48, Train Loss: 1.0583, Test Loss: 0.9418\n",
      "Epoch 42/48, Train Loss: 1.0243, Test Loss: 0.9419\n",
      "Epoch 43/48, Train Loss: 1.0103, Test Loss: 0.9417\n",
      "Epoch 44/48, Train Loss: 1.0463, Test Loss: 0.9385\n",
      "Epoch 45/48, Train Loss: 1.0268, Test Loss: 0.9398\n",
      "Epoch 46/48, Train Loss: 1.0753, Test Loss: 0.9401\n",
      "Epoch 47/48, Train Loss: 1.0491, Test Loss: 0.9417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:03:28,572] Trial 158 finished with value: 0.9403968155384064 and parameters: {'num_hidden_layers': 16, 'layer_0_size': 102, 'layer_1_size': 144, 'layer_2_size': 135, 'layer_3_size': 104, 'layer_4_size': 172, 'layer_5_size': 210, 'layer_6_size': 91, 'layer_7_size': 43, 'layer_8_size': 162, 'layer_9_size': 253, 'layer_10_size': 244, 'layer_11_size': 168, 'layer_12_size': 195, 'layer_13_size': 187, 'layer_14_size': 161, 'layer_15_size': 55, 'dropout_rate': 0.21176263838178744, 'learning_rate': 0.00021288021200956575, 'batch_size': 128, 'epochs': 48}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/48, Train Loss: 1.0502, Test Loss: 0.9404\n",
      "Epoch 1/56, Train Loss: 1.3642, Test Loss: 0.9183\n",
      "Epoch 2/56, Train Loss: 1.3772, Test Loss: 0.9298\n",
      "Epoch 3/56, Train Loss: 1.3143, Test Loss: 0.9451\n",
      "Epoch 4/56, Train Loss: 1.3397, Test Loss: 0.9613\n",
      "Epoch 5/56, Train Loss: 1.3536, Test Loss: 0.9741\n",
      "Epoch 6/56, Train Loss: 1.2776, Test Loss: 0.9841\n",
      "Epoch 7/56, Train Loss: 1.3472, Test Loss: 0.9935\n",
      "Epoch 8/56, Train Loss: 1.3033, Test Loss: 0.9977\n",
      "Epoch 9/56, Train Loss: 1.2891, Test Loss: 0.9991\n",
      "Epoch 10/56, Train Loss: 1.2657, Test Loss: 1.0005\n",
      "Epoch 11/56, Train Loss: 1.2657, Test Loss: 0.9956\n",
      "Epoch 12/56, Train Loss: 1.2830, Test Loss: 0.9920\n",
      "Epoch 13/56, Train Loss: 1.2910, Test Loss: 0.9877\n",
      "Epoch 14/56, Train Loss: 1.2735, Test Loss: 0.9855\n",
      "Epoch 15/56, Train Loss: 1.2617, Test Loss: 0.9785\n",
      "Epoch 16/56, Train Loss: 1.2449, Test Loss: 0.9769\n",
      "Epoch 17/56, Train Loss: 1.2574, Test Loss: 0.9751\n",
      "Epoch 18/56, Train Loss: 1.2728, Test Loss: 0.9701\n",
      "Epoch 19/56, Train Loss: 1.3022, Test Loss: 0.9695\n",
      "Epoch 20/56, Train Loss: 1.2255, Test Loss: 0.9645\n",
      "Epoch 21/56, Train Loss: 1.1873, Test Loss: 0.9660\n",
      "Epoch 22/56, Train Loss: 1.2044, Test Loss: 0.9632\n",
      "Epoch 23/56, Train Loss: 1.1807, Test Loss: 0.9581\n",
      "Epoch 24/56, Train Loss: 1.2217, Test Loss: 0.9549\n",
      "Epoch 25/56, Train Loss: 1.2303, Test Loss: 0.9587\n",
      "Epoch 26/56, Train Loss: 1.1763, Test Loss: 0.9519\n",
      "Epoch 27/56, Train Loss: 1.2062, Test Loss: 0.9513\n",
      "Epoch 28/56, Train Loss: 1.1900, Test Loss: 0.9524\n",
      "Epoch 29/56, Train Loss: 1.2282, Test Loss: 0.9514\n",
      "Epoch 30/56, Train Loss: 1.1715, Test Loss: 0.9493\n",
      "Epoch 31/56, Train Loss: 1.1932, Test Loss: 0.9449\n",
      "Epoch 32/56, Train Loss: 1.2413, Test Loss: 0.9414\n",
      "Epoch 33/56, Train Loss: 1.2152, Test Loss: 0.9378\n",
      "Epoch 34/56, Train Loss: 1.1716, Test Loss: 0.9394\n",
      "Epoch 35/56, Train Loss: 1.1964, Test Loss: 0.9386\n",
      "Epoch 36/56, Train Loss: 1.1851, Test Loss: 0.9354\n",
      "Epoch 37/56, Train Loss: 1.1911, Test Loss: 0.9349\n",
      "Epoch 38/56, Train Loss: 1.1622, Test Loss: 0.9358\n",
      "Epoch 39/56, Train Loss: 1.1791, Test Loss: 0.9339\n",
      "Epoch 40/56, Train Loss: 1.1888, Test Loss: 0.9316\n",
      "Epoch 41/56, Train Loss: 1.0979, Test Loss: 0.9283\n",
      "Epoch 42/56, Train Loss: 1.1699, Test Loss: 0.9271\n",
      "Epoch 43/56, Train Loss: 1.1615, Test Loss: 0.9279\n",
      "Epoch 44/56, Train Loss: 1.1540, Test Loss: 0.9288\n",
      "Epoch 45/56, Train Loss: 1.1106, Test Loss: 0.9289\n",
      "Epoch 46/56, Train Loss: 1.1415, Test Loss: 0.9269\n",
      "Epoch 47/56, Train Loss: 1.1699, Test Loss: 0.9217\n",
      "Epoch 48/56, Train Loss: 1.1551, Test Loss: 0.9184\n",
      "Epoch 49/56, Train Loss: 1.1118, Test Loss: 0.9187\n",
      "Epoch 50/56, Train Loss: 1.1729, Test Loss: 0.9181\n",
      "Epoch 51/56, Train Loss: 1.1535, Test Loss: 0.9179\n",
      "Epoch 52/56, Train Loss: 1.1507, Test Loss: 0.9184\n",
      "Epoch 53/56, Train Loss: 1.1707, Test Loss: 0.9188\n",
      "Epoch 54/56, Train Loss: 1.1107, Test Loss: 0.9173\n",
      "Epoch 55/56, Train Loss: 1.1318, Test Loss: 0.9156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:03:30,855] Trial 159 finished with value: 0.9151861667633057 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 77, 'layer_1_size': 139, 'layer_2_size': 224, 'layer_3_size': 233, 'layer_4_size': 204, 'layer_5_size': 177, 'layer_6_size': 41, 'layer_7_size': 102, 'dropout_rate': 0.2841507825328769, 'learning_rate': 5.079262025294689e-05, 'batch_size': 256, 'epochs': 56}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/56, Train Loss: 1.1251, Test Loss: 0.9152\n",
      "Epoch 1/74, Train Loss: 1.2273, Test Loss: 0.7837\n",
      "Epoch 2/74, Train Loss: 1.1816, Test Loss: 0.7863\n",
      "Epoch 3/74, Train Loss: 1.1554, Test Loss: 0.7974\n",
      "Epoch 4/74, Train Loss: 1.1497, Test Loss: 0.7856\n",
      "Epoch 5/74, Train Loss: 1.1114, Test Loss: 0.7884\n",
      "Epoch 6/74, Train Loss: 1.1380, Test Loss: 0.7861\n",
      "Epoch 7/74, Train Loss: 1.1420, Test Loss: 0.7845\n",
      "Epoch 8/74, Train Loss: 1.0875, Test Loss: 0.7803\n",
      "Epoch 9/74, Train Loss: 1.1326, Test Loss: 0.7765\n",
      "Epoch 10/74, Train Loss: 1.1259, Test Loss: 0.7789\n",
      "Epoch 11/74, Train Loss: 1.1386, Test Loss: 0.7826\n",
      "Epoch 12/74, Train Loss: 1.1329, Test Loss: 0.7848\n",
      "Epoch 13/74, Train Loss: 1.0842, Test Loss: 0.7837\n",
      "Epoch 14/74, Train Loss: 1.1180, Test Loss: 0.7873\n",
      "Epoch 15/74, Train Loss: 1.1234, Test Loss: 0.7807\n",
      "Epoch 16/74, Train Loss: 1.1079, Test Loss: 0.7827\n",
      "Epoch 17/74, Train Loss: 1.0891, Test Loss: 0.7782\n",
      "Epoch 18/74, Train Loss: 1.1013, Test Loss: 0.7782\n",
      "Epoch 19/74, Train Loss: 1.1009, Test Loss: 0.7755\n",
      "Epoch 20/74, Train Loss: 1.0916, Test Loss: 0.7777\n",
      "Epoch 21/74, Train Loss: 1.0994, Test Loss: 0.7755\n",
      "Epoch 22/74, Train Loss: 1.1327, Test Loss: 0.7738\n",
      "Epoch 23/74, Train Loss: 1.1061, Test Loss: 0.7790\n",
      "Epoch 24/74, Train Loss: 1.0938, Test Loss: 0.7807\n",
      "Epoch 25/74, Train Loss: 1.1059, Test Loss: 0.7777\n",
      "Epoch 26/74, Train Loss: 1.1057, Test Loss: 0.7808\n",
      "Epoch 27/74, Train Loss: 1.0847, Test Loss: 0.7828\n",
      "Epoch 28/74, Train Loss: 1.0688, Test Loss: 0.7793\n",
      "Epoch 29/74, Train Loss: 1.0946, Test Loss: 0.7830\n",
      "Epoch 30/74, Train Loss: 1.0973, Test Loss: 0.7801\n",
      "Epoch 31/74, Train Loss: 1.0744, Test Loss: 0.7794\n",
      "Epoch 32/74, Train Loss: 1.0663, Test Loss: 0.7834\n",
      "Epoch 33/74, Train Loss: 1.0811, Test Loss: 0.7788\n",
      "Epoch 34/74, Train Loss: 1.0696, Test Loss: 0.7780\n",
      "Epoch 35/74, Train Loss: 1.0703, Test Loss: 0.7777\n",
      "Epoch 36/74, Train Loss: 1.0591, Test Loss: 0.7798\n",
      "Epoch 37/74, Train Loss: 1.0874, Test Loss: 0.7798\n",
      "Epoch 38/74, Train Loss: 1.0861, Test Loss: 0.7794\n",
      "Epoch 39/74, Train Loss: 1.0778, Test Loss: 0.7802\n",
      "Epoch 40/74, Train Loss: 1.0798, Test Loss: 0.7829\n",
      "Epoch 41/74, Train Loss: 1.0628, Test Loss: 0.7798\n",
      "Epoch 42/74, Train Loss: 1.0631, Test Loss: 0.7819\n",
      "Epoch 43/74, Train Loss: 1.0706, Test Loss: 0.7814\n",
      "Epoch 44/74, Train Loss: 1.0793, Test Loss: 0.7817\n",
      "Epoch 45/74, Train Loss: 1.0740, Test Loss: 0.7800\n",
      "Epoch 46/74, Train Loss: 1.0673, Test Loss: 0.7800\n",
      "Epoch 47/74, Train Loss: 1.0863, Test Loss: 0.7786\n",
      "Epoch 48/74, Train Loss: 1.0785, Test Loss: 0.7825\n",
      "Epoch 49/74, Train Loss: 1.0968, Test Loss: 0.7818\n",
      "Epoch 50/74, Train Loss: 1.0801, Test Loss: 0.7815\n",
      "Epoch 51/74, Train Loss: 1.0541, Test Loss: 0.7790\n",
      "Epoch 52/74, Train Loss: 1.0608, Test Loss: 0.7805\n",
      "Epoch 53/74, Train Loss: 1.1185, Test Loss: 0.7782\n",
      "Epoch 54/74, Train Loss: 1.0764, Test Loss: 0.7782\n",
      "Epoch 55/74, Train Loss: 1.0971, Test Loss: 0.7799\n",
      "Epoch 56/74, Train Loss: 1.0678, Test Loss: 0.7804\n",
      "Epoch 57/74, Train Loss: 1.0913, Test Loss: 0.7794\n",
      "Epoch 58/74, Train Loss: 1.0868, Test Loss: 0.7822\n",
      "Epoch 59/74, Train Loss: 1.0589, Test Loss: 0.7799\n",
      "Epoch 60/74, Train Loss: 1.0654, Test Loss: 0.7818\n",
      "Epoch 61/74, Train Loss: 1.0837, Test Loss: 0.7795\n",
      "Epoch 62/74, Train Loss: 1.0573, Test Loss: 0.7798\n",
      "Epoch 63/74, Train Loss: 1.0721, Test Loss: 0.7797\n",
      "Epoch 64/74, Train Loss: 1.0729, Test Loss: 0.7841\n",
      "Epoch 65/74, Train Loss: 1.0783, Test Loss: 0.7822\n",
      "Epoch 66/74, Train Loss: 1.0817, Test Loss: 0.7795\n",
      "Epoch 67/74, Train Loss: 1.0801, Test Loss: 0.7764\n",
      "Epoch 68/74, Train Loss: 1.0720, Test Loss: 0.7787\n",
      "Epoch 69/74, Train Loss: 1.0577, Test Loss: 0.7784\n",
      "Epoch 70/74, Train Loss: 1.0690, Test Loss: 0.7808\n",
      "Epoch 71/74, Train Loss: 1.0523, Test Loss: 0.7783\n",
      "Epoch 72/74, Train Loss: 1.0740, Test Loss: 0.7797\n",
      "Epoch 73/74, Train Loss: 1.0791, Test Loss: 0.7778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:03:48,992] Trial 160 finished with value: 0.7781093120574951 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 131, 'layer_1_size': 132, 'layer_2_size': 147, 'layer_3_size': 151, 'layer_4_size': 110, 'layer_5_size': 135, 'layer_6_size': 75, 'layer_7_size': 116, 'layer_8_size': 176, 'layer_9_size': 228, 'layer_10_size': 219, 'layer_11_size': 173, 'layer_12_size': 47, 'layer_13_size': 121, 'dropout_rate': 0.24815291317290053, 'learning_rate': 0.0004020370069882703, 'batch_size': 32, 'epochs': 74}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/74, Train Loss: 1.0796, Test Loss: 0.7781\n",
      "Epoch 1/74, Train Loss: 1.1524, Test Loss: 0.8217\n",
      "Epoch 2/74, Train Loss: 1.0638, Test Loss: 0.8188\n",
      "Epoch 3/74, Train Loss: 1.0448, Test Loss: 0.8089\n",
      "Epoch 4/74, Train Loss: 1.0248, Test Loss: 0.8128\n",
      "Epoch 5/74, Train Loss: 1.0360, Test Loss: 0.8166\n",
      "Epoch 6/74, Train Loss: 1.0019, Test Loss: 0.8045\n",
      "Epoch 7/74, Train Loss: 1.0306, Test Loss: 0.8007\n",
      "Epoch 8/74, Train Loss: 1.0353, Test Loss: 0.8008\n",
      "Epoch 9/74, Train Loss: 1.0126, Test Loss: 0.8045\n",
      "Epoch 10/74, Train Loss: 1.0484, Test Loss: 0.8101\n",
      "Epoch 11/74, Train Loss: 1.0230, Test Loss: 0.8051\n",
      "Epoch 12/74, Train Loss: 1.0250, Test Loss: 0.8023\n",
      "Epoch 13/74, Train Loss: 0.9986, Test Loss: 0.8054\n",
      "Epoch 14/74, Train Loss: 1.0170, Test Loss: 0.7992\n",
      "Epoch 15/74, Train Loss: 1.0343, Test Loss: 0.8049\n",
      "Epoch 16/74, Train Loss: 1.0110, Test Loss: 0.8054\n",
      "Epoch 17/74, Train Loss: 1.0134, Test Loss: 0.8024\n",
      "Epoch 18/74, Train Loss: 1.0203, Test Loss: 0.8034\n",
      "Epoch 19/74, Train Loss: 1.0064, Test Loss: 0.7969\n",
      "Epoch 20/74, Train Loss: 0.9978, Test Loss: 0.7988\n",
      "Epoch 21/74, Train Loss: 1.0202, Test Loss: 0.7999\n",
      "Epoch 22/74, Train Loss: 1.0181, Test Loss: 0.7970\n",
      "Epoch 23/74, Train Loss: 1.0104, Test Loss: 0.7982\n",
      "Epoch 24/74, Train Loss: 1.0138, Test Loss: 0.7995\n",
      "Epoch 25/74, Train Loss: 0.9922, Test Loss: 0.7969\n",
      "Epoch 26/74, Train Loss: 0.9789, Test Loss: 0.7947\n",
      "Epoch 27/74, Train Loss: 0.9986, Test Loss: 0.7958\n",
      "Epoch 28/74, Train Loss: 0.9955, Test Loss: 0.7970\n",
      "Epoch 29/74, Train Loss: 0.9817, Test Loss: 0.7960\n",
      "Epoch 30/74, Train Loss: 0.9917, Test Loss: 0.8044\n",
      "Epoch 31/74, Train Loss: 1.0169, Test Loss: 0.8041\n",
      "Epoch 32/74, Train Loss: 0.9839, Test Loss: 0.8110\n",
      "Epoch 33/74, Train Loss: 0.9674, Test Loss: 0.8245\n",
      "Epoch 34/74, Train Loss: 0.9877, Test Loss: 0.8048\n",
      "Epoch 35/74, Train Loss: 0.9652, Test Loss: 0.7970\n",
      "Epoch 36/74, Train Loss: 0.9988, Test Loss: 0.7957\n",
      "Epoch 37/74, Train Loss: 1.0065, Test Loss: 0.7974\n",
      "Epoch 38/74, Train Loss: 1.0054, Test Loss: 0.7979\n",
      "Epoch 39/74, Train Loss: 0.9851, Test Loss: 0.7951\n",
      "Epoch 40/74, Train Loss: 0.9727, Test Loss: 0.7964\n",
      "Epoch 41/74, Train Loss: 0.9619, Test Loss: 0.7982\n",
      "Epoch 42/74, Train Loss: 0.9793, Test Loss: 0.7962\n",
      "Epoch 43/74, Train Loss: 0.9799, Test Loss: 0.7948\n",
      "Epoch 44/74, Train Loss: 0.9765, Test Loss: 0.7942\n",
      "Epoch 45/74, Train Loss: 0.9723, Test Loss: 0.7973\n",
      "Epoch 46/74, Train Loss: 0.9982, Test Loss: 0.7939\n",
      "Epoch 47/74, Train Loss: 0.9881, Test Loss: 0.7935\n",
      "Epoch 48/74, Train Loss: 0.9897, Test Loss: 0.7939\n",
      "Epoch 49/74, Train Loss: 0.9733, Test Loss: 0.7935\n",
      "Epoch 50/74, Train Loss: 0.9932, Test Loss: 0.7943\n",
      "Epoch 51/74, Train Loss: 0.9935, Test Loss: 0.7946\n",
      "Epoch 52/74, Train Loss: 0.9734, Test Loss: 0.7946\n",
      "Epoch 53/74, Train Loss: 1.0008, Test Loss: 0.8021\n",
      "Epoch 54/74, Train Loss: 0.9601, Test Loss: 0.7963\n",
      "Epoch 55/74, Train Loss: 0.9790, Test Loss: 0.7961\n",
      "Epoch 56/74, Train Loss: 1.0293, Test Loss: 0.7949\n",
      "Epoch 57/74, Train Loss: 0.9840, Test Loss: 0.7995\n",
      "Epoch 58/74, Train Loss: 0.9705, Test Loss: 0.7961\n",
      "Epoch 59/74, Train Loss: 0.9764, Test Loss: 0.7996\n",
      "Epoch 60/74, Train Loss: 0.9759, Test Loss: 0.7985\n",
      "Epoch 61/74, Train Loss: 0.9769, Test Loss: 0.7936\n",
      "Epoch 62/74, Train Loss: 0.9935, Test Loss: 0.7971\n",
      "Epoch 63/74, Train Loss: 0.9860, Test Loss: 0.7963\n",
      "Epoch 64/74, Train Loss: 0.9773, Test Loss: 0.7968\n",
      "Epoch 65/74, Train Loss: 0.9633, Test Loss: 0.7958\n",
      "Epoch 66/74, Train Loss: 0.9587, Test Loss: 0.7977\n",
      "Epoch 67/74, Train Loss: 0.9936, Test Loss: 0.7959\n",
      "Epoch 68/74, Train Loss: 0.9845, Test Loss: 0.8031\n",
      "Epoch 69/74, Train Loss: 0.9736, Test Loss: 0.7993\n",
      "Epoch 70/74, Train Loss: 0.9636, Test Loss: 0.8074\n",
      "Epoch 71/74, Train Loss: 0.9797, Test Loss: 0.7999\n",
      "Epoch 72/74, Train Loss: 0.9899, Test Loss: 0.8001\n",
      "Epoch 73/74, Train Loss: 0.9768, Test Loss: 0.8003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:04:07,961] Trial 161 finished with value: 0.7973646010671344 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 135, 'layer_1_size': 129, 'layer_2_size': 146, 'layer_3_size': 147, 'layer_4_size': 111, 'layer_5_size': 136, 'layer_6_size': 73, 'layer_7_size': 113, 'layer_8_size': 179, 'layer_9_size': 239, 'layer_10_size': 234, 'layer_11_size': 178, 'layer_12_size': 45, 'layer_13_size': 169, 'dropout_rate': 0.24963236352898646, 'learning_rate': 0.00035127144769740937, 'batch_size': 32, 'epochs': 74}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/74, Train Loss: 0.9658, Test Loss: 0.7974\n",
      "Epoch 1/76, Train Loss: 1.1661, Test Loss: 0.9667\n",
      "Epoch 2/76, Train Loss: 1.1486, Test Loss: 0.9767\n",
      "Epoch 3/76, Train Loss: 1.1017, Test Loss: 0.9719\n",
      "Epoch 4/76, Train Loss: 1.1533, Test Loss: 0.9770\n",
      "Epoch 5/76, Train Loss: 1.1378, Test Loss: 0.9716\n",
      "Epoch 6/76, Train Loss: 1.1886, Test Loss: 0.9788\n",
      "Epoch 7/76, Train Loss: 1.1140, Test Loss: 0.9776\n",
      "Epoch 8/76, Train Loss: 1.1046, Test Loss: 0.9708\n",
      "Epoch 9/76, Train Loss: 1.1206, Test Loss: 0.9675\n",
      "Epoch 10/76, Train Loss: 1.1192, Test Loss: 0.9589\n",
      "Epoch 11/76, Train Loss: 1.1001, Test Loss: 0.9614\n",
      "Epoch 12/76, Train Loss: 1.1145, Test Loss: 0.9617\n",
      "Epoch 13/76, Train Loss: 1.1181, Test Loss: 0.9587\n",
      "Epoch 14/76, Train Loss: 1.0836, Test Loss: 0.9593\n",
      "Epoch 15/76, Train Loss: 1.0712, Test Loss: 0.9635\n",
      "Epoch 16/76, Train Loss: 1.0766, Test Loss: 0.9657\n",
      "Epoch 17/76, Train Loss: 1.0947, Test Loss: 0.9681\n",
      "Epoch 18/76, Train Loss: 1.0937, Test Loss: 0.9674\n",
      "Epoch 19/76, Train Loss: 1.0909, Test Loss: 0.9705\n",
      "Epoch 20/76, Train Loss: 1.0935, Test Loss: 0.9708\n",
      "Epoch 21/76, Train Loss: 1.0927, Test Loss: 0.9786\n",
      "Epoch 22/76, Train Loss: 1.0969, Test Loss: 0.9656\n",
      "Epoch 23/76, Train Loss: 1.0990, Test Loss: 0.9625\n",
      "Epoch 24/76, Train Loss: 1.1068, Test Loss: 0.9653\n",
      "Epoch 25/76, Train Loss: 1.0987, Test Loss: 0.9621\n",
      "Epoch 26/76, Train Loss: 1.0852, Test Loss: 0.9653\n",
      "Epoch 27/76, Train Loss: 1.1063, Test Loss: 0.9641\n",
      "Epoch 28/76, Train Loss: 1.0662, Test Loss: 0.9636\n",
      "Epoch 29/76, Train Loss: 1.0749, Test Loss: 0.9677\n",
      "Epoch 30/76, Train Loss: 1.0739, Test Loss: 0.9648\n",
      "Epoch 31/76, Train Loss: 1.0750, Test Loss: 0.9641\n",
      "Epoch 32/76, Train Loss: 1.1370, Test Loss: 0.9649\n",
      "Epoch 33/76, Train Loss: 1.1158, Test Loss: 0.9668\n",
      "Epoch 34/76, Train Loss: 1.0501, Test Loss: 0.9614\n",
      "Epoch 35/76, Train Loss: 1.0564, Test Loss: 0.9628\n",
      "Epoch 36/76, Train Loss: 1.0741, Test Loss: 0.9667\n",
      "Epoch 37/76, Train Loss: 1.0685, Test Loss: 0.9688\n",
      "Epoch 38/76, Train Loss: 1.0620, Test Loss: 0.9666\n",
      "Epoch 39/76, Train Loss: 1.0656, Test Loss: 0.9649\n",
      "Epoch 40/76, Train Loss: 1.0818, Test Loss: 0.9648\n",
      "Epoch 41/76, Train Loss: 1.0496, Test Loss: 0.9656\n",
      "Epoch 42/76, Train Loss: 1.0576, Test Loss: 0.9663\n",
      "Epoch 43/76, Train Loss: 1.0837, Test Loss: 0.9684\n",
      "Epoch 44/76, Train Loss: 1.0776, Test Loss: 0.9673\n",
      "Epoch 45/76, Train Loss: 1.0634, Test Loss: 0.9684\n",
      "Epoch 46/76, Train Loss: 1.0554, Test Loss: 0.9692\n",
      "Epoch 47/76, Train Loss: 1.0852, Test Loss: 0.9670\n",
      "Epoch 48/76, Train Loss: 1.0646, Test Loss: 0.9674\n",
      "Epoch 49/76, Train Loss: 1.0561, Test Loss: 0.9651\n",
      "Epoch 50/76, Train Loss: 1.0578, Test Loss: 0.9661\n",
      "Epoch 51/76, Train Loss: 1.0666, Test Loss: 0.9660\n",
      "Epoch 52/76, Train Loss: 1.0649, Test Loss: 0.9662\n",
      "Epoch 53/76, Train Loss: 1.0546, Test Loss: 0.9643\n",
      "Epoch 54/76, Train Loss: 1.0629, Test Loss: 0.9630\n",
      "Epoch 55/76, Train Loss: 1.0776, Test Loss: 0.9646\n",
      "Epoch 56/76, Train Loss: 1.0621, Test Loss: 0.9653\n",
      "Epoch 57/76, Train Loss: 1.0452, Test Loss: 0.9660\n",
      "Epoch 58/76, Train Loss: 1.0494, Test Loss: 0.9655\n",
      "Epoch 59/76, Train Loss: 1.0589, Test Loss: 0.9670\n",
      "Epoch 60/76, Train Loss: 1.0354, Test Loss: 0.9673\n",
      "Epoch 61/76, Train Loss: 1.0522, Test Loss: 0.9667\n",
      "Epoch 62/76, Train Loss: 1.0336, Test Loss: 0.9662\n",
      "Epoch 63/76, Train Loss: 1.0519, Test Loss: 0.9641\n",
      "Epoch 64/76, Train Loss: 1.0395, Test Loss: 0.9682\n",
      "Epoch 65/76, Train Loss: 1.0514, Test Loss: 0.9670\n",
      "Epoch 66/76, Train Loss: 1.0570, Test Loss: 0.9681\n",
      "Epoch 67/76, Train Loss: 1.0674, Test Loss: 0.9682\n",
      "Epoch 68/76, Train Loss: 1.0572, Test Loss: 0.9680\n",
      "Epoch 69/76, Train Loss: 1.0605, Test Loss: 0.9675\n",
      "Epoch 70/76, Train Loss: 1.0587, Test Loss: 0.9684\n",
      "Epoch 71/76, Train Loss: 1.0663, Test Loss: 0.9677\n",
      "Epoch 72/76, Train Loss: 1.0551, Test Loss: 0.9670\n",
      "Epoch 73/76, Train Loss: 1.0596, Test Loss: 0.9649\n",
      "Epoch 74/76, Train Loss: 1.0525, Test Loss: 0.9647\n",
      "Epoch 75/76, Train Loss: 1.0368, Test Loss: 0.9667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:04:26,445] Trial 162 finished with value: 0.9639849407332284 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 136, 'layer_1_size': 129, 'layer_2_size': 146, 'layer_3_size': 151, 'layer_4_size': 100, 'layer_5_size': 134, 'layer_6_size': 76, 'layer_7_size': 114, 'layer_8_size': 177, 'layer_9_size': 231, 'layer_10_size': 219, 'layer_11_size': 174, 'layer_12_size': 45, 'layer_13_size': 121, 'dropout_rate': 0.25126741882183645, 'learning_rate': 0.00037311179204733226, 'batch_size': 32, 'epochs': 76}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/76, Train Loss: 1.0588, Test Loss: 0.9640\n",
      "Epoch 1/74, Train Loss: 1.1189, Test Loss: 1.0345\n",
      "Epoch 2/74, Train Loss: 1.0743, Test Loss: 1.0371\n",
      "Epoch 3/74, Train Loss: 1.0520, Test Loss: 1.0363\n",
      "Epoch 4/74, Train Loss: 1.0555, Test Loss: 1.0424\n",
      "Epoch 5/74, Train Loss: 1.0645, Test Loss: 1.0386\n",
      "Epoch 6/74, Train Loss: 1.0483, Test Loss: 1.0343\n",
      "Epoch 7/74, Train Loss: 1.0477, Test Loss: 1.0350\n",
      "Epoch 8/74, Train Loss: 1.0498, Test Loss: 1.0412\n",
      "Epoch 9/74, Train Loss: 1.0429, Test Loss: 1.0371\n",
      "Epoch 10/74, Train Loss: 1.0572, Test Loss: 1.0409\n",
      "Epoch 11/74, Train Loss: 1.0440, Test Loss: 1.0394\n",
      "Epoch 12/74, Train Loss: 1.0383, Test Loss: 1.0409\n",
      "Epoch 13/74, Train Loss: 1.0253, Test Loss: 1.0399\n",
      "Epoch 14/74, Train Loss: 1.0946, Test Loss: 1.0398\n",
      "Epoch 15/74, Train Loss: 1.0149, Test Loss: 1.0482\n",
      "Epoch 16/74, Train Loss: 1.0130, Test Loss: 1.0456\n",
      "Epoch 17/74, Train Loss: 1.0633, Test Loss: 1.0409\n",
      "Epoch 18/74, Train Loss: 1.0260, Test Loss: 1.0361\n",
      "Epoch 19/74, Train Loss: 1.0496, Test Loss: 1.0384\n",
      "Epoch 20/74, Train Loss: 1.0199, Test Loss: 1.0396\n",
      "Epoch 21/74, Train Loss: 1.0153, Test Loss: 1.0370\n",
      "Epoch 22/74, Train Loss: 1.0258, Test Loss: 1.0398\n",
      "Epoch 23/74, Train Loss: 1.0355, Test Loss: 1.0384\n",
      "Epoch 24/74, Train Loss: 1.0128, Test Loss: 1.0385\n",
      "Epoch 25/74, Train Loss: 1.0543, Test Loss: 1.0396\n",
      "Epoch 26/74, Train Loss: 1.0194, Test Loss: 1.0412\n",
      "Epoch 27/74, Train Loss: 1.0154, Test Loss: 1.0396\n",
      "Epoch 28/74, Train Loss: 1.0008, Test Loss: 1.0374\n",
      "Epoch 29/74, Train Loss: 1.0116, Test Loss: 1.0378\n",
      "Epoch 30/74, Train Loss: 0.9920, Test Loss: 1.0380\n",
      "Epoch 31/74, Train Loss: 1.0123, Test Loss: 1.0429\n",
      "Epoch 32/74, Train Loss: 1.0120, Test Loss: 1.0363\n",
      "Epoch 33/74, Train Loss: 1.0041, Test Loss: 1.0407\n",
      "Epoch 34/74, Train Loss: 1.0106, Test Loss: 1.0419\n",
      "Epoch 35/74, Train Loss: 1.0043, Test Loss: 1.0411\n",
      "Epoch 36/74, Train Loss: 1.0161, Test Loss: 1.0386\n",
      "Epoch 37/74, Train Loss: 1.0305, Test Loss: 1.0412\n",
      "Epoch 38/74, Train Loss: 1.0101, Test Loss: 1.0387\n",
      "Epoch 39/74, Train Loss: 1.0165, Test Loss: 1.0413\n",
      "Epoch 40/74, Train Loss: 1.0123, Test Loss: 1.0398\n",
      "Epoch 41/74, Train Loss: 0.9930, Test Loss: 1.0395\n",
      "Epoch 42/74, Train Loss: 1.0168, Test Loss: 1.0406\n",
      "Epoch 43/74, Train Loss: 1.0161, Test Loss: 1.0436\n",
      "Epoch 44/74, Train Loss: 1.0180, Test Loss: 1.0415\n",
      "Epoch 45/74, Train Loss: 1.0196, Test Loss: 1.0408\n",
      "Epoch 46/74, Train Loss: 1.0177, Test Loss: 1.0377\n",
      "Epoch 47/74, Train Loss: 1.0431, Test Loss: 1.0390\n",
      "Epoch 48/74, Train Loss: 1.0178, Test Loss: 1.0386\n",
      "Epoch 49/74, Train Loss: 1.0094, Test Loss: 1.0363\n",
      "Epoch 50/74, Train Loss: 1.0006, Test Loss: 1.0328\n",
      "Epoch 51/74, Train Loss: 1.0065, Test Loss: 1.0371\n",
      "Epoch 52/74, Train Loss: 1.0059, Test Loss: 1.0378\n",
      "Epoch 53/74, Train Loss: 1.0495, Test Loss: 1.0423\n",
      "Epoch 54/74, Train Loss: 0.9856, Test Loss: 1.0399\n",
      "Epoch 55/74, Train Loss: 1.0380, Test Loss: 1.0428\n",
      "Epoch 56/74, Train Loss: 0.9947, Test Loss: 1.0407\n",
      "Epoch 57/74, Train Loss: 1.0250, Test Loss: 1.0428\n",
      "Epoch 58/74, Train Loss: 1.0161, Test Loss: 1.0403\n",
      "Epoch 59/74, Train Loss: 0.9929, Test Loss: 1.0415\n",
      "Epoch 60/74, Train Loss: 1.0094, Test Loss: 1.0405\n",
      "Epoch 61/74, Train Loss: 1.0044, Test Loss: 1.0389\n",
      "Epoch 62/74, Train Loss: 1.0128, Test Loss: 1.0392\n",
      "Epoch 63/74, Train Loss: 1.0021, Test Loss: 1.0392\n",
      "Epoch 64/74, Train Loss: 1.0169, Test Loss: 1.0387\n",
      "Epoch 65/74, Train Loss: 1.0062, Test Loss: 1.0342\n",
      "Epoch 66/74, Train Loss: 0.9905, Test Loss: 1.0333\n",
      "Epoch 67/74, Train Loss: 1.0101, Test Loss: 1.0315\n",
      "Epoch 68/74, Train Loss: 0.9990, Test Loss: 1.0334\n",
      "Epoch 69/74, Train Loss: 0.9969, Test Loss: 1.0335\n",
      "Epoch 70/74, Train Loss: 1.0046, Test Loss: 1.0342\n",
      "Epoch 71/74, Train Loss: 0.9922, Test Loss: 1.0364\n",
      "Epoch 72/74, Train Loss: 1.0080, Test Loss: 1.0357\n",
      "Epoch 73/74, Train Loss: 1.0274, Test Loss: 1.0380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:04:44,400] Trial 163 finished with value: 1.0385619550943375 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 141, 'layer_1_size': 133, 'layer_2_size': 146, 'layer_3_size': 152, 'layer_4_size': 115, 'layer_5_size': 144, 'layer_6_size': 110, 'layer_7_size': 123, 'layer_8_size': 168, 'layer_9_size': 223, 'layer_10_size': 228, 'layer_11_size': 179, 'layer_12_size': 39, 'layer_13_size': 171, 'dropout_rate': 0.24321419385475307, 'learning_rate': 0.0004963114093017073, 'batch_size': 32, 'epochs': 74}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/74, Train Loss: 1.0197, Test Loss: 1.0386\n",
      "Epoch 1/75, Train Loss: 1.1311, Test Loss: 0.9181\n",
      "Epoch 2/75, Train Loss: 1.0766, Test Loss: 0.9246\n",
      "Epoch 3/75, Train Loss: 1.1049, Test Loss: 0.9356\n",
      "Epoch 4/75, Train Loss: 1.0823, Test Loss: 0.9621\n",
      "Epoch 5/75, Train Loss: 1.1167, Test Loss: 0.9437\n",
      "Epoch 6/75, Train Loss: 1.0641, Test Loss: 0.9694\n",
      "Epoch 7/75, Train Loss: 1.0352, Test Loss: 0.9424\n",
      "Epoch 8/75, Train Loss: 1.0222, Test Loss: 0.9527\n",
      "Epoch 9/75, Train Loss: 1.0620, Test Loss: 0.9547\n",
      "Epoch 10/75, Train Loss: 1.0490, Test Loss: 0.9308\n",
      "Epoch 11/75, Train Loss: 1.0416, Test Loss: 0.9302\n",
      "Epoch 12/75, Train Loss: 1.0501, Test Loss: 0.9162\n",
      "Epoch 13/75, Train Loss: 1.0159, Test Loss: 0.9087\n",
      "Epoch 14/75, Train Loss: 1.0443, Test Loss: 0.9313\n",
      "Epoch 15/75, Train Loss: 1.0081, Test Loss: 0.9145\n",
      "Epoch 16/75, Train Loss: 1.0412, Test Loss: 0.9064\n",
      "Epoch 17/75, Train Loss: 1.0249, Test Loss: 0.9169\n",
      "Epoch 18/75, Train Loss: 1.0293, Test Loss: 0.9098\n",
      "Epoch 19/75, Train Loss: 1.0076, Test Loss: 0.9289\n",
      "Epoch 20/75, Train Loss: 1.0343, Test Loss: 0.9237\n",
      "Epoch 21/75, Train Loss: 1.0318, Test Loss: 0.9267\n",
      "Epoch 22/75, Train Loss: 1.0278, Test Loss: 0.9179\n",
      "Epoch 23/75, Train Loss: 1.0239, Test Loss: 0.9079\n",
      "Epoch 24/75, Train Loss: 1.0149, Test Loss: 0.9053\n",
      "Epoch 25/75, Train Loss: 1.0102, Test Loss: 0.9003\n",
      "Epoch 26/75, Train Loss: 1.0010, Test Loss: 0.9133\n",
      "Epoch 27/75, Train Loss: 1.0208, Test Loss: 0.9116\n",
      "Epoch 28/75, Train Loss: 0.9952, Test Loss: 0.9101\n",
      "Epoch 29/75, Train Loss: 0.9913, Test Loss: 0.9334\n",
      "Epoch 30/75, Train Loss: 0.9812, Test Loss: 0.9283\n",
      "Epoch 31/75, Train Loss: 0.9890, Test Loss: 0.9326\n",
      "Epoch 32/75, Train Loss: 1.0278, Test Loss: 0.9333\n",
      "Epoch 33/75, Train Loss: 0.9859, Test Loss: 0.9206\n",
      "Epoch 34/75, Train Loss: 0.9955, Test Loss: 0.9218\n",
      "Epoch 35/75, Train Loss: 1.0409, Test Loss: 0.9239\n",
      "Epoch 36/75, Train Loss: 1.0097, Test Loss: 0.9162\n",
      "Epoch 37/75, Train Loss: 1.0139, Test Loss: 0.9194\n",
      "Epoch 38/75, Train Loss: 0.9972, Test Loss: 0.9238\n",
      "Epoch 39/75, Train Loss: 0.9748, Test Loss: 0.9109\n",
      "Epoch 40/75, Train Loss: 0.9790, Test Loss: 0.9074\n",
      "Epoch 41/75, Train Loss: 1.0062, Test Loss: 0.9035\n",
      "Epoch 42/75, Train Loss: 0.9851, Test Loss: 0.9128\n",
      "Epoch 43/75, Train Loss: 0.9863, Test Loss: 0.9101\n",
      "Epoch 44/75, Train Loss: 0.9888, Test Loss: 0.9165\n",
      "Epoch 45/75, Train Loss: 0.9939, Test Loss: 0.9017\n",
      "Epoch 46/75, Train Loss: 0.9831, Test Loss: 0.9103\n",
      "Epoch 47/75, Train Loss: 1.0042, Test Loss: 0.9056\n",
      "Epoch 48/75, Train Loss: 0.9814, Test Loss: 0.9064\n",
      "Epoch 49/75, Train Loss: 0.9664, Test Loss: 0.9120\n",
      "Epoch 50/75, Train Loss: 0.9979, Test Loss: 0.9100\n",
      "Epoch 51/75, Train Loss: 0.9823, Test Loss: 0.9116\n",
      "Epoch 52/75, Train Loss: 0.9898, Test Loss: 0.9169\n",
      "Epoch 53/75, Train Loss: 0.9759, Test Loss: 0.9158\n",
      "Epoch 54/75, Train Loss: 0.9913, Test Loss: 0.9240\n",
      "Epoch 55/75, Train Loss: 0.9804, Test Loss: 0.9299\n",
      "Epoch 56/75, Train Loss: 1.0033, Test Loss: 0.9335\n",
      "Epoch 57/75, Train Loss: 0.9770, Test Loss: 0.9440\n",
      "Epoch 58/75, Train Loss: 0.9766, Test Loss: 0.9417\n",
      "Epoch 59/75, Train Loss: 1.0097, Test Loss: 0.9391\n",
      "Epoch 60/75, Train Loss: 0.9929, Test Loss: 0.9342\n",
      "Epoch 61/75, Train Loss: 0.9898, Test Loss: 0.9336\n",
      "Epoch 62/75, Train Loss: 0.9710, Test Loss: 0.9229\n",
      "Epoch 63/75, Train Loss: 1.0089, Test Loss: 0.9236\n",
      "Epoch 64/75, Train Loss: 0.9839, Test Loss: 0.9186\n",
      "Epoch 65/75, Train Loss: 0.9676, Test Loss: 0.9174\n",
      "Epoch 66/75, Train Loss: 0.9947, Test Loss: 0.9177\n",
      "Epoch 67/75, Train Loss: 0.9710, Test Loss: 0.9189\n",
      "Epoch 68/75, Train Loss: 0.9825, Test Loss: 0.9149\n",
      "Epoch 69/75, Train Loss: 0.9766, Test Loss: 0.9171\n",
      "Epoch 70/75, Train Loss: 0.9789, Test Loss: 0.9156\n",
      "Epoch 71/75, Train Loss: 0.9832, Test Loss: 0.9074\n",
      "Epoch 72/75, Train Loss: 1.0086, Test Loss: 0.9092\n",
      "Epoch 73/75, Train Loss: 0.9762, Test Loss: 0.9053\n",
      "Epoch 74/75, Train Loss: 0.9800, Test Loss: 0.9032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:05:01,398] Trial 164 finished with value: 0.9056230059691838 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 133, 'layer_1_size': 138, 'layer_2_size': 138, 'layer_3_size': 250, 'layer_4_size': 121, 'layer_5_size': 139, 'layer_6_size': 74, 'layer_7_size': 108, 'layer_8_size': 194, 'layer_9_size': 212, 'layer_10_size': 233, 'layer_11_size': 192, 'layer_12_size': 45, 'dropout_rate': 0.2707173492570107, 'learning_rate': 0.0003178740836252991, 'batch_size': 32, 'epochs': 75}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/75, Train Loss: 0.9764, Test Loss: 0.9056\n",
      "Epoch 1/82, Train Loss: 1.1888, Test Loss: 1.1990\n",
      "Epoch 2/82, Train Loss: 1.1098, Test Loss: 1.2129\n",
      "Epoch 3/82, Train Loss: 1.0591, Test Loss: 1.2012\n",
      "Epoch 4/82, Train Loss: 1.0777, Test Loss: 1.1885\n",
      "Epoch 5/82, Train Loss: 1.1108, Test Loss: 1.1817\n",
      "Epoch 6/82, Train Loss: 1.0841, Test Loss: 1.1851\n",
      "Epoch 7/82, Train Loss: 1.0784, Test Loss: 1.1861\n",
      "Epoch 8/82, Train Loss: 1.1088, Test Loss: 1.1905\n",
      "Epoch 9/82, Train Loss: 1.0835, Test Loss: 1.1924\n",
      "Epoch 10/82, Train Loss: 1.0565, Test Loss: 1.1951\n",
      "Epoch 11/82, Train Loss: 1.0592, Test Loss: 1.1951\n",
      "Epoch 12/82, Train Loss: 1.0780, Test Loss: 1.1928\n",
      "Epoch 13/82, Train Loss: 1.0544, Test Loss: 1.2066\n",
      "Epoch 14/82, Train Loss: 1.0605, Test Loss: 1.2009\n",
      "Epoch 15/82, Train Loss: 1.0497, Test Loss: 1.1961\n",
      "Epoch 16/82, Train Loss: 1.0643, Test Loss: 1.1914\n",
      "Epoch 17/82, Train Loss: 1.0520, Test Loss: 1.1877\n",
      "Epoch 18/82, Train Loss: 1.0250, Test Loss: 1.1850\n",
      "Epoch 19/82, Train Loss: 1.0367, Test Loss: 1.1889\n",
      "Epoch 20/82, Train Loss: 1.0462, Test Loss: 1.1924\n",
      "Epoch 21/82, Train Loss: 1.0419, Test Loss: 1.1886\n",
      "Epoch 22/82, Train Loss: 1.0115, Test Loss: 1.1875\n",
      "Epoch 23/82, Train Loss: 1.0378, Test Loss: 1.1866\n",
      "Epoch 24/82, Train Loss: 1.0693, Test Loss: 1.1858\n",
      "Epoch 25/82, Train Loss: 1.0390, Test Loss: 1.1807\n",
      "Epoch 26/82, Train Loss: 1.0435, Test Loss: 1.1827\n",
      "Epoch 27/82, Train Loss: 1.0335, Test Loss: 1.1833\n",
      "Epoch 28/82, Train Loss: 1.0190, Test Loss: 1.1845\n",
      "Epoch 29/82, Train Loss: 1.0179, Test Loss: 1.1870\n",
      "Epoch 30/82, Train Loss: 1.0264, Test Loss: 1.1861\n",
      "Epoch 31/82, Train Loss: 1.0298, Test Loss: 1.1863\n",
      "Epoch 32/82, Train Loss: 1.0342, Test Loss: 1.1870\n",
      "Epoch 33/82, Train Loss: 1.0319, Test Loss: 1.1894\n",
      "Epoch 34/82, Train Loss: 1.0576, Test Loss: 1.1846\n",
      "Epoch 35/82, Train Loss: 1.0071, Test Loss: 1.1870\n",
      "Epoch 36/82, Train Loss: 1.0184, Test Loss: 1.1843\n",
      "Epoch 37/82, Train Loss: 1.0404, Test Loss: 1.1849\n",
      "Epoch 38/82, Train Loss: 1.0330, Test Loss: 1.1889\n",
      "Epoch 39/82, Train Loss: 1.0439, Test Loss: 1.1872\n",
      "Epoch 40/82, Train Loss: 1.0143, Test Loss: 1.1884\n",
      "Epoch 41/82, Train Loss: 1.0047, Test Loss: 1.1862\n",
      "Epoch 42/82, Train Loss: 0.9957, Test Loss: 1.1896\n",
      "Epoch 43/82, Train Loss: 1.0202, Test Loss: 1.1874\n",
      "Epoch 44/82, Train Loss: 1.0055, Test Loss: 1.1898\n",
      "Epoch 45/82, Train Loss: 1.0023, Test Loss: 1.1943\n",
      "Epoch 46/82, Train Loss: 1.0267, Test Loss: 1.1932\n",
      "Epoch 47/82, Train Loss: 0.9987, Test Loss: 1.1898\n",
      "Epoch 48/82, Train Loss: 1.0087, Test Loss: 1.1911\n",
      "Epoch 49/82, Train Loss: 1.0033, Test Loss: 1.1904\n",
      "Epoch 50/82, Train Loss: 1.0152, Test Loss: 1.1909\n",
      "Epoch 51/82, Train Loss: 1.0095, Test Loss: 1.1886\n",
      "Epoch 52/82, Train Loss: 1.0054, Test Loss: 1.1903\n",
      "Epoch 53/82, Train Loss: 1.0001, Test Loss: 1.1885\n",
      "Epoch 54/82, Train Loss: 1.0289, Test Loss: 1.1899\n",
      "Epoch 55/82, Train Loss: 1.0058, Test Loss: 1.1899\n",
      "Epoch 56/82, Train Loss: 1.0057, Test Loss: 1.1918\n",
      "Epoch 57/82, Train Loss: 1.0029, Test Loss: 1.1881\n",
      "Epoch 58/82, Train Loss: 0.9903, Test Loss: 1.1871\n",
      "Epoch 59/82, Train Loss: 1.0086, Test Loss: 1.1894\n",
      "Epoch 60/82, Train Loss: 1.0313, Test Loss: 1.1911\n",
      "Epoch 61/82, Train Loss: 1.0087, Test Loss: 1.1907\n",
      "Epoch 62/82, Train Loss: 0.9987, Test Loss: 1.1912\n",
      "Epoch 63/82, Train Loss: 1.0003, Test Loss: 1.1977\n",
      "Epoch 64/82, Train Loss: 1.0160, Test Loss: 1.1925\n",
      "Epoch 65/82, Train Loss: 1.0077, Test Loss: 1.1906\n",
      "Epoch 66/82, Train Loss: 1.0064, Test Loss: 1.1924\n",
      "Epoch 67/82, Train Loss: 1.0067, Test Loss: 1.1914\n",
      "Epoch 68/82, Train Loss: 0.9953, Test Loss: 1.1888\n",
      "Epoch 69/82, Train Loss: 0.9976, Test Loss: 1.1943\n",
      "Epoch 70/82, Train Loss: 1.0428, Test Loss: 1.1924\n",
      "Epoch 71/82, Train Loss: 1.0041, Test Loss: 1.1930\n",
      "Epoch 72/82, Train Loss: 1.0002, Test Loss: 1.1960\n",
      "Epoch 73/82, Train Loss: 0.9915, Test Loss: 1.1903\n",
      "Epoch 74/82, Train Loss: 1.0124, Test Loss: 1.1919\n",
      "Epoch 75/82, Train Loss: 1.0056, Test Loss: 1.1934\n",
      "Epoch 76/82, Train Loss: 1.0098, Test Loss: 1.1939\n",
      "Epoch 77/82, Train Loss: 0.9971, Test Loss: 1.1982\n",
      "Epoch 78/82, Train Loss: 1.0156, Test Loss: 1.1902\n",
      "Epoch 79/82, Train Loss: 1.0082, Test Loss: 1.1896\n",
      "Epoch 80/82, Train Loss: 0.9940, Test Loss: 1.1883\n",
      "Epoch 81/82, Train Loss: 1.0003, Test Loss: 1.1906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:05:21,733] Trial 165 finished with value: 1.187404888016837 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 164, 'layer_1_size': 122, 'layer_2_size': 196, 'layer_3_size': 241, 'layer_4_size': 109, 'layer_5_size': 130, 'layer_6_size': 84, 'layer_7_size': 120, 'layer_8_size': 183, 'layer_9_size': 237, 'layer_10_size': 241, 'layer_11_size': 129, 'layer_12_size': 32, 'layer_13_size': 160, 'dropout_rate': 0.2615099397201694, 'learning_rate': 0.0003011776794025756, 'batch_size': 32, 'epochs': 82}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/82, Train Loss: 0.9960, Test Loss: 1.1874\n",
      "Epoch 1/71, Train Loss: 1.1669, Test Loss: 0.8281\n",
      "Epoch 2/71, Train Loss: 1.1041, Test Loss: 0.8085\n",
      "Epoch 3/71, Train Loss: 1.0682, Test Loss: 0.7967\n",
      "Epoch 4/71, Train Loss: 1.0977, Test Loss: 0.7967\n",
      "Epoch 5/71, Train Loss: 1.0896, Test Loss: 0.7979\n",
      "Epoch 6/71, Train Loss: 1.0510, Test Loss: 0.8012\n",
      "Epoch 7/71, Train Loss: 1.0434, Test Loss: 0.7963\n",
      "Epoch 8/71, Train Loss: 1.0557, Test Loss: 0.7880\n",
      "Epoch 9/71, Train Loss: 1.0460, Test Loss: 0.7937\n",
      "Epoch 10/71, Train Loss: 1.0420, Test Loss: 0.7945\n",
      "Epoch 11/71, Train Loss: 1.0299, Test Loss: 0.7925\n",
      "Epoch 12/71, Train Loss: 1.0070, Test Loss: 0.7935\n",
      "Epoch 13/71, Train Loss: 1.0315, Test Loss: 0.7945\n",
      "Epoch 14/71, Train Loss: 1.0358, Test Loss: 0.7928\n",
      "Epoch 15/71, Train Loss: 1.0179, Test Loss: 0.7938\n",
      "Epoch 16/71, Train Loss: 1.0383, Test Loss: 0.7950\n",
      "Epoch 17/71, Train Loss: 1.0464, Test Loss: 0.7955\n",
      "Epoch 18/71, Train Loss: 1.0055, Test Loss: 0.7958\n",
      "Epoch 19/71, Train Loss: 1.0144, Test Loss: 0.7938\n",
      "Epoch 20/71, Train Loss: 1.0102, Test Loss: 0.7901\n",
      "Epoch 21/71, Train Loss: 1.0079, Test Loss: 0.7907\n",
      "Epoch 22/71, Train Loss: 0.9990, Test Loss: 0.7908\n",
      "Epoch 23/71, Train Loss: 0.9824, Test Loss: 0.7938\n",
      "Epoch 24/71, Train Loss: 0.9953, Test Loss: 0.7912\n",
      "Epoch 25/71, Train Loss: 1.0175, Test Loss: 0.7901\n",
      "Epoch 26/71, Train Loss: 1.0153, Test Loss: 0.7951\n",
      "Epoch 27/71, Train Loss: 1.0113, Test Loss: 0.7943\n",
      "Epoch 28/71, Train Loss: 0.9958, Test Loss: 0.7944\n",
      "Epoch 29/71, Train Loss: 1.0112, Test Loss: 0.7960\n",
      "Epoch 30/71, Train Loss: 1.0086, Test Loss: 0.7946\n",
      "Epoch 31/71, Train Loss: 0.9890, Test Loss: 0.7911\n",
      "Epoch 32/71, Train Loss: 0.9724, Test Loss: 0.7923\n",
      "Epoch 33/71, Train Loss: 0.9849, Test Loss: 0.7941\n",
      "Epoch 34/71, Train Loss: 0.9964, Test Loss: 0.8003\n",
      "Epoch 35/71, Train Loss: 0.9853, Test Loss: 0.8026\n",
      "Epoch 36/71, Train Loss: 1.0096, Test Loss: 0.8070\n",
      "Epoch 37/71, Train Loss: 0.9896, Test Loss: 0.8047\n",
      "Epoch 38/71, Train Loss: 0.9852, Test Loss: 0.8115\n",
      "Epoch 39/71, Train Loss: 0.9910, Test Loss: 0.8066\n",
      "Epoch 40/71, Train Loss: 1.0066, Test Loss: 0.8032\n",
      "Epoch 41/71, Train Loss: 1.0383, Test Loss: 0.8064\n",
      "Epoch 42/71, Train Loss: 0.9811, Test Loss: 0.8061\n",
      "Epoch 43/71, Train Loss: 0.9966, Test Loss: 0.8127\n",
      "Epoch 44/71, Train Loss: 0.9930, Test Loss: 0.8121\n",
      "Epoch 45/71, Train Loss: 1.0108, Test Loss: 0.8113\n",
      "Epoch 46/71, Train Loss: 1.0207, Test Loss: 0.8112\n",
      "Epoch 47/71, Train Loss: 0.9814, Test Loss: 0.8007\n",
      "Epoch 48/71, Train Loss: 0.9829, Test Loss: 0.8000\n",
      "Epoch 49/71, Train Loss: 0.9739, Test Loss: 0.8033\n",
      "Epoch 50/71, Train Loss: 1.0064, Test Loss: 0.8077\n",
      "Epoch 51/71, Train Loss: 0.9933, Test Loss: 0.8007\n",
      "Epoch 52/71, Train Loss: 0.9908, Test Loss: 0.8075\n",
      "Epoch 53/71, Train Loss: 0.9795, Test Loss: 0.8039\n",
      "Epoch 54/71, Train Loss: 0.9825, Test Loss: 0.8109\n",
      "Epoch 55/71, Train Loss: 0.9625, Test Loss: 0.8205\n",
      "Epoch 56/71, Train Loss: 0.9981, Test Loss: 0.8112\n",
      "Epoch 57/71, Train Loss: 1.0019, Test Loss: 0.8074\n",
      "Epoch 58/71, Train Loss: 0.9824, Test Loss: 0.8034\n",
      "Epoch 59/71, Train Loss: 0.9779, Test Loss: 0.8040\n",
      "Epoch 60/71, Train Loss: 0.9880, Test Loss: 0.8008\n",
      "Epoch 61/71, Train Loss: 0.9964, Test Loss: 0.8031\n",
      "Epoch 62/71, Train Loss: 0.9847, Test Loss: 0.8045\n",
      "Epoch 63/71, Train Loss: 0.9769, Test Loss: 0.7984\n",
      "Epoch 64/71, Train Loss: 0.9730, Test Loss: 0.8018\n",
      "Epoch 65/71, Train Loss: 0.9911, Test Loss: 0.8070\n",
      "Epoch 66/71, Train Loss: 0.9802, Test Loss: 0.8033\n",
      "Epoch 67/71, Train Loss: 0.9655, Test Loss: 0.8018\n",
      "Epoch 68/71, Train Loss: 0.9820, Test Loss: 0.7988\n",
      "Epoch 69/71, Train Loss: 0.9745, Test Loss: 0.7983\n",
      "Epoch 70/71, Train Loss: 0.9807, Test Loss: 0.7975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:05:39,207] Trial 166 finished with value: 0.8015028067997524 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 207, 'layer_1_size': 115, 'layer_2_size': 241, 'layer_3_size': 147, 'layer_4_size': 113, 'layer_5_size': 125, 'layer_6_size': 97, 'layer_7_size': 250, 'layer_8_size': 175, 'layer_9_size': 227, 'layer_10_size': 150, 'layer_11_size': 154, 'layer_12_size': 58, 'layer_13_size': 148, 'dropout_rate': 0.2991335227379107, 'learning_rate': 0.0004189372135691784, 'batch_size': 32, 'epochs': 71}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/71, Train Loss: 0.9639, Test Loss: 0.8015\n",
      "Epoch 1/71, Train Loss: 1.2767, Test Loss: 1.1378\n",
      "Epoch 2/71, Train Loss: 1.1822, Test Loss: 1.1361\n",
      "Epoch 3/71, Train Loss: 1.2029, Test Loss: 1.1274\n",
      "Epoch 4/71, Train Loss: 1.1738, Test Loss: 1.1193\n",
      "Epoch 5/71, Train Loss: 1.1278, Test Loss: 1.1209\n",
      "Epoch 6/71, Train Loss: 1.0956, Test Loss: 1.1236\n",
      "Epoch 7/71, Train Loss: 1.1679, Test Loss: 1.1310\n",
      "Epoch 8/71, Train Loss: 1.1422, Test Loss: 1.1337\n",
      "Epoch 9/71, Train Loss: 1.1100, Test Loss: 1.1258\n",
      "Epoch 10/71, Train Loss: 1.1015, Test Loss: 1.1171\n",
      "Epoch 11/71, Train Loss: 1.1363, Test Loss: 1.1158\n",
      "Epoch 12/71, Train Loss: 1.1373, Test Loss: 1.1216\n",
      "Epoch 13/71, Train Loss: 1.1149, Test Loss: 1.1261\n",
      "Epoch 14/71, Train Loss: 1.1203, Test Loss: 1.1340\n",
      "Epoch 15/71, Train Loss: 1.0956, Test Loss: 1.1312\n",
      "Epoch 16/71, Train Loss: 1.0680, Test Loss: 1.1259\n",
      "Epoch 17/71, Train Loss: 1.1229, Test Loss: 1.1259\n",
      "Epoch 18/71, Train Loss: 1.1180, Test Loss: 1.1223\n",
      "Epoch 19/71, Train Loss: 1.1232, Test Loss: 1.1255\n",
      "Epoch 20/71, Train Loss: 1.1122, Test Loss: 1.1344\n",
      "Epoch 21/71, Train Loss: 1.0948, Test Loss: 1.1331\n",
      "Epoch 22/71, Train Loss: 1.0775, Test Loss: 1.1345\n",
      "Epoch 23/71, Train Loss: 1.0864, Test Loss: 1.1289\n",
      "Epoch 24/71, Train Loss: 1.0848, Test Loss: 1.1306\n",
      "Epoch 25/71, Train Loss: 1.1010, Test Loss: 1.1256\n",
      "Epoch 26/71, Train Loss: 1.0887, Test Loss: 1.1250\n",
      "Epoch 27/71, Train Loss: 1.0848, Test Loss: 1.1234\n",
      "Epoch 28/71, Train Loss: 1.0584, Test Loss: 1.1279\n",
      "Epoch 29/71, Train Loss: 1.0617, Test Loss: 1.1268\n",
      "Epoch 30/71, Train Loss: 1.0694, Test Loss: 1.1303\n",
      "Epoch 31/71, Train Loss: 1.0709, Test Loss: 1.1350\n",
      "Epoch 32/71, Train Loss: 1.0639, Test Loss: 1.1374\n",
      "Epoch 33/71, Train Loss: 1.0628, Test Loss: 1.1277\n",
      "Epoch 34/71, Train Loss: 1.0448, Test Loss: 1.1268\n",
      "Epoch 35/71, Train Loss: 1.0684, Test Loss: 1.1364\n",
      "Epoch 36/71, Train Loss: 1.0771, Test Loss: 1.1322\n",
      "Epoch 37/71, Train Loss: 1.0652, Test Loss: 1.1344\n",
      "Epoch 38/71, Train Loss: 1.0816, Test Loss: 1.1298\n",
      "Epoch 39/71, Train Loss: 1.0751, Test Loss: 1.1297\n",
      "Epoch 40/71, Train Loss: 1.0685, Test Loss: 1.1304\n",
      "Epoch 41/71, Train Loss: 1.0499, Test Loss: 1.1337\n",
      "Epoch 42/71, Train Loss: 1.0619, Test Loss: 1.1330\n",
      "Epoch 43/71, Train Loss: 1.0810, Test Loss: 1.1328\n",
      "Epoch 44/71, Train Loss: 1.0889, Test Loss: 1.1346\n",
      "Epoch 45/71, Train Loss: 1.0667, Test Loss: 1.1285\n",
      "Epoch 46/71, Train Loss: 1.0635, Test Loss: 1.1272\n",
      "Epoch 47/71, Train Loss: 1.0488, Test Loss: 1.1259\n",
      "Epoch 48/71, Train Loss: 1.0483, Test Loss: 1.1289\n",
      "Epoch 49/71, Train Loss: 1.0636, Test Loss: 1.1311\n",
      "Epoch 50/71, Train Loss: 1.0560, Test Loss: 1.1291\n",
      "Epoch 51/71, Train Loss: 1.0653, Test Loss: 1.1281\n",
      "Epoch 52/71, Train Loss: 1.0553, Test Loss: 1.1296\n",
      "Epoch 53/71, Train Loss: 1.0429, Test Loss: 1.1358\n",
      "Epoch 54/71, Train Loss: 1.0507, Test Loss: 1.1278\n",
      "Epoch 55/71, Train Loss: 1.0599, Test Loss: 1.1298\n",
      "Epoch 56/71, Train Loss: 1.0701, Test Loss: 1.1312\n",
      "Epoch 57/71, Train Loss: 1.0616, Test Loss: 1.1305\n",
      "Epoch 58/71, Train Loss: 1.0480, Test Loss: 1.1296\n",
      "Epoch 59/71, Train Loss: 1.0707, Test Loss: 1.1315\n",
      "Epoch 60/71, Train Loss: 1.0373, Test Loss: 1.1287\n",
      "Epoch 61/71, Train Loss: 1.0584, Test Loss: 1.1307\n",
      "Epoch 62/71, Train Loss: 1.0589, Test Loss: 1.1317\n",
      "Epoch 63/71, Train Loss: 1.0524, Test Loss: 1.1305\n",
      "Epoch 64/71, Train Loss: 1.0331, Test Loss: 1.1328\n",
      "Epoch 65/71, Train Loss: 1.0399, Test Loss: 1.1317\n",
      "Epoch 66/71, Train Loss: 1.0446, Test Loss: 1.1318\n",
      "Epoch 67/71, Train Loss: 1.0362, Test Loss: 1.1320\n",
      "Epoch 68/71, Train Loss: 1.0626, Test Loss: 1.1332\n",
      "Epoch 69/71, Train Loss: 1.0243, Test Loss: 1.1329\n",
      "Epoch 70/71, Train Loss: 1.0173, Test Loss: 1.1392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:05:56,063] Trial 167 finished with value: 1.1428389719554357 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 202, 'layer_1_size': 113, 'layer_2_size': 243, 'layer_3_size': 172, 'layer_4_size': 111, 'layer_5_size': 136, 'layer_6_size': 106, 'layer_7_size': 252, 'layer_8_size': 176, 'layer_9_size': 218, 'layer_10_size': 163, 'layer_11_size': 154, 'layer_12_size': 56, 'dropout_rate': 0.23716892755322494, 'learning_rate': 0.000425665013262837, 'batch_size': 32, 'epochs': 71}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/71, Train Loss: 1.0519, Test Loss: 1.1428\n",
      "Epoch 1/73, Train Loss: 1.1724, Test Loss: 1.0832\n",
      "Epoch 2/73, Train Loss: 1.1441, Test Loss: 1.0760\n",
      "Epoch 3/73, Train Loss: 1.1109, Test Loss: 1.0614\n",
      "Epoch 4/73, Train Loss: 1.0767, Test Loss: 1.0708\n",
      "Epoch 5/73, Train Loss: 1.0678, Test Loss: 1.0672\n",
      "Epoch 6/73, Train Loss: 1.0370, Test Loss: 1.0672\n",
      "Epoch 7/73, Train Loss: 1.0563, Test Loss: 1.0689\n",
      "Epoch 8/73, Train Loss: 1.0953, Test Loss: 1.0837\n",
      "Epoch 9/73, Train Loss: 1.0222, Test Loss: 1.0782\n",
      "Epoch 10/73, Train Loss: 1.0302, Test Loss: 1.0925\n",
      "Epoch 11/73, Train Loss: 1.0728, Test Loss: 1.0671\n",
      "Epoch 12/73, Train Loss: 1.0390, Test Loss: 1.0743\n",
      "Epoch 13/73, Train Loss: 1.0169, Test Loss: 1.0676\n",
      "Epoch 14/73, Train Loss: 1.0482, Test Loss: 1.0686\n",
      "Epoch 15/73, Train Loss: 1.0216, Test Loss: 1.0675\n",
      "Epoch 16/73, Train Loss: 1.0122, Test Loss: 1.0653\n",
      "Epoch 17/73, Train Loss: 1.0322, Test Loss: 1.0667\n",
      "Epoch 18/73, Train Loss: 1.0112, Test Loss: 1.0659\n",
      "Epoch 19/73, Train Loss: 1.0097, Test Loss: 1.0669\n",
      "Epoch 20/73, Train Loss: 1.0423, Test Loss: 1.0659\n",
      "Epoch 21/73, Train Loss: 1.0828, Test Loss: 1.0625\n",
      "Epoch 22/73, Train Loss: 1.0112, Test Loss: 1.0594\n",
      "Epoch 23/73, Train Loss: 1.0010, Test Loss: 1.0592\n",
      "Epoch 24/73, Train Loss: 1.0298, Test Loss: 1.0572\n",
      "Epoch 25/73, Train Loss: 1.0173, Test Loss: 1.0611\n",
      "Epoch 26/73, Train Loss: 0.9959, Test Loss: 1.0615\n",
      "Epoch 27/73, Train Loss: 0.9981, Test Loss: 1.0617\n",
      "Epoch 28/73, Train Loss: 1.0103, Test Loss: 1.0636\n",
      "Epoch 29/73, Train Loss: 1.0057, Test Loss: 1.0608\n",
      "Epoch 30/73, Train Loss: 1.0017, Test Loss: 1.0602\n",
      "Epoch 31/73, Train Loss: 1.0199, Test Loss: 1.0596\n",
      "Epoch 32/73, Train Loss: 0.9974, Test Loss: 1.0678\n",
      "Epoch 33/73, Train Loss: 0.9953, Test Loss: 1.0672\n",
      "Epoch 34/73, Train Loss: 1.0161, Test Loss: 1.0628\n",
      "Epoch 35/73, Train Loss: 1.0242, Test Loss: 1.0623\n",
      "Epoch 36/73, Train Loss: 0.9931, Test Loss: 1.0659\n",
      "Epoch 37/73, Train Loss: 0.9944, Test Loss: 1.0672\n",
      "Epoch 38/73, Train Loss: 1.0180, Test Loss: 1.0642\n",
      "Epoch 39/73, Train Loss: 1.0195, Test Loss: 1.0598\n",
      "Epoch 40/73, Train Loss: 1.0104, Test Loss: 1.0601\n",
      "Epoch 41/73, Train Loss: 1.0350, Test Loss: 1.0600\n",
      "Epoch 42/73, Train Loss: 1.0149, Test Loss: 1.0637\n",
      "Epoch 43/73, Train Loss: 1.0235, Test Loss: 1.0626\n",
      "Epoch 44/73, Train Loss: 0.9940, Test Loss: 1.0608\n",
      "Epoch 45/73, Train Loss: 1.0040, Test Loss: 1.0605\n",
      "Epoch 46/73, Train Loss: 1.0084, Test Loss: 1.0584\n",
      "Epoch 47/73, Train Loss: 1.0024, Test Loss: 1.0578\n",
      "Epoch 48/73, Train Loss: 0.9874, Test Loss: 1.0622\n",
      "Epoch 49/73, Train Loss: 1.0007, Test Loss: 1.0618\n",
      "Epoch 50/73, Train Loss: 0.9930, Test Loss: 1.0593\n",
      "Epoch 51/73, Train Loss: 1.0171, Test Loss: 1.0605\n",
      "Epoch 52/73, Train Loss: 1.0191, Test Loss: 1.0610\n",
      "Epoch 53/73, Train Loss: 1.0118, Test Loss: 1.0636\n",
      "Epoch 54/73, Train Loss: 1.0209, Test Loss: 1.0657\n",
      "Epoch 55/73, Train Loss: 1.0248, Test Loss: 1.0645\n",
      "Epoch 56/73, Train Loss: 0.9988, Test Loss: 1.0673\n",
      "Epoch 57/73, Train Loss: 0.9953, Test Loss: 1.0668\n",
      "Epoch 58/73, Train Loss: 1.0139, Test Loss: 1.0677\n",
      "Epoch 59/73, Train Loss: 0.9904, Test Loss: 1.0674\n",
      "Epoch 60/73, Train Loss: 1.0279, Test Loss: 1.0666\n",
      "Epoch 61/73, Train Loss: 0.9946, Test Loss: 1.0710\n",
      "Epoch 62/73, Train Loss: 0.9999, Test Loss: 1.0654\n",
      "Epoch 63/73, Train Loss: 0.9874, Test Loss: 1.0646\n",
      "Epoch 64/73, Train Loss: 0.9935, Test Loss: 1.0636\n",
      "Epoch 65/73, Train Loss: 0.9963, Test Loss: 1.0625\n",
      "Epoch 66/73, Train Loss: 1.0253, Test Loss: 1.0610\n",
      "Epoch 67/73, Train Loss: 1.0018, Test Loss: 1.0605\n",
      "Epoch 68/73, Train Loss: 0.9966, Test Loss: 1.0614\n",
      "Epoch 69/73, Train Loss: 1.0050, Test Loss: 1.0608\n",
      "Epoch 70/73, Train Loss: 1.0172, Test Loss: 1.0600\n",
      "Epoch 71/73, Train Loss: 0.9989, Test Loss: 1.0620\n",
      "Epoch 72/73, Train Loss: 0.9982, Test Loss: 1.0617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:06:14,538] Trial 168 finished with value: 1.060493869440896 and parameters: {'num_hidden_layers': 15, 'layer_0_size': 207, 'layer_1_size': 117, 'layer_2_size': 119, 'layer_3_size': 157, 'layer_4_size': 117, 'layer_5_size': 123, 'layer_6_size': 97, 'layer_7_size': 251, 'layer_8_size': 178, 'layer_9_size': 229, 'layer_10_size': 151, 'layer_11_size': 162, 'layer_12_size': 49, 'layer_13_size': 136, 'layer_14_size': 36, 'dropout_rate': 0.3001709441126869, 'learning_rate': 0.0006481193974712991, 'batch_size': 32, 'epochs': 73}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/73, Train Loss: 1.0016, Test Loss: 1.0605\n",
      "Epoch 1/71, Train Loss: 1.2393, Test Loss: 1.0796\n",
      "Epoch 2/71, Train Loss: 1.1596, Test Loss: 1.0841\n",
      "Epoch 3/71, Train Loss: 1.1229, Test Loss: 1.1115\n",
      "Epoch 4/71, Train Loss: 1.1405, Test Loss: 1.0964\n",
      "Epoch 5/71, Train Loss: 1.1103, Test Loss: 1.0759\n",
      "Epoch 6/71, Train Loss: 1.0776, Test Loss: 1.0791\n",
      "Epoch 7/71, Train Loss: 1.1108, Test Loss: 1.0772\n",
      "Epoch 8/71, Train Loss: 1.1028, Test Loss: 1.0841\n",
      "Epoch 9/71, Train Loss: 1.1005, Test Loss: 1.0778\n",
      "Epoch 10/71, Train Loss: 1.1107, Test Loss: 1.0830\n",
      "Epoch 11/71, Train Loss: 1.0610, Test Loss: 1.0686\n",
      "Epoch 12/71, Train Loss: 1.0833, Test Loss: 1.0683\n",
      "Epoch 13/71, Train Loss: 1.0677, Test Loss: 1.0734\n",
      "Epoch 14/71, Train Loss: 1.0703, Test Loss: 1.0695\n",
      "Epoch 15/71, Train Loss: 1.0513, Test Loss: 1.0695\n",
      "Epoch 16/71, Train Loss: 1.0963, Test Loss: 1.0725\n",
      "Epoch 17/71, Train Loss: 1.0665, Test Loss: 1.0853\n",
      "Epoch 18/71, Train Loss: 1.0784, Test Loss: 1.0740\n",
      "Epoch 19/71, Train Loss: 1.0343, Test Loss: 1.0731\n",
      "Epoch 20/71, Train Loss: 1.0318, Test Loss: 1.0707\n",
      "Epoch 21/71, Train Loss: 1.0421, Test Loss: 1.0746\n",
      "Epoch 22/71, Train Loss: 1.0659, Test Loss: 1.0825\n",
      "Epoch 23/71, Train Loss: 1.0215, Test Loss: 1.0843\n",
      "Epoch 24/71, Train Loss: 1.0562, Test Loss: 1.0835\n",
      "Epoch 25/71, Train Loss: 1.0249, Test Loss: 1.0852\n",
      "Epoch 26/71, Train Loss: 1.0415, Test Loss: 1.0775\n",
      "Epoch 27/71, Train Loss: 1.0709, Test Loss: 1.0695\n",
      "Epoch 28/71, Train Loss: 1.0399, Test Loss: 1.0728\n",
      "Epoch 29/71, Train Loss: 1.0442, Test Loss: 1.0702\n",
      "Epoch 30/71, Train Loss: 1.0283, Test Loss: 1.0783\n",
      "Epoch 31/71, Train Loss: 1.0295, Test Loss: 1.0755\n",
      "Epoch 32/71, Train Loss: 1.0493, Test Loss: 1.0722\n",
      "Epoch 33/71, Train Loss: 1.0421, Test Loss: 1.0789\n",
      "Epoch 34/71, Train Loss: 1.0411, Test Loss: 1.0787\n",
      "Epoch 35/71, Train Loss: 1.0404, Test Loss: 1.0741\n",
      "Epoch 36/71, Train Loss: 1.0368, Test Loss: 1.0790\n",
      "Epoch 37/71, Train Loss: 1.0194, Test Loss: 1.0753\n",
      "Epoch 38/71, Train Loss: 1.0319, Test Loss: 1.0773\n",
      "Epoch 39/71, Train Loss: 1.0345, Test Loss: 1.0725\n",
      "Epoch 40/71, Train Loss: 1.0898, Test Loss: 1.0752\n",
      "Epoch 41/71, Train Loss: 1.0127, Test Loss: 1.0708\n",
      "Epoch 42/71, Train Loss: 1.0372, Test Loss: 1.0752\n",
      "Epoch 43/71, Train Loss: 1.0365, Test Loss: 1.0767\n",
      "Epoch 44/71, Train Loss: 1.0689, Test Loss: 1.0701\n",
      "Epoch 45/71, Train Loss: 1.0385, Test Loss: 1.0698\n",
      "Epoch 46/71, Train Loss: 1.0196, Test Loss: 1.0719\n",
      "Epoch 47/71, Train Loss: 1.0229, Test Loss: 1.0715\n",
      "Epoch 48/71, Train Loss: 1.0287, Test Loss: 1.0776\n",
      "Epoch 49/71, Train Loss: 1.0368, Test Loss: 1.0699\n",
      "Epoch 50/71, Train Loss: 1.0118, Test Loss: 1.0715\n",
      "Epoch 51/71, Train Loss: 1.0313, Test Loss: 1.0703\n",
      "Epoch 52/71, Train Loss: 1.0312, Test Loss: 1.0701\n",
      "Epoch 53/71, Train Loss: 1.0208, Test Loss: 1.0700\n",
      "Epoch 54/71, Train Loss: 1.0136, Test Loss: 1.0694\n",
      "Epoch 55/71, Train Loss: 1.0334, Test Loss: 1.0715\n",
      "Epoch 56/71, Train Loss: 1.0279, Test Loss: 1.0742\n",
      "Epoch 57/71, Train Loss: 1.0545, Test Loss: 1.0691\n",
      "Epoch 58/71, Train Loss: 1.0075, Test Loss: 1.0699\n",
      "Epoch 59/71, Train Loss: 1.0327, Test Loss: 1.0695\n",
      "Epoch 60/71, Train Loss: 1.0419, Test Loss: 1.0716\n",
      "Epoch 61/71, Train Loss: 1.0497, Test Loss: 1.0760\n",
      "Epoch 62/71, Train Loss: 1.0225, Test Loss: 1.0740\n",
      "Epoch 63/71, Train Loss: 1.0297, Test Loss: 1.0706\n",
      "Epoch 64/71, Train Loss: 1.0280, Test Loss: 1.0709\n",
      "Epoch 65/71, Train Loss: 1.0489, Test Loss: 1.0735\n",
      "Epoch 66/71, Train Loss: 1.0295, Test Loss: 1.0807\n",
      "Epoch 67/71, Train Loss: 1.0230, Test Loss: 1.0836\n",
      "Epoch 68/71, Train Loss: 1.0165, Test Loss: 1.0846\n",
      "Epoch 69/71, Train Loss: 1.0086, Test Loss: 1.0999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:06:29,386] Trial 169 finished with value: 1.0916477867535181 and parameters: {'num_hidden_layers': 12, 'layer_0_size': 131, 'layer_1_size': 199, 'layer_2_size': 149, 'layer_3_size': 147, 'layer_4_size': 105, 'layer_5_size': 149, 'layer_6_size': 117, 'layer_7_size': 256, 'layer_8_size': 190, 'layer_9_size': 226, 'layer_10_size': 142, 'layer_11_size': 170, 'dropout_rate': 0.3470864904432742, 'learning_rate': 0.00041266955264334043, 'batch_size': 32, 'epochs': 71}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/71, Train Loss: 1.0177, Test Loss: 1.0886\n",
      "Epoch 71/71, Train Loss: 1.0409, Test Loss: 1.0916\n",
      "Epoch 1/77, Train Loss: 1.2860, Test Loss: 1.2467\n",
      "Epoch 2/77, Train Loss: 1.2096, Test Loss: 1.1229\n",
      "Epoch 3/77, Train Loss: 1.1888, Test Loss: 1.1039\n",
      "Epoch 4/77, Train Loss: 1.1608, Test Loss: 1.1047\n",
      "Epoch 5/77, Train Loss: 1.1912, Test Loss: 1.1174\n",
      "Epoch 6/77, Train Loss: 1.1080, Test Loss: 1.1303\n",
      "Epoch 7/77, Train Loss: 1.1128, Test Loss: 1.1128\n",
      "Epoch 8/77, Train Loss: 1.1096, Test Loss: 1.0950\n",
      "Epoch 9/77, Train Loss: 1.1070, Test Loss: 1.1037\n",
      "Epoch 10/77, Train Loss: 1.1026, Test Loss: 1.1039\n",
      "Epoch 11/77, Train Loss: 1.0861, Test Loss: 1.1083\n",
      "Epoch 12/77, Train Loss: 1.0888, Test Loss: 1.1068\n",
      "Epoch 13/77, Train Loss: 1.0681, Test Loss: 1.1123\n",
      "Epoch 14/77, Train Loss: 1.0887, Test Loss: 1.1117\n",
      "Epoch 15/77, Train Loss: 1.0999, Test Loss: 1.1053\n",
      "Epoch 16/77, Train Loss: 1.0798, Test Loss: 1.1072\n",
      "Epoch 17/77, Train Loss: 1.1021, Test Loss: 1.1133\n",
      "Epoch 18/77, Train Loss: 1.0769, Test Loss: 1.1062\n",
      "Epoch 19/77, Train Loss: 1.1071, Test Loss: 1.1060\n",
      "Epoch 20/77, Train Loss: 1.0871, Test Loss: 1.1068\n",
      "Epoch 21/77, Train Loss: 1.0708, Test Loss: 1.1034\n",
      "Epoch 22/77, Train Loss: 1.0883, Test Loss: 1.1020\n",
      "Epoch 23/77, Train Loss: 1.0545, Test Loss: 1.1050\n",
      "Epoch 24/77, Train Loss: 1.0803, Test Loss: 1.1136\n",
      "Epoch 25/77, Train Loss: 1.0703, Test Loss: 1.1059\n",
      "Epoch 26/77, Train Loss: 1.0698, Test Loss: 1.1073\n",
      "Epoch 27/77, Train Loss: 1.0592, Test Loss: 1.1039\n",
      "Epoch 28/77, Train Loss: 1.0745, Test Loss: 1.1046\n",
      "Epoch 29/77, Train Loss: 1.0774, Test Loss: 1.1062\n",
      "Epoch 30/77, Train Loss: 1.0569, Test Loss: 1.1100\n",
      "Epoch 31/77, Train Loss: 1.0654, Test Loss: 1.1049\n",
      "Epoch 32/77, Train Loss: 1.0787, Test Loss: 1.1056\n",
      "Epoch 33/77, Train Loss: 1.0771, Test Loss: 1.1067\n",
      "Epoch 34/77, Train Loss: 1.0657, Test Loss: 1.1051\n",
      "Epoch 35/77, Train Loss: 1.0458, Test Loss: 1.1050\n",
      "Epoch 36/77, Train Loss: 1.0618, Test Loss: 1.1053\n",
      "Epoch 37/77, Train Loss: 1.0822, Test Loss: 1.1062\n",
      "Epoch 38/77, Train Loss: 1.0589, Test Loss: 1.1021\n",
      "Epoch 39/77, Train Loss: 1.0645, Test Loss: 1.1045\n",
      "Epoch 40/77, Train Loss: 1.0679, Test Loss: 1.1058\n",
      "Epoch 41/77, Train Loss: 1.0691, Test Loss: 1.1070\n",
      "Epoch 42/77, Train Loss: 1.0764, Test Loss: 1.1121\n",
      "Epoch 43/77, Train Loss: 1.0546, Test Loss: 1.1116\n",
      "Epoch 44/77, Train Loss: 1.0668, Test Loss: 1.1133\n",
      "Epoch 45/77, Train Loss: 1.0434, Test Loss: 1.1188\n",
      "Epoch 46/77, Train Loss: 1.0463, Test Loss: 1.1196\n",
      "Epoch 47/77, Train Loss: 1.0609, Test Loss: 1.1155\n",
      "Epoch 48/77, Train Loss: 1.0376, Test Loss: 1.1110\n",
      "Epoch 49/77, Train Loss: 1.0705, Test Loss: 1.1125\n",
      "Epoch 50/77, Train Loss: 1.0539, Test Loss: 1.1062\n",
      "Epoch 51/77, Train Loss: 1.0522, Test Loss: 1.1112\n",
      "Epoch 52/77, Train Loss: 1.0493, Test Loss: 1.1075\n",
      "Epoch 53/77, Train Loss: 1.0797, Test Loss: 1.1110\n",
      "Epoch 54/77, Train Loss: 1.0640, Test Loss: 1.1131\n",
      "Epoch 55/77, Train Loss: 1.0394, Test Loss: 1.1092\n",
      "Epoch 56/77, Train Loss: 1.0796, Test Loss: 1.1084\n",
      "Epoch 57/77, Train Loss: 1.0725, Test Loss: 1.1074\n",
      "Epoch 58/77, Train Loss: 1.0431, Test Loss: 1.1079\n",
      "Epoch 59/77, Train Loss: 1.0655, Test Loss: 1.1088\n",
      "Epoch 60/77, Train Loss: 1.0521, Test Loss: 1.1126\n",
      "Epoch 61/77, Train Loss: 1.0442, Test Loss: 1.1127\n",
      "Epoch 62/77, Train Loss: 1.0464, Test Loss: 1.1181\n",
      "Epoch 63/77, Train Loss: 1.0667, Test Loss: 1.1166\n",
      "Epoch 64/77, Train Loss: 1.0558, Test Loss: 1.1219\n",
      "Epoch 65/77, Train Loss: 1.0687, Test Loss: 1.1193\n",
      "Epoch 66/77, Train Loss: 1.0556, Test Loss: 1.1133\n",
      "Epoch 67/77, Train Loss: 1.0453, Test Loss: 1.1109\n",
      "Epoch 68/77, Train Loss: 1.0594, Test Loss: 1.1186\n",
      "Epoch 69/77, Train Loss: 1.0404, Test Loss: 1.1257\n",
      "Epoch 70/77, Train Loss: 1.0569, Test Loss: 1.1380\n",
      "Epoch 71/77, Train Loss: 1.0642, Test Loss: 1.1296\n",
      "Epoch 72/77, Train Loss: 1.0455, Test Loss: 1.1168\n",
      "Epoch 73/77, Train Loss: 1.0479, Test Loss: 1.1143\n",
      "Epoch 74/77, Train Loss: 1.1021, Test Loss: 1.1174\n",
      "Epoch 75/77, Train Loss: 1.0637, Test Loss: 1.1085\n",
      "Epoch 76/77, Train Loss: 1.0366, Test Loss: 1.1110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:06:48,672] Trial 170 finished with value: 1.1119128380502974 and parameters: {'num_hidden_layers': 15, 'layer_0_size': 214, 'layer_1_size': 239, 'layer_2_size': 130, 'layer_3_size': 157, 'layer_4_size': 93, 'layer_5_size': 112, 'layer_6_size': 100, 'layer_7_size': 245, 'layer_8_size': 94, 'layer_9_size': 207, 'layer_10_size': 137, 'layer_11_size': 158, 'layer_12_size': 70, 'layer_13_size': 147, 'layer_14_size': 76, 'dropout_rate': 0.2792560181875894, 'learning_rate': 0.0005598671517949682, 'batch_size': 32, 'epochs': 77}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/77, Train Loss: 1.0459, Test Loss: 1.1119\n",
      "Epoch 1/67, Train Loss: 1.2348, Test Loss: 0.9537\n",
      "Epoch 2/67, Train Loss: 1.1380, Test Loss: 0.9595\n",
      "Epoch 3/67, Train Loss: 1.1084, Test Loss: 0.9529\n",
      "Epoch 4/67, Train Loss: 1.0967, Test Loss: 0.9534\n",
      "Epoch 5/67, Train Loss: 1.0453, Test Loss: 0.9577\n",
      "Epoch 6/67, Train Loss: 1.0455, Test Loss: 0.9585\n",
      "Epoch 7/67, Train Loss: 1.0519, Test Loss: 0.9563\n",
      "Epoch 8/67, Train Loss: 1.0929, Test Loss: 0.9548\n",
      "Epoch 9/67, Train Loss: 1.0708, Test Loss: 0.9418\n",
      "Epoch 10/67, Train Loss: 1.1120, Test Loss: 0.9490\n",
      "Epoch 11/67, Train Loss: 1.0396, Test Loss: 0.9468\n",
      "Epoch 12/67, Train Loss: 1.0428, Test Loss: 0.9496\n",
      "Epoch 13/67, Train Loss: 1.0589, Test Loss: 0.9611\n",
      "Epoch 14/67, Train Loss: 1.0625, Test Loss: 0.9760\n",
      "Epoch 15/67, Train Loss: 1.0612, Test Loss: 0.9780\n",
      "Epoch 16/67, Train Loss: 1.0346, Test Loss: 0.9563\n",
      "Epoch 17/67, Train Loss: 1.0348, Test Loss: 0.9592\n",
      "Epoch 18/67, Train Loss: 1.0353, Test Loss: 0.9456\n",
      "Epoch 19/67, Train Loss: 1.0238, Test Loss: 0.9574\n",
      "Epoch 20/67, Train Loss: 1.0224, Test Loss: 0.9520\n",
      "Epoch 21/67, Train Loss: 1.0235, Test Loss: 0.9558\n",
      "Epoch 22/67, Train Loss: 1.0224, Test Loss: 0.9695\n",
      "Epoch 23/67, Train Loss: 1.0077, Test Loss: 0.9711\n",
      "Epoch 24/67, Train Loss: 1.0275, Test Loss: 0.9607\n",
      "Epoch 25/67, Train Loss: 1.0074, Test Loss: 0.9513\n",
      "Epoch 26/67, Train Loss: 0.9892, Test Loss: 0.9631\n",
      "Epoch 27/67, Train Loss: 1.0291, Test Loss: 0.9689\n",
      "Epoch 28/67, Train Loss: 1.0171, Test Loss: 0.9617\n",
      "Epoch 29/67, Train Loss: 1.0058, Test Loss: 0.9694\n",
      "Epoch 30/67, Train Loss: 1.0035, Test Loss: 0.9666\n",
      "Epoch 31/67, Train Loss: 1.0226, Test Loss: 0.9725\n",
      "Epoch 32/67, Train Loss: 0.9743, Test Loss: 0.9712\n",
      "Epoch 33/67, Train Loss: 1.0002, Test Loss: 0.9630\n",
      "Epoch 34/67, Train Loss: 1.0143, Test Loss: 0.9656\n",
      "Epoch 35/67, Train Loss: 1.0218, Test Loss: 0.9608\n",
      "Epoch 36/67, Train Loss: 0.9887, Test Loss: 0.9635\n",
      "Epoch 37/67, Train Loss: 1.0097, Test Loss: 0.9606\n",
      "Epoch 38/67, Train Loss: 0.9998, Test Loss: 0.9741\n",
      "Epoch 39/67, Train Loss: 1.0085, Test Loss: 0.9652\n",
      "Epoch 40/67, Train Loss: 1.0099, Test Loss: 0.9647\n",
      "Epoch 41/67, Train Loss: 1.0169, Test Loss: 0.9634\n",
      "Epoch 42/67, Train Loss: 1.0259, Test Loss: 0.9602\n",
      "Epoch 43/67, Train Loss: 1.0242, Test Loss: 0.9625\n",
      "Epoch 44/67, Train Loss: 1.0189, Test Loss: 0.9659\n",
      "Epoch 45/67, Train Loss: 1.0140, Test Loss: 0.9777\n",
      "Epoch 46/67, Train Loss: 0.9940, Test Loss: 0.9896\n",
      "Epoch 47/67, Train Loss: 0.9988, Test Loss: 0.9834\n",
      "Epoch 48/67, Train Loss: 1.0143, Test Loss: 0.9652\n",
      "Epoch 49/67, Train Loss: 1.0095, Test Loss: 0.9537\n",
      "Epoch 50/67, Train Loss: 0.9923, Test Loss: 0.9677\n",
      "Epoch 51/67, Train Loss: 1.0034, Test Loss: 0.9701\n",
      "Epoch 52/67, Train Loss: 1.0227, Test Loss: 0.9660\n",
      "Epoch 53/67, Train Loss: 1.0092, Test Loss: 0.9642\n",
      "Epoch 54/67, Train Loss: 0.9848, Test Loss: 0.9690\n",
      "Epoch 55/67, Train Loss: 0.9918, Test Loss: 0.9641\n",
      "Epoch 56/67, Train Loss: 0.9930, Test Loss: 0.9605\n",
      "Epoch 57/67, Train Loss: 0.9843, Test Loss: 0.9509\n",
      "Epoch 58/67, Train Loss: 1.0013, Test Loss: 0.9619\n",
      "Epoch 59/67, Train Loss: 1.0076, Test Loss: 0.9627\n",
      "Epoch 60/67, Train Loss: 1.0188, Test Loss: 0.9576\n",
      "Epoch 61/67, Train Loss: 0.9856, Test Loss: 0.9604\n",
      "Epoch 62/67, Train Loss: 1.0143, Test Loss: 0.9572\n",
      "Epoch 63/67, Train Loss: 0.9779, Test Loss: 0.9617\n",
      "Epoch 64/67, Train Loss: 0.9817, Test Loss: 0.9566\n",
      "Epoch 65/67, Train Loss: 0.9913, Test Loss: 0.9575\n",
      "Epoch 66/67, Train Loss: 0.9988, Test Loss: 0.9549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:07:04,568] Trial 171 finished with value: 0.9503216232572284 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 144, 'layer_1_size': 125, 'layer_2_size': 249, 'layer_3_size': 141, 'layer_4_size': 126, 'layer_5_size': 116, 'layer_6_size': 92, 'layer_7_size': 113, 'layer_8_size': 151, 'layer_9_size': 239, 'layer_10_size': 229, 'layer_11_size': 76, 'layer_12_size': 63, 'layer_13_size': 112, 'dropout_rate': 0.3175935229628143, 'learning_rate': 0.0004624015275111028, 'batch_size': 32, 'epochs': 67}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/67, Train Loss: 0.9752, Test Loss: 0.9503\n",
      "Epoch 1/62, Train Loss: 1.2112, Test Loss: 1.0395\n",
      "Epoch 2/62, Train Loss: 1.2572, Test Loss: 1.0401\n",
      "Epoch 3/62, Train Loss: 1.2245, Test Loss: 1.0062\n",
      "Epoch 4/62, Train Loss: 1.2027, Test Loss: 0.9992\n",
      "Epoch 5/62, Train Loss: 1.1452, Test Loss: 0.9867\n",
      "Epoch 6/62, Train Loss: 1.1115, Test Loss: 0.9823\n",
      "Epoch 7/62, Train Loss: 1.1390, Test Loss: 0.9856\n",
      "Epoch 8/62, Train Loss: 1.1948, Test Loss: 0.9821\n",
      "Epoch 9/62, Train Loss: 1.1345, Test Loss: 0.9874\n",
      "Epoch 10/62, Train Loss: 1.1388, Test Loss: 0.9888\n",
      "Epoch 11/62, Train Loss: 1.1489, Test Loss: 0.9900\n",
      "Epoch 12/62, Train Loss: 1.1807, Test Loss: 0.9999\n",
      "Epoch 13/62, Train Loss: 1.1544, Test Loss: 0.9845\n",
      "Epoch 14/62, Train Loss: 1.0795, Test Loss: 0.9883\n",
      "Epoch 15/62, Train Loss: 1.1390, Test Loss: 0.9943\n",
      "Epoch 16/62, Train Loss: 1.1282, Test Loss: 1.0019\n",
      "Epoch 17/62, Train Loss: 1.1491, Test Loss: 0.9961\n",
      "Epoch 18/62, Train Loss: 1.0962, Test Loss: 1.0002\n",
      "Epoch 19/62, Train Loss: 1.0970, Test Loss: 0.9939\n",
      "Epoch 20/62, Train Loss: 1.1371, Test Loss: 0.9858\n",
      "Epoch 21/62, Train Loss: 1.1202, Test Loss: 0.9822\n",
      "Epoch 22/62, Train Loss: 1.0961, Test Loss: 0.9892\n",
      "Epoch 23/62, Train Loss: 1.0956, Test Loss: 0.9929\n",
      "Epoch 24/62, Train Loss: 1.1435, Test Loss: 0.9881\n",
      "Epoch 25/62, Train Loss: 1.0899, Test Loss: 1.0009\n",
      "Epoch 26/62, Train Loss: 1.1091, Test Loss: 1.0059\n",
      "Epoch 27/62, Train Loss: 1.1031, Test Loss: 1.0013\n",
      "Epoch 28/62, Train Loss: 1.1087, Test Loss: 1.0007\n",
      "Epoch 29/62, Train Loss: 1.1043, Test Loss: 0.9971\n",
      "Epoch 30/62, Train Loss: 1.1104, Test Loss: 0.9988\n",
      "Epoch 31/62, Train Loss: 1.1010, Test Loss: 0.9904\n",
      "Epoch 32/62, Train Loss: 1.1021, Test Loss: 0.9888\n",
      "Epoch 33/62, Train Loss: 1.0834, Test Loss: 0.9918\n",
      "Epoch 34/62, Train Loss: 1.0786, Test Loss: 0.9908\n",
      "Epoch 35/62, Train Loss: 1.0790, Test Loss: 0.9989\n",
      "Epoch 36/62, Train Loss: 1.0899, Test Loss: 1.0004\n",
      "Epoch 37/62, Train Loss: 1.0935, Test Loss: 0.9961\n",
      "Epoch 38/62, Train Loss: 1.0793, Test Loss: 0.9823\n",
      "Epoch 39/62, Train Loss: 1.1135, Test Loss: 0.9809\n",
      "Epoch 40/62, Train Loss: 1.0967, Test Loss: 0.9816\n",
      "Epoch 41/62, Train Loss: 1.0922, Test Loss: 0.9850\n",
      "Epoch 42/62, Train Loss: 1.0990, Test Loss: 0.9853\n",
      "Epoch 43/62, Train Loss: 1.0857, Test Loss: 0.9815\n",
      "Epoch 44/62, Train Loss: 1.0776, Test Loss: 0.9810\n",
      "Epoch 45/62, Train Loss: 1.1071, Test Loss: 0.9823\n",
      "Epoch 46/62, Train Loss: 1.0694, Test Loss: 0.9838\n",
      "Epoch 47/62, Train Loss: 1.1307, Test Loss: 0.9835\n",
      "Epoch 48/62, Train Loss: 1.0757, Test Loss: 0.9860\n",
      "Epoch 49/62, Train Loss: 1.1249, Test Loss: 0.9816\n",
      "Epoch 50/62, Train Loss: 1.1009, Test Loss: 0.9855\n",
      "Epoch 51/62, Train Loss: 1.0871, Test Loss: 0.9854\n",
      "Epoch 52/62, Train Loss: 1.0962, Test Loss: 0.9847\n",
      "Epoch 53/62, Train Loss: 1.1011, Test Loss: 0.9884\n",
      "Epoch 54/62, Train Loss: 1.0849, Test Loss: 0.9878\n",
      "Epoch 55/62, Train Loss: 1.1125, Test Loss: 0.9911\n",
      "Epoch 56/62, Train Loss: 1.0767, Test Loss: 0.9841\n",
      "Epoch 57/62, Train Loss: 1.0915, Test Loss: 0.9825\n",
      "Epoch 58/62, Train Loss: 1.0831, Test Loss: 0.9840\n",
      "Epoch 59/62, Train Loss: 1.0667, Test Loss: 0.9815\n",
      "Epoch 60/62, Train Loss: 1.0666, Test Loss: 0.9813\n",
      "Epoch 61/62, Train Loss: 1.0721, Test Loss: 0.9818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:07:19,021] Trial 172 finished with value: 0.9851815274783543 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 156, 'layer_1_size': 133, 'layer_2_size': 239, 'layer_3_size': 144, 'layer_4_size': 130, 'layer_5_size': 126, 'layer_6_size': 64, 'layer_7_size': 247, 'layer_8_size': 156, 'layer_9_size': 32, 'layer_10_size': 248, 'layer_11_size': 183, 'layer_12_size': 77, 'layer_13_size': 123, 'dropout_rate': 0.293046343534861, 'learning_rate': 0.0003526188986329793, 'batch_size': 32, 'epochs': 62}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/62, Train Loss: 1.0954, Test Loss: 0.9852\n",
      "Epoch 1/69, Train Loss: 1.2438, Test Loss: 1.1209\n",
      "Epoch 2/69, Train Loss: 1.1760, Test Loss: 1.1207\n",
      "Epoch 3/69, Train Loss: 1.1523, Test Loss: 1.1462\n",
      "Epoch 4/69, Train Loss: 1.1340, Test Loss: 1.1390\n",
      "Epoch 5/69, Train Loss: 1.1194, Test Loss: 1.1647\n",
      "Epoch 6/69, Train Loss: 1.0992, Test Loss: 1.1514\n",
      "Epoch 7/69, Train Loss: 1.0930, Test Loss: 1.1257\n",
      "Epoch 8/69, Train Loss: 1.1569, Test Loss: 1.1121\n",
      "Epoch 9/69, Train Loss: 1.0961, Test Loss: 1.1064\n",
      "Epoch 10/69, Train Loss: 1.1118, Test Loss: 1.1142\n",
      "Epoch 11/69, Train Loss: 1.0876, Test Loss: 1.1062\n",
      "Epoch 12/69, Train Loss: 1.1112, Test Loss: 1.1113\n",
      "Epoch 13/69, Train Loss: 1.0972, Test Loss: 1.1073\n",
      "Epoch 14/69, Train Loss: 1.0618, Test Loss: 1.1091\n",
      "Epoch 15/69, Train Loss: 1.0694, Test Loss: 1.1105\n",
      "Epoch 16/69, Train Loss: 1.0841, Test Loss: 1.1094\n",
      "Epoch 17/69, Train Loss: 1.0605, Test Loss: 1.1109\n",
      "Epoch 18/69, Train Loss: 1.0958, Test Loss: 1.1132\n",
      "Epoch 19/69, Train Loss: 1.0724, Test Loss: 1.1082\n",
      "Epoch 20/69, Train Loss: 1.0766, Test Loss: 1.1125\n",
      "Epoch 21/69, Train Loss: 1.0779, Test Loss: 1.1080\n",
      "Epoch 22/69, Train Loss: 1.0559, Test Loss: 1.1069\n",
      "Epoch 23/69, Train Loss: 1.0756, Test Loss: 1.1062\n",
      "Epoch 24/69, Train Loss: 1.1153, Test Loss: 1.1084\n",
      "Epoch 25/69, Train Loss: 1.0739, Test Loss: 1.1083\n",
      "Epoch 26/69, Train Loss: 1.0824, Test Loss: 1.1078\n",
      "Epoch 27/69, Train Loss: 1.0962, Test Loss: 1.1067\n",
      "Epoch 28/69, Train Loss: 1.0553, Test Loss: 1.1071\n",
      "Epoch 29/69, Train Loss: 1.0472, Test Loss: 1.1071\n",
      "Epoch 30/69, Train Loss: 1.0585, Test Loss: 1.1094\n",
      "Epoch 31/69, Train Loss: 1.0527, Test Loss: 1.1100\n",
      "Epoch 32/69, Train Loss: 1.0523, Test Loss: 1.1125\n",
      "Epoch 33/69, Train Loss: 1.0652, Test Loss: 1.1115\n",
      "Epoch 34/69, Train Loss: 1.0567, Test Loss: 1.1088\n",
      "Epoch 35/69, Train Loss: 1.0617, Test Loss: 1.1113\n",
      "Epoch 36/69, Train Loss: 1.0605, Test Loss: 1.1127\n",
      "Epoch 37/69, Train Loss: 1.0428, Test Loss: 1.1137\n",
      "Epoch 38/69, Train Loss: 1.0667, Test Loss: 1.1165\n",
      "Epoch 39/69, Train Loss: 1.0622, Test Loss: 1.1149\n",
      "Epoch 40/69, Train Loss: 1.0397, Test Loss: 1.1180\n",
      "Epoch 41/69, Train Loss: 1.0563, Test Loss: 1.1125\n",
      "Epoch 42/69, Train Loss: 1.0337, Test Loss: 1.1096\n",
      "Epoch 43/69, Train Loss: 1.0470, Test Loss: 1.1104\n",
      "Epoch 44/69, Train Loss: 1.0517, Test Loss: 1.1120\n",
      "Epoch 45/69, Train Loss: 1.0760, Test Loss: 1.1097\n",
      "Epoch 46/69, Train Loss: 1.0267, Test Loss: 1.1167\n",
      "Epoch 47/69, Train Loss: 1.0505, Test Loss: 1.1125\n",
      "Epoch 48/69, Train Loss: 1.0533, Test Loss: 1.1178\n",
      "Epoch 49/69, Train Loss: 1.0241, Test Loss: 1.1144\n",
      "Epoch 50/69, Train Loss: 1.0339, Test Loss: 1.1132\n",
      "Epoch 51/69, Train Loss: 1.0306, Test Loss: 1.1123\n",
      "Epoch 52/69, Train Loss: 1.0575, Test Loss: 1.1113\n",
      "Epoch 53/69, Train Loss: 1.0344, Test Loss: 1.1067\n",
      "Epoch 54/69, Train Loss: 1.0406, Test Loss: 1.1092\n",
      "Epoch 55/69, Train Loss: 1.0486, Test Loss: 1.1155\n",
      "Epoch 56/69, Train Loss: 1.0839, Test Loss: 1.1129\n",
      "Epoch 57/69, Train Loss: 1.0719, Test Loss: 1.1122\n",
      "Epoch 58/69, Train Loss: 1.0713, Test Loss: 1.1105\n",
      "Epoch 59/69, Train Loss: 1.0311, Test Loss: 1.1181\n",
      "Epoch 60/69, Train Loss: 1.0328, Test Loss: 1.1185\n",
      "Epoch 61/69, Train Loss: 1.0257, Test Loss: 1.1134\n",
      "Epoch 62/69, Train Loss: 1.0456, Test Loss: 1.1145\n",
      "Epoch 63/69, Train Loss: 1.0437, Test Loss: 1.1162\n",
      "Epoch 64/69, Train Loss: 1.0384, Test Loss: 1.1117\n",
      "Epoch 65/69, Train Loss: 1.0387, Test Loss: 1.1167\n",
      "Epoch 66/69, Train Loss: 1.0261, Test Loss: 1.1176\n",
      "Epoch 67/69, Train Loss: 1.0498, Test Loss: 1.1169\n",
      "Epoch 68/69, Train Loss: 1.0340, Test Loss: 1.1159\n",
      "Epoch 69/69, Train Loss: 1.0420, Test Loss: 1.1164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:07:32,889] Trial 173 finished with value: 1.116352617740631 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 220, 'layer_1_size': 147, 'layer_2_size': 157, 'layer_3_size': 138, 'layer_4_size': 119, 'layer_5_size': 144, 'layer_6_size': 81, 'layer_7_size': 241, 'layer_8_size': 64, 'layer_9_size': 39, 'layer_10_size': 175, 'layer_11_size': 136, 'layer_12_size': 38, 'dropout_rate': 0.3069914325085763, 'learning_rate': 0.0002526719850732272, 'batch_size': 32, 'epochs': 69}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80, Train Loss: 1.1396, Test Loss: 1.0553\n",
      "Epoch 2/80, Train Loss: 1.1224, Test Loss: 1.0254\n",
      "Epoch 3/80, Train Loss: 1.0775, Test Loss: 1.0025\n",
      "Epoch 4/80, Train Loss: 1.0855, Test Loss: 1.0273\n",
      "Epoch 5/80, Train Loss: 1.0775, Test Loss: 1.0149\n",
      "Epoch 6/80, Train Loss: 1.0759, Test Loss: 1.0169\n",
      "Epoch 7/80, Train Loss: 1.0408, Test Loss: 1.0215\n",
      "Epoch 8/80, Train Loss: 1.0733, Test Loss: 1.0147\n",
      "Epoch 9/80, Train Loss: 1.0784, Test Loss: 1.0152\n",
      "Epoch 10/80, Train Loss: 1.0517, Test Loss: 1.0142\n",
      "Epoch 11/80, Train Loss: 1.0769, Test Loss: 1.0206\n",
      "Epoch 12/80, Train Loss: 1.0736, Test Loss: 1.0185\n",
      "Epoch 13/80, Train Loss: 1.0297, Test Loss: 1.0064\n",
      "Epoch 14/80, Train Loss: 1.0874, Test Loss: 1.0060\n",
      "Epoch 15/80, Train Loss: 1.0296, Test Loss: 1.0014\n",
      "Epoch 16/80, Train Loss: 1.0200, Test Loss: 1.0008\n",
      "Epoch 17/80, Train Loss: 1.0150, Test Loss: 1.0093\n",
      "Epoch 18/80, Train Loss: 1.0719, Test Loss: 1.0113\n",
      "Epoch 19/80, Train Loss: 1.0749, Test Loss: 1.0038\n",
      "Epoch 20/80, Train Loss: 1.0342, Test Loss: 1.0054\n",
      "Epoch 21/80, Train Loss: 1.0142, Test Loss: 1.0076\n",
      "Epoch 22/80, Train Loss: 1.0690, Test Loss: 1.0125\n",
      "Epoch 23/80, Train Loss: 1.0572, Test Loss: 1.0073\n",
      "Epoch 24/80, Train Loss: 1.0302, Test Loss: 0.9996\n",
      "Epoch 25/80, Train Loss: 1.0585, Test Loss: 1.0012\n",
      "Epoch 26/80, Train Loss: 1.0047, Test Loss: 1.0014\n",
      "Epoch 27/80, Train Loss: 1.0407, Test Loss: 1.0057\n",
      "Epoch 28/80, Train Loss: 1.0121, Test Loss: 1.0049\n",
      "Epoch 29/80, Train Loss: 1.0447, Test Loss: 0.9984\n",
      "Epoch 30/80, Train Loss: 1.0258, Test Loss: 1.0011\n",
      "Epoch 31/80, Train Loss: 1.0724, Test Loss: 0.9966\n",
      "Epoch 32/80, Train Loss: 1.0651, Test Loss: 0.9930\n",
      "Epoch 33/80, Train Loss: 1.0193, Test Loss: 0.9926\n",
      "Epoch 34/80, Train Loss: 1.0089, Test Loss: 0.9933\n",
      "Epoch 35/80, Train Loss: 1.0178, Test Loss: 0.9905\n",
      "Epoch 36/80, Train Loss: 1.0160, Test Loss: 0.9920\n",
      "Epoch 37/80, Train Loss: 1.0300, Test Loss: 0.9839\n",
      "Epoch 38/80, Train Loss: 1.0244, Test Loss: 0.9987\n",
      "Epoch 39/80, Train Loss: 1.0058, Test Loss: 1.0012\n",
      "Epoch 40/80, Train Loss: 1.0390, Test Loss: 0.9998\n",
      "Epoch 41/80, Train Loss: 1.0172, Test Loss: 0.9930\n",
      "Epoch 42/80, Train Loss: 1.0066, Test Loss: 0.9975\n",
      "Epoch 43/80, Train Loss: 1.0077, Test Loss: 0.9973\n",
      "Epoch 44/80, Train Loss: 1.0201, Test Loss: 0.9938\n",
      "Epoch 45/80, Train Loss: 1.0257, Test Loss: 0.9937\n",
      "Epoch 46/80, Train Loss: 1.0422, Test Loss: 0.9911\n",
      "Epoch 47/80, Train Loss: 0.9994, Test Loss: 0.9953\n",
      "Epoch 48/80, Train Loss: 1.0132, Test Loss: 1.0011\n",
      "Epoch 49/80, Train Loss: 0.9933, Test Loss: 1.0020\n",
      "Epoch 50/80, Train Loss: 1.0136, Test Loss: 1.0031\n",
      "Epoch 51/80, Train Loss: 1.0364, Test Loss: 0.9989\n",
      "Epoch 52/80, Train Loss: 1.0069, Test Loss: 0.9999\n",
      "Epoch 53/80, Train Loss: 0.9958, Test Loss: 1.0063\n",
      "Epoch 54/80, Train Loss: 1.0095, Test Loss: 1.0058\n",
      "Epoch 55/80, Train Loss: 0.9925, Test Loss: 1.0089\n",
      "Epoch 56/80, Train Loss: 1.0022, Test Loss: 1.0085\n",
      "Epoch 57/80, Train Loss: 1.0240, Test Loss: 1.0082\n",
      "Epoch 58/80, Train Loss: 0.9922, Test Loss: 1.0095\n",
      "Epoch 59/80, Train Loss: 1.0213, Test Loss: 1.0100\n",
      "Epoch 60/80, Train Loss: 1.0216, Test Loss: 1.0180\n",
      "Epoch 61/80, Train Loss: 1.0142, Test Loss: 1.0101\n",
      "Epoch 62/80, Train Loss: 1.0089, Test Loss: 1.0094\n",
      "Epoch 63/80, Train Loss: 1.0179, Test Loss: 1.0142\n",
      "Epoch 64/80, Train Loss: 0.9952, Test Loss: 1.0130\n",
      "Epoch 65/80, Train Loss: 1.0104, Test Loss: 1.0020\n",
      "Epoch 66/80, Train Loss: 1.0131, Test Loss: 1.0072\n",
      "Epoch 67/80, Train Loss: 1.0001, Test Loss: 1.0016\n",
      "Epoch 68/80, Train Loss: 1.0314, Test Loss: 1.0092\n",
      "Epoch 69/80, Train Loss: 1.0023, Test Loss: 1.0049\n",
      "Epoch 70/80, Train Loss: 1.0116, Test Loss: 1.0054\n",
      "Epoch 71/80, Train Loss: 1.0023, Test Loss: 1.0034\n",
      "Epoch 72/80, Train Loss: 1.0000, Test Loss: 0.9998\n",
      "Epoch 73/80, Train Loss: 1.0129, Test Loss: 1.0022\n",
      "Epoch 74/80, Train Loss: 0.9893, Test Loss: 1.0098\n",
      "Epoch 75/80, Train Loss: 1.0078, Test Loss: 1.0155\n",
      "Epoch 76/80, Train Loss: 0.9895, Test Loss: 1.0160\n",
      "Epoch 77/80, Train Loss: 1.0225, Test Loss: 1.0120\n",
      "Epoch 78/80, Train Loss: 0.9963, Test Loss: 1.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:07:48,755] Trial 174 finished with value: 0.9990712404251099 and parameters: {'num_hidden_layers': 12, 'layer_0_size': 181, 'layer_1_size': 96, 'layer_2_size': 230, 'layer_3_size': 205, 'layer_4_size': 97, 'layer_5_size': 82, 'layer_6_size': 104, 'layer_7_size': 206, 'layer_8_size': 171, 'layer_9_size': 65, 'layer_10_size': 130, 'layer_11_size': 147, 'dropout_rate': 0.2492497806133827, 'learning_rate': 0.00027583925972534556, 'batch_size': 32, 'epochs': 80}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/80, Train Loss: 1.0005, Test Loss: 0.9985\n",
      "Epoch 80/80, Train Loss: 1.0180, Test Loss: 0.9991\n",
      "Epoch 1/73, Train Loss: 1.4791, Test Loss: 0.9984\n",
      "Epoch 2/73, Train Loss: 1.4419, Test Loss: 1.0024\n",
      "Epoch 3/73, Train Loss: 1.5386, Test Loss: 1.0068\n",
      "Epoch 4/73, Train Loss: 1.4801, Test Loss: 1.0050\n",
      "Epoch 5/73, Train Loss: 1.3760, Test Loss: 1.0005\n",
      "Epoch 6/73, Train Loss: 1.4814, Test Loss: 1.0075\n",
      "Epoch 7/73, Train Loss: 1.4163, Test Loss: 1.0019\n",
      "Epoch 8/73, Train Loss: 1.4426, Test Loss: 1.0051\n",
      "Epoch 9/73, Train Loss: 1.3999, Test Loss: 1.0174\n",
      "Epoch 10/73, Train Loss: 1.3965, Test Loss: 1.0079\n",
      "Epoch 11/73, Train Loss: 1.4089, Test Loss: 1.0132\n",
      "Epoch 12/73, Train Loss: 1.3511, Test Loss: 1.0044\n",
      "Epoch 13/73, Train Loss: 1.3619, Test Loss: 1.0336\n",
      "Epoch 14/73, Train Loss: 1.3474, Test Loss: 1.0088\n",
      "Epoch 15/73, Train Loss: 1.3118, Test Loss: 1.0287\n",
      "Epoch 16/73, Train Loss: 1.3638, Test Loss: 1.0173\n",
      "Epoch 17/73, Train Loss: 1.3299, Test Loss: 1.0140\n",
      "Epoch 18/73, Train Loss: 1.3241, Test Loss: 1.0208\n",
      "Epoch 19/73, Train Loss: 1.3753, Test Loss: 1.0282\n",
      "Epoch 20/73, Train Loss: 1.3484, Test Loss: 1.0161\n",
      "Epoch 21/73, Train Loss: 1.3575, Test Loss: 1.0253\n",
      "Epoch 22/73, Train Loss: 1.3486, Test Loss: 1.0220\n",
      "Epoch 23/73, Train Loss: 1.2740, Test Loss: 1.0131\n",
      "Epoch 24/73, Train Loss: 1.3044, Test Loss: 1.0178\n",
      "Epoch 25/73, Train Loss: 1.3250, Test Loss: 1.0201\n",
      "Epoch 26/73, Train Loss: 1.3132, Test Loss: 1.0302\n",
      "Epoch 27/73, Train Loss: 1.2896, Test Loss: 1.0186\n",
      "Epoch 28/73, Train Loss: 1.2716, Test Loss: 1.0242\n",
      "Epoch 29/73, Train Loss: 1.3099, Test Loss: 1.0207\n",
      "Epoch 30/73, Train Loss: 1.3116, Test Loss: 1.0226\n",
      "Epoch 31/73, Train Loss: 1.2558, Test Loss: 1.0427\n",
      "Epoch 32/73, Train Loss: 1.2043, Test Loss: 1.0402\n",
      "Epoch 33/73, Train Loss: 1.2520, Test Loss: 1.0260\n",
      "Epoch 34/73, Train Loss: 1.2900, Test Loss: 1.0110\n",
      "Epoch 35/73, Train Loss: 1.2811, Test Loss: 1.0341\n",
      "Epoch 36/73, Train Loss: 1.2575, Test Loss: 1.0334\n",
      "Epoch 37/73, Train Loss: 1.3002, Test Loss: 1.0144\n",
      "Epoch 38/73, Train Loss: 1.2568, Test Loss: 1.0541\n",
      "Epoch 39/73, Train Loss: 1.2126, Test Loss: 1.0272\n",
      "Epoch 40/73, Train Loss: 1.2525, Test Loss: 1.0401\n",
      "Epoch 41/73, Train Loss: 1.2313, Test Loss: 1.0229\n",
      "Epoch 42/73, Train Loss: 1.2293, Test Loss: 1.0180\n",
      "Epoch 43/73, Train Loss: 1.2563, Test Loss: 1.0199\n",
      "Epoch 44/73, Train Loss: 1.1724, Test Loss: 1.0328\n",
      "Epoch 45/73, Train Loss: 1.2378, Test Loss: 1.0281\n",
      "Epoch 46/73, Train Loss: 1.2401, Test Loss: 1.0361\n",
      "Epoch 47/73, Train Loss: 1.2339, Test Loss: 1.0139\n",
      "Epoch 48/73, Train Loss: 1.2591, Test Loss: 1.0174\n",
      "Epoch 49/73, Train Loss: 1.2382, Test Loss: 1.0135\n",
      "Epoch 50/73, Train Loss: 1.2030, Test Loss: 1.0270\n",
      "Epoch 51/73, Train Loss: 1.2081, Test Loss: 1.0358\n",
      "Epoch 52/73, Train Loss: 1.1849, Test Loss: 1.0255\n",
      "Epoch 53/73, Train Loss: 1.1888, Test Loss: 1.0356\n",
      "Epoch 54/73, Train Loss: 1.2340, Test Loss: 1.0274\n",
      "Epoch 55/73, Train Loss: 1.2454, Test Loss: 1.0322\n",
      "Epoch 56/73, Train Loss: 1.1957, Test Loss: 1.0252\n",
      "Epoch 57/73, Train Loss: 1.1722, Test Loss: 1.0179\n",
      "Epoch 58/73, Train Loss: 1.2202, Test Loss: 1.0300\n",
      "Epoch 59/73, Train Loss: 1.2135, Test Loss: 1.0056\n",
      "Epoch 60/73, Train Loss: 1.1719, Test Loss: 1.0221\n",
      "Epoch 61/73, Train Loss: 1.2156, Test Loss: 1.0083\n",
      "Epoch 62/73, Train Loss: 1.1862, Test Loss: 1.0225\n",
      "Epoch 63/73, Train Loss: 1.1786, Test Loss: 1.0111\n",
      "Epoch 64/73, Train Loss: 1.2215, Test Loss: 1.0151\n",
      "Epoch 65/73, Train Loss: 1.1651, Test Loss: 1.0020\n",
      "Epoch 66/73, Train Loss: 1.2031, Test Loss: 1.0164\n",
      "Epoch 67/73, Train Loss: 1.2104, Test Loss: 1.0140\n",
      "Epoch 68/73, Train Loss: 1.1752, Test Loss: 1.0033\n",
      "Epoch 69/73, Train Loss: 1.1812, Test Loss: 1.0052\n",
      "Epoch 70/73, Train Loss: 1.1756, Test Loss: 1.0173\n",
      "Epoch 71/73, Train Loss: 1.1612, Test Loss: 1.0153\n",
      "Epoch 72/73, Train Loss: 1.1632, Test Loss: 1.0206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:08:05,154] Trial 175 finished with value: 1.00661016362054 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 227, 'layer_1_size': 132, 'layer_2_size': 142, 'layer_3_size': 151, 'layer_4_size': 137, 'layer_5_size': 253, 'layer_6_size': 209, 'layer_7_size': 104, 'layer_8_size': 161, 'layer_9_size': 148, 'layer_10_size': 148, 'layer_11_size': 141, 'layer_12_size': 49, 'dropout_rate': 0.28850133004809686, 'learning_rate': 1.1922432156761594e-05, 'batch_size': 32, 'epochs': 73}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/73, Train Loss: 1.1639, Test Loss: 1.0066\n",
      "Epoch 1/60, Train Loss: 1.2928, Test Loss: 1.1082\n",
      "Epoch 2/60, Train Loss: 1.2247, Test Loss: 1.1076\n",
      "Epoch 3/60, Train Loss: 1.1708, Test Loss: 1.1076\n",
      "Epoch 4/60, Train Loss: 1.1852, Test Loss: 1.1077\n",
      "Epoch 5/60, Train Loss: 1.1337, Test Loss: 1.1081\n",
      "Epoch 6/60, Train Loss: 1.1785, Test Loss: 1.1088\n",
      "Epoch 7/60, Train Loss: 1.1468, Test Loss: 1.1092\n",
      "Epoch 8/60, Train Loss: 1.1398, Test Loss: 1.1077\n",
      "Epoch 9/60, Train Loss: 1.1535, Test Loss: 1.1085\n",
      "Epoch 10/60, Train Loss: 1.1396, Test Loss: 1.1080\n",
      "Epoch 11/60, Train Loss: 1.1596, Test Loss: 1.1082\n",
      "Epoch 12/60, Train Loss: 1.1430, Test Loss: 1.1098\n",
      "Epoch 13/60, Train Loss: 1.1219, Test Loss: 1.1099\n",
      "Epoch 14/60, Train Loss: 1.1180, Test Loss: 1.1113\n",
      "Epoch 15/60, Train Loss: 1.1009, Test Loss: 1.1094\n",
      "Epoch 16/60, Train Loss: 1.1340, Test Loss: 1.1095\n",
      "Epoch 17/60, Train Loss: 1.1326, Test Loss: 1.1081\n",
      "Epoch 18/60, Train Loss: 1.1319, Test Loss: 1.1086\n",
      "Epoch 19/60, Train Loss: 1.0826, Test Loss: 1.1102\n",
      "Epoch 20/60, Train Loss: 1.1359, Test Loss: 1.1125\n",
      "Epoch 21/60, Train Loss: 1.1418, Test Loss: 1.1126\n",
      "Epoch 22/60, Train Loss: 1.1179, Test Loss: 1.1125\n",
      "Epoch 23/60, Train Loss: 1.1282, Test Loss: 1.1116\n",
      "Epoch 24/60, Train Loss: 1.1179, Test Loss: 1.1123\n",
      "Epoch 25/60, Train Loss: 1.1561, Test Loss: 1.1110\n",
      "Epoch 26/60, Train Loss: 1.0877, Test Loss: 1.1112\n",
      "Epoch 27/60, Train Loss: 1.0856, Test Loss: 1.1107\n",
      "Epoch 28/60, Train Loss: 1.1103, Test Loss: 1.1100\n",
      "Epoch 29/60, Train Loss: 1.0888, Test Loss: 1.1112\n",
      "Epoch 30/60, Train Loss: 1.1114, Test Loss: 1.1101\n",
      "Epoch 31/60, Train Loss: 1.1579, Test Loss: 1.1098\n",
      "Epoch 32/60, Train Loss: 1.1417, Test Loss: 1.1095\n",
      "Epoch 33/60, Train Loss: 1.1213, Test Loss: 1.1090\n",
      "Epoch 34/60, Train Loss: 1.1460, Test Loss: 1.1101\n",
      "Epoch 35/60, Train Loss: 1.0964, Test Loss: 1.1112\n",
      "Epoch 36/60, Train Loss: 1.1015, Test Loss: 1.1135\n",
      "Epoch 37/60, Train Loss: 1.1360, Test Loss: 1.1128\n",
      "Epoch 38/60, Train Loss: 1.1344, Test Loss: 1.1135\n",
      "Epoch 39/60, Train Loss: 1.1137, Test Loss: 1.1121\n",
      "Epoch 40/60, Train Loss: 1.0909, Test Loss: 1.1129\n",
      "Epoch 41/60, Train Loss: 1.1172, Test Loss: 1.1139\n",
      "Epoch 42/60, Train Loss: 1.1058, Test Loss: 1.1126\n",
      "Epoch 43/60, Train Loss: 1.1054, Test Loss: 1.1119\n",
      "Epoch 44/60, Train Loss: 1.0991, Test Loss: 1.1118\n",
      "Epoch 45/60, Train Loss: 1.0992, Test Loss: 1.1107\n",
      "Epoch 46/60, Train Loss: 1.0926, Test Loss: 1.1109\n",
      "Epoch 47/60, Train Loss: 1.0956, Test Loss: 1.1104\n",
      "Epoch 48/60, Train Loss: 1.0769, Test Loss: 1.1106\n",
      "Epoch 49/60, Train Loss: 1.1078, Test Loss: 1.1096\n",
      "Epoch 50/60, Train Loss: 1.0997, Test Loss: 1.1078\n",
      "Epoch 51/60, Train Loss: 1.1045, Test Loss: 1.1068\n",
      "Epoch 52/60, Train Loss: 1.0836, Test Loss: 1.1058\n",
      "Epoch 53/60, Train Loss: 1.0974, Test Loss: 1.1063\n",
      "Epoch 54/60, Train Loss: 1.0785, Test Loss: 1.1067\n",
      "Epoch 55/60, Train Loss: 1.1012, Test Loss: 1.1077\n",
      "Epoch 56/60, Train Loss: 1.1092, Test Loss: 1.1049\n",
      "Epoch 57/60, Train Loss: 1.1035, Test Loss: 1.1045\n",
      "Epoch 58/60, Train Loss: 1.0939, Test Loss: 1.1054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:08:08,636] Trial 176 finished with value: 1.1045318841934204 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 124, 'layer_1_size': 40, 'layer_2_size': 243, 'layer_3_size': 129, 'layer_4_size': 113, 'layer_5_size': 245, 'layer_6_size': 86, 'layer_7_size': 117, 'layer_8_size': 150, 'layer_9_size': 55, 'layer_10_size': 225, 'layer_11_size': 166, 'layer_12_size': 58, 'layer_13_size': 153, 'dropout_rate': 0.2545757597836334, 'learning_rate': 0.00020777812084628124, 'batch_size': 256, 'epochs': 60}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/60, Train Loss: 1.0811, Test Loss: 1.1046\n",
      "Epoch 60/60, Train Loss: 1.0723, Test Loss: 1.1045\n",
      "Epoch 1/57, Train Loss: 1.1365, Test Loss: 1.1454\n",
      "Epoch 2/57, Train Loss: 1.0985, Test Loss: 1.1404\n",
      "Epoch 3/57, Train Loss: 1.1020, Test Loss: 1.1203\n",
      "Epoch 4/57, Train Loss: 1.0190, Test Loss: 1.1325\n",
      "Epoch 5/57, Train Loss: 1.0691, Test Loss: 1.1878\n",
      "Epoch 6/57, Train Loss: 1.0367, Test Loss: 1.1827\n",
      "Epoch 7/57, Train Loss: 1.0570, Test Loss: 1.1383\n",
      "Epoch 8/57, Train Loss: 1.0220, Test Loss: 1.1442\n",
      "Epoch 9/57, Train Loss: 1.0118, Test Loss: 1.1297\n",
      "Epoch 10/57, Train Loss: 1.0218, Test Loss: 1.1260\n",
      "Epoch 11/57, Train Loss: 1.0234, Test Loss: 1.1465\n",
      "Epoch 12/57, Train Loss: 1.0329, Test Loss: 1.1512\n",
      "Epoch 13/57, Train Loss: 1.0164, Test Loss: 1.1463\n",
      "Epoch 14/57, Train Loss: 1.0065, Test Loss: 1.1399\n",
      "Epoch 15/57, Train Loss: 0.9912, Test Loss: 1.1605\n",
      "Epoch 16/57, Train Loss: 0.9866, Test Loss: 1.1644\n",
      "Epoch 17/57, Train Loss: 0.9978, Test Loss: 1.1487\n",
      "Epoch 18/57, Train Loss: 1.0164, Test Loss: 1.1447\n",
      "Epoch 19/57, Train Loss: 1.0108, Test Loss: 1.1522\n",
      "Epoch 20/57, Train Loss: 0.9744, Test Loss: 1.1525\n",
      "Epoch 21/57, Train Loss: 0.9748, Test Loss: 1.1609\n",
      "Epoch 22/57, Train Loss: 0.9752, Test Loss: 1.1528\n",
      "Epoch 23/57, Train Loss: 0.9814, Test Loss: 1.1377\n",
      "Epoch 24/57, Train Loss: 0.9859, Test Loss: 1.1504\n",
      "Epoch 25/57, Train Loss: 0.9812, Test Loss: 1.1458\n",
      "Epoch 26/57, Train Loss: 0.9784, Test Loss: 1.1616\n",
      "Epoch 27/57, Train Loss: 0.9812, Test Loss: 1.1567\n",
      "Epoch 28/57, Train Loss: 0.9716, Test Loss: 1.1697\n",
      "Epoch 29/57, Train Loss: 0.9691, Test Loss: 1.1597\n",
      "Epoch 30/57, Train Loss: 0.9692, Test Loss: 1.1424\n",
      "Epoch 31/57, Train Loss: 0.9737, Test Loss: 1.1446\n",
      "Epoch 32/57, Train Loss: 0.9875, Test Loss: 1.1542\n",
      "Epoch 33/57, Train Loss: 0.9595, Test Loss: 1.1885\n",
      "Epoch 34/57, Train Loss: 0.9591, Test Loss: 1.1753\n",
      "Epoch 35/57, Train Loss: 0.9718, Test Loss: 1.1748\n",
      "Epoch 36/57, Train Loss: 0.9730, Test Loss: 1.1806\n",
      "Epoch 37/57, Train Loss: 0.9690, Test Loss: 1.1798\n",
      "Epoch 38/57, Train Loss: 1.0047, Test Loss: 1.1907\n",
      "Epoch 39/57, Train Loss: 0.9541, Test Loss: 1.1782\n",
      "Epoch 40/57, Train Loss: 0.9571, Test Loss: 1.2148\n",
      "Epoch 41/57, Train Loss: 0.9719, Test Loss: 1.2081\n",
      "Epoch 42/57, Train Loss: 0.9227, Test Loss: 1.1886\n",
      "Epoch 43/57, Train Loss: 0.9473, Test Loss: 1.2026\n",
      "Epoch 44/57, Train Loss: 0.9216, Test Loss: 1.2026\n",
      "Epoch 45/57, Train Loss: 0.9335, Test Loss: 1.2113\n",
      "Epoch 46/57, Train Loss: 0.9106, Test Loss: 1.2441\n",
      "Epoch 47/57, Train Loss: 0.9046, Test Loss: 1.2446\n",
      "Epoch 48/57, Train Loss: 0.9027, Test Loss: 1.2372\n",
      "Epoch 49/57, Train Loss: 0.9276, Test Loss: 1.2439\n",
      "Epoch 50/57, Train Loss: 0.8878, Test Loss: 1.2823\n",
      "Epoch 51/57, Train Loss: 0.8908, Test Loss: 1.2519\n",
      "Epoch 52/57, Train Loss: 0.8624, Test Loss: 1.2528\n",
      "Epoch 53/57, Train Loss: 0.8527, Test Loss: 1.3412\n",
      "Epoch 54/57, Train Loss: 0.8550, Test Loss: 1.3362\n",
      "Epoch 55/57, Train Loss: 0.8558, Test Loss: 1.2645\n",
      "Epoch 56/57, Train Loss: 0.8269, Test Loss: 1.3731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:08:19,139] Trial 177 finished with value: 1.3396676778793335 and parameters: {'num_hidden_layers': 11, 'layer_0_size': 139, 'layer_1_size': 106, 'layer_2_size': 234, 'layer_3_size': 110, 'layer_4_size': 250, 'layer_5_size': 132, 'layer_6_size': 72, 'layer_7_size': 128, 'layer_8_size': 77, 'layer_9_size': 133, 'layer_10_size': 118, 'dropout_rate': 0.30178993036060586, 'learning_rate': 0.0009314949668365333, 'batch_size': 32, 'epochs': 57}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/57, Train Loss: 0.8092, Test Loss: 1.3397\n",
      "Epoch 1/53, Train Loss: 1.2222, Test Loss: 0.8492\n",
      "Epoch 2/53, Train Loss: 1.0960, Test Loss: 0.8520\n",
      "Epoch 3/53, Train Loss: 1.1236, Test Loss: 0.8635\n",
      "Epoch 4/53, Train Loss: 1.1137, Test Loss: 0.8547\n",
      "Epoch 5/53, Train Loss: 1.0753, Test Loss: 0.8491\n",
      "Epoch 6/53, Train Loss: 1.0698, Test Loss: 0.8521\n",
      "Epoch 7/53, Train Loss: 1.0765, Test Loss: 0.8482\n",
      "Epoch 8/53, Train Loss: 1.1159, Test Loss: 0.8524\n",
      "Epoch 9/53, Train Loss: 1.0865, Test Loss: 0.8645\n",
      "Epoch 10/53, Train Loss: 1.0385, Test Loss: 0.8801\n",
      "Epoch 11/53, Train Loss: 1.0568, Test Loss: 0.8676\n",
      "Epoch 12/53, Train Loss: 1.0564, Test Loss: 0.8611\n",
      "Epoch 13/53, Train Loss: 1.0462, Test Loss: 0.8588\n",
      "Epoch 14/53, Train Loss: 1.0629, Test Loss: 0.8501\n",
      "Epoch 15/53, Train Loss: 1.0243, Test Loss: 0.8531\n",
      "Epoch 16/53, Train Loss: 1.0546, Test Loss: 0.8616\n",
      "Epoch 17/53, Train Loss: 1.0607, Test Loss: 0.8704\n",
      "Epoch 18/53, Train Loss: 1.0140, Test Loss: 0.8754\n",
      "Epoch 19/53, Train Loss: 1.0488, Test Loss: 0.8799\n",
      "Epoch 20/53, Train Loss: 1.0672, Test Loss: 0.8936\n",
      "Epoch 21/53, Train Loss: 1.0637, Test Loss: 0.8899\n",
      "Epoch 22/53, Train Loss: 1.0445, Test Loss: 0.8801\n",
      "Epoch 23/53, Train Loss: 1.0349, Test Loss: 0.8654\n",
      "Epoch 24/53, Train Loss: 1.0151, Test Loss: 0.8591\n",
      "Epoch 25/53, Train Loss: 1.0178, Test Loss: 0.8525\n",
      "Epoch 26/53, Train Loss: 1.0552, Test Loss: 0.8536\n",
      "Epoch 27/53, Train Loss: 1.0347, Test Loss: 0.8589\n",
      "Epoch 28/53, Train Loss: 1.0397, Test Loss: 0.8663\n",
      "Epoch 29/53, Train Loss: 1.0158, Test Loss: 0.8636\n",
      "Epoch 30/53, Train Loss: 0.9930, Test Loss: 0.8541\n",
      "Epoch 31/53, Train Loss: 1.0185, Test Loss: 0.8530\n",
      "Epoch 32/53, Train Loss: 0.9981, Test Loss: 0.8507\n",
      "Epoch 33/53, Train Loss: 1.0476, Test Loss: 0.8595\n",
      "Epoch 34/53, Train Loss: 0.9861, Test Loss: 0.8564\n",
      "Epoch 35/53, Train Loss: 1.0205, Test Loss: 0.8490\n",
      "Epoch 36/53, Train Loss: 1.0143, Test Loss: 0.8484\n",
      "Epoch 37/53, Train Loss: 1.0354, Test Loss: 0.8576\n",
      "Epoch 38/53, Train Loss: 1.0250, Test Loss: 0.8572\n",
      "Epoch 39/53, Train Loss: 1.0237, Test Loss: 0.8599\n",
      "Epoch 40/53, Train Loss: 1.0139, Test Loss: 0.8510\n",
      "Epoch 41/53, Train Loss: 1.0277, Test Loss: 0.8580\n",
      "Epoch 42/53, Train Loss: 0.9894, Test Loss: 0.8625\n",
      "Epoch 43/53, Train Loss: 0.9988, Test Loss: 0.8672\n",
      "Epoch 44/53, Train Loss: 1.0240, Test Loss: 0.8600\n",
      "Epoch 45/53, Train Loss: 1.0108, Test Loss: 0.8518\n",
      "Epoch 46/53, Train Loss: 1.0356, Test Loss: 0.8516\n",
      "Epoch 47/53, Train Loss: 1.0009, Test Loss: 0.8492\n",
      "Epoch 48/53, Train Loss: 1.0177, Test Loss: 0.8503\n",
      "Epoch 49/53, Train Loss: 1.0136, Test Loss: 0.8596\n",
      "Epoch 50/53, Train Loss: 1.0026, Test Loss: 0.8593\n",
      "Epoch 51/53, Train Loss: 1.0176, Test Loss: 0.8592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:08:26,619] Trial 178 finished with value: 0.8518556654453278 and parameters: {'num_hidden_layers': 15, 'layer_0_size': 199, 'layer_1_size': 34, 'layer_2_size': 134, 'layer_3_size': 148, 'layer_4_size': 100, 'layer_5_size': 188, 'layer_6_size': 32, 'layer_7_size': 161, 'layer_8_size': 89, 'layer_9_size': 226, 'layer_10_size': 253, 'layer_11_size': 120, 'layer_12_size': 167, 'layer_13_size': 171, 'layer_14_size': 102, 'dropout_rate': 0.23226862271926374, 'learning_rate': 0.0003688468222766114, 'batch_size': 64, 'epochs': 53}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/53, Train Loss: 0.9863, Test Loss: 0.8541\n",
      "Epoch 53/53, Train Loss: 1.0226, Test Loss: 0.8519\n",
      "Epoch 1/51, Train Loss: 1.0658, Test Loss: 1.2150\n",
      "Epoch 2/51, Train Loss: 1.0598, Test Loss: 1.2069\n",
      "Epoch 3/51, Train Loss: 1.0301, Test Loss: 1.1968\n",
      "Epoch 4/51, Train Loss: 1.0456, Test Loss: 1.2178\n",
      "Epoch 5/51, Train Loss: 1.0149, Test Loss: 1.2271\n",
      "Epoch 6/51, Train Loss: 1.0288, Test Loss: 1.2338\n",
      "Epoch 7/51, Train Loss: 0.9948, Test Loss: 1.2278\n",
      "Epoch 8/51, Train Loss: 0.9723, Test Loss: 1.2351\n",
      "Epoch 9/51, Train Loss: 1.0282, Test Loss: 1.2513\n",
      "Epoch 10/51, Train Loss: 0.9748, Test Loss: 1.2275\n",
      "Epoch 11/51, Train Loss: 0.9904, Test Loss: 1.2297\n",
      "Epoch 12/51, Train Loss: 1.0053, Test Loss: 1.2246\n",
      "Epoch 13/51, Train Loss: 0.9661, Test Loss: 1.2263\n",
      "Epoch 14/51, Train Loss: 0.9841, Test Loss: 1.2266\n",
      "Epoch 15/51, Train Loss: 0.9926, Test Loss: 1.2275\n",
      "Epoch 16/51, Train Loss: 0.9896, Test Loss: 1.2320\n",
      "Epoch 17/51, Train Loss: 0.9693, Test Loss: 1.2283\n",
      "Epoch 18/51, Train Loss: 1.0042, Test Loss: 1.2348\n",
      "Epoch 19/51, Train Loss: 0.9543, Test Loss: 1.2396\n",
      "Epoch 20/51, Train Loss: 0.9727, Test Loss: 1.2372\n",
      "Epoch 21/51, Train Loss: 0.9851, Test Loss: 1.2329\n",
      "Epoch 22/51, Train Loss: 0.9817, Test Loss: 1.2520\n",
      "Epoch 23/51, Train Loss: 0.9405, Test Loss: 1.2521\n",
      "Epoch 24/51, Train Loss: 0.9548, Test Loss: 1.2387\n",
      "Epoch 25/51, Train Loss: 0.9733, Test Loss: 1.2369\n",
      "Epoch 26/51, Train Loss: 0.9772, Test Loss: 1.2328\n",
      "Epoch 27/51, Train Loss: 0.9852, Test Loss: 1.2303\n",
      "Epoch 28/51, Train Loss: 0.9722, Test Loss: 1.2319\n",
      "Epoch 29/51, Train Loss: 0.9846, Test Loss: 1.2234\n",
      "Epoch 30/51, Train Loss: 0.9549, Test Loss: 1.2169\n",
      "Epoch 31/51, Train Loss: 0.9542, Test Loss: 1.2129\n",
      "Epoch 32/51, Train Loss: 0.9489, Test Loss: 1.2116\n",
      "Epoch 33/51, Train Loss: 0.9679, Test Loss: 1.2051\n",
      "Epoch 34/51, Train Loss: 0.9526, Test Loss: 1.2028\n",
      "Epoch 35/51, Train Loss: 0.9658, Test Loss: 1.2106\n",
      "Epoch 36/51, Train Loss: 0.9536, Test Loss: 1.2146\n",
      "Epoch 37/51, Train Loss: 0.9509, Test Loss: 1.2185\n",
      "Epoch 38/51, Train Loss: 0.9554, Test Loss: 1.2253\n",
      "Epoch 39/51, Train Loss: 0.9471, Test Loss: 1.2303\n",
      "Epoch 40/51, Train Loss: 0.9665, Test Loss: 1.2360\n",
      "Epoch 41/51, Train Loss: 0.9521, Test Loss: 1.2316\n",
      "Epoch 42/51, Train Loss: 0.9581, Test Loss: 1.2324\n",
      "Epoch 43/51, Train Loss: 0.9504, Test Loss: 1.2322\n",
      "Epoch 44/51, Train Loss: 0.9452, Test Loss: 1.2409\n",
      "Epoch 45/51, Train Loss: 0.9334, Test Loss: 1.2492\n",
      "Epoch 46/51, Train Loss: 0.9317, Test Loss: 1.2432\n",
      "Epoch 47/51, Train Loss: 0.9520, Test Loss: 1.2455\n",
      "Epoch 48/51, Train Loss: 0.9734, Test Loss: 1.2501\n",
      "Epoch 49/51, Train Loss: 0.9512, Test Loss: 1.2511\n",
      "Epoch 50/51, Train Loss: 0.9388, Test Loss: 1.2425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:08:34,102] Trial 179 finished with value: 1.2497609704732895 and parameters: {'num_hidden_layers': 16, 'layer_0_size': 197, 'layer_1_size': 35, 'layer_2_size': 127, 'layer_3_size': 161, 'layer_4_size': 104, 'layer_5_size': 119, 'layer_6_size': 32, 'layer_7_size': 151, 'layer_8_size': 99, 'layer_9_size': 225, 'layer_10_size': 255, 'layer_11_size': 179, 'layer_12_size': 166, 'layer_13_size': 134, 'layer_14_size': 101, 'layer_15_size': 101, 'dropout_rate': 0.23412791774106945, 'learning_rate': 0.00035307574885911844, 'batch_size': 64, 'epochs': 51}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/51, Train Loss: 0.9568, Test Loss: 1.2498\n",
      "Epoch 1/55, Train Loss: 1.0729, Test Loss: 1.0283\n",
      "Epoch 2/55, Train Loss: 1.0338, Test Loss: 1.0463\n",
      "Epoch 3/55, Train Loss: 1.0608, Test Loss: 1.0280\n",
      "Epoch 4/55, Train Loss: 1.1130, Test Loss: 1.0119\n",
      "Epoch 5/55, Train Loss: 1.0432, Test Loss: 1.0267\n",
      "Epoch 6/55, Train Loss: 1.0850, Test Loss: 1.0344\n",
      "Epoch 7/55, Train Loss: 1.0055, Test Loss: 1.0235\n",
      "Epoch 8/55, Train Loss: 1.0408, Test Loss: 1.0342\n",
      "Epoch 9/55, Train Loss: 1.0207, Test Loss: 1.0617\n",
      "Epoch 10/55, Train Loss: 1.0249, Test Loss: 1.0653\n",
      "Epoch 11/55, Train Loss: 1.0380, Test Loss: 1.0501\n",
      "Epoch 12/55, Train Loss: 1.0381, Test Loss: 1.0474\n",
      "Epoch 13/55, Train Loss: 1.0137, Test Loss: 1.0480\n",
      "Epoch 14/55, Train Loss: 1.0302, Test Loss: 1.0671\n",
      "Epoch 15/55, Train Loss: 1.0227, Test Loss: 1.0488\n",
      "Epoch 16/55, Train Loss: 0.9882, Test Loss: 1.0249\n",
      "Epoch 17/55, Train Loss: 1.0074, Test Loss: 1.0286\n",
      "Epoch 18/55, Train Loss: 1.0092, Test Loss: 1.0351\n",
      "Epoch 19/55, Train Loss: 1.0272, Test Loss: 1.0326\n",
      "Epoch 20/55, Train Loss: 1.0258, Test Loss: 1.0326\n",
      "Epoch 21/55, Train Loss: 1.0048, Test Loss: 1.0379\n",
      "Epoch 22/55, Train Loss: 1.0191, Test Loss: 1.0341\n",
      "Epoch 23/55, Train Loss: 1.0121, Test Loss: 1.0324\n",
      "Epoch 24/55, Train Loss: 1.0069, Test Loss: 1.0297\n",
      "Epoch 25/55, Train Loss: 1.0029, Test Loss: 1.0299\n",
      "Epoch 26/55, Train Loss: 1.0108, Test Loss: 1.0444\n",
      "Epoch 27/55, Train Loss: 0.9910, Test Loss: 1.0389\n",
      "Epoch 28/55, Train Loss: 1.0088, Test Loss: 1.0314\n",
      "Epoch 29/55, Train Loss: 1.0113, Test Loss: 1.0317\n",
      "Epoch 30/55, Train Loss: 1.0197, Test Loss: 1.0475\n",
      "Epoch 31/55, Train Loss: 1.0085, Test Loss: 1.0593\n",
      "Epoch 32/55, Train Loss: 0.9893, Test Loss: 1.0571\n",
      "Epoch 33/55, Train Loss: 0.9995, Test Loss: 1.0562\n",
      "Epoch 34/55, Train Loss: 0.9938, Test Loss: 1.0463\n",
      "Epoch 35/55, Train Loss: 1.0084, Test Loss: 1.0528\n",
      "Epoch 36/55, Train Loss: 1.0073, Test Loss: 1.0506\n",
      "Epoch 37/55, Train Loss: 0.9872, Test Loss: 1.0435\n",
      "Epoch 38/55, Train Loss: 1.0033, Test Loss: 1.0380\n",
      "Epoch 39/55, Train Loss: 0.9852, Test Loss: 1.0420\n",
      "Epoch 40/55, Train Loss: 0.9708, Test Loss: 1.0347\n",
      "Epoch 41/55, Train Loss: 0.9828, Test Loss: 1.0278\n",
      "Epoch 42/55, Train Loss: 0.9909, Test Loss: 1.0269\n",
      "Epoch 43/55, Train Loss: 0.9837, Test Loss: 1.0317\n",
      "Epoch 44/55, Train Loss: 0.9948, Test Loss: 1.0308\n",
      "Epoch 45/55, Train Loss: 1.0053, Test Loss: 1.0315\n",
      "Epoch 46/55, Train Loss: 0.9954, Test Loss: 1.0331\n",
      "Epoch 47/55, Train Loss: 0.9918, Test Loss: 1.0310\n",
      "Epoch 48/55, Train Loss: 1.0034, Test Loss: 1.0262\n",
      "Epoch 49/55, Train Loss: 0.9991, Test Loss: 1.0259\n",
      "Epoch 50/55, Train Loss: 0.9983, Test Loss: 1.0221\n",
      "Epoch 51/55, Train Loss: 1.0053, Test Loss: 1.0356\n",
      "Epoch 52/55, Train Loss: 0.9954, Test Loss: 1.0279\n",
      "Epoch 53/55, Train Loss: 0.9843, Test Loss: 1.0284\n",
      "Epoch 54/55, Train Loss: 1.0043, Test Loss: 1.0272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:08:41,938] Trial 180 finished with value: 1.0352337956428528 and parameters: {'num_hidden_layers': 15, 'layer_0_size': 210, 'layer_1_size': 37, 'layer_2_size': 111, 'layer_3_size': 146, 'layer_4_size': 108, 'layer_5_size': 185, 'layer_6_size': 45, 'layer_7_size': 164, 'layer_8_size': 105, 'layer_9_size': 233, 'layer_10_size': 246, 'layer_11_size': 154, 'layer_12_size': 156, 'layer_13_size': 142, 'layer_14_size': 119, 'dropout_rate': 0.23029564350818635, 'learning_rate': 0.0004812114308100127, 'batch_size': 64, 'epochs': 55}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/55, Train Loss: 0.9902, Test Loss: 1.0352\n",
      "Epoch 1/53, Train Loss: 1.1867, Test Loss: 0.9396\n",
      "Epoch 2/53, Train Loss: 1.0617, Test Loss: 0.9631\n",
      "Epoch 3/53, Train Loss: 1.0695, Test Loss: 0.9623\n",
      "Epoch 4/53, Train Loss: 1.0650, Test Loss: 0.9594\n",
      "Epoch 5/53, Train Loss: 1.0431, Test Loss: 0.9608\n",
      "Epoch 6/53, Train Loss: 1.0646, Test Loss: 0.9596\n",
      "Epoch 7/53, Train Loss: 1.0391, Test Loss: 0.9542\n",
      "Epoch 8/53, Train Loss: 1.0724, Test Loss: 0.9423\n",
      "Epoch 9/53, Train Loss: 1.0207, Test Loss: 0.9482\n",
      "Epoch 10/53, Train Loss: 1.0577, Test Loss: 0.9532\n",
      "Epoch 11/53, Train Loss: 1.0275, Test Loss: 0.9516\n",
      "Epoch 12/53, Train Loss: 1.0294, Test Loss: 0.9583\n",
      "Epoch 13/53, Train Loss: 1.0649, Test Loss: 0.9614\n",
      "Epoch 14/53, Train Loss: 1.0219, Test Loss: 0.9639\n",
      "Epoch 15/53, Train Loss: 1.0258, Test Loss: 0.9563\n",
      "Epoch 16/53, Train Loss: 1.0439, Test Loss: 0.9565\n",
      "Epoch 17/53, Train Loss: 1.0147, Test Loss: 0.9577\n",
      "Epoch 18/53, Train Loss: 0.9912, Test Loss: 0.9597\n",
      "Epoch 19/53, Train Loss: 1.0000, Test Loss: 0.9460\n",
      "Epoch 20/53, Train Loss: 1.0488, Test Loss: 0.9405\n",
      "Epoch 21/53, Train Loss: 0.9837, Test Loss: 0.9388\n",
      "Epoch 22/53, Train Loss: 1.0079, Test Loss: 0.9354\n",
      "Epoch 23/53, Train Loss: 1.0283, Test Loss: 0.9378\n",
      "Epoch 24/53, Train Loss: 0.9897, Test Loss: 0.9532\n",
      "Epoch 25/53, Train Loss: 1.0303, Test Loss: 0.9485\n",
      "Epoch 26/53, Train Loss: 1.0268, Test Loss: 0.9509\n",
      "Epoch 27/53, Train Loss: 1.0443, Test Loss: 0.9475\n",
      "Epoch 28/53, Train Loss: 1.0287, Test Loss: 0.9435\n",
      "Epoch 29/53, Train Loss: 1.0157, Test Loss: 0.9459\n",
      "Epoch 30/53, Train Loss: 1.0186, Test Loss: 0.9450\n",
      "Epoch 31/53, Train Loss: 1.0256, Test Loss: 0.9429\n",
      "Epoch 32/53, Train Loss: 1.0145, Test Loss: 0.9471\n",
      "Epoch 33/53, Train Loss: 1.0141, Test Loss: 0.9381\n",
      "Epoch 34/53, Train Loss: 1.0025, Test Loss: 0.9350\n",
      "Epoch 35/53, Train Loss: 1.0308, Test Loss: 0.9450\n",
      "Epoch 36/53, Train Loss: 1.0102, Test Loss: 0.9420\n",
      "Epoch 37/53, Train Loss: 0.9968, Test Loss: 0.9419\n",
      "Epoch 38/53, Train Loss: 1.0044, Test Loss: 0.9445\n",
      "Epoch 39/53, Train Loss: 1.0013, Test Loss: 0.9543\n",
      "Epoch 40/53, Train Loss: 1.0009, Test Loss: 0.9617\n",
      "Epoch 41/53, Train Loss: 1.0162, Test Loss: 0.9556\n",
      "Epoch 42/53, Train Loss: 1.0113, Test Loss: 0.9465\n",
      "Epoch 43/53, Train Loss: 1.0130, Test Loss: 0.9425\n",
      "Epoch 44/53, Train Loss: 1.0066, Test Loss: 0.9429\n",
      "Epoch 45/53, Train Loss: 1.0165, Test Loss: 0.9419\n",
      "Epoch 46/53, Train Loss: 1.0124, Test Loss: 0.9410\n",
      "Epoch 47/53, Train Loss: 1.0046, Test Loss: 0.9359\n",
      "Epoch 48/53, Train Loss: 1.0031, Test Loss: 0.9367\n",
      "Epoch 49/53, Train Loss: 0.9946, Test Loss: 0.9376\n",
      "Epoch 50/53, Train Loss: 0.9800, Test Loss: 0.9469\n",
      "Epoch 51/53, Train Loss: 0.9940, Test Loss: 0.9503\n",
      "Epoch 52/53, Train Loss: 1.0021, Test Loss: 0.9533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:08:49,400] Trial 181 finished with value: 0.9485874623060226 and parameters: {'num_hidden_layers': 15, 'layer_0_size': 192, 'layer_1_size': 142, 'layer_2_size': 133, 'layer_3_size': 148, 'layer_4_size': 95, 'layer_5_size': 190, 'layer_6_size': 38, 'layer_7_size': 185, 'layer_8_size': 86, 'layer_9_size': 221, 'layer_10_size': 238, 'layer_11_size': 119, 'layer_12_size': 175, 'layer_13_size': 170, 'layer_14_size': 136, 'dropout_rate': 0.2204363896840097, 'learning_rate': 0.0002878086397964112, 'batch_size': 64, 'epochs': 53}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/53, Train Loss: 1.0075, Test Loss: 0.9486\n",
      "Epoch 1/75, Train Loss: 1.1790, Test Loss: 0.8549\n",
      "Epoch 2/75, Train Loss: 1.1787, Test Loss: 0.8576\n",
      "Epoch 3/75, Train Loss: 1.0863, Test Loss: 0.8538\n",
      "Epoch 4/75, Train Loss: 1.1034, Test Loss: 0.8580\n",
      "Epoch 5/75, Train Loss: 1.0926, Test Loss: 0.8604\n",
      "Epoch 6/75, Train Loss: 1.0549, Test Loss: 0.8623\n",
      "Epoch 7/75, Train Loss: 1.0376, Test Loss: 0.8659\n",
      "Epoch 8/75, Train Loss: 1.0826, Test Loss: 0.8504\n",
      "Epoch 9/75, Train Loss: 1.0300, Test Loss: 0.8482\n",
      "Epoch 10/75, Train Loss: 1.0495, Test Loss: 0.8481\n",
      "Epoch 11/75, Train Loss: 1.0544, Test Loss: 0.8518\n",
      "Epoch 12/75, Train Loss: 1.0711, Test Loss: 0.8483\n",
      "Epoch 13/75, Train Loss: 1.0687, Test Loss: 0.8438\n",
      "Epoch 14/75, Train Loss: 1.0447, Test Loss: 0.8549\n",
      "Epoch 15/75, Train Loss: 1.0061, Test Loss: 0.8555\n",
      "Epoch 16/75, Train Loss: 1.0625, Test Loss: 0.8547\n",
      "Epoch 17/75, Train Loss: 1.0539, Test Loss: 0.8571\n",
      "Epoch 18/75, Train Loss: 1.0759, Test Loss: 0.8575\n",
      "Epoch 19/75, Train Loss: 1.0343, Test Loss: 0.8653\n",
      "Epoch 20/75, Train Loss: 1.0442, Test Loss: 0.8662\n",
      "Epoch 21/75, Train Loss: 1.0468, Test Loss: 0.8620\n",
      "Epoch 22/75, Train Loss: 1.0397, Test Loss: 0.8609\n",
      "Epoch 23/75, Train Loss: 1.0724, Test Loss: 0.8564\n",
      "Epoch 24/75, Train Loss: 1.0451, Test Loss: 0.8536\n",
      "Epoch 25/75, Train Loss: 1.0282, Test Loss: 0.8473\n",
      "Epoch 26/75, Train Loss: 1.0261, Test Loss: 0.8520\n",
      "Epoch 27/75, Train Loss: 1.0372, Test Loss: 0.8547\n",
      "Epoch 28/75, Train Loss: 1.0156, Test Loss: 0.8518\n",
      "Epoch 29/75, Train Loss: 1.0089, Test Loss: 0.8484\n",
      "Epoch 30/75, Train Loss: 1.0663, Test Loss: 0.8516\n",
      "Epoch 31/75, Train Loss: 1.0166, Test Loss: 0.8516\n",
      "Epoch 32/75, Train Loss: 1.0320, Test Loss: 0.8462\n",
      "Epoch 33/75, Train Loss: 1.0249, Test Loss: 0.8499\n",
      "Epoch 34/75, Train Loss: 1.0284, Test Loss: 0.8447\n",
      "Epoch 35/75, Train Loss: 1.0176, Test Loss: 0.8446\n",
      "Epoch 36/75, Train Loss: 1.0386, Test Loss: 0.8429\n",
      "Epoch 37/75, Train Loss: 1.0316, Test Loss: 0.8475\n",
      "Epoch 38/75, Train Loss: 1.0075, Test Loss: 0.8558\n",
      "Epoch 39/75, Train Loss: 1.0143, Test Loss: 0.8587\n",
      "Epoch 40/75, Train Loss: 1.0002, Test Loss: 0.8622\n",
      "Epoch 41/75, Train Loss: 1.0203, Test Loss: 0.8631\n",
      "Epoch 42/75, Train Loss: 1.0188, Test Loss: 0.8613\n",
      "Epoch 43/75, Train Loss: 1.0135, Test Loss: 0.8574\n",
      "Epoch 44/75, Train Loss: 1.0048, Test Loss: 0.8604\n",
      "Epoch 45/75, Train Loss: 1.0223, Test Loss: 0.8610\n",
      "Epoch 46/75, Train Loss: 1.0101, Test Loss: 0.8609\n",
      "Epoch 47/75, Train Loss: 1.0318, Test Loss: 0.8616\n",
      "Epoch 48/75, Train Loss: 1.0031, Test Loss: 0.8641\n",
      "Epoch 49/75, Train Loss: 1.0115, Test Loss: 0.8606\n",
      "Epoch 50/75, Train Loss: 1.0099, Test Loss: 0.8680\n",
      "Epoch 51/75, Train Loss: 1.0304, Test Loss: 0.8680\n",
      "Epoch 52/75, Train Loss: 0.9992, Test Loss: 0.8685\n",
      "Epoch 53/75, Train Loss: 1.0141, Test Loss: 0.8622\n",
      "Epoch 54/75, Train Loss: 1.0075, Test Loss: 0.8635\n",
      "Epoch 55/75, Train Loss: 1.0234, Test Loss: 0.8630\n",
      "Epoch 56/75, Train Loss: 1.0066, Test Loss: 0.8602\n",
      "Epoch 57/75, Train Loss: 0.9980, Test Loss: 0.8615\n",
      "Epoch 58/75, Train Loss: 0.9990, Test Loss: 0.8634\n",
      "Epoch 59/75, Train Loss: 1.0201, Test Loss: 0.8590\n",
      "Epoch 60/75, Train Loss: 1.0136, Test Loss: 0.8598\n",
      "Epoch 61/75, Train Loss: 1.0176, Test Loss: 0.8591\n",
      "Epoch 62/75, Train Loss: 0.9863, Test Loss: 0.8614\n",
      "Epoch 63/75, Train Loss: 1.0113, Test Loss: 0.8636\n",
      "Epoch 64/75, Train Loss: 1.0052, Test Loss: 0.8606\n",
      "Epoch 65/75, Train Loss: 1.0195, Test Loss: 0.8546\n",
      "Epoch 66/75, Train Loss: 1.0095, Test Loss: 0.8557\n",
      "Epoch 67/75, Train Loss: 1.0097, Test Loss: 0.8634\n",
      "Epoch 68/75, Train Loss: 1.0073, Test Loss: 0.8639\n",
      "Epoch 69/75, Train Loss: 1.0075, Test Loss: 0.8598\n",
      "Epoch 70/75, Train Loss: 1.0001, Test Loss: 0.8658\n",
      "Epoch 71/75, Train Loss: 1.0007, Test Loss: 0.8684\n",
      "Epoch 72/75, Train Loss: 1.0122, Test Loss: 0.8656\n",
      "Epoch 73/75, Train Loss: 1.0076, Test Loss: 0.8629\n",
      "Epoch 74/75, Train Loss: 1.0003, Test Loss: 0.8656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:08:59,472] Trial 182 finished with value: 0.87272147834301 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 202, 'layer_1_size': 47, 'layer_2_size': 138, 'layer_3_size': 139, 'layer_4_size': 87, 'layer_5_size': 236, 'layer_6_size': 219, 'layer_7_size': 147, 'layer_8_size': 91, 'layer_9_size': 90, 'layer_10_size': 250, 'layer_11_size': 134, 'layer_12_size': 87, 'layer_13_size': 183, 'dropout_rate': 0.24260455922523189, 'learning_rate': 0.00042137844070138953, 'batch_size': 64, 'epochs': 75}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/75, Train Loss: 0.9963, Test Loss: 0.8727\n",
      "Epoch 1/54, Train Loss: 1.1468, Test Loss: 0.9920\n",
      "Epoch 2/54, Train Loss: 1.1227, Test Loss: 0.9610\n",
      "Epoch 3/54, Train Loss: 1.0847, Test Loss: 0.9525\n",
      "Epoch 4/54, Train Loss: 1.0881, Test Loss: 0.9783\n",
      "Epoch 5/54, Train Loss: 1.0615, Test Loss: 0.9755\n",
      "Epoch 6/54, Train Loss: 1.0568, Test Loss: 0.9699\n",
      "Epoch 7/54, Train Loss: 1.0435, Test Loss: 0.9570\n",
      "Epoch 8/54, Train Loss: 1.0528, Test Loss: 0.9611\n",
      "Epoch 9/54, Train Loss: 1.0551, Test Loss: 0.9505\n",
      "Epoch 10/54, Train Loss: 1.0171, Test Loss: 0.9428\n",
      "Epoch 11/54, Train Loss: 1.0496, Test Loss: 0.9231\n",
      "Epoch 12/54, Train Loss: 1.0337, Test Loss: 0.9218\n",
      "Epoch 13/54, Train Loss: 1.0241, Test Loss: 0.9375\n",
      "Epoch 14/54, Train Loss: 1.0152, Test Loss: 0.9491\n",
      "Epoch 15/54, Train Loss: 1.0008, Test Loss: 0.9353\n",
      "Epoch 16/54, Train Loss: 1.0079, Test Loss: 0.9197\n",
      "Epoch 17/54, Train Loss: 1.0220, Test Loss: 0.9498\n",
      "Epoch 18/54, Train Loss: 1.0129, Test Loss: 0.9420\n",
      "Epoch 19/54, Train Loss: 1.0174, Test Loss: 0.9356\n",
      "Epoch 20/54, Train Loss: 1.0027, Test Loss: 0.9239\n",
      "Epoch 21/54, Train Loss: 1.0136, Test Loss: 0.9302\n",
      "Epoch 22/54, Train Loss: 0.9927, Test Loss: 0.9449\n",
      "Epoch 23/54, Train Loss: 1.0029, Test Loss: 0.9492\n",
      "Epoch 24/54, Train Loss: 1.0188, Test Loss: 0.9543\n",
      "Epoch 25/54, Train Loss: 1.0026, Test Loss: 0.9515\n",
      "Epoch 26/54, Train Loss: 0.9936, Test Loss: 0.9347\n",
      "Epoch 27/54, Train Loss: 0.9743, Test Loss: 0.9287\n",
      "Epoch 28/54, Train Loss: 0.9882, Test Loss: 0.9453\n",
      "Epoch 29/54, Train Loss: 0.9996, Test Loss: 0.9509\n",
      "Epoch 30/54, Train Loss: 0.9934, Test Loss: 0.9451\n",
      "Epoch 31/54, Train Loss: 0.9889, Test Loss: 0.9462\n",
      "Epoch 32/54, Train Loss: 1.0044, Test Loss: 0.9401\n",
      "Epoch 33/54, Train Loss: 0.9848, Test Loss: 0.9368\n",
      "Epoch 34/54, Train Loss: 0.9939, Test Loss: 0.9342\n",
      "Epoch 35/54, Train Loss: 0.9813, Test Loss: 0.9481\n",
      "Epoch 36/54, Train Loss: 0.9852, Test Loss: 0.9458\n",
      "Epoch 37/54, Train Loss: 0.9947, Test Loss: 0.9521\n",
      "Epoch 38/54, Train Loss: 0.9829, Test Loss: 0.9509\n",
      "Epoch 39/54, Train Loss: 0.9779, Test Loss: 0.9486\n",
      "Epoch 40/54, Train Loss: 0.9829, Test Loss: 0.9477\n",
      "Epoch 41/54, Train Loss: 0.9839, Test Loss: 0.9482\n",
      "Epoch 42/54, Train Loss: 0.9796, Test Loss: 0.9455\n",
      "Epoch 43/54, Train Loss: 0.9946, Test Loss: 0.9481\n",
      "Epoch 44/54, Train Loss: 0.9827, Test Loss: 0.9585\n",
      "Epoch 45/54, Train Loss: 0.9806, Test Loss: 0.9453\n",
      "Epoch 46/54, Train Loss: 0.9847, Test Loss: 0.9451\n",
      "Epoch 47/54, Train Loss: 0.9752, Test Loss: 0.9417\n",
      "Epoch 48/54, Train Loss: 0.9865, Test Loss: 0.9408\n",
      "Epoch 49/54, Train Loss: 0.9675, Test Loss: 0.9463\n",
      "Epoch 50/54, Train Loss: 0.9979, Test Loss: 0.9407\n",
      "Epoch 51/54, Train Loss: 0.9853, Test Loss: 0.9481\n",
      "Epoch 52/54, Train Loss: 0.9655, Test Loss: 0.9367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:09:07,829] Trial 183 finished with value: 0.9396344721317291 and parameters: {'num_hidden_layers': 17, 'layer_0_size': 215, 'layer_1_size': 34, 'layer_2_size': 146, 'layer_3_size': 153, 'layer_4_size': 102, 'layer_5_size': 95, 'layer_6_size': 250, 'layer_7_size': 133, 'layer_8_size': 69, 'layer_9_size': 247, 'layer_10_size': 167, 'layer_11_size': 127, 'layer_12_size': 72, 'layer_13_size': 170, 'layer_14_size': 82, 'layer_15_size': 209, 'layer_16_size': 80, 'dropout_rate': 0.31095599293224546, 'learning_rate': 0.0006767209795067693, 'batch_size': 64, 'epochs': 54}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/54, Train Loss: 0.9796, Test Loss: 0.9309\n",
      "Epoch 54/54, Train Loss: 0.9841, Test Loss: 0.9396\n",
      "Epoch 1/66, Train Loss: 1.1762, Test Loss: 1.0245\n",
      "Epoch 2/66, Train Loss: 1.1641, Test Loss: 1.0184\n",
      "Epoch 3/66, Train Loss: 1.1124, Test Loss: 0.9963\n",
      "Epoch 4/66, Train Loss: 1.1561, Test Loss: 0.9985\n",
      "Epoch 5/66, Train Loss: 1.0889, Test Loss: 1.0078\n",
      "Epoch 6/66, Train Loss: 1.1422, Test Loss: 1.0035\n",
      "Epoch 7/66, Train Loss: 1.1236, Test Loss: 1.0015\n",
      "Epoch 8/66, Train Loss: 1.0956, Test Loss: 1.0104\n",
      "Epoch 9/66, Train Loss: 1.1207, Test Loss: 1.0092\n",
      "Epoch 10/66, Train Loss: 1.1297, Test Loss: 1.0024\n",
      "Epoch 11/66, Train Loss: 1.1052, Test Loss: 1.0105\n",
      "Epoch 12/66, Train Loss: 1.1087, Test Loss: 1.0245\n",
      "Epoch 13/66, Train Loss: 1.0792, Test Loss: 1.0205\n",
      "Epoch 14/66, Train Loss: 1.1105, Test Loss: 1.0170\n",
      "Epoch 15/66, Train Loss: 1.0863, Test Loss: 1.0137\n",
      "Epoch 16/66, Train Loss: 1.0918, Test Loss: 1.0093\n",
      "Epoch 17/66, Train Loss: 1.1343, Test Loss: 1.0133\n",
      "Epoch 18/66, Train Loss: 1.0842, Test Loss: 1.0102\n",
      "Epoch 19/66, Train Loss: 1.0709, Test Loss: 1.0049\n",
      "Epoch 20/66, Train Loss: 1.0755, Test Loss: 1.0105\n",
      "Epoch 21/66, Train Loss: 1.0810, Test Loss: 1.0060\n",
      "Epoch 22/66, Train Loss: 1.0771, Test Loss: 1.0105\n",
      "Epoch 23/66, Train Loss: 1.0601, Test Loss: 1.0125\n",
      "Epoch 24/66, Train Loss: 1.0489, Test Loss: 1.0067\n",
      "Epoch 25/66, Train Loss: 1.0788, Test Loss: 1.0009\n",
      "Epoch 26/66, Train Loss: 1.0579, Test Loss: 1.0034\n",
      "Epoch 27/66, Train Loss: 1.0862, Test Loss: 1.0031\n",
      "Epoch 28/66, Train Loss: 1.0469, Test Loss: 1.0058\n",
      "Epoch 29/66, Train Loss: 1.0495, Test Loss: 1.0076\n",
      "Epoch 30/66, Train Loss: 1.0534, Test Loss: 1.0096\n",
      "Epoch 31/66, Train Loss: 1.0608, Test Loss: 1.0083\n",
      "Epoch 32/66, Train Loss: 1.0725, Test Loss: 1.0092\n",
      "Epoch 33/66, Train Loss: 1.0828, Test Loss: 1.0101\n",
      "Epoch 34/66, Train Loss: 1.0649, Test Loss: 1.0038\n",
      "Epoch 35/66, Train Loss: 1.0666, Test Loss: 1.0064\n",
      "Epoch 36/66, Train Loss: 1.0609, Test Loss: 1.0073\n",
      "Epoch 37/66, Train Loss: 1.0566, Test Loss: 1.0026\n",
      "Epoch 38/66, Train Loss: 1.0636, Test Loss: 1.0049\n",
      "Epoch 39/66, Train Loss: 1.0473, Test Loss: 1.0084\n",
      "Epoch 40/66, Train Loss: 1.0502, Test Loss: 1.0102\n",
      "Epoch 41/66, Train Loss: 1.0649, Test Loss: 1.0052\n",
      "Epoch 42/66, Train Loss: 1.0573, Test Loss: 1.0009\n",
      "Epoch 43/66, Train Loss: 1.0419, Test Loss: 1.0022\n",
      "Epoch 44/66, Train Loss: 1.0451, Test Loss: 1.0006\n",
      "Epoch 45/66, Train Loss: 1.0460, Test Loss: 1.0056\n",
      "Epoch 46/66, Train Loss: 1.0482, Test Loss: 1.0107\n",
      "Epoch 47/66, Train Loss: 1.0242, Test Loss: 1.0045\n",
      "Epoch 48/66, Train Loss: 1.0496, Test Loss: 1.0042\n",
      "Epoch 49/66, Train Loss: 1.0346, Test Loss: 1.0073\n",
      "Epoch 50/66, Train Loss: 1.0321, Test Loss: 1.0020\n",
      "Epoch 51/66, Train Loss: 1.0317, Test Loss: 1.0026\n",
      "Epoch 52/66, Train Loss: 1.0244, Test Loss: 1.0028\n",
      "Epoch 53/66, Train Loss: 1.0378, Test Loss: 1.0033\n",
      "Epoch 54/66, Train Loss: 1.0518, Test Loss: 1.0103\n",
      "Epoch 55/66, Train Loss: 1.0572, Test Loss: 1.0139\n",
      "Epoch 56/66, Train Loss: 1.0321, Test Loss: 1.0104\n",
      "Epoch 57/66, Train Loss: 1.0187, Test Loss: 1.0167\n",
      "Epoch 58/66, Train Loss: 1.0402, Test Loss: 1.0141\n",
      "Epoch 59/66, Train Loss: 1.0202, Test Loss: 1.0069\n",
      "Epoch 60/66, Train Loss: 1.0154, Test Loss: 1.0079\n",
      "Epoch 61/66, Train Loss: 1.0438, Test Loss: 1.0038\n",
      "Epoch 62/66, Train Loss: 1.0675, Test Loss: 1.0061\n",
      "Epoch 63/66, Train Loss: 1.0413, Test Loss: 1.0059\n",
      "Epoch 64/66, Train Loss: 1.0493, Test Loss: 1.0105\n",
      "Epoch 65/66, Train Loss: 1.0350, Test Loss: 1.0072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:09:28,189] Trial 184 finished with value: 1.0087768180029733 and parameters: {'num_hidden_layers': 15, 'layer_0_size': 207, 'layer_1_size': 137, 'layer_2_size': 153, 'layer_3_size': 155, 'layer_4_size': 99, 'layer_5_size': 193, 'layer_6_size': 232, 'layer_7_size': 212, 'layer_8_size': 204, 'layer_9_size': 239, 'layer_10_size': 256, 'layer_11_size': 143, 'layer_12_size': 65, 'layer_13_size': 192, 'layer_14_size': 110, 'dropout_rate': 0.26666629460669194, 'learning_rate': 0.000372932816956389, 'batch_size': 32, 'epochs': 66}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/66, Train Loss: 1.0483, Test Loss: 1.0088\n",
      "Epoch 1/58, Train Loss: 1.2107, Test Loss: 0.8705\n",
      "Epoch 2/58, Train Loss: 1.1621, Test Loss: 0.8705\n",
      "Epoch 3/58, Train Loss: 1.1872, Test Loss: 0.8703\n",
      "Epoch 4/58, Train Loss: 1.1721, Test Loss: 0.8704\n",
      "Epoch 5/58, Train Loss: 1.1608, Test Loss: 0.8696\n",
      "Epoch 6/58, Train Loss: 1.1220, Test Loss: 0.8692\n",
      "Epoch 7/58, Train Loss: 1.1754, Test Loss: 0.8689\n",
      "Epoch 8/58, Train Loss: 1.2040, Test Loss: 0.8695\n",
      "Epoch 9/58, Train Loss: 1.1789, Test Loss: 0.8703\n",
      "Epoch 10/58, Train Loss: 1.1399, Test Loss: 0.8704\n",
      "Epoch 11/58, Train Loss: 1.1533, Test Loss: 0.8718\n",
      "Epoch 12/58, Train Loss: 1.1369, Test Loss: 0.8714\n",
      "Epoch 13/58, Train Loss: 1.1637, Test Loss: 0.8718\n",
      "Epoch 14/58, Train Loss: 1.0876, Test Loss: 0.8715\n",
      "Epoch 15/58, Train Loss: 1.1403, Test Loss: 0.8712\n",
      "Epoch 16/58, Train Loss: 1.1628, Test Loss: 0.8717\n",
      "Epoch 17/58, Train Loss: 1.1531, Test Loss: 0.8721\n",
      "Epoch 18/58, Train Loss: 1.1240, Test Loss: 0.8713\n",
      "Epoch 19/58, Train Loss: 1.1205, Test Loss: 0.8712\n",
      "Epoch 20/58, Train Loss: 1.1223, Test Loss: 0.8726\n",
      "Epoch 21/58, Train Loss: 1.1533, Test Loss: 0.8739\n",
      "Epoch 22/58, Train Loss: 1.0932, Test Loss: 0.8743\n",
      "Epoch 23/58, Train Loss: 1.1547, Test Loss: 0.8738\n",
      "Epoch 24/58, Train Loss: 1.1031, Test Loss: 0.8739\n",
      "Epoch 25/58, Train Loss: 1.0864, Test Loss: 0.8738\n",
      "Epoch 26/58, Train Loss: 1.1465, Test Loss: 0.8745\n",
      "Epoch 27/58, Train Loss: 1.0773, Test Loss: 0.8743\n",
      "Epoch 28/58, Train Loss: 1.0890, Test Loss: 0.8731\n",
      "Epoch 29/58, Train Loss: 1.1302, Test Loss: 0.8740\n",
      "Epoch 30/58, Train Loss: 1.1053, Test Loss: 0.8738\n",
      "Epoch 31/58, Train Loss: 1.1344, Test Loss: 0.8733\n",
      "Epoch 32/58, Train Loss: 1.0671, Test Loss: 0.8725\n",
      "Epoch 33/58, Train Loss: 1.1099, Test Loss: 0.8732\n",
      "Epoch 34/58, Train Loss: 1.1421, Test Loss: 0.8735\n",
      "Epoch 35/58, Train Loss: 1.1387, Test Loss: 0.8739\n",
      "Epoch 36/58, Train Loss: 1.0816, Test Loss: 0.8742\n",
      "Epoch 37/58, Train Loss: 1.0735, Test Loss: 0.8751\n",
      "Epoch 38/58, Train Loss: 1.0963, Test Loss: 0.8745\n",
      "Epoch 39/58, Train Loss: 1.1251, Test Loss: 0.8749\n",
      "Epoch 40/58, Train Loss: 1.1170, Test Loss: 0.8736\n",
      "Epoch 41/58, Train Loss: 1.1494, Test Loss: 0.8756\n",
      "Epoch 42/58, Train Loss: 1.0810, Test Loss: 0.8731\n",
      "Epoch 43/58, Train Loss: 1.0849, Test Loss: 0.8744\n",
      "Epoch 44/58, Train Loss: 1.1191, Test Loss: 0.8745\n",
      "Epoch 45/58, Train Loss: 1.1006, Test Loss: 0.8732\n",
      "Epoch 46/58, Train Loss: 1.0901, Test Loss: 0.8720\n",
      "Epoch 47/58, Train Loss: 1.1465, Test Loss: 0.8717\n",
      "Epoch 48/58, Train Loss: 1.1007, Test Loss: 0.8729\n",
      "Epoch 49/58, Train Loss: 1.1099, Test Loss: 0.8738\n",
      "Epoch 50/58, Train Loss: 1.0517, Test Loss: 0.8736\n",
      "Epoch 51/58, Train Loss: 1.0792, Test Loss: 0.8724\n",
      "Epoch 52/58, Train Loss: 1.0442, Test Loss: 0.8741\n",
      "Epoch 53/58, Train Loss: 1.1327, Test Loss: 0.8741\n",
      "Epoch 54/58, Train Loss: 1.1063, Test Loss: 0.8747\n",
      "Epoch 55/58, Train Loss: 1.0770, Test Loss: 0.8743\n",
      "Epoch 56/58, Train Loss: 1.1057, Test Loss: 0.8740\n",
      "Epoch 57/58, Train Loss: 1.0818, Test Loss: 0.8751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:09:32,530] Trial 185 finished with value: 0.8748629093170166 and parameters: {'num_hidden_layers': 16, 'layer_0_size': 234, 'layer_1_size': 46, 'layer_2_size': 49, 'layer_3_size': 115, 'layer_4_size': 60, 'layer_5_size': 138, 'layer_6_size': 32, 'layer_7_size': 160, 'layer_8_size': 118, 'layer_9_size': 215, 'layer_10_size': 233, 'layer_11_size': 187, 'layer_12_size': 44, 'layer_13_size': 147, 'layer_14_size': 43, 'layer_15_size': 247, 'dropout_rate': 0.2770321720804237, 'learning_rate': 1.5362482704904283e-05, 'batch_size': 256, 'epochs': 58}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/58, Train Loss: 1.0790, Test Loss: 0.8749\n",
      "Epoch 1/77, Train Loss: 1.1008, Test Loss: 1.0998\n",
      "Epoch 2/77, Train Loss: 1.0391, Test Loss: 1.1394\n",
      "Epoch 3/77, Train Loss: 1.0195, Test Loss: 1.1145\n",
      "Epoch 4/77, Train Loss: 1.0908, Test Loss: 1.1080\n",
      "Epoch 5/77, Train Loss: 1.0217, Test Loss: 1.1070\n",
      "Epoch 6/77, Train Loss: 1.0065, Test Loss: 1.1169\n",
      "Epoch 7/77, Train Loss: 1.0342, Test Loss: 1.1270\n",
      "Epoch 8/77, Train Loss: 1.0329, Test Loss: 1.1000\n",
      "Epoch 9/77, Train Loss: 0.9960, Test Loss: 1.1069\n",
      "Epoch 10/77, Train Loss: 1.0216, Test Loss: 1.1172\n",
      "Epoch 11/77, Train Loss: 1.0198, Test Loss: 1.0973\n",
      "Epoch 12/77, Train Loss: 0.9988, Test Loss: 1.1002\n",
      "Epoch 13/77, Train Loss: 0.9793, Test Loss: 1.0857\n",
      "Epoch 14/77, Train Loss: 0.9939, Test Loss: 1.1139\n",
      "Epoch 15/77, Train Loss: 0.9952, Test Loss: 1.1116\n",
      "Epoch 16/77, Train Loss: 0.9634, Test Loss: 1.1224\n",
      "Epoch 17/77, Train Loss: 0.9706, Test Loss: 1.0922\n",
      "Epoch 18/77, Train Loss: 0.9765, Test Loss: 1.0978\n",
      "Epoch 19/77, Train Loss: 0.9798, Test Loss: 1.1388\n",
      "Epoch 20/77, Train Loss: 0.9547, Test Loss: 1.1014\n",
      "Epoch 21/77, Train Loss: 0.9670, Test Loss: 1.1443\n",
      "Epoch 22/77, Train Loss: 0.9425, Test Loss: 1.1300\n",
      "Epoch 23/77, Train Loss: 0.9502, Test Loss: 1.1677\n",
      "Epoch 24/77, Train Loss: 0.9338, Test Loss: 1.1392\n",
      "Epoch 25/77, Train Loss: 0.9230, Test Loss: 1.1267\n",
      "Epoch 26/77, Train Loss: 0.9108, Test Loss: 1.1304\n",
      "Epoch 27/77, Train Loss: 0.8941, Test Loss: 1.1115\n",
      "Epoch 28/77, Train Loss: 0.8863, Test Loss: 1.1384\n",
      "Epoch 29/77, Train Loss: 0.8793, Test Loss: 1.1770\n",
      "Epoch 30/77, Train Loss: 0.8741, Test Loss: 1.1778\n",
      "Epoch 31/77, Train Loss: 0.8753, Test Loss: 1.1627\n",
      "Epoch 32/77, Train Loss: 0.9001, Test Loss: 1.1370\n",
      "Epoch 33/77, Train Loss: 0.8406, Test Loss: 1.1712\n",
      "Epoch 34/77, Train Loss: 0.8196, Test Loss: 1.1894\n",
      "Epoch 35/77, Train Loss: 0.8504, Test Loss: 1.1369\n",
      "Epoch 36/77, Train Loss: 0.8167, Test Loss: 1.1511\n",
      "Epoch 37/77, Train Loss: 0.8399, Test Loss: 1.1759\n",
      "Epoch 38/77, Train Loss: 0.7416, Test Loss: 1.1974\n",
      "Epoch 39/77, Train Loss: 0.7405, Test Loss: 1.1850\n",
      "Epoch 40/77, Train Loss: 0.7490, Test Loss: 1.2177\n",
      "Epoch 41/77, Train Loss: 0.7520, Test Loss: 1.2188\n",
      "Epoch 42/77, Train Loss: 0.7002, Test Loss: 1.2762\n",
      "Epoch 43/77, Train Loss: 0.7392, Test Loss: 1.3034\n",
      "Epoch 44/77, Train Loss: 0.6625, Test Loss: 1.2992\n",
      "Epoch 45/77, Train Loss: 0.6475, Test Loss: 1.3433\n",
      "Epoch 46/77, Train Loss: 0.6667, Test Loss: 1.3052\n",
      "Epoch 47/77, Train Loss: 0.6515, Test Loss: 1.3677\n",
      "Epoch 48/77, Train Loss: 0.6492, Test Loss: 1.3038\n",
      "Epoch 49/77, Train Loss: 0.5846, Test Loss: 1.3095\n",
      "Epoch 50/77, Train Loss: 0.6036, Test Loss: 1.2532\n",
      "Epoch 51/77, Train Loss: 0.5661, Test Loss: 1.3485\n",
      "Epoch 52/77, Train Loss: 0.5553, Test Loss: 1.3777\n",
      "Epoch 53/77, Train Loss: 0.5176, Test Loss: 1.3489\n",
      "Epoch 54/77, Train Loss: 0.5858, Test Loss: 1.2734\n",
      "Epoch 55/77, Train Loss: 0.5172, Test Loss: 1.3076\n",
      "Epoch 56/77, Train Loss: 0.5216, Test Loss: 1.3317\n",
      "Epoch 57/77, Train Loss: 0.5178, Test Loss: 1.3528\n",
      "Epoch 58/77, Train Loss: 0.5127, Test Loss: 1.3268\n",
      "Epoch 59/77, Train Loss: 0.5103, Test Loss: 1.3930\n",
      "Epoch 60/77, Train Loss: 0.4622, Test Loss: 1.3492\n",
      "Epoch 61/77, Train Loss: 0.4883, Test Loss: 1.3210\n",
      "Epoch 62/77, Train Loss: 0.4617, Test Loss: 1.3387\n",
      "Epoch 63/77, Train Loss: 0.4614, Test Loss: 1.3962\n",
      "Epoch 64/77, Train Loss: 0.4584, Test Loss: 1.4148\n",
      "Epoch 65/77, Train Loss: 0.4634, Test Loss: 1.3393\n",
      "Epoch 66/77, Train Loss: 0.4595, Test Loss: 1.4339\n",
      "Epoch 67/77, Train Loss: 0.4194, Test Loss: 1.4168\n",
      "Epoch 68/77, Train Loss: 0.4560, Test Loss: 1.4119\n",
      "Epoch 69/77, Train Loss: 0.3899, Test Loss: 1.3920\n",
      "Epoch 70/77, Train Loss: 0.4342, Test Loss: 1.4302\n",
      "Epoch 71/77, Train Loss: 0.4161, Test Loss: 1.3808\n",
      "Epoch 72/77, Train Loss: 0.4119, Test Loss: 1.3709\n",
      "Epoch 73/77, Train Loss: 0.3879, Test Loss: 1.3540\n",
      "Epoch 74/77, Train Loss: 0.3777, Test Loss: 1.3209\n",
      "Epoch 75/77, Train Loss: 0.3898, Test Loss: 1.3630\n",
      "Epoch 76/77, Train Loss: 0.4061, Test Loss: 1.3304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:09:50,072] Trial 186 finished with value: 1.5066705771854945 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 149, 'layer_1_size': 250, 'layer_2_size': 237, 'layer_3_size': 69, 'layer_4_size': 110, 'layer_5_size': 187, 'layer_6_size': 41, 'layer_7_size': 97, 'layer_8_size': 158, 'layer_9_size': 227, 'layer_10_size': 216, 'layer_11_size': 172, 'layer_12_size': 183, 'dropout_rate': 0.1413550633350974, 'learning_rate': 0.0005674602833021862, 'batch_size': 32, 'epochs': 77}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/77, Train Loss: 0.3930, Test Loss: 1.5067\n",
      "Epoch 1/5, Train Loss: 1.2101, Test Loss: 0.9899\n",
      "Epoch 2/5, Train Loss: 1.1772, Test Loss: 0.9929\n",
      "Epoch 3/5, Train Loss: 1.1500, Test Loss: 0.9940\n",
      "Epoch 4/5, Train Loss: 1.1405, Test Loss: 0.9899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:09:50,520] Trial 187 finished with value: 0.9860519170761108 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 130, 'layer_1_size': 195, 'layer_2_size': 123, 'layer_3_size': 142, 'layer_4_size': 146, 'layer_5_size': 125, 'layer_6_size': 36, 'layer_7_size': 221, 'layer_8_size': 139, 'layer_9_size': 60, 'layer_10_size': 242, 'layer_11_size': 160, 'layer_12_size': 148, 'layer_13_size': 200, 'dropout_rate': 0.2536942830098838, 'learning_rate': 0.00031806588858075403, 'batch_size': 256, 'epochs': 5}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Train Loss: 1.1190, Test Loss: 0.9861\n",
      "Epoch 1/38, Train Loss: 1.1076, Test Loss: 1.2403\n",
      "Epoch 2/38, Train Loss: 1.1170, Test Loss: 1.2203\n",
      "Epoch 3/38, Train Loss: 1.1200, Test Loss: 1.2155\n",
      "Epoch 4/38, Train Loss: 1.0464, Test Loss: 1.2175\n",
      "Epoch 5/38, Train Loss: 1.1077, Test Loss: 1.2151\n",
      "Epoch 6/38, Train Loss: 1.0687, Test Loss: 1.2098\n",
      "Epoch 7/38, Train Loss: 1.0626, Test Loss: 1.2050\n",
      "Epoch 8/38, Train Loss: 1.0774, Test Loss: 1.2097\n",
      "Epoch 9/38, Train Loss: 1.1130, Test Loss: 1.2032\n",
      "Epoch 10/38, Train Loss: 1.0570, Test Loss: 1.2063\n",
      "Epoch 11/38, Train Loss: 1.0354, Test Loss: 1.2071\n",
      "Epoch 12/38, Train Loss: 1.0494, Test Loss: 1.2080\n",
      "Epoch 13/38, Train Loss: 1.0579, Test Loss: 1.2032\n",
      "Epoch 14/38, Train Loss: 1.0505, Test Loss: 1.2075\n",
      "Epoch 15/38, Train Loss: 1.0267, Test Loss: 1.2080\n",
      "Epoch 16/38, Train Loss: 1.0408, Test Loss: 1.2145\n",
      "Epoch 17/38, Train Loss: 1.0257, Test Loss: 1.2190\n",
      "Epoch 18/38, Train Loss: 1.0254, Test Loss: 1.2229\n",
      "Epoch 19/38, Train Loss: 1.0359, Test Loss: 1.2161\n",
      "Epoch 20/38, Train Loss: 1.0077, Test Loss: 1.2071\n",
      "Epoch 21/38, Train Loss: 1.0602, Test Loss: 1.2053\n",
      "Epoch 22/38, Train Loss: 1.0281, Test Loss: 1.2009\n",
      "Epoch 23/38, Train Loss: 1.0317, Test Loss: 1.2000\n",
      "Epoch 24/38, Train Loss: 1.0257, Test Loss: 1.1990\n",
      "Epoch 25/38, Train Loss: 1.0490, Test Loss: 1.2020\n",
      "Epoch 26/38, Train Loss: 1.0214, Test Loss: 1.1994\n",
      "Epoch 27/38, Train Loss: 1.0051, Test Loss: 1.1996\n",
      "Epoch 28/38, Train Loss: 0.9938, Test Loss: 1.2006\n",
      "Epoch 29/38, Train Loss: 0.9998, Test Loss: 1.1975\n",
      "Epoch 30/38, Train Loss: 1.0175, Test Loss: 1.1981\n",
      "Epoch 31/38, Train Loss: 0.9929, Test Loss: 1.2013\n",
      "Epoch 32/38, Train Loss: 1.0151, Test Loss: 1.2028\n",
      "Epoch 33/38, Train Loss: 1.0061, Test Loss: 1.2072\n",
      "Epoch 34/38, Train Loss: 1.0030, Test Loss: 1.2099\n",
      "Epoch 35/38, Train Loss: 1.0179, Test Loss: 1.2083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:09:54,899] Trial 188 finished with value: 1.2296251058578491 and parameters: {'num_hidden_layers': 12, 'layer_0_size': 70, 'layer_1_size': 231, 'layer_2_size': 116, 'layer_3_size': 133, 'layer_4_size': 80, 'layer_5_size': 171, 'layer_6_size': 205, 'layer_7_size': 140, 'layer_8_size': 172, 'layer_9_size': 79, 'layer_10_size': 223, 'layer_11_size': 123, 'dropout_rate': 0.2956402401308432, 'learning_rate': 0.0001680137774276107, 'batch_size': 64, 'epochs': 38}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/38, Train Loss: 1.0306, Test Loss: 1.2120\n",
      "Epoch 37/38, Train Loss: 1.0185, Test Loss: 1.2181\n",
      "Epoch 38/38, Train Loss: 1.0210, Test Loss: 1.2296\n",
      "Epoch 1/42, Train Loss: 1.0818, Test Loss: 0.8334\n",
      "Epoch 2/42, Train Loss: 1.1038, Test Loss: 0.8188\n",
      "Epoch 3/42, Train Loss: 1.0697, Test Loss: 0.8279\n",
      "Epoch 4/42, Train Loss: 1.0370, Test Loss: 0.8356\n",
      "Epoch 5/42, Train Loss: 1.0369, Test Loss: 0.8230\n",
      "Epoch 6/42, Train Loss: 1.0185, Test Loss: 0.8389\n",
      "Epoch 7/42, Train Loss: 1.0831, Test Loss: 0.8153\n",
      "Epoch 8/42, Train Loss: 1.0316, Test Loss: 0.8173\n",
      "Epoch 9/42, Train Loss: 1.0271, Test Loss: 0.8152\n",
      "Epoch 10/42, Train Loss: 1.0298, Test Loss: 0.8181\n",
      "Epoch 11/42, Train Loss: 1.0418, Test Loss: 0.8228\n",
      "Epoch 12/42, Train Loss: 0.9957, Test Loss: 0.8167\n",
      "Epoch 13/42, Train Loss: 1.0288, Test Loss: 0.8188\n",
      "Epoch 14/42, Train Loss: 1.0149, Test Loss: 0.8170\n",
      "Epoch 15/42, Train Loss: 1.0130, Test Loss: 0.8135\n",
      "Epoch 16/42, Train Loss: 1.0007, Test Loss: 0.8206\n",
      "Epoch 17/42, Train Loss: 1.0309, Test Loss: 0.8134\n",
      "Epoch 18/42, Train Loss: 1.0133, Test Loss: 0.8134\n",
      "Epoch 19/42, Train Loss: 1.0433, Test Loss: 0.8124\n",
      "Epoch 20/42, Train Loss: 1.0105, Test Loss: 0.8146\n",
      "Epoch 21/42, Train Loss: 1.0169, Test Loss: 0.8095\n",
      "Epoch 22/42, Train Loss: 1.0035, Test Loss: 0.8099\n",
      "Epoch 23/42, Train Loss: 0.9825, Test Loss: 0.8068\n",
      "Epoch 24/42, Train Loss: 1.0091, Test Loss: 0.8111\n",
      "Epoch 25/42, Train Loss: 0.9968, Test Loss: 0.8048\n",
      "Epoch 26/42, Train Loss: 1.0160, Test Loss: 0.8082\n",
      "Epoch 27/42, Train Loss: 1.0145, Test Loss: 0.8141\n",
      "Epoch 28/42, Train Loss: 1.0101, Test Loss: 0.8087\n",
      "Epoch 29/42, Train Loss: 0.9868, Test Loss: 0.8113\n",
      "Epoch 30/42, Train Loss: 1.0005, Test Loss: 0.8059\n",
      "Epoch 31/42, Train Loss: 0.9932, Test Loss: 0.8110\n",
      "Epoch 32/42, Train Loss: 0.9948, Test Loss: 0.8105\n",
      "Epoch 33/42, Train Loss: 0.9697, Test Loss: 0.8145\n",
      "Epoch 34/42, Train Loss: 0.9893, Test Loss: 0.8137\n",
      "Epoch 35/42, Train Loss: 0.9862, Test Loss: 0.8129\n",
      "Epoch 36/42, Train Loss: 0.9798, Test Loss: 0.8109\n",
      "Epoch 37/42, Train Loss: 0.9816, Test Loss: 0.8094\n",
      "Epoch 38/42, Train Loss: 0.9770, Test Loss: 0.8118\n",
      "Epoch 39/42, Train Loss: 0.9606, Test Loss: 0.8113\n",
      "Epoch 40/42, Train Loss: 0.9777, Test Loss: 0.8107\n",
      "Epoch 41/42, Train Loss: 0.9899, Test Loss: 0.8072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:10:04,788] Trial 189 finished with value: 0.8096154545034681 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 82, 'layer_1_size': 68, 'layer_2_size': 105, 'layer_3_size': 126, 'layer_4_size': 140, 'layer_5_size': 155, 'layer_6_size': 94, 'layer_7_size': 174, 'layer_8_size': 165, 'layer_9_size': 70, 'layer_10_size': 183, 'layer_11_size': 177, 'layer_12_size': 138, 'layer_13_size': 160, 'dropout_rate': 0.21569212487104042, 'learning_rate': 0.00025741334174347023, 'batch_size': 32, 'epochs': 42}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/42, Train Loss: 0.9939, Test Loss: 0.8096\n",
      "Epoch 1/68, Train Loss: 1.6905, Test Loss: 0.9830\n",
      "Epoch 2/68, Train Loss: 1.6023, Test Loss: 0.9940\n",
      "Epoch 3/68, Train Loss: 1.5116, Test Loss: 0.9905\n",
      "Epoch 4/68, Train Loss: 1.5272, Test Loss: 0.9843\n",
      "Epoch 5/68, Train Loss: 1.5170, Test Loss: 0.9924\n",
      "Epoch 6/68, Train Loss: 1.4931, Test Loss: 0.9988\n",
      "Epoch 7/68, Train Loss: 1.4548, Test Loss: 0.9989\n",
      "Epoch 8/68, Train Loss: 1.4412, Test Loss: 0.9856\n",
      "Epoch 9/68, Train Loss: 1.4274, Test Loss: 0.9982\n",
      "Epoch 10/68, Train Loss: 1.4167, Test Loss: 0.9833\n",
      "Epoch 11/68, Train Loss: 1.4004, Test Loss: 0.9881\n",
      "Epoch 12/68, Train Loss: 1.3940, Test Loss: 0.9825\n",
      "Epoch 13/68, Train Loss: 1.3649, Test Loss: 0.9841\n",
      "Epoch 14/68, Train Loss: 1.3465, Test Loss: 0.9940\n",
      "Epoch 15/68, Train Loss: 1.3611, Test Loss: 0.9828\n",
      "Epoch 16/68, Train Loss: 1.3268, Test Loss: 0.9905\n",
      "Epoch 17/68, Train Loss: 1.3486, Test Loss: 0.9875\n",
      "Epoch 18/68, Train Loss: 1.3279, Test Loss: 0.9853\n",
      "Epoch 19/68, Train Loss: 1.2668, Test Loss: 0.9880\n",
      "Epoch 20/68, Train Loss: 1.2738, Test Loss: 0.9726\n",
      "Epoch 21/68, Train Loss: 1.3038, Test Loss: 0.9799\n",
      "Epoch 22/68, Train Loss: 1.2541, Test Loss: 0.9904\n",
      "Epoch 23/68, Train Loss: 1.2659, Test Loss: 0.9766\n",
      "Epoch 24/68, Train Loss: 1.2412, Test Loss: 0.9770\n",
      "Epoch 25/68, Train Loss: 1.2296, Test Loss: 0.9819\n",
      "Epoch 26/68, Train Loss: 1.2583, Test Loss: 0.9807\n",
      "Epoch 27/68, Train Loss: 1.2680, Test Loss: 0.9829\n",
      "Epoch 28/68, Train Loss: 1.2585, Test Loss: 0.9798\n",
      "Epoch 29/68, Train Loss: 1.2754, Test Loss: 0.9880\n",
      "Epoch 30/68, Train Loss: 1.2227, Test Loss: 0.9876\n",
      "Epoch 31/68, Train Loss: 1.2104, Test Loss: 0.9773\n",
      "Epoch 32/68, Train Loss: 1.2119, Test Loss: 0.9758\n",
      "Epoch 33/68, Train Loss: 1.2418, Test Loss: 0.9814\n",
      "Epoch 34/68, Train Loss: 1.2523, Test Loss: 0.9780\n",
      "Epoch 35/68, Train Loss: 1.2092, Test Loss: 0.9782\n",
      "Epoch 36/68, Train Loss: 1.2297, Test Loss: 0.9761\n",
      "Epoch 37/68, Train Loss: 1.2358, Test Loss: 0.9806\n",
      "Epoch 38/68, Train Loss: 1.2153, Test Loss: 0.9749\n",
      "Epoch 39/68, Train Loss: 1.2267, Test Loss: 0.9885\n",
      "Epoch 40/68, Train Loss: 1.1736, Test Loss: 0.9779\n",
      "Epoch 41/68, Train Loss: 1.2019, Test Loss: 0.9755\n",
      "Epoch 42/68, Train Loss: 1.2198, Test Loss: 0.9817\n",
      "Epoch 43/68, Train Loss: 1.1564, Test Loss: 0.9956\n",
      "Epoch 44/68, Train Loss: 1.2123, Test Loss: 0.9859\n",
      "Epoch 45/68, Train Loss: 1.2121, Test Loss: 0.9762\n",
      "Epoch 46/68, Train Loss: 1.1857, Test Loss: 0.9876\n",
      "Epoch 47/68, Train Loss: 1.1849, Test Loss: 0.9774\n",
      "Epoch 48/68, Train Loss: 1.1894, Test Loss: 0.9870\n",
      "Epoch 49/68, Train Loss: 1.1366, Test Loss: 0.9872\n",
      "Epoch 50/68, Train Loss: 1.1795, Test Loss: 0.9870\n",
      "Epoch 51/68, Train Loss: 1.1588, Test Loss: 0.9964\n",
      "Epoch 52/68, Train Loss: 1.1846, Test Loss: 0.9838\n",
      "Epoch 53/68, Train Loss: 1.1504, Test Loss: 0.9775\n",
      "Epoch 54/68, Train Loss: 1.1522, Test Loss: 0.9807\n",
      "Epoch 55/68, Train Loss: 1.1515, Test Loss: 0.9828\n",
      "Epoch 56/68, Train Loss: 1.1764, Test Loss: 0.9774\n",
      "Epoch 57/68, Train Loss: 1.1788, Test Loss: 0.9945\n",
      "Epoch 58/68, Train Loss: 1.1558, Test Loss: 0.9869\n",
      "Epoch 59/68, Train Loss: 1.1567, Test Loss: 0.9821\n",
      "Epoch 60/68, Train Loss: 1.2041, Test Loss: 0.9867\n",
      "Epoch 61/68, Train Loss: 1.1472, Test Loss: 0.9799\n",
      "Epoch 62/68, Train Loss: 1.1606, Test Loss: 0.9821\n",
      "Epoch 63/68, Train Loss: 1.1591, Test Loss: 0.9963\n",
      "Epoch 64/68, Train Loss: 1.1631, Test Loss: 0.9957\n",
      "Epoch 65/68, Train Loss: 1.1461, Test Loss: 0.9829\n",
      "Epoch 66/68, Train Loss: 1.1309, Test Loss: 0.9815\n",
      "Epoch 67/68, Train Loss: 1.1784, Test Loss: 1.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:10:20,378] Trial 190 finished with value: 0.9962241053581238 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 80, 'layer_1_size': 57, 'layer_2_size': 107, 'layer_3_size': 118, 'layer_4_size': 138, 'layer_5_size': 162, 'layer_6_size': 239, 'layer_7_size': 190, 'layer_8_size': 167, 'layer_9_size': 65, 'layer_10_size': 182, 'layer_11_size': 177, 'layer_12_size': 156, 'dropout_rate': 0.20788168294636566, 'learning_rate': 1.0176009119321657e-05, 'batch_size': 32, 'epochs': 68}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/68, Train Loss: 1.1607, Test Loss: 0.9962\n",
      "Epoch 1/28, Train Loss: 1.1344, Test Loss: 1.1851\n",
      "Epoch 2/28, Train Loss: 1.1629, Test Loss: 1.1605\n",
      "Epoch 3/28, Train Loss: 1.1353, Test Loss: 1.1647\n",
      "Epoch 4/28, Train Loss: 1.0919, Test Loss: 1.1773\n",
      "Epoch 5/28, Train Loss: 1.1559, Test Loss: 1.1722\n",
      "Epoch 6/28, Train Loss: 1.1023, Test Loss: 1.1617\n",
      "Epoch 7/28, Train Loss: 1.1016, Test Loss: 1.1632\n",
      "Epoch 8/28, Train Loss: 1.1226, Test Loss: 1.1544\n",
      "Epoch 9/28, Train Loss: 1.1159, Test Loss: 1.1598\n",
      "Epoch 10/28, Train Loss: 1.1278, Test Loss: 1.1543\n",
      "Epoch 11/28, Train Loss: 1.1083, Test Loss: 1.1466\n",
      "Epoch 12/28, Train Loss: 1.1150, Test Loss: 1.1504\n",
      "Epoch 13/28, Train Loss: 1.1075, Test Loss: 1.1676\n",
      "Epoch 14/28, Train Loss: 1.1071, Test Loss: 1.1719\n",
      "Epoch 15/28, Train Loss: 1.1360, Test Loss: 1.1624\n",
      "Epoch 16/28, Train Loss: 1.0509, Test Loss: 1.1580\n",
      "Epoch 17/28, Train Loss: 1.0789, Test Loss: 1.1642\n",
      "Epoch 18/28, Train Loss: 1.0877, Test Loss: 1.1620\n",
      "Epoch 19/28, Train Loss: 1.1204, Test Loss: 1.1681\n",
      "Epoch 20/28, Train Loss: 1.1089, Test Loss: 1.1538\n",
      "Epoch 21/28, Train Loss: 1.0899, Test Loss: 1.1491\n",
      "Epoch 22/28, Train Loss: 1.0767, Test Loss: 1.1494\n",
      "Epoch 23/28, Train Loss: 1.0805, Test Loss: 1.1514\n",
      "Epoch 24/28, Train Loss: 1.0875, Test Loss: 1.1547\n",
      "Epoch 25/28, Train Loss: 1.0718, Test Loss: 1.1547\n",
      "Epoch 26/28, Train Loss: 1.1156, Test Loss: 1.1535\n",
      "Epoch 27/28, Train Loss: 1.0503, Test Loss: 1.1533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:10:27,377] Trial 191 finished with value: 1.15701505116054 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 82, 'layer_1_size': 67, 'layer_2_size': 135, 'layer_3_size': 123, 'layer_4_size': 132, 'layer_5_size': 156, 'layer_6_size': 101, 'layer_7_size': 183, 'layer_8_size': 182, 'layer_9_size': 68, 'layer_10_size': 190, 'layer_11_size': 57, 'layer_12_size': 131, 'layer_13_size': 159, 'dropout_rate': 0.2159105617402146, 'learning_rate': 0.0002451320264270674, 'batch_size': 32, 'epochs': 28}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/28, Train Loss: 1.0769, Test Loss: 1.1570\n",
      "Epoch 1/62, Train Loss: 1.1865, Test Loss: 1.2459\n",
      "Epoch 2/62, Train Loss: 1.1407, Test Loss: 1.2537\n",
      "Epoch 3/62, Train Loss: 1.1375, Test Loss: 1.2136\n",
      "Epoch 4/62, Train Loss: 1.1514, Test Loss: 1.2163\n",
      "Epoch 5/62, Train Loss: 1.1126, Test Loss: 1.1951\n",
      "Epoch 6/62, Train Loss: 1.0837, Test Loss: 1.2105\n",
      "Epoch 7/62, Train Loss: 1.0652, Test Loss: 1.2136\n",
      "Epoch 8/62, Train Loss: 1.0716, Test Loss: 1.2066\n",
      "Epoch 9/62, Train Loss: 1.0991, Test Loss: 1.2140\n",
      "Epoch 10/62, Train Loss: 1.0792, Test Loss: 1.2121\n",
      "Epoch 11/62, Train Loss: 1.0635, Test Loss: 1.2073\n",
      "Epoch 12/62, Train Loss: 1.0511, Test Loss: 1.2222\n",
      "Epoch 13/62, Train Loss: 1.0700, Test Loss: 1.2430\n",
      "Epoch 14/62, Train Loss: 1.0676, Test Loss: 1.2035\n",
      "Epoch 15/62, Train Loss: 1.0854, Test Loss: 1.2153\n",
      "Epoch 16/62, Train Loss: 1.0542, Test Loss: 1.2041\n",
      "Epoch 17/62, Train Loss: 1.0994, Test Loss: 1.2124\n",
      "Epoch 18/62, Train Loss: 1.0764, Test Loss: 1.2353\n",
      "Epoch 19/62, Train Loss: 1.0742, Test Loss: 1.2202\n",
      "Epoch 20/62, Train Loss: 1.0598, Test Loss: 1.2327\n",
      "Epoch 21/62, Train Loss: 1.0501, Test Loss: 1.2244\n",
      "Epoch 22/62, Train Loss: 1.0322, Test Loss: 1.2198\n",
      "Epoch 23/62, Train Loss: 1.0657, Test Loss: 1.2132\n",
      "Epoch 24/62, Train Loss: 1.0619, Test Loss: 1.2053\n",
      "Epoch 25/62, Train Loss: 1.0645, Test Loss: 1.1930\n",
      "Epoch 26/62, Train Loss: 1.0896, Test Loss: 1.2140\n",
      "Epoch 27/62, Train Loss: 1.0159, Test Loss: 1.2125\n",
      "Epoch 28/62, Train Loss: 1.0512, Test Loss: 1.1947\n",
      "Epoch 29/62, Train Loss: 1.0481, Test Loss: 1.2122\n",
      "Epoch 30/62, Train Loss: 1.0436, Test Loss: 1.2228\n",
      "Epoch 31/62, Train Loss: 1.0548, Test Loss: 1.2149\n",
      "Epoch 32/62, Train Loss: 1.0376, Test Loss: 1.2177\n",
      "Epoch 33/62, Train Loss: 1.0531, Test Loss: 1.2060\n",
      "Epoch 34/62, Train Loss: 1.0410, Test Loss: 1.2200\n",
      "Epoch 35/62, Train Loss: 1.0553, Test Loss: 1.2337\n",
      "Epoch 36/62, Train Loss: 1.0401, Test Loss: 1.2261\n",
      "Epoch 37/62, Train Loss: 1.0418, Test Loss: 1.2091\n",
      "Epoch 38/62, Train Loss: 1.0439, Test Loss: 1.2137\n",
      "Epoch 39/62, Train Loss: 1.0468, Test Loss: 1.2036\n",
      "Epoch 40/62, Train Loss: 1.0569, Test Loss: 1.2097\n",
      "Epoch 41/62, Train Loss: 1.0675, Test Loss: 1.2008\n",
      "Epoch 42/62, Train Loss: 1.0649, Test Loss: 1.2135\n",
      "Epoch 43/62, Train Loss: 1.0184, Test Loss: 1.1962\n",
      "Epoch 44/62, Train Loss: 1.0495, Test Loss: 1.1980\n",
      "Epoch 45/62, Train Loss: 1.0411, Test Loss: 1.2060\n",
      "Epoch 46/62, Train Loss: 1.0425, Test Loss: 1.2161\n",
      "Epoch 47/62, Train Loss: 1.0285, Test Loss: 1.2223\n",
      "Epoch 48/62, Train Loss: 1.0459, Test Loss: 1.2041\n",
      "Epoch 49/62, Train Loss: 1.0413, Test Loss: 1.1952\n",
      "Epoch 50/62, Train Loss: 1.0449, Test Loss: 1.1917\n",
      "Epoch 51/62, Train Loss: 1.0536, Test Loss: 1.1882\n",
      "Epoch 52/62, Train Loss: 1.0772, Test Loss: 1.1915\n",
      "Epoch 53/62, Train Loss: 1.0285, Test Loss: 1.1965\n",
      "Epoch 54/62, Train Loss: 1.0260, Test Loss: 1.1935\n",
      "Epoch 55/62, Train Loss: 1.0230, Test Loss: 1.1966\n",
      "Epoch 56/62, Train Loss: 1.0448, Test Loss: 1.1948\n",
      "Epoch 57/62, Train Loss: 1.0461, Test Loss: 1.1963\n",
      "Epoch 58/62, Train Loss: 1.0293, Test Loss: 1.1975\n",
      "Epoch 59/62, Train Loss: 1.0219, Test Loss: 1.1971\n",
      "Epoch 60/62, Train Loss: 1.0414, Test Loss: 1.1883\n",
      "Epoch 61/62, Train Loss: 1.0366, Test Loss: 1.1937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:10:43,457] Trial 192 finished with value: 1.1909711956977844 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 155, 'layer_1_size': 85, 'layer_2_size': 144, 'layer_3_size': 127, 'layer_4_size': 93, 'layer_5_size': 151, 'layer_6_size': 95, 'layer_7_size': 174, 'layer_8_size': 163, 'layer_9_size': 74, 'layer_10_size': 203, 'layer_11_size': 183, 'layer_12_size': 137, 'layer_13_size': 175, 'dropout_rate': 0.22647721610802127, 'learning_rate': 0.00022424456710867514, 'batch_size': 32, 'epochs': 62}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/62, Train Loss: 1.0104, Test Loss: 1.1910\n",
      "Epoch 1/46, Train Loss: 1.0874, Test Loss: 1.1188\n",
      "Epoch 2/46, Train Loss: 1.0454, Test Loss: 1.0543\n",
      "Epoch 3/46, Train Loss: 1.0342, Test Loss: 1.0647\n",
      "Epoch 4/46, Train Loss: 1.0212, Test Loss: 1.0574\n",
      "Epoch 5/46, Train Loss: 0.9964, Test Loss: 1.0744\n",
      "Epoch 6/46, Train Loss: 0.9821, Test Loss: 1.0600\n",
      "Epoch 7/46, Train Loss: 0.9949, Test Loss: 1.1017\n",
      "Epoch 8/46, Train Loss: 1.0080, Test Loss: 1.0853\n",
      "Epoch 9/46, Train Loss: 0.9673, Test Loss: 1.0847\n",
      "Epoch 10/46, Train Loss: 1.0056, Test Loss: 1.0651\n",
      "Epoch 11/46, Train Loss: 0.9750, Test Loss: 1.0710\n",
      "Epoch 12/46, Train Loss: 0.9603, Test Loss: 1.0748\n",
      "Epoch 13/46, Train Loss: 0.9513, Test Loss: 1.0650\n",
      "Epoch 14/46, Train Loss: 0.9741, Test Loss: 1.0569\n",
      "Epoch 15/46, Train Loss: 0.9787, Test Loss: 1.0738\n",
      "Epoch 16/46, Train Loss: 0.9517, Test Loss: 1.0791\n",
      "Epoch 17/46, Train Loss: 0.9449, Test Loss: 1.1034\n",
      "Epoch 18/46, Train Loss: 0.9638, Test Loss: 1.0809\n",
      "Epoch 19/46, Train Loss: 0.9572, Test Loss: 1.0584\n",
      "Epoch 20/46, Train Loss: 0.9548, Test Loss: 1.0421\n",
      "Epoch 21/46, Train Loss: 0.9456, Test Loss: 1.0537\n",
      "Epoch 22/46, Train Loss: 0.9519, Test Loss: 1.0390\n",
      "Epoch 23/46, Train Loss: 0.9506, Test Loss: 1.0495\n",
      "Epoch 24/46, Train Loss: 0.9558, Test Loss: 1.0706\n",
      "Epoch 25/46, Train Loss: 0.9545, Test Loss: 1.0701\n",
      "Epoch 26/46, Train Loss: 0.9488, Test Loss: 1.0417\n",
      "Epoch 27/46, Train Loss: 0.9374, Test Loss: 1.0364\n",
      "Epoch 28/46, Train Loss: 0.9347, Test Loss: 1.0487\n",
      "Epoch 29/46, Train Loss: 0.9375, Test Loss: 1.0480\n",
      "Epoch 30/46, Train Loss: 0.9537, Test Loss: 1.0449\n",
      "Epoch 31/46, Train Loss: 0.9705, Test Loss: 1.0491\n",
      "Epoch 32/46, Train Loss: 0.9507, Test Loss: 1.0575\n",
      "Epoch 33/46, Train Loss: 0.9782, Test Loss: 1.0829\n",
      "Epoch 34/46, Train Loss: 0.9527, Test Loss: 1.0878\n",
      "Epoch 35/46, Train Loss: 0.9375, Test Loss: 1.0855\n",
      "Epoch 36/46, Train Loss: 0.9403, Test Loss: 1.0687\n",
      "Epoch 37/46, Train Loss: 0.9209, Test Loss: 1.0556\n",
      "Epoch 38/46, Train Loss: 0.9311, Test Loss: 1.0590\n",
      "Epoch 39/46, Train Loss: 0.9506, Test Loss: 1.0504\n",
      "Epoch 40/46, Train Loss: 0.9288, Test Loss: 1.0715\n",
      "Epoch 41/46, Train Loss: 0.9494, Test Loss: 1.0845\n",
      "Epoch 42/46, Train Loss: 0.9184, Test Loss: 1.0836\n",
      "Epoch 43/46, Train Loss: 0.9332, Test Loss: 1.0846\n",
      "Epoch 44/46, Train Loss: 0.9581, Test Loss: 1.0694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:10:54,875] Trial 193 finished with value: 1.072140097618103 and parameters: {'num_hidden_layers': 15, 'layer_0_size': 74, 'layer_1_size': 44, 'layer_2_size': 103, 'layer_3_size': 137, 'layer_4_size': 164, 'layer_5_size': 167, 'layer_6_size': 198, 'layer_7_size': 177, 'layer_8_size': 155, 'layer_9_size': 126, 'layer_10_size': 175, 'layer_11_size': 165, 'layer_12_size': 167, 'layer_13_size': 162, 'layer_14_size': 152, 'dropout_rate': 0.24284274283151247, 'learning_rate': 0.00038426195551154516, 'batch_size': 32, 'epochs': 46}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/46, Train Loss: 0.9427, Test Loss: 1.0573\n",
      "Epoch 46/46, Train Loss: 0.9365, Test Loss: 1.0721\n",
      "Epoch 1/42, Train Loss: 1.1376, Test Loss: 1.2178\n",
      "Epoch 2/42, Train Loss: 1.1710, Test Loss: 1.2040\n",
      "Epoch 3/42, Train Loss: 1.0991, Test Loss: 1.1964\n",
      "Epoch 4/42, Train Loss: 1.1084, Test Loss: 1.1719\n",
      "Epoch 5/42, Train Loss: 1.0771, Test Loss: 1.1747\n",
      "Epoch 6/42, Train Loss: 1.0800, Test Loss: 1.1717\n",
      "Epoch 7/42, Train Loss: 1.0772, Test Loss: 1.1713\n",
      "Epoch 8/42, Train Loss: 1.1078, Test Loss: 1.1705\n",
      "Epoch 9/42, Train Loss: 1.0769, Test Loss: 1.1669\n",
      "Epoch 10/42, Train Loss: 1.1131, Test Loss: 1.1688\n",
      "Epoch 11/42, Train Loss: 1.0796, Test Loss: 1.1687\n",
      "Epoch 12/42, Train Loss: 1.0542, Test Loss: 1.1680\n",
      "Epoch 13/42, Train Loss: 1.0556, Test Loss: 1.1682\n",
      "Epoch 14/42, Train Loss: 1.0712, Test Loss: 1.1656\n",
      "Epoch 15/42, Train Loss: 1.0612, Test Loss: 1.1659\n",
      "Epoch 16/42, Train Loss: 1.0760, Test Loss: 1.1692\n",
      "Epoch 17/42, Train Loss: 1.0258, Test Loss: 1.1742\n",
      "Epoch 18/42, Train Loss: 1.0577, Test Loss: 1.1715\n",
      "Epoch 19/42, Train Loss: 1.0818, Test Loss: 1.1677\n",
      "Epoch 20/42, Train Loss: 1.0685, Test Loss: 1.1686\n",
      "Epoch 21/42, Train Loss: 1.0722, Test Loss: 1.1686\n",
      "Epoch 22/42, Train Loss: 1.0578, Test Loss: 1.1645\n",
      "Epoch 23/42, Train Loss: 1.0332, Test Loss: 1.1681\n",
      "Epoch 24/42, Train Loss: 1.0594, Test Loss: 1.1643\n",
      "Epoch 25/42, Train Loss: 1.0519, Test Loss: 1.1624\n",
      "Epoch 26/42, Train Loss: 1.0440, Test Loss: 1.1633\n",
      "Epoch 27/42, Train Loss: 1.0278, Test Loss: 1.1638\n",
      "Epoch 28/42, Train Loss: 1.0289, Test Loss: 1.1634\n",
      "Epoch 29/42, Train Loss: 1.0324, Test Loss: 1.1654\n",
      "Epoch 30/42, Train Loss: 1.0480, Test Loss: 1.1652\n",
      "Epoch 31/42, Train Loss: 1.0449, Test Loss: 1.1633\n",
      "Epoch 32/42, Train Loss: 1.0451, Test Loss: 1.1646\n",
      "Epoch 33/42, Train Loss: 1.0882, Test Loss: 1.1632\n",
      "Epoch 34/42, Train Loss: 1.0504, Test Loss: 1.1623\n",
      "Epoch 35/42, Train Loss: 1.0418, Test Loss: 1.1622\n",
      "Epoch 36/42, Train Loss: 1.0206, Test Loss: 1.1608\n",
      "Epoch 37/42, Train Loss: 1.0539, Test Loss: 1.1637\n",
      "Epoch 38/42, Train Loss: 1.0319, Test Loss: 1.1621\n",
      "Epoch 39/42, Train Loss: 1.0213, Test Loss: 1.1630\n",
      "Epoch 40/42, Train Loss: 1.0333, Test Loss: 1.1644\n",
      "Epoch 41/42, Train Loss: 1.0353, Test Loss: 1.1650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:11:03,952] Trial 194 finished with value: 1.1635121277400426 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 136, 'layer_1_size': 51, 'layer_2_size': 204, 'layer_3_size': 224, 'layer_4_size': 55, 'layer_5_size': 182, 'layer_6_size': 89, 'layer_7_size': 167, 'layer_8_size': 82, 'layer_9_size': 84, 'layer_10_size': 211, 'layer_11_size': 190, 'layer_12_size': 120, 'layer_13_size': 127, 'dropout_rate': 0.324282621381083, 'learning_rate': 0.00019216082378835508, 'batch_size': 32, 'epochs': 42}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/42, Train Loss: 1.0181, Test Loss: 1.1635\n",
      "Epoch 1/49, Train Loss: 1.2013, Test Loss: 0.8337\n",
      "Epoch 2/49, Train Loss: 1.1984, Test Loss: 0.8425\n",
      "Epoch 3/49, Train Loss: 1.2107, Test Loss: 0.8359\n",
      "Epoch 4/49, Train Loss: 1.2279, Test Loss: 0.8354\n",
      "Epoch 5/49, Train Loss: 1.2160, Test Loss: 0.8296\n",
      "Epoch 6/49, Train Loss: 1.1609, Test Loss: 0.8342\n",
      "Epoch 7/49, Train Loss: 1.2021, Test Loss: 0.8382\n",
      "Epoch 8/49, Train Loss: 1.1787, Test Loss: 0.8440\n",
      "Epoch 9/49, Train Loss: 1.1942, Test Loss: 0.8384\n",
      "Epoch 10/49, Train Loss: 1.1865, Test Loss: 0.8344\n",
      "Epoch 11/49, Train Loss: 1.1843, Test Loss: 0.8399\n",
      "Epoch 12/49, Train Loss: 1.1623, Test Loss: 0.8328\n",
      "Epoch 13/49, Train Loss: 1.1992, Test Loss: 0.8381\n",
      "Epoch 14/49, Train Loss: 1.2121, Test Loss: 0.8341\n",
      "Epoch 15/49, Train Loss: 1.2097, Test Loss: 0.8351\n",
      "Epoch 16/49, Train Loss: 1.1663, Test Loss: 0.8380\n",
      "Epoch 17/49, Train Loss: 1.1311, Test Loss: 0.8313\n",
      "Epoch 18/49, Train Loss: 1.1565, Test Loss: 0.8401\n",
      "Epoch 19/49, Train Loss: 1.1935, Test Loss: 0.8311\n",
      "Epoch 20/49, Train Loss: 1.1273, Test Loss: 0.8310\n",
      "Epoch 21/49, Train Loss: 1.1612, Test Loss: 0.8373\n",
      "Epoch 22/49, Train Loss: 1.1438, Test Loss: 0.8296\n",
      "Epoch 23/49, Train Loss: 1.1820, Test Loss: 0.8343\n",
      "Epoch 24/49, Train Loss: 1.2260, Test Loss: 0.8288\n",
      "Epoch 25/49, Train Loss: 1.1519, Test Loss: 0.8382\n",
      "Epoch 26/49, Train Loss: 1.1651, Test Loss: 0.8275\n",
      "Epoch 27/49, Train Loss: 1.1567, Test Loss: 0.8316\n",
      "Epoch 28/49, Train Loss: 1.1717, Test Loss: 0.8380\n",
      "Epoch 29/49, Train Loss: 1.1379, Test Loss: 0.8322\n",
      "Epoch 30/49, Train Loss: 1.1101, Test Loss: 0.8342\n",
      "Epoch 31/49, Train Loss: 1.1167, Test Loss: 0.8315\n",
      "Epoch 32/49, Train Loss: 1.1252, Test Loss: 0.8424\n",
      "Epoch 33/49, Train Loss: 1.1618, Test Loss: 0.8310\n",
      "Epoch 34/49, Train Loss: 1.1659, Test Loss: 0.8321\n",
      "Epoch 35/49, Train Loss: 1.1527, Test Loss: 0.8266\n",
      "Epoch 36/49, Train Loss: 1.1960, Test Loss: 0.8339\n",
      "Epoch 37/49, Train Loss: 1.1440, Test Loss: 0.8345\n",
      "Epoch 38/49, Train Loss: 1.1315, Test Loss: 0.8346\n",
      "Epoch 39/49, Train Loss: 1.1516, Test Loss: 0.8286\n",
      "Epoch 40/49, Train Loss: 1.2043, Test Loss: 0.8356\n",
      "Epoch 41/49, Train Loss: 1.1620, Test Loss: 0.8322\n",
      "Epoch 42/49, Train Loss: 1.1650, Test Loss: 0.8366\n",
      "Epoch 43/49, Train Loss: 1.1759, Test Loss: 0.8296\n",
      "Epoch 44/49, Train Loss: 1.1648, Test Loss: 0.8344\n",
      "Epoch 45/49, Train Loss: 1.1046, Test Loss: 0.8314\n",
      "Epoch 46/49, Train Loss: 1.1370, Test Loss: 0.8391\n",
      "Epoch 47/49, Train Loss: 1.1587, Test Loss: 0.8333\n",
      "Epoch 48/49, Train Loss: 1.2048, Test Loss: 0.8339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:11:15,812] Trial 195 finished with value: 0.8350738372121539 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 198, 'layer_1_size': 32, 'layer_2_size': 149, 'layer_3_size': 146, 'layer_4_size': 141, 'layer_5_size': 161, 'layer_6_size': 191, 'layer_7_size': 156, 'layer_8_size': 187, 'layer_9_size': 43, 'layer_10_size': 194, 'layer_11_size': 169, 'layer_12_size': 142, 'dropout_rate': 0.31586341926161354, 'learning_rate': 1.3065994466736686e-05, 'batch_size': 32, 'epochs': 49}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/49, Train Loss: 1.1397, Test Loss: 0.8351\n",
      "Epoch 1/49, Train Loss: 1.2285, Test Loss: 0.9486\n",
      "Epoch 2/49, Train Loss: 1.2129, Test Loss: 0.9436\n",
      "Epoch 3/49, Train Loss: 1.2678, Test Loss: 0.9494\n",
      "Epoch 4/49, Train Loss: 1.3099, Test Loss: 0.9442\n",
      "Epoch 5/49, Train Loss: 1.2587, Test Loss: 0.9457\n",
      "Epoch 6/49, Train Loss: 1.2254, Test Loss: 0.9467\n",
      "Epoch 7/49, Train Loss: 1.2677, Test Loss: 0.9462\n",
      "Epoch 8/49, Train Loss: 1.2260, Test Loss: 0.9468\n",
      "Epoch 9/49, Train Loss: 1.2241, Test Loss: 0.9489\n",
      "Epoch 10/49, Train Loss: 1.2429, Test Loss: 0.9480\n",
      "Epoch 11/49, Train Loss: 1.2719, Test Loss: 0.9461\n",
      "Epoch 12/49, Train Loss: 1.1755, Test Loss: 0.9508\n",
      "Epoch 13/49, Train Loss: 1.1878, Test Loss: 0.9464\n",
      "Epoch 14/49, Train Loss: 1.1990, Test Loss: 0.9463\n",
      "Epoch 15/49, Train Loss: 1.1745, Test Loss: 0.9477\n",
      "Epoch 16/49, Train Loss: 1.2402, Test Loss: 0.9454\n",
      "Epoch 17/49, Train Loss: 1.2400, Test Loss: 0.9475\n",
      "Epoch 18/49, Train Loss: 1.2539, Test Loss: 0.9430\n",
      "Epoch 19/49, Train Loss: 1.2166, Test Loss: 0.9443\n",
      "Epoch 20/49, Train Loss: 1.2129, Test Loss: 0.9484\n",
      "Epoch 21/49, Train Loss: 1.1763, Test Loss: 0.9424\n",
      "Epoch 22/49, Train Loss: 1.1948, Test Loss: 0.9426\n",
      "Epoch 23/49, Train Loss: 1.2476, Test Loss: 0.9447\n",
      "Epoch 24/49, Train Loss: 1.1402, Test Loss: 0.9448\n",
      "Epoch 25/49, Train Loss: 1.2087, Test Loss: 0.9446\n",
      "Epoch 26/49, Train Loss: 1.2434, Test Loss: 0.9502\n",
      "Epoch 27/49, Train Loss: 1.1509, Test Loss: 0.9476\n",
      "Epoch 28/49, Train Loss: 1.1996, Test Loss: 0.9452\n",
      "Epoch 29/49, Train Loss: 1.1733, Test Loss: 0.9433\n",
      "Epoch 30/49, Train Loss: 1.2100, Test Loss: 0.9497\n",
      "Epoch 31/49, Train Loss: 1.1730, Test Loss: 0.9456\n",
      "Epoch 32/49, Train Loss: 1.1709, Test Loss: 0.9465\n",
      "Epoch 33/49, Train Loss: 1.1547, Test Loss: 0.9451\n",
      "Epoch 34/49, Train Loss: 1.2095, Test Loss: 0.9449\n",
      "Epoch 35/49, Train Loss: 1.1407, Test Loss: 0.9472\n",
      "Epoch 36/49, Train Loss: 1.2100, Test Loss: 0.9427\n",
      "Epoch 37/49, Train Loss: 1.1268, Test Loss: 0.9484\n",
      "Epoch 38/49, Train Loss: 1.2164, Test Loss: 0.9480\n",
      "Epoch 39/49, Train Loss: 1.2078, Test Loss: 0.9462\n",
      "Epoch 40/49, Train Loss: 1.1555, Test Loss: 0.9445\n",
      "Epoch 41/49, Train Loss: 1.1677, Test Loss: 0.9440\n",
      "Epoch 42/49, Train Loss: 1.1693, Test Loss: 0.9461\n",
      "Epoch 43/49, Train Loss: 1.1518, Test Loss: 0.9445\n",
      "Epoch 44/49, Train Loss: 1.2118, Test Loss: 0.9412\n",
      "Epoch 45/49, Train Loss: 1.1684, Test Loss: 0.9443\n",
      "Epoch 46/49, Train Loss: 1.1483, Test Loss: 0.9429\n",
      "Epoch 47/49, Train Loss: 1.1849, Test Loss: 0.9463\n",
      "Epoch 48/49, Train Loss: 1.2830, Test Loss: 0.9459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:11:27,121] Trial 196 finished with value: 0.9431613513401577 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 199, 'layer_1_size': 39, 'layer_2_size': 151, 'layer_3_size': 149, 'layer_4_size': 140, 'layer_5_size': 163, 'layer_6_size': 190, 'layer_7_size': 156, 'layer_8_size': 186, 'layer_9_size': 40, 'layer_10_size': 195, 'layer_11_size': 170, 'layer_12_size': 144, 'dropout_rate': 0.3084525231805445, 'learning_rate': 1.5093316673544832e-05, 'batch_size': 32, 'epochs': 49}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/49, Train Loss: 1.1475, Test Loss: 0.9432\n",
      "Epoch 1/52, Train Loss: 1.3671, Test Loss: 1.1857\n",
      "Epoch 2/52, Train Loss: 1.2926, Test Loss: 1.2110\n",
      "Epoch 3/52, Train Loss: 1.2612, Test Loss: 1.1859\n",
      "Epoch 4/52, Train Loss: 1.2909, Test Loss: 1.1909\n",
      "Epoch 5/52, Train Loss: 1.3153, Test Loss: 1.1847\n",
      "Epoch 6/52, Train Loss: 1.3203, Test Loss: 1.1719\n",
      "Epoch 7/52, Train Loss: 1.2959, Test Loss: 1.2070\n",
      "Epoch 8/52, Train Loss: 1.2509, Test Loss: 1.1798\n",
      "Epoch 9/52, Train Loss: 1.3446, Test Loss: 1.2123\n",
      "Epoch 10/52, Train Loss: 1.2457, Test Loss: 1.1975\n",
      "Epoch 11/52, Train Loss: 1.2449, Test Loss: 1.1910\n",
      "Epoch 12/52, Train Loss: 1.2805, Test Loss: 1.1903\n",
      "Epoch 13/52, Train Loss: 1.2620, Test Loss: 1.1764\n",
      "Epoch 14/52, Train Loss: 1.3061, Test Loss: 1.1957\n",
      "Epoch 15/52, Train Loss: 1.2140, Test Loss: 1.1893\n",
      "Epoch 16/52, Train Loss: 1.1473, Test Loss: 1.1609\n",
      "Epoch 17/52, Train Loss: 1.2073, Test Loss: 1.1908\n",
      "Epoch 18/52, Train Loss: 1.2226, Test Loss: 1.1930\n",
      "Epoch 19/52, Train Loss: 1.2134, Test Loss: 1.1749\n",
      "Epoch 20/52, Train Loss: 1.2023, Test Loss: 1.1834\n",
      "Epoch 21/52, Train Loss: 1.2460, Test Loss: 1.1709\n",
      "Epoch 22/52, Train Loss: 1.1426, Test Loss: 1.1853\n",
      "Epoch 23/52, Train Loss: 1.2121, Test Loss: 1.1699\n",
      "Epoch 24/52, Train Loss: 1.1922, Test Loss: 1.1605\n",
      "Epoch 25/52, Train Loss: 1.1759, Test Loss: 1.1610\n",
      "Epoch 26/52, Train Loss: 1.1441, Test Loss: 1.1551\n",
      "Epoch 27/52, Train Loss: 1.2605, Test Loss: 1.1683\n",
      "Epoch 28/52, Train Loss: 1.2004, Test Loss: 1.1871\n",
      "Epoch 29/52, Train Loss: 1.1772, Test Loss: 1.1701\n",
      "Epoch 30/52, Train Loss: 1.2003, Test Loss: 1.1702\n",
      "Epoch 31/52, Train Loss: 1.1580, Test Loss: 1.1763\n",
      "Epoch 32/52, Train Loss: 1.2069, Test Loss: 1.1776\n",
      "Epoch 33/52, Train Loss: 1.2132, Test Loss: 1.1798\n",
      "Epoch 34/52, Train Loss: 1.2045, Test Loss: 1.1775\n",
      "Epoch 35/52, Train Loss: 1.1876, Test Loss: 1.1717\n",
      "Epoch 36/52, Train Loss: 1.2311, Test Loss: 1.1629\n",
      "Epoch 37/52, Train Loss: 1.2535, Test Loss: 1.1596\n",
      "Epoch 38/52, Train Loss: 1.2053, Test Loss: 1.1544\n",
      "Epoch 39/52, Train Loss: 1.1553, Test Loss: 1.1613\n",
      "Epoch 40/52, Train Loss: 1.1380, Test Loss: 1.1703\n",
      "Epoch 41/52, Train Loss: 1.1515, Test Loss: 1.1637\n",
      "Epoch 42/52, Train Loss: 1.1629, Test Loss: 1.1631\n",
      "Epoch 43/52, Train Loss: 1.1492, Test Loss: 1.1634\n",
      "Epoch 44/52, Train Loss: 1.1780, Test Loss: 1.1670\n",
      "Epoch 45/52, Train Loss: 1.1606, Test Loss: 1.1626\n",
      "Epoch 46/52, Train Loss: 1.1747, Test Loss: 1.1554\n",
      "Epoch 47/52, Train Loss: 1.1618, Test Loss: 1.1533\n",
      "Epoch 48/52, Train Loss: 1.1644, Test Loss: 1.1564\n",
      "Epoch 49/52, Train Loss: 1.1609, Test Loss: 1.1672\n",
      "Epoch 50/52, Train Loss: 1.1825, Test Loss: 1.1618\n",
      "Epoch 51/52, Train Loss: 1.1804, Test Loss: 1.1644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:11:37,951] Trial 197 finished with value: 1.1534340977668762 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 211, 'layer_1_size': 41, 'layer_2_size': 87, 'layer_3_size': 147, 'layer_4_size': 150, 'layer_5_size': 256, 'layer_6_size': 183, 'layer_7_size': 160, 'layer_8_size': 178, 'layer_9_size': 46, 'layer_10_size': 202, 'layer_11_size': 175, 'layer_12_size': 160, 'dropout_rate': 0.31684884801411217, 'learning_rate': 1.207602130838173e-05, 'batch_size': 32, 'epochs': 52}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/52, Train Loss: 1.1466, Test Loss: 1.1534\n",
      "Epoch 1/47, Train Loss: 1.3297, Test Loss: 1.0750\n",
      "Epoch 2/47, Train Loss: 1.2150, Test Loss: 1.0987\n",
      "Epoch 3/47, Train Loss: 1.2504, Test Loss: 1.1107\n",
      "Epoch 4/47, Train Loss: 1.2455, Test Loss: 1.0949\n",
      "Epoch 5/47, Train Loss: 1.1651, Test Loss: 1.0622\n",
      "Epoch 6/47, Train Loss: 1.2781, Test Loss: 1.0916\n",
      "Epoch 7/47, Train Loss: 1.2074, Test Loss: 1.0874\n",
      "Epoch 8/47, Train Loss: 1.2042, Test Loss: 1.0796\n",
      "Epoch 9/47, Train Loss: 1.1826, Test Loss: 1.0514\n",
      "Epoch 10/47, Train Loss: 1.1899, Test Loss: 1.0867\n",
      "Epoch 11/47, Train Loss: 1.1877, Test Loss: 1.0756\n",
      "Epoch 12/47, Train Loss: 1.2332, Test Loss: 1.0650\n",
      "Epoch 13/47, Train Loss: 1.1945, Test Loss: 1.0542\n",
      "Epoch 14/47, Train Loss: 1.1817, Test Loss: 1.0464\n",
      "Epoch 15/47, Train Loss: 1.1706, Test Loss: 1.0764\n",
      "Epoch 16/47, Train Loss: 1.1795, Test Loss: 1.0576\n",
      "Epoch 17/47, Train Loss: 1.1936, Test Loss: 1.0588\n",
      "Epoch 18/47, Train Loss: 1.2404, Test Loss: 1.0610\n",
      "Epoch 19/47, Train Loss: 1.1700, Test Loss: 1.0484\n",
      "Epoch 20/47, Train Loss: 1.2036, Test Loss: 1.0519\n",
      "Epoch 21/47, Train Loss: 1.1855, Test Loss: 1.0512\n",
      "Epoch 22/47, Train Loss: 1.1904, Test Loss: 1.0393\n",
      "Epoch 23/47, Train Loss: 1.1646, Test Loss: 1.0286\n",
      "Epoch 24/47, Train Loss: 1.2011, Test Loss: 1.0296\n",
      "Epoch 25/47, Train Loss: 1.1988, Test Loss: 1.0272\n",
      "Epoch 26/47, Train Loss: 1.2070, Test Loss: 1.0389\n",
      "Epoch 27/47, Train Loss: 1.1695, Test Loss: 1.0283\n",
      "Epoch 28/47, Train Loss: 1.1667, Test Loss: 1.0182\n",
      "Epoch 29/47, Train Loss: 1.1785, Test Loss: 1.0223\n",
      "Epoch 30/47, Train Loss: 1.1639, Test Loss: 1.0222\n",
      "Epoch 31/47, Train Loss: 1.1795, Test Loss: 1.0227\n",
      "Epoch 32/47, Train Loss: 1.1566, Test Loss: 1.0167\n",
      "Epoch 33/47, Train Loss: 1.1192, Test Loss: 1.0044\n",
      "Epoch 34/47, Train Loss: 1.1394, Test Loss: 1.0276\n",
      "Epoch 35/47, Train Loss: 1.1353, Test Loss: 1.0276\n",
      "Epoch 36/47, Train Loss: 1.1238, Test Loss: 0.9996\n",
      "Epoch 37/47, Train Loss: 1.1436, Test Loss: 1.0310\n",
      "Epoch 38/47, Train Loss: 1.1946, Test Loss: 1.0121\n",
      "Epoch 39/47, Train Loss: 1.1350, Test Loss: 1.0099\n",
      "Epoch 40/47, Train Loss: 1.1806, Test Loss: 1.0086\n",
      "Epoch 41/47, Train Loss: 1.1420, Test Loss: 1.0061\n",
      "Epoch 42/47, Train Loss: 1.1324, Test Loss: 1.0088\n",
      "Epoch 43/47, Train Loss: 1.1365, Test Loss: 1.0057\n",
      "Epoch 44/47, Train Loss: 1.1554, Test Loss: 1.0033\n",
      "Epoch 45/47, Train Loss: 1.1518, Test Loss: 1.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:11:45,959] Trial 198 finished with value: 0.9997798034123012 and parameters: {'num_hidden_layers': 12, 'layer_0_size': 186, 'layer_1_size': 35, 'layer_2_size': 162, 'layer_3_size': 161, 'layer_4_size': 146, 'layer_5_size': 159, 'layer_6_size': 164, 'layer_7_size': 152, 'layer_8_size': 190, 'layer_9_size': 50, 'layer_10_size': 185, 'layer_11_size': 49, 'dropout_rate': 0.3339894820569473, 'learning_rate': 1.3294798321623336e-05, 'batch_size': 32, 'epochs': 47}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/47, Train Loss: 1.1712, Test Loss: 1.0173\n",
      "Epoch 47/47, Train Loss: 1.1529, Test Loss: 0.9998\n",
      "Epoch 1/44, Train Loss: 1.1752, Test Loss: 0.9635\n",
      "Epoch 2/44, Train Loss: 1.1173, Test Loss: 0.9618\n",
      "Epoch 3/44, Train Loss: 1.0595, Test Loss: 0.9633\n",
      "Epoch 4/44, Train Loss: 1.0545, Test Loss: 0.9530\n",
      "Epoch 5/44, Train Loss: 1.0462, Test Loss: 0.9618\n",
      "Epoch 6/44, Train Loss: 1.0495, Test Loss: 0.9511\n",
      "Epoch 7/44, Train Loss: 1.0917, Test Loss: 0.9686\n",
      "Epoch 8/44, Train Loss: 1.0243, Test Loss: 0.9664\n",
      "Epoch 9/44, Train Loss: 1.0497, Test Loss: 0.9652\n",
      "Epoch 10/44, Train Loss: 1.0349, Test Loss: 0.9646\n",
      "Epoch 11/44, Train Loss: 1.0465, Test Loss: 0.9643\n",
      "Epoch 12/44, Train Loss: 1.0354, Test Loss: 0.9605\n",
      "Epoch 13/44, Train Loss: 0.9818, Test Loss: 0.9614\n",
      "Epoch 14/44, Train Loss: 1.0209, Test Loss: 0.9576\n",
      "Epoch 15/44, Train Loss: 1.0300, Test Loss: 0.9605\n",
      "Epoch 16/44, Train Loss: 1.0317, Test Loss: 0.9615\n",
      "Epoch 17/44, Train Loss: 1.0501, Test Loss: 0.9639\n",
      "Epoch 18/44, Train Loss: 1.0064, Test Loss: 0.9656\n",
      "Epoch 19/44, Train Loss: 0.9666, Test Loss: 0.9637\n",
      "Epoch 20/44, Train Loss: 1.0543, Test Loss: 0.9618\n",
      "Epoch 21/44, Train Loss: 1.0445, Test Loss: 0.9636\n",
      "Epoch 22/44, Train Loss: 1.0048, Test Loss: 0.9613\n",
      "Epoch 23/44, Train Loss: 1.0503, Test Loss: 0.9654\n",
      "Epoch 24/44, Train Loss: 1.0214, Test Loss: 0.9587\n",
      "Epoch 25/44, Train Loss: 1.0374, Test Loss: 0.9669\n",
      "Epoch 26/44, Train Loss: 0.9647, Test Loss: 0.9621\n",
      "Epoch 27/44, Train Loss: 1.0271, Test Loss: 0.9667\n",
      "Epoch 28/44, Train Loss: 1.0265, Test Loss: 0.9632\n",
      "Epoch 29/44, Train Loss: 0.9860, Test Loss: 0.9596\n",
      "Epoch 30/44, Train Loss: 1.0144, Test Loss: 0.9630\n",
      "Epoch 31/44, Train Loss: 1.0182, Test Loss: 0.9637\n",
      "Epoch 32/44, Train Loss: 1.0546, Test Loss: 0.9633\n",
      "Epoch 33/44, Train Loss: 0.9910, Test Loss: 0.9631\n",
      "Epoch 34/44, Train Loss: 1.0254, Test Loss: 0.9640\n",
      "Epoch 35/44, Train Loss: 0.9904, Test Loss: 0.9679\n",
      "Epoch 36/44, Train Loss: 0.9908, Test Loss: 0.9616\n",
      "Epoch 37/44, Train Loss: 1.0118, Test Loss: 0.9658\n",
      "Epoch 38/44, Train Loss: 1.0284, Test Loss: 0.9648\n",
      "Epoch 39/44, Train Loss: 0.9992, Test Loss: 0.9648\n",
      "Epoch 40/44, Train Loss: 1.0108, Test Loss: 0.9654\n",
      "Epoch 41/44, Train Loss: 0.9960, Test Loss: 0.9656\n",
      "Epoch 42/44, Train Loss: 1.0136, Test Loss: 0.9673\n",
      "Epoch 43/44, Train Loss: 0.9877, Test Loss: 0.9642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:11:54,095] Trial 199 finished with value: 0.9570889217512948 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 195, 'layer_1_size': 192, 'layer_2_size': 99, 'layer_3_size': 143, 'layer_4_size': 134, 'layer_5_size': 154, 'layer_6_size': 55, 'layer_7_size': 171, 'layer_8_size': 172, 'layer_9_size': 36, 'layer_10_size': 110, 'layer_11_size': 102, 'layer_12_size': 140, 'dropout_rate': 0.23162847470035, 'learning_rate': 0.00012226950774535056, 'batch_size': 32, 'epochs': 44}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/44, Train Loss: 1.0175, Test Loss: 0.9571\n",
      "Epoch 1/14, Train Loss: 1.1639, Test Loss: 0.9955\n",
      "Epoch 2/14, Train Loss: 1.1167, Test Loss: 0.9996\n",
      "Epoch 3/14, Train Loss: 1.1144, Test Loss: 0.9998\n",
      "Epoch 4/14, Train Loss: 1.1298, Test Loss: 0.9905\n",
      "Epoch 5/14, Train Loss: 1.0849, Test Loss: 0.9985\n",
      "Epoch 6/14, Train Loss: 1.1052, Test Loss: 1.0013\n",
      "Epoch 7/14, Train Loss: 1.1120, Test Loss: 0.9879\n",
      "Epoch 8/14, Train Loss: 1.0913, Test Loss: 0.9899\n",
      "Epoch 9/14, Train Loss: 1.0543, Test Loss: 0.9846\n",
      "Epoch 10/14, Train Loss: 1.0767, Test Loss: 0.9902\n",
      "Epoch 11/14, Train Loss: 1.0820, Test Loss: 0.9880\n",
      "Epoch 12/14, Train Loss: 1.0813, Test Loss: 0.9891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:11:56,445] Trial 200 finished with value: 0.992565223148891 and parameters: {'num_hidden_layers': 12, 'layer_0_size': 118, 'layer_1_size': 204, 'layer_2_size': 141, 'layer_3_size': 86, 'layer_4_size': 125, 'layer_5_size': 172, 'layer_6_size': 111, 'layer_7_size': 199, 'layer_8_size': 74, 'layer_9_size': 55, 'layer_10_size': 191, 'layer_11_size': 180, 'dropout_rate': 0.19083098715164468, 'learning_rate': 2.3444557543673933e-05, 'batch_size': 32, 'epochs': 14}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/14, Train Loss: 1.0888, Test Loss: 0.9992\n",
      "Epoch 14/14, Train Loss: 1.0907, Test Loss: 0.9926\n",
      "Epoch 1/40, Train Loss: 1.1744, Test Loss: 0.8580\n",
      "Epoch 2/40, Train Loss: 1.0759, Test Loss: 0.8567\n",
      "Epoch 3/40, Train Loss: 1.0874, Test Loss: 0.8569\n",
      "Epoch 4/40, Train Loss: 1.0995, Test Loss: 0.8629\n",
      "Epoch 5/40, Train Loss: 1.0946, Test Loss: 0.8656\n",
      "Epoch 6/40, Train Loss: 1.0795, Test Loss: 0.8667\n",
      "Epoch 7/40, Train Loss: 1.0276, Test Loss: 0.8643\n",
      "Epoch 8/40, Train Loss: 1.1055, Test Loss: 0.8586\n",
      "Epoch 9/40, Train Loss: 1.0435, Test Loss: 0.8548\n",
      "Epoch 10/40, Train Loss: 1.0161, Test Loss: 0.8537\n",
      "Epoch 11/40, Train Loss: 1.0505, Test Loss: 0.8511\n",
      "Epoch 12/40, Train Loss: 0.9834, Test Loss: 0.8546\n",
      "Epoch 13/40, Train Loss: 1.0454, Test Loss: 0.8534\n",
      "Epoch 14/40, Train Loss: 1.0761, Test Loss: 0.8522\n",
      "Epoch 15/40, Train Loss: 1.0242, Test Loss: 0.8532\n",
      "Epoch 16/40, Train Loss: 1.0192, Test Loss: 0.8561\n",
      "Epoch 17/40, Train Loss: 1.0073, Test Loss: 0.8578\n",
      "Epoch 18/40, Train Loss: 1.0116, Test Loss: 0.8582\n",
      "Epoch 19/40, Train Loss: 1.0211, Test Loss: 0.8571\n",
      "Epoch 20/40, Train Loss: 0.9946, Test Loss: 0.8590\n",
      "Epoch 21/40, Train Loss: 0.9958, Test Loss: 0.8608\n",
      "Epoch 22/40, Train Loss: 1.0226, Test Loss: 0.8600\n",
      "Epoch 23/40, Train Loss: 0.9990, Test Loss: 0.8569\n",
      "Epoch 24/40, Train Loss: 1.0350, Test Loss: 0.8516\n",
      "Epoch 25/40, Train Loss: 0.9898, Test Loss: 0.8486\n",
      "Epoch 26/40, Train Loss: 1.0128, Test Loss: 0.8451\n",
      "Epoch 27/40, Train Loss: 0.9772, Test Loss: 0.8448\n",
      "Epoch 28/40, Train Loss: 0.9542, Test Loss: 0.8450\n",
      "Epoch 29/40, Train Loss: 0.9987, Test Loss: 0.8449\n",
      "Epoch 30/40, Train Loss: 0.9914, Test Loss: 0.8450\n",
      "Epoch 31/40, Train Loss: 0.9972, Test Loss: 0.8431\n",
      "Epoch 32/40, Train Loss: 0.9903, Test Loss: 0.8443\n",
      "Epoch 33/40, Train Loss: 0.9738, Test Loss: 0.8463\n",
      "Epoch 34/40, Train Loss: 0.9977, Test Loss: 0.8465\n",
      "Epoch 35/40, Train Loss: 0.9735, Test Loss: 0.8477\n",
      "Epoch 36/40, Train Loss: 0.9787, Test Loss: 0.8499\n",
      "Epoch 37/40, Train Loss: 0.9762, Test Loss: 0.8523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:11:59,200] Trial 201 finished with value: 0.8547835350036621 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 222, 'layer_1_size': 71, 'layer_2_size': 148, 'layer_3_size': 132, 'layer_4_size': 155, 'layer_5_size': 197, 'layer_6_size': 191, 'layer_7_size': 145, 'layer_8_size': 166, 'layer_9_size': 44, 'layer_10_size': 237, 'layer_11_size': 152, 'layer_12_size': 129, 'layer_13_size': 181, 'dropout_rate': 0.31527152948381243, 'learning_rate': 0.0002688775736497782, 'batch_size': 256, 'epochs': 40}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/40, Train Loss: 0.9954, Test Loss: 0.8534\n",
      "Epoch 39/40, Train Loss: 0.9726, Test Loss: 0.8544\n",
      "Epoch 40/40, Train Loss: 0.9728, Test Loss: 0.8548\n",
      "Epoch 1/34, Train Loss: 1.2047, Test Loss: 0.8955\n",
      "Epoch 2/34, Train Loss: 1.0965, Test Loss: 0.8975\n",
      "Epoch 3/34, Train Loss: 1.1048, Test Loss: 0.8931\n",
      "Epoch 4/34, Train Loss: 1.0319, Test Loss: 0.9162\n",
      "Epoch 5/34, Train Loss: 1.0706, Test Loss: 0.9035\n",
      "Epoch 6/34, Train Loss: 1.0485, Test Loss: 0.8958\n",
      "Epoch 7/34, Train Loss: 1.0508, Test Loss: 0.8953\n",
      "Epoch 8/34, Train Loss: 1.0318, Test Loss: 0.8883\n",
      "Epoch 9/34, Train Loss: 1.0387, Test Loss: 0.8932\n",
      "Epoch 10/34, Train Loss: 1.0145, Test Loss: 0.8888\n",
      "Epoch 11/34, Train Loss: 1.0088, Test Loss: 0.8935\n",
      "Epoch 12/34, Train Loss: 1.0320, Test Loss: 0.8925\n",
      "Epoch 13/34, Train Loss: 1.0015, Test Loss: 0.8886\n",
      "Epoch 14/34, Train Loss: 1.0297, Test Loss: 0.8944\n",
      "Epoch 15/34, Train Loss: 0.9946, Test Loss: 0.8954\n",
      "Epoch 16/34, Train Loss: 1.0233, Test Loss: 0.8927\n",
      "Epoch 17/34, Train Loss: 0.9976, Test Loss: 0.8936\n",
      "Epoch 18/34, Train Loss: 0.9784, Test Loss: 0.8959\n",
      "Epoch 19/34, Train Loss: 1.0112, Test Loss: 0.8965\n",
      "Epoch 20/34, Train Loss: 0.9870, Test Loss: 0.8953\n",
      "Epoch 21/34, Train Loss: 0.9969, Test Loss: 0.8934\n",
      "Epoch 22/34, Train Loss: 1.0251, Test Loss: 0.9001\n",
      "Epoch 23/34, Train Loss: 0.9771, Test Loss: 0.9080\n",
      "Epoch 24/34, Train Loss: 1.0076, Test Loss: 0.9059\n",
      "Epoch 25/34, Train Loss: 0.9698, Test Loss: 0.9040\n",
      "Epoch 26/34, Train Loss: 0.9438, Test Loss: 0.9034\n",
      "Epoch 27/34, Train Loss: 0.9563, Test Loss: 0.9094\n",
      "Epoch 28/34, Train Loss: 0.9932, Test Loss: 0.8996\n",
      "Epoch 29/34, Train Loss: 0.9707, Test Loss: 0.8981\n",
      "Epoch 30/34, Train Loss: 0.9726, Test Loss: 0.9002\n",
      "Epoch 31/34, Train Loss: 0.9951, Test Loss: 0.8964\n",
      "Epoch 32/34, Train Loss: 0.9583, Test Loss: 0.9013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:12:05,946] Trial 202 finished with value: 0.9071541513715472 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 220, 'layer_1_size': 199, 'layer_2_size': 149, 'layer_3_size': 130, 'layer_4_size': 155, 'layer_5_size': 146, 'layer_6_size': 69, 'layer_7_size': 143, 'layer_8_size': 168, 'layer_9_size': 45, 'layer_10_size': 252, 'layer_11_size': 163, 'layer_12_size': 127, 'dropout_rate': 0.3276412464537113, 'learning_rate': 0.00027657121172959835, 'batch_size': 32, 'epochs': 34}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/34, Train Loss: 1.0024, Test Loss: 0.9119\n",
      "Epoch 34/34, Train Loss: 0.9747, Test Loss: 0.9072\n",
      "Epoch 1/41, Train Loss: 1.3141, Test Loss: 0.9048\n",
      "Epoch 2/41, Train Loss: 1.2200, Test Loss: 0.9044\n",
      "Epoch 3/41, Train Loss: 1.1972, Test Loss: 0.9047\n",
      "Epoch 4/41, Train Loss: 1.2165, Test Loss: 0.9055\n",
      "Epoch 5/41, Train Loss: 1.1922, Test Loss: 0.9032\n",
      "Epoch 6/41, Train Loss: 1.1871, Test Loss: 0.9009\n",
      "Epoch 7/41, Train Loss: 1.1389, Test Loss: 0.9006\n",
      "Epoch 8/41, Train Loss: 1.1302, Test Loss: 0.8996\n",
      "Epoch 9/41, Train Loss: 1.1901, Test Loss: 0.9005\n",
      "Epoch 10/41, Train Loss: 1.1287, Test Loss: 0.9013\n",
      "Epoch 11/41, Train Loss: 1.1357, Test Loss: 0.8996\n",
      "Epoch 12/41, Train Loss: 1.1244, Test Loss: 0.8991\n",
      "Epoch 13/41, Train Loss: 1.1324, Test Loss: 0.8983\n",
      "Epoch 14/41, Train Loss: 1.1418, Test Loss: 0.8977\n",
      "Epoch 15/41, Train Loss: 1.1311, Test Loss: 0.8968\n",
      "Epoch 16/41, Train Loss: 1.1224, Test Loss: 0.8966\n",
      "Epoch 17/41, Train Loss: 1.1260, Test Loss: 0.8964\n",
      "Epoch 18/41, Train Loss: 1.1233, Test Loss: 0.8959\n",
      "Epoch 19/41, Train Loss: 1.1613, Test Loss: 0.8962\n",
      "Epoch 20/41, Train Loss: 1.1075, Test Loss: 0.8978\n",
      "Epoch 21/41, Train Loss: 1.0882, Test Loss: 0.8989\n",
      "Epoch 22/41, Train Loss: 1.1542, Test Loss: 0.8989\n",
      "Epoch 23/41, Train Loss: 1.0912, Test Loss: 0.8992\n",
      "Epoch 24/41, Train Loss: 1.1066, Test Loss: 0.9011\n",
      "Epoch 25/41, Train Loss: 1.1215, Test Loss: 0.9019\n",
      "Epoch 26/41, Train Loss: 1.1265, Test Loss: 0.9051\n",
      "Epoch 27/41, Train Loss: 1.1673, Test Loss: 0.9052\n",
      "Epoch 28/41, Train Loss: 1.1018, Test Loss: 0.9039\n",
      "Epoch 29/41, Train Loss: 1.1056, Test Loss: 0.9027\n",
      "Epoch 30/41, Train Loss: 1.0893, Test Loss: 0.9027\n",
      "Epoch 31/41, Train Loss: 1.1224, Test Loss: 0.9027\n",
      "Epoch 32/41, Train Loss: 1.0953, Test Loss: 0.9007\n",
      "Epoch 33/41, Train Loss: 1.1094, Test Loss: 0.8989\n",
      "Epoch 34/41, Train Loss: 1.1267, Test Loss: 0.8979\n",
      "Epoch 35/41, Train Loss: 1.1234, Test Loss: 0.8982\n",
      "Epoch 36/41, Train Loss: 1.0937, Test Loss: 0.8983\n",
      "Epoch 37/41, Train Loss: 1.0748, Test Loss: 0.8980\n",
      "Epoch 38/41, Train Loss: 1.1054, Test Loss: 0.8981\n",
      "Epoch 39/41, Train Loss: 1.0983, Test Loss: 0.8991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:12:08,255] Trial 203 finished with value: 0.8994501233100891 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 226, 'layer_1_size': 73, 'layer_2_size': 156, 'layer_3_size': 125, 'layer_4_size': 143, 'layer_5_size': 130, 'layer_6_size': 192, 'layer_7_size': 146, 'layer_8_size': 173, 'layer_9_size': 43, 'layer_10_size': 198, 'layer_11_size': 150, 'layer_12_size': 152, 'layer_13_size': 155, 'dropout_rate': 0.308690491253365, 'learning_rate': 0.00033758401673887297, 'batch_size': 256, 'epochs': 41}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/41, Train Loss: 1.1037, Test Loss: 0.8989\n",
      "Epoch 41/41, Train Loss: 1.0617, Test Loss: 0.8995\n",
      "Epoch 1/38, Train Loss: 1.2744, Test Loss: 0.8879\n",
      "Epoch 2/38, Train Loss: 1.2852, Test Loss: 0.9228\n",
      "Epoch 3/38, Train Loss: 1.3037, Test Loss: 0.9296\n",
      "Epoch 4/38, Train Loss: 1.2625, Test Loss: 0.9358\n",
      "Epoch 5/38, Train Loss: 1.2559, Test Loss: 0.9035\n",
      "Epoch 6/38, Train Loss: 1.2709, Test Loss: 0.9217\n",
      "Epoch 7/38, Train Loss: 1.2576, Test Loss: 0.9123\n",
      "Epoch 8/38, Train Loss: 1.3011, Test Loss: 0.9035\n",
      "Epoch 9/38, Train Loss: 1.3055, Test Loss: 0.9037\n",
      "Epoch 10/38, Train Loss: 1.2415, Test Loss: 0.8932\n",
      "Epoch 11/38, Train Loss: 1.2967, Test Loss: 0.9099\n",
      "Epoch 12/38, Train Loss: 1.2562, Test Loss: 0.9043\n",
      "Epoch 13/38, Train Loss: 1.2598, Test Loss: 0.9023\n",
      "Epoch 14/38, Train Loss: 1.2473, Test Loss: 0.8984\n",
      "Epoch 15/38, Train Loss: 1.2521, Test Loss: 0.8913\n",
      "Epoch 16/38, Train Loss: 1.2747, Test Loss: 0.8913\n",
      "Epoch 17/38, Train Loss: 1.2165, Test Loss: 0.8944\n",
      "Epoch 18/38, Train Loss: 1.1991, Test Loss: 0.8914\n",
      "Epoch 19/38, Train Loss: 1.2389, Test Loss: 0.8899\n",
      "Epoch 20/38, Train Loss: 1.2296, Test Loss: 0.8908\n",
      "Epoch 21/38, Train Loss: 1.2180, Test Loss: 0.8818\n",
      "Epoch 22/38, Train Loss: 1.2066, Test Loss: 0.8831\n",
      "Epoch 23/38, Train Loss: 1.2602, Test Loss: 0.8807\n",
      "Epoch 24/38, Train Loss: 1.1655, Test Loss: 0.8843\n",
      "Epoch 25/38, Train Loss: 1.2394, Test Loss: 0.8859\n",
      "Epoch 26/38, Train Loss: 1.2971, Test Loss: 0.8878\n",
      "Epoch 27/38, Train Loss: 1.2244, Test Loss: 0.8853\n",
      "Epoch 28/38, Train Loss: 1.2451, Test Loss: 0.8863\n",
      "Epoch 29/38, Train Loss: 1.2209, Test Loss: 0.8853\n",
      "Epoch 30/38, Train Loss: 1.2408, Test Loss: 0.8840\n",
      "Epoch 31/38, Train Loss: 1.2523, Test Loss: 0.8851\n",
      "Epoch 32/38, Train Loss: 1.2118, Test Loss: 0.8817\n",
      "Epoch 33/38, Train Loss: 1.1729, Test Loss: 0.8802\n",
      "Epoch 34/38, Train Loss: 1.2049, Test Loss: 0.8818\n",
      "Epoch 35/38, Train Loss: 1.2415, Test Loss: 0.8827\n",
      "Epoch 36/38, Train Loss: 1.1812, Test Loss: 0.8809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:12:13,498] Trial 204 finished with value: 0.8814759477972984 and parameters: {'num_hidden_layers': 15, 'layer_0_size': 65, 'layer_1_size': 33, 'layer_2_size': 144, 'layer_3_size': 143, 'layer_4_size': 161, 'layer_5_size': 177, 'layer_6_size': 174, 'layer_7_size': 150, 'layer_8_size': 164, 'layer_9_size': 51, 'layer_10_size': 135, 'layer_11_size': 168, 'layer_12_size': 112, 'layer_13_size': 150, 'layer_14_size': 95, 'dropout_rate': 0.32274059731102384, 'learning_rate': 1.9269373008731597e-05, 'batch_size': 64, 'epochs': 38}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/38, Train Loss: 1.2223, Test Loss: 0.8853\n",
      "Epoch 38/38, Train Loss: 1.2340, Test Loss: 0.8815\n",
      "Epoch 1/50, Train Loss: 1.3573, Test Loss: 1.0167\n",
      "Epoch 2/50, Train Loss: 1.2364, Test Loss: 1.0249\n",
      "Epoch 3/50, Train Loss: 1.2034, Test Loss: 1.0249\n",
      "Epoch 4/50, Train Loss: 1.2258, Test Loss: 1.0054\n",
      "Epoch 5/50, Train Loss: 1.2774, Test Loss: 1.0020\n",
      "Epoch 6/50, Train Loss: 1.2832, Test Loss: 1.0248\n",
      "Epoch 7/50, Train Loss: 1.2363, Test Loss: 1.0017\n",
      "Epoch 8/50, Train Loss: 1.3079, Test Loss: 0.9953\n",
      "Epoch 9/50, Train Loss: 1.2586, Test Loss: 0.9964\n",
      "Epoch 10/50, Train Loss: 1.2628, Test Loss: 0.9906\n",
      "Epoch 11/50, Train Loss: 1.1933, Test Loss: 0.9971\n",
      "Epoch 12/50, Train Loss: 1.2125, Test Loss: 0.9944\n",
      "Epoch 13/50, Train Loss: 1.2191, Test Loss: 1.0024\n",
      "Epoch 14/50, Train Loss: 1.1898, Test Loss: 1.0001\n",
      "Epoch 15/50, Train Loss: 1.2106, Test Loss: 0.9983\n",
      "Epoch 16/50, Train Loss: 1.2213, Test Loss: 0.9970\n",
      "Epoch 17/50, Train Loss: 1.2239, Test Loss: 0.9988\n",
      "Epoch 18/50, Train Loss: 1.2165, Test Loss: 0.9999\n",
      "Epoch 19/50, Train Loss: 1.1751, Test Loss: 1.0027\n",
      "Epoch 20/50, Train Loss: 1.2041, Test Loss: 1.0011\n",
      "Epoch 21/50, Train Loss: 1.2011, Test Loss: 0.9934\n",
      "Epoch 22/50, Train Loss: 1.1833, Test Loss: 1.0000\n",
      "Epoch 23/50, Train Loss: 1.2259, Test Loss: 1.0028\n",
      "Epoch 24/50, Train Loss: 1.1596, Test Loss: 0.9917\n",
      "Epoch 25/50, Train Loss: 1.1909, Test Loss: 0.9907\n",
      "Epoch 26/50, Train Loss: 1.1596, Test Loss: 0.9796\n",
      "Epoch 27/50, Train Loss: 1.2247, Test Loss: 0.9885\n",
      "Epoch 28/50, Train Loss: 1.1767, Test Loss: 0.9881\n",
      "Epoch 29/50, Train Loss: 1.1775, Test Loss: 0.9885\n",
      "Epoch 30/50, Train Loss: 1.1971, Test Loss: 1.0038\n",
      "Epoch 31/50, Train Loss: 1.2307, Test Loss: 1.0046\n",
      "Epoch 32/50, Train Loss: 1.1817, Test Loss: 0.9924\n",
      "Epoch 33/50, Train Loss: 1.1950, Test Loss: 0.9896\n",
      "Epoch 34/50, Train Loss: 1.2014, Test Loss: 0.9896\n",
      "Epoch 35/50, Train Loss: 1.1436, Test Loss: 0.9938\n",
      "Epoch 36/50, Train Loss: 1.1829, Test Loss: 0.9945\n",
      "Epoch 37/50, Train Loss: 1.2071, Test Loss: 0.9914\n",
      "Epoch 38/50, Train Loss: 1.1548, Test Loss: 0.9898\n",
      "Epoch 39/50, Train Loss: 1.1894, Test Loss: 0.9907\n",
      "Epoch 40/50, Train Loss: 1.2051, Test Loss: 0.9807\n",
      "Epoch 41/50, Train Loss: 1.1450, Test Loss: 0.9810\n",
      "Epoch 42/50, Train Loss: 1.1238, Test Loss: 0.9775\n",
      "Epoch 43/50, Train Loss: 1.1680, Test Loss: 0.9863\n",
      "Epoch 44/50, Train Loss: 1.1997, Test Loss: 0.9768\n",
      "Epoch 45/50, Train Loss: 1.1840, Test Loss: 0.9828\n",
      "Epoch 46/50, Train Loss: 1.1802, Test Loss: 0.9928\n",
      "Epoch 47/50, Train Loss: 1.2012, Test Loss: 0.9863\n",
      "Epoch 48/50, Train Loss: 1.1415, Test Loss: 0.9893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:12:20,035] Trial 205 finished with value: 0.9762369564601353 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 203, 'layer_1_size': 57, 'layer_2_size': 137, 'layer_3_size': 120, 'layer_4_size': 149, 'layer_5_size': 165, 'layer_6_size': 186, 'dropout_rate': 0.31550122273339487, 'learning_rate': 1.249542120979191e-05, 'batch_size': 32, 'epochs': 50}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50, Train Loss: 1.1566, Test Loss: 0.9859\n",
      "Epoch 50/50, Train Loss: 1.1528, Test Loss: 0.9762\n",
      "Epoch 1/43, Train Loss: 1.1030, Test Loss: 1.0311\n",
      "Epoch 2/43, Train Loss: 1.1113, Test Loss: 1.0436\n",
      "Epoch 3/43, Train Loss: 1.1636, Test Loss: 1.0418\n",
      "Epoch 4/43, Train Loss: 1.0969, Test Loss: 1.0276\n",
      "Epoch 5/43, Train Loss: 1.1370, Test Loss: 1.0413\n",
      "Epoch 6/43, Train Loss: 1.1294, Test Loss: 1.0342\n",
      "Epoch 7/43, Train Loss: 1.1362, Test Loss: 1.0319\n",
      "Epoch 8/43, Train Loss: 1.0605, Test Loss: 1.0315\n",
      "Epoch 9/43, Train Loss: 1.0567, Test Loss: 1.0246\n",
      "Epoch 10/43, Train Loss: 1.1069, Test Loss: 1.0233\n",
      "Epoch 11/43, Train Loss: 1.0557, Test Loss: 1.0220\n",
      "Epoch 12/43, Train Loss: 1.0739, Test Loss: 1.0233\n",
      "Epoch 13/43, Train Loss: 1.0380, Test Loss: 1.0184\n",
      "Epoch 14/43, Train Loss: 1.1260, Test Loss: 1.0240\n",
      "Epoch 15/43, Train Loss: 1.0627, Test Loss: 1.0401\n",
      "Epoch 16/43, Train Loss: 1.0661, Test Loss: 1.0476\n",
      "Epoch 17/43, Train Loss: 1.1071, Test Loss: 1.0220\n",
      "Epoch 18/43, Train Loss: 1.0882, Test Loss: 1.0213\n",
      "Epoch 19/43, Train Loss: 1.0653, Test Loss: 1.0265\n",
      "Epoch 20/43, Train Loss: 1.0387, Test Loss: 1.0148\n",
      "Epoch 21/43, Train Loss: 1.1212, Test Loss: 1.0271\n",
      "Epoch 22/43, Train Loss: 1.0650, Test Loss: 1.0191\n",
      "Epoch 23/43, Train Loss: 1.0911, Test Loss: 1.0137\n",
      "Epoch 24/43, Train Loss: 1.0515, Test Loss: 1.0159\n",
      "Epoch 25/43, Train Loss: 1.0226, Test Loss: 1.0162\n",
      "Epoch 26/43, Train Loss: 1.0640, Test Loss: 1.0163\n",
      "Epoch 27/43, Train Loss: 1.0528, Test Loss: 1.0161\n",
      "Epoch 28/43, Train Loss: 1.0650, Test Loss: 1.0165\n",
      "Epoch 29/43, Train Loss: 1.0740, Test Loss: 1.0134\n",
      "Epoch 30/43, Train Loss: 1.0268, Test Loss: 1.0186\n",
      "Epoch 31/43, Train Loss: 1.0716, Test Loss: 1.0111\n",
      "Epoch 32/43, Train Loss: 1.0669, Test Loss: 1.0185\n",
      "Epoch 33/43, Train Loss: 1.0104, Test Loss: 1.0169\n",
      "Epoch 34/43, Train Loss: 1.0257, Test Loss: 1.0117\n",
      "Epoch 35/43, Train Loss: 1.0827, Test Loss: 1.0113\n",
      "Epoch 36/43, Train Loss: 1.0289, Test Loss: 1.0166\n",
      "Epoch 37/43, Train Loss: 1.0299, Test Loss: 1.0224\n",
      "Epoch 38/43, Train Loss: 1.0321, Test Loss: 1.0179\n",
      "Epoch 39/43, Train Loss: 1.0789, Test Loss: 1.0162\n",
      "Epoch 40/43, Train Loss: 0.9896, Test Loss: 1.0201\n",
      "Epoch 41/43, Train Loss: 1.0815, Test Loss: 1.0172\n",
      "Epoch 42/43, Train Loss: 1.0318, Test Loss: 1.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:12:31,238] Trial 206 finished with value: 1.0119990365845817 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 216, 'layer_1_size': 112, 'layer_2_size': 147, 'layer_3_size': 134, 'layer_4_size': 156, 'layer_5_size': 79, 'layer_6_size': 93, 'layer_7_size': 155, 'layer_8_size': 108, 'layer_9_size': 59, 'layer_10_size': 237, 'layer_11_size': 174, 'layer_12_size': 134, 'layer_13_size': 182, 'dropout_rate': 0.2597293223037969, 'learning_rate': 1.109507708716596e-05, 'batch_size': 32, 'epochs': 43}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/43, Train Loss: 1.0300, Test Loss: 1.0120\n",
      "Epoch 1/40, Train Loss: 1.1527, Test Loss: 1.0411\n",
      "Epoch 2/40, Train Loss: 1.1613, Test Loss: 1.0412\n",
      "Epoch 3/40, Train Loss: 1.1476, Test Loss: 1.0394\n",
      "Epoch 4/40, Train Loss: 1.1472, Test Loss: 1.0372\n",
      "Epoch 5/40, Train Loss: 1.1128, Test Loss: 1.0363\n",
      "Epoch 6/40, Train Loss: 1.0726, Test Loss: 1.0339\n",
      "Epoch 7/40, Train Loss: 1.0591, Test Loss: 1.0298\n",
      "Epoch 8/40, Train Loss: 1.0805, Test Loss: 1.0251\n",
      "Epoch 9/40, Train Loss: 1.0922, Test Loss: 1.0228\n",
      "Epoch 10/40, Train Loss: 1.1079, Test Loss: 1.0231\n",
      "Epoch 11/40, Train Loss: 1.1068, Test Loss: 1.0229\n",
      "Epoch 12/40, Train Loss: 1.0866, Test Loss: 1.0239\n",
      "Epoch 13/40, Train Loss: 1.1007, Test Loss: 1.0266\n",
      "Epoch 14/40, Train Loss: 1.0972, Test Loss: 1.0280\n",
      "Epoch 15/40, Train Loss: 1.0796, Test Loss: 1.0275\n",
      "Epoch 16/40, Train Loss: 1.0666, Test Loss: 1.0262\n",
      "Epoch 17/40, Train Loss: 1.0562, Test Loss: 1.0245\n",
      "Epoch 18/40, Train Loss: 1.0594, Test Loss: 1.0236\n",
      "Epoch 19/40, Train Loss: 1.0415, Test Loss: 1.0253\n",
      "Epoch 20/40, Train Loss: 1.0704, Test Loss: 1.0260\n",
      "Epoch 21/40, Train Loss: 1.0324, Test Loss: 1.0260\n",
      "Epoch 22/40, Train Loss: 1.0594, Test Loss: 1.0272\n",
      "Epoch 23/40, Train Loss: 1.0786, Test Loss: 1.0282\n",
      "Epoch 24/40, Train Loss: 1.0511, Test Loss: 1.0289\n",
      "Epoch 25/40, Train Loss: 1.0527, Test Loss: 1.0301\n",
      "Epoch 26/40, Train Loss: 1.0251, Test Loss: 1.0304\n",
      "Epoch 27/40, Train Loss: 1.0448, Test Loss: 1.0299\n",
      "Epoch 28/40, Train Loss: 1.0391, Test Loss: 1.0301\n",
      "Epoch 29/40, Train Loss: 1.0329, Test Loss: 1.0290\n",
      "Epoch 30/40, Train Loss: 1.0379, Test Loss: 1.0271\n",
      "Epoch 31/40, Train Loss: 1.0191, Test Loss: 1.0270\n",
      "Epoch 32/40, Train Loss: 1.0400, Test Loss: 1.0265\n",
      "Epoch 33/40, Train Loss: 1.0516, Test Loss: 1.0267\n",
      "Epoch 34/40, Train Loss: 1.0227, Test Loss: 1.0251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:12:32,220] Trial 207 finished with value: 1.0323622226715088 and parameters: {'num_hidden_layers': 5, 'layer_0_size': 189, 'layer_1_size': 79, 'layer_2_size': 154, 'layer_3_size': 107, 'layer_4_size': 143, 'dropout_rate': 0.30167022818374767, 'learning_rate': 0.00040106354345737926, 'batch_size': 256, 'epochs': 40}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/40, Train Loss: 1.0213, Test Loss: 1.0249\n",
      "Epoch 36/40, Train Loss: 1.0212, Test Loss: 1.0272\n",
      "Epoch 37/40, Train Loss: 1.0088, Test Loss: 1.0292\n",
      "Epoch 38/40, Train Loss: 1.0274, Test Loss: 1.0303\n",
      "Epoch 39/40, Train Loss: 0.9898, Test Loss: 1.0310\n",
      "Epoch 40/40, Train Loss: 0.9931, Test Loss: 1.0324\n",
      "Epoch 1/30, Train Loss: 1.0889, Test Loss: 1.2320\n",
      "Epoch 2/30, Train Loss: 1.1222, Test Loss: 1.2109\n",
      "Epoch 3/30, Train Loss: 1.1039, Test Loss: 1.2389\n",
      "Epoch 4/30, Train Loss: 1.0975, Test Loss: 1.2379\n",
      "Epoch 5/30, Train Loss: 1.0771, Test Loss: 1.2410\n",
      "Epoch 6/30, Train Loss: 1.1099, Test Loss: 1.2309\n",
      "Epoch 7/30, Train Loss: 1.0879, Test Loss: 1.2362\n",
      "Epoch 8/30, Train Loss: 1.0893, Test Loss: 1.2244\n",
      "Epoch 9/30, Train Loss: 1.0928, Test Loss: 1.2334\n",
      "Epoch 10/30, Train Loss: 1.1428, Test Loss: 1.2392\n",
      "Epoch 11/30, Train Loss: 1.1025, Test Loss: 1.2305\n",
      "Epoch 12/30, Train Loss: 1.0899, Test Loss: 1.2146\n",
      "Epoch 13/30, Train Loss: 1.0646, Test Loss: 1.2268\n",
      "Epoch 14/30, Train Loss: 1.0678, Test Loss: 1.2078\n",
      "Epoch 15/30, Train Loss: 1.0818, Test Loss: 1.2169\n",
      "Epoch 16/30, Train Loss: 1.0874, Test Loss: 1.2285\n",
      "Epoch 17/30, Train Loss: 1.1497, Test Loss: 1.2067\n",
      "Epoch 18/30, Train Loss: 1.0796, Test Loss: 1.2395\n",
      "Epoch 19/30, Train Loss: 1.0668, Test Loss: 1.2235\n",
      "Epoch 20/30, Train Loss: 1.1044, Test Loss: 1.2188\n",
      "Epoch 21/30, Train Loss: 1.0801, Test Loss: 1.2285\n",
      "Epoch 22/30, Train Loss: 1.0785, Test Loss: 1.2277\n",
      "Epoch 23/30, Train Loss: 1.0648, Test Loss: 1.2250\n",
      "Epoch 24/30, Train Loss: 1.0134, Test Loss: 1.2144\n",
      "Epoch 25/30, Train Loss: 1.0867, Test Loss: 1.2252\n",
      "Epoch 26/30, Train Loss: 1.0820, Test Loss: 1.2163\n",
      "Epoch 27/30, Train Loss: 1.0756, Test Loss: 1.2257\n",
      "Epoch 28/30, Train Loss: 1.0618, Test Loss: 1.2169\n",
      "Epoch 29/30, Train Loss: 1.0231, Test Loss: 1.2283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:12:39,129] Trial 208 finished with value: 1.215706901890891 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 207, 'layer_1_size': 74, 'layer_2_size': 214, 'layer_3_size': 154, 'layer_4_size': 237, 'layer_5_size': 121, 'layer_6_size': 127, 'layer_7_size': 109, 'layer_8_size': 181, 'layer_9_size': 40, 'layer_10_size': 245, 'layer_11_size': 157, 'layer_12_size': 143, 'dropout_rate': 0.161088208822878, 'learning_rate': 1.0009646273609984e-05, 'batch_size': 32, 'epochs': 30}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30, Train Loss: 1.0949, Test Loss: 1.2157\n",
      "Epoch 1/10, Train Loss: 1.1440, Test Loss: 1.0768\n",
      "Epoch 2/10, Train Loss: 1.1858, Test Loss: 1.0858\n",
      "Epoch 3/10, Train Loss: 1.1011, Test Loss: 1.0921\n",
      "Epoch 4/10, Train Loss: 1.1854, Test Loss: 1.0830\n",
      "Epoch 5/10, Train Loss: 1.1327, Test Loss: 1.0819\n",
      "Epoch 6/10, Train Loss: 1.1134, Test Loss: 1.0900\n",
      "Epoch 7/10, Train Loss: 1.1201, Test Loss: 1.1007\n",
      "Epoch 8/10, Train Loss: 1.0922, Test Loss: 1.0925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:12:40,832] Trial 209 finished with value: 1.081275028841836 and parameters: {'num_hidden_layers': 11, 'layer_0_size': 222, 'layer_1_size': 32, 'layer_2_size': 131, 'layer_3_size': 131, 'layer_4_size': 120, 'layer_5_size': 86, 'layer_6_size': 49, 'layer_7_size': 142, 'layer_8_size': 146, 'layer_9_size': 234, 'layer_10_size': 187, 'dropout_rate': 0.24785334468488063, 'learning_rate': 0.0003206341127996551, 'batch_size': 32, 'epochs': 10}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Train Loss: 1.0973, Test Loss: 1.0760\n",
      "Epoch 10/10, Train Loss: 1.1060, Test Loss: 1.0813\n",
      "Epoch 1/55, Train Loss: 1.2191, Test Loss: 0.9082\n",
      "Epoch 2/55, Train Loss: 1.1603, Test Loss: 0.9124\n",
      "Epoch 3/55, Train Loss: 1.1020, Test Loss: 0.9154\n",
      "Epoch 4/55, Train Loss: 1.1095, Test Loss: 0.9216\n",
      "Epoch 5/55, Train Loss: 1.0964, Test Loss: 0.9274\n",
      "Epoch 6/55, Train Loss: 1.1029, Test Loss: 0.9337\n",
      "Epoch 7/55, Train Loss: 1.0708, Test Loss: 0.9316\n",
      "Epoch 8/55, Train Loss: 1.0916, Test Loss: 0.9224\n",
      "Epoch 9/55, Train Loss: 1.0596, Test Loss: 0.9144\n",
      "Epoch 10/55, Train Loss: 1.1035, Test Loss: 0.9124\n",
      "Epoch 11/55, Train Loss: 1.0725, Test Loss: 0.9129\n",
      "Epoch 12/55, Train Loss: 1.0625, Test Loss: 0.9176\n",
      "Epoch 13/55, Train Loss: 1.1100, Test Loss: 0.9236\n",
      "Epoch 14/55, Train Loss: 1.0620, Test Loss: 0.9296\n",
      "Epoch 15/55, Train Loss: 1.0592, Test Loss: 0.9357\n",
      "Epoch 16/55, Train Loss: 1.0633, Test Loss: 0.9379\n",
      "Epoch 17/55, Train Loss: 1.1001, Test Loss: 0.9339\n",
      "Epoch 18/55, Train Loss: 1.0702, Test Loss: 0.9306\n",
      "Epoch 19/55, Train Loss: 1.0583, Test Loss: 0.9214\n",
      "Epoch 20/55, Train Loss: 1.0654, Test Loss: 0.9141\n",
      "Epoch 21/55, Train Loss: 1.0397, Test Loss: 0.9111\n",
      "Epoch 22/55, Train Loss: 1.0576, Test Loss: 0.9108\n",
      "Epoch 23/55, Train Loss: 1.0366, Test Loss: 0.9123\n",
      "Epoch 24/55, Train Loss: 1.0704, Test Loss: 0.9149\n",
      "Epoch 25/55, Train Loss: 1.0689, Test Loss: 0.9149\n",
      "Epoch 26/55, Train Loss: 1.0510, Test Loss: 0.9160\n",
      "Epoch 27/55, Train Loss: 1.0432, Test Loss: 0.9155\n",
      "Epoch 28/55, Train Loss: 1.0421, Test Loss: 0.9144\n",
      "Epoch 29/55, Train Loss: 1.0443, Test Loss: 0.9145\n",
      "Epoch 30/55, Train Loss: 1.0411, Test Loss: 0.9154\n",
      "Epoch 31/55, Train Loss: 1.0262, Test Loss: 0.9140\n",
      "Epoch 32/55, Train Loss: 1.0269, Test Loss: 0.9130\n",
      "Epoch 33/55, Train Loss: 1.0477, Test Loss: 0.9095\n",
      "Epoch 34/55, Train Loss: 1.0377, Test Loss: 0.9086\n",
      "Epoch 35/55, Train Loss: 1.0296, Test Loss: 0.9087\n",
      "Epoch 36/55, Train Loss: 1.0243, Test Loss: 0.9084\n",
      "Epoch 37/55, Train Loss: 1.0434, Test Loss: 0.9091\n",
      "Epoch 38/55, Train Loss: 1.0164, Test Loss: 0.9118\n",
      "Epoch 39/55, Train Loss: 1.0217, Test Loss: 0.9140\n",
      "Epoch 40/55, Train Loss: 1.0320, Test Loss: 0.9153\n",
      "Epoch 41/55, Train Loss: 1.0225, Test Loss: 0.9137\n",
      "Epoch 42/55, Train Loss: 1.0254, Test Loss: 0.9128\n",
      "Epoch 43/55, Train Loss: 1.0288, Test Loss: 0.9120\n",
      "Epoch 44/55, Train Loss: 1.0180, Test Loss: 0.9116\n",
      "Epoch 45/55, Train Loss: 1.0183, Test Loss: 0.9098\n",
      "Epoch 46/55, Train Loss: 1.0206, Test Loss: 0.9099\n",
      "Epoch 47/55, Train Loss: 1.0351, Test Loss: 0.9092\n",
      "Epoch 48/55, Train Loss: 1.0354, Test Loss: 0.9075\n",
      "Epoch 49/55, Train Loss: 1.0343, Test Loss: 0.9070\n",
      "Epoch 50/55, Train Loss: 1.0298, Test Loss: 0.9069\n",
      "Epoch 51/55, Train Loss: 1.0433, Test Loss: 0.9064\n",
      "Epoch 52/55, Train Loss: 1.0263, Test Loss: 0.9073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:12:44,026] Trial 210 finished with value: 0.9113696217536926 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 85, 'layer_1_size': 64, 'layer_2_size': 171, 'layer_3_size': 112, 'layer_4_size': 105, 'layer_5_size': 141, 'layer_6_size': 83, 'layer_7_size': 249, 'layer_8_size': 176, 'layer_9_size': 32, 'layer_10_size': 220, 'layer_11_size': 152, 'layer_12_size': 149, 'layer_13_size': 106, 'dropout_rate': 0.21746646602518738, 'learning_rate': 0.0005061624960128847, 'batch_size': 256, 'epochs': 55}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/55, Train Loss: 1.0308, Test Loss: 0.9087\n",
      "Epoch 54/55, Train Loss: 1.0242, Test Loss: 0.9096\n",
      "Epoch 55/55, Train Loss: 1.0224, Test Loss: 0.9114\n",
      "Epoch 1/57, Train Loss: 1.2314, Test Loss: 0.8805\n",
      "Epoch 2/57, Train Loss: 1.2008, Test Loss: 0.8786\n",
      "Epoch 3/57, Train Loss: 1.2104, Test Loss: 0.8775\n",
      "Epoch 4/57, Train Loss: 1.2294, Test Loss: 0.8770\n",
      "Epoch 5/57, Train Loss: 1.2035, Test Loss: 0.8772\n",
      "Epoch 6/57, Train Loss: 1.1866, Test Loss: 0.8777\n",
      "Epoch 7/57, Train Loss: 1.1231, Test Loss: 0.8779\n",
      "Epoch 8/57, Train Loss: 1.1426, Test Loss: 0.8783\n",
      "Epoch 9/57, Train Loss: 1.1621, Test Loss: 0.8783\n",
      "Epoch 10/57, Train Loss: 1.1420, Test Loss: 0.8790\n",
      "Epoch 11/57, Train Loss: 1.1235, Test Loss: 0.8806\n",
      "Epoch 12/57, Train Loss: 1.1591, Test Loss: 0.8817\n",
      "Epoch 13/57, Train Loss: 1.0998, Test Loss: 0.8825\n",
      "Epoch 14/57, Train Loss: 1.1482, Test Loss: 0.8822\n",
      "Epoch 15/57, Train Loss: 1.1426, Test Loss: 0.8820\n",
      "Epoch 16/57, Train Loss: 1.1075, Test Loss: 0.8812\n",
      "Epoch 17/57, Train Loss: 1.1333, Test Loss: 0.8817\n",
      "Epoch 18/57, Train Loss: 1.1098, Test Loss: 0.8825\n",
      "Epoch 19/57, Train Loss: 1.1310, Test Loss: 0.8827\n",
      "Epoch 20/57, Train Loss: 1.1144, Test Loss: 0.8836\n",
      "Epoch 21/57, Train Loss: 1.1173, Test Loss: 0.8846\n",
      "Epoch 22/57, Train Loss: 1.1025, Test Loss: 0.8853\n",
      "Epoch 23/57, Train Loss: 1.1159, Test Loss: 0.8857\n",
      "Epoch 24/57, Train Loss: 1.1222, Test Loss: 0.8861\n",
      "Epoch 25/57, Train Loss: 1.1001, Test Loss: 0.8862\n",
      "Epoch 26/57, Train Loss: 1.1314, Test Loss: 0.8861\n",
      "Epoch 27/57, Train Loss: 1.1131, Test Loss: 0.8872\n",
      "Epoch 28/57, Train Loss: 1.0901, Test Loss: 0.8878\n",
      "Epoch 29/57, Train Loss: 1.1034, Test Loss: 0.8874\n",
      "Epoch 30/57, Train Loss: 1.1038, Test Loss: 0.8867\n",
      "Epoch 31/57, Train Loss: 1.1433, Test Loss: 0.8841\n",
      "Epoch 32/57, Train Loss: 1.1100, Test Loss: 0.8825\n",
      "Epoch 33/57, Train Loss: 1.0558, Test Loss: 0.8809\n",
      "Epoch 34/57, Train Loss: 1.1048, Test Loss: 0.8802\n",
      "Epoch 35/57, Train Loss: 1.1223, Test Loss: 0.8796\n",
      "Epoch 36/57, Train Loss: 1.0744, Test Loss: 0.8795\n",
      "Epoch 37/57, Train Loss: 1.1019, Test Loss: 0.8789\n",
      "Epoch 38/57, Train Loss: 1.0579, Test Loss: 0.8797\n",
      "Epoch 39/57, Train Loss: 1.0982, Test Loss: 0.8789\n",
      "Epoch 40/57, Train Loss: 1.1000, Test Loss: 0.8790\n",
      "Epoch 41/57, Train Loss: 1.0949, Test Loss: 0.8782\n",
      "Epoch 42/57, Train Loss: 1.1090, Test Loss: 0.8781\n",
      "Epoch 43/57, Train Loss: 1.0699, Test Loss: 0.8780\n",
      "Epoch 44/57, Train Loss: 1.0604, Test Loss: 0.8779\n",
      "Epoch 45/57, Train Loss: 1.0915, Test Loss: 0.8775\n",
      "Epoch 46/57, Train Loss: 1.0766, Test Loss: 0.8773\n",
      "Epoch 47/57, Train Loss: 1.0764, Test Loss: 0.8771\n",
      "Epoch 48/57, Train Loss: 1.0922, Test Loss: 0.8773\n",
      "Epoch 49/57, Train Loss: 1.0595, Test Loss: 0.8772\n",
      "Epoch 50/57, Train Loss: 1.0606, Test Loss: 0.8767\n",
      "Epoch 51/57, Train Loss: 1.0907, Test Loss: 0.8765\n",
      "Epoch 52/57, Train Loss: 1.0686, Test Loss: 0.8762\n",
      "Epoch 53/57, Train Loss: 1.0516, Test Loss: 0.8760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:12:47,827] Trial 211 finished with value: 0.8763940930366516 and parameters: {'num_hidden_layers': 15, 'layer_0_size': 129, 'layer_1_size': 129, 'layer_2_size': 250, 'layer_3_size': 138, 'layer_4_size': 169, 'layer_5_size': 158, 'layer_6_size': 201, 'layer_7_size': 137, 'layer_8_size': 160, 'layer_9_size': 103, 'layer_10_size': 233, 'layer_11_size': 141, 'layer_12_size': 53, 'layer_13_size': 169, 'layer_14_size': 62, 'dropout_rate': 0.2897946380856808, 'learning_rate': 0.00025689500952521784, 'batch_size': 256, 'epochs': 57}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/57, Train Loss: 1.1005, Test Loss: 0.8759\n",
      "Epoch 55/57, Train Loss: 1.0706, Test Loss: 0.8760\n",
      "Epoch 56/57, Train Loss: 1.0804, Test Loss: 0.8764\n",
      "Epoch 57/57, Train Loss: 1.0613, Test Loss: 0.8764\n",
      "Epoch 1/52, Train Loss: 1.2264, Test Loss: 0.9452\n",
      "Epoch 2/52, Train Loss: 1.2131, Test Loss: 0.9449\n",
      "Epoch 3/52, Train Loss: 1.1985, Test Loss: 0.9457\n",
      "Epoch 4/52, Train Loss: 1.1832, Test Loss: 0.9485\n",
      "Epoch 5/52, Train Loss: 1.1445, Test Loss: 0.9525\n",
      "Epoch 6/52, Train Loss: 1.1360, Test Loss: 0.9548\n",
      "Epoch 7/52, Train Loss: 1.1740, Test Loss: 0.9566\n",
      "Epoch 8/52, Train Loss: 1.1655, Test Loss: 0.9558\n",
      "Epoch 9/52, Train Loss: 1.1243, Test Loss: 0.9543\n",
      "Epoch 10/52, Train Loss: 1.1313, Test Loss: 0.9515\n",
      "Epoch 11/52, Train Loss: 1.1321, Test Loss: 0.9479\n",
      "Epoch 12/52, Train Loss: 1.1161, Test Loss: 0.9457\n",
      "Epoch 13/52, Train Loss: 1.1166, Test Loss: 0.9445\n",
      "Epoch 14/52, Train Loss: 1.0938, Test Loss: 0.9426\n",
      "Epoch 15/52, Train Loss: 1.1008, Test Loss: 0.9426\n",
      "Epoch 16/52, Train Loss: 1.1032, Test Loss: 0.9413\n",
      "Epoch 17/52, Train Loss: 1.0939, Test Loss: 0.9425\n",
      "Epoch 18/52, Train Loss: 1.0952, Test Loss: 0.9431\n",
      "Epoch 19/52, Train Loss: 1.1197, Test Loss: 0.9436\n",
      "Epoch 20/52, Train Loss: 1.1348, Test Loss: 0.9454\n",
      "Epoch 21/52, Train Loss: 1.0998, Test Loss: 0.9444\n",
      "Epoch 22/52, Train Loss: 1.1068, Test Loss: 0.9431\n",
      "Epoch 23/52, Train Loss: 1.0861, Test Loss: 0.9435\n",
      "Epoch 24/52, Train Loss: 1.0969, Test Loss: 0.9443\n",
      "Epoch 25/52, Train Loss: 1.1321, Test Loss: 0.9461\n",
      "Epoch 26/52, Train Loss: 1.0859, Test Loss: 0.9466\n",
      "Epoch 27/52, Train Loss: 1.1018, Test Loss: 0.9474\n",
      "Epoch 28/52, Train Loss: 1.0978, Test Loss: 0.9471\n",
      "Epoch 29/52, Train Loss: 1.1097, Test Loss: 0.9472\n",
      "Epoch 30/52, Train Loss: 1.0847, Test Loss: 0.9469\n",
      "Epoch 31/52, Train Loss: 1.0622, Test Loss: 0.9463\n",
      "Epoch 32/52, Train Loss: 1.1017, Test Loss: 0.9454\n",
      "Epoch 33/52, Train Loss: 1.0980, Test Loss: 0.9444\n",
      "Epoch 34/52, Train Loss: 1.0949, Test Loss: 0.9442\n",
      "Epoch 35/52, Train Loss: 1.0929, Test Loss: 0.9464\n",
      "Epoch 36/52, Train Loss: 1.0823, Test Loss: 0.9463\n",
      "Epoch 37/52, Train Loss: 1.1159, Test Loss: 0.9464\n",
      "Epoch 38/52, Train Loss: 1.0849, Test Loss: 0.9468\n",
      "Epoch 39/52, Train Loss: 1.1083, Test Loss: 0.9465\n",
      "Epoch 40/52, Train Loss: 1.0879, Test Loss: 0.9467\n",
      "Epoch 41/52, Train Loss: 1.1059, Test Loss: 0.9471\n",
      "Epoch 42/52, Train Loss: 1.0851, Test Loss: 0.9466\n",
      "Epoch 43/52, Train Loss: 1.0949, Test Loss: 0.9458\n",
      "Epoch 44/52, Train Loss: 1.0674, Test Loss: 0.9452\n",
      "Epoch 45/52, Train Loss: 1.0705, Test Loss: 0.9466\n",
      "Epoch 46/52, Train Loss: 1.0875, Test Loss: 0.9469\n",
      "Epoch 47/52, Train Loss: 1.0928, Test Loss: 0.9464\n",
      "Epoch 48/52, Train Loss: 1.0746, Test Loss: 0.9442\n",
      "Epoch 49/52, Train Loss: 1.1000, Test Loss: 0.9441\n",
      "Epoch 50/52, Train Loss: 1.0630, Test Loss: 0.9451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:12:51,120] Trial 212 finished with value: 0.9447627067565918 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 174, 'layer_1_size': 69, 'layer_2_size': 113, 'layer_3_size': 149, 'layer_4_size': 64, 'layer_5_size': 195, 'layer_6_size': 195, 'layer_7_size': 113, 'layer_8_size': 153, 'layer_9_size': 70, 'layer_10_size': 229, 'layer_11_size': 132, 'layer_12_size': 82, 'layer_13_size': 182, 'dropout_rate': 0.29716178807836563, 'learning_rate': 0.0002812289652162501, 'batch_size': 256, 'epochs': 52}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/52, Train Loss: 1.0625, Test Loss: 0.9449\n",
      "Epoch 52/52, Train Loss: 1.0724, Test Loss: 0.9448\n",
      "Epoch 1/37, Train Loss: 1.1434, Test Loss: 0.9147\n",
      "Epoch 2/37, Train Loss: 1.1566, Test Loss: 0.9110\n",
      "Epoch 3/37, Train Loss: 1.1298, Test Loss: 0.9090\n",
      "Epoch 4/37, Train Loss: 1.1581, Test Loss: 0.9087\n",
      "Epoch 5/37, Train Loss: 1.1499, Test Loss: 0.9095\n",
      "Epoch 6/37, Train Loss: 1.1101, Test Loss: 0.9109\n",
      "Epoch 7/37, Train Loss: 1.1587, Test Loss: 0.9133\n",
      "Epoch 8/37, Train Loss: 1.1011, Test Loss: 0.9111\n",
      "Epoch 9/37, Train Loss: 1.1016, Test Loss: 0.9083\n",
      "Epoch 10/37, Train Loss: 1.0983, Test Loss: 0.9100\n",
      "Epoch 11/37, Train Loss: 1.0937, Test Loss: 0.9095\n",
      "Epoch 12/37, Train Loss: 1.1001, Test Loss: 0.9083\n",
      "Epoch 13/37, Train Loss: 1.1331, Test Loss: 0.9098\n",
      "Epoch 14/37, Train Loss: 1.0937, Test Loss: 0.9111\n",
      "Epoch 15/37, Train Loss: 1.1213, Test Loss: 0.9099\n",
      "Epoch 16/37, Train Loss: 1.1119, Test Loss: 0.9105\n",
      "Epoch 17/37, Train Loss: 1.0686, Test Loss: 0.9094\n",
      "Epoch 18/37, Train Loss: 1.0929, Test Loss: 0.9094\n",
      "Epoch 19/37, Train Loss: 1.0798, Test Loss: 0.9116\n",
      "Epoch 20/37, Train Loss: 1.1041, Test Loss: 0.9122\n",
      "Epoch 21/37, Train Loss: 1.0711, Test Loss: 0.9141\n",
      "Epoch 22/37, Train Loss: 1.0688, Test Loss: 0.9155\n",
      "Epoch 23/37, Train Loss: 1.0819, Test Loss: 0.9157\n",
      "Epoch 24/37, Train Loss: 1.0635, Test Loss: 0.9159\n",
      "Epoch 25/37, Train Loss: 1.0579, Test Loss: 0.9156\n",
      "Epoch 26/37, Train Loss: 1.0655, Test Loss: 0.9151\n",
      "Epoch 27/37, Train Loss: 1.0786, Test Loss: 0.9141\n",
      "Epoch 28/37, Train Loss: 1.0580, Test Loss: 0.9150\n",
      "Epoch 29/37, Train Loss: 1.0663, Test Loss: 0.9147\n",
      "Epoch 30/37, Train Loss: 1.0706, Test Loss: 0.9144\n",
      "Epoch 31/37, Train Loss: 1.0789, Test Loss: 0.9139\n",
      "Epoch 32/37, Train Loss: 1.1050, Test Loss: 0.9129\n",
      "Epoch 33/37, Train Loss: 1.0552, Test Loss: 0.9118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:12:53,731] Trial 213 finished with value: 0.9108723402023315 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 144, 'layer_1_size': 120, 'layer_2_size': 142, 'layer_3_size': 140, 'layer_4_size': 85, 'layer_5_size': 197, 'layer_6_size': 190, 'layer_7_size': 255, 'layer_8_size': 163, 'layer_9_size': 95, 'layer_10_size': 181, 'layer_11_size': 145, 'layer_12_size': 124, 'layer_13_size': 206, 'dropout_rate': 0.23623870244322837, 'learning_rate': 0.00022358634476847168, 'batch_size': 256, 'epochs': 37}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/37, Train Loss: 1.0946, Test Loss: 0.9111\n",
      "Epoch 35/37, Train Loss: 1.0658, Test Loss: 0.9107\n",
      "Epoch 36/37, Train Loss: 1.0894, Test Loss: 0.9102\n",
      "Epoch 37/37, Train Loss: 1.0828, Test Loss: 0.9109\n",
      "Epoch 1/60, Train Loss: 1.3647, Test Loss: 0.9637\n",
      "Epoch 2/60, Train Loss: 1.3108, Test Loss: 0.9756\n",
      "Epoch 3/60, Train Loss: 1.1717, Test Loss: 0.9767\n",
      "Epoch 4/60, Train Loss: 1.1304, Test Loss: 0.9697\n",
      "Epoch 5/60, Train Loss: 1.1561, Test Loss: 0.9612\n",
      "Epoch 6/60, Train Loss: 1.1348, Test Loss: 0.9564\n",
      "Epoch 7/60, Train Loss: 1.1277, Test Loss: 0.9583\n",
      "Epoch 8/60, Train Loss: 1.1069, Test Loss: 0.9643\n",
      "Epoch 9/60, Train Loss: 1.1109, Test Loss: 0.9667\n",
      "Epoch 10/60, Train Loss: 1.1329, Test Loss: 0.9635\n",
      "Epoch 11/60, Train Loss: 1.0875, Test Loss: 0.9604\n",
      "Epoch 12/60, Train Loss: 1.1191, Test Loss: 0.9582\n",
      "Epoch 13/60, Train Loss: 1.1114, Test Loss: 0.9541\n",
      "Epoch 14/60, Train Loss: 1.0705, Test Loss: 0.9536\n",
      "Epoch 15/60, Train Loss: 1.0868, Test Loss: 0.9543\n",
      "Epoch 16/60, Train Loss: 1.0832, Test Loss: 0.9487\n",
      "Epoch 17/60, Train Loss: 1.0735, Test Loss: 0.9433\n",
      "Epoch 18/60, Train Loss: 1.1018, Test Loss: 0.9390\n",
      "Epoch 19/60, Train Loss: 1.0641, Test Loss: 0.9361\n",
      "Epoch 20/60, Train Loss: 1.0724, Test Loss: 0.9366\n",
      "Epoch 21/60, Train Loss: 1.0790, Test Loss: 0.9369\n",
      "Epoch 22/60, Train Loss: 1.0674, Test Loss: 0.9379\n",
      "Epoch 23/60, Train Loss: 1.0304, Test Loss: 0.9419\n",
      "Epoch 24/60, Train Loss: 1.0536, Test Loss: 0.9464\n",
      "Epoch 25/60, Train Loss: 1.0587, Test Loss: 0.9520\n",
      "Epoch 26/60, Train Loss: 1.0432, Test Loss: 0.9567\n",
      "Epoch 27/60, Train Loss: 1.0376, Test Loss: 0.9604\n",
      "Epoch 28/60, Train Loss: 1.0492, Test Loss: 0.9596\n",
      "Epoch 29/60, Train Loss: 1.0529, Test Loss: 0.9595\n",
      "Epoch 30/60, Train Loss: 1.0477, Test Loss: 0.9594\n",
      "Epoch 31/60, Train Loss: 1.0469, Test Loss: 0.9585\n",
      "Epoch 32/60, Train Loss: 1.0322, Test Loss: 0.9569\n",
      "Epoch 33/60, Train Loss: 1.0445, Test Loss: 0.9525\n",
      "Epoch 34/60, Train Loss: 1.0382, Test Loss: 0.9488\n",
      "Epoch 35/60, Train Loss: 1.0337, Test Loss: 0.9500\n",
      "Epoch 36/60, Train Loss: 1.0296, Test Loss: 0.9508\n",
      "Epoch 37/60, Train Loss: 1.0454, Test Loss: 0.9502\n",
      "Epoch 38/60, Train Loss: 1.0346, Test Loss: 0.9465\n",
      "Epoch 39/60, Train Loss: 1.0495, Test Loss: 0.9467\n",
      "Epoch 40/60, Train Loss: 1.0242, Test Loss: 0.9474\n",
      "Epoch 41/60, Train Loss: 1.0388, Test Loss: 0.9496\n",
      "Epoch 42/60, Train Loss: 1.0125, Test Loss: 0.9518\n",
      "Epoch 43/60, Train Loss: 1.0373, Test Loss: 0.9483\n",
      "Epoch 44/60, Train Loss: 1.0136, Test Loss: 0.9467\n",
      "Epoch 45/60, Train Loss: 1.0144, Test Loss: 0.9466\n",
      "Epoch 46/60, Train Loss: 1.0244, Test Loss: 0.9447\n",
      "Epoch 47/60, Train Loss: 1.0276, Test Loss: 0.9446\n",
      "Epoch 48/60, Train Loss: 1.0130, Test Loss: 0.9470\n",
      "Epoch 49/60, Train Loss: 1.0135, Test Loss: 0.9478\n",
      "Epoch 50/60, Train Loss: 1.0190, Test Loss: 0.9497\n",
      "Epoch 51/60, Train Loss: 1.0018, Test Loss: 0.9516\n",
      "Epoch 52/60, Train Loss: 1.0039, Test Loss: 0.9538\n",
      "Epoch 53/60, Train Loss: 1.0175, Test Loss: 0.9577\n",
      "Epoch 54/60, Train Loss: 1.0238, Test Loss: 0.9603\n",
      "Epoch 55/60, Train Loss: 1.0081, Test Loss: 0.9589\n",
      "Epoch 56/60, Train Loss: 1.0168, Test Loss: 0.9591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:12:57,406] Trial 214 finished with value: 0.9521116614341736 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 243, 'layer_1_size': 255, 'layer_2_size': 160, 'layer_3_size': 137, 'layer_4_size': 51, 'layer_5_size': 106, 'layer_6_size': 197, 'layer_7_size': 166, 'layer_8_size': 155, 'layer_9_size': 38, 'layer_10_size': 241, 'layer_11_size': 136, 'layer_12_size': 130, 'dropout_rate': 0.3148269831165713, 'learning_rate': 0.0007714709183030207, 'batch_size': 256, 'epochs': 60}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/60, Train Loss: 1.0032, Test Loss: 0.9564\n",
      "Epoch 58/60, Train Loss: 1.0265, Test Loss: 0.9534\n",
      "Epoch 59/60, Train Loss: 1.0028, Test Loss: 0.9514\n",
      "Epoch 60/60, Train Loss: 1.0156, Test Loss: 0.9521\n",
      "Epoch 1/72, Train Loss: 1.2536, Test Loss: 1.4654\n",
      "Epoch 2/72, Train Loss: 1.1403, Test Loss: 1.4971\n",
      "Epoch 3/72, Train Loss: 1.1584, Test Loss: 1.4633\n",
      "Epoch 4/72, Train Loss: 1.1136, Test Loss: 1.4139\n",
      "Epoch 5/72, Train Loss: 1.1135, Test Loss: 1.3990\n",
      "Epoch 6/72, Train Loss: 1.1392, Test Loss: 1.3904\n",
      "Epoch 7/72, Train Loss: 1.0538, Test Loss: 1.3785\n",
      "Epoch 8/72, Train Loss: 1.1046, Test Loss: 1.3667\n",
      "Epoch 9/72, Train Loss: 1.0768, Test Loss: 1.3704\n",
      "Epoch 10/72, Train Loss: 1.0501, Test Loss: 1.3635\n",
      "Epoch 11/72, Train Loss: 1.0646, Test Loss: 1.3583\n",
      "Epoch 12/72, Train Loss: 1.0795, Test Loss: 1.3586\n",
      "Epoch 13/72, Train Loss: 1.0444, Test Loss: 1.3616\n",
      "Epoch 14/72, Train Loss: 1.0274, Test Loss: 1.3623\n",
      "Epoch 15/72, Train Loss: 1.0879, Test Loss: 1.3639\n",
      "Epoch 16/72, Train Loss: 1.0418, Test Loss: 1.3560\n",
      "Epoch 17/72, Train Loss: 1.0637, Test Loss: 1.3474\n",
      "Epoch 18/72, Train Loss: 1.0393, Test Loss: 1.3475\n",
      "Epoch 19/72, Train Loss: 1.0459, Test Loss: 1.3431\n",
      "Epoch 20/72, Train Loss: 1.0629, Test Loss: 1.3385\n",
      "Epoch 21/72, Train Loss: 1.0499, Test Loss: 1.3471\n",
      "Epoch 22/72, Train Loss: 1.0577, Test Loss: 1.3495\n",
      "Epoch 23/72, Train Loss: 1.0171, Test Loss: 1.3507\n",
      "Epoch 24/72, Train Loss: 1.0112, Test Loss: 1.3620\n",
      "Epoch 25/72, Train Loss: 1.0333, Test Loss: 1.3601\n",
      "Epoch 26/72, Train Loss: 1.0768, Test Loss: 1.3618\n",
      "Epoch 27/72, Train Loss: 1.0166, Test Loss: 1.3517\n",
      "Epoch 28/72, Train Loss: 1.0762, Test Loss: 1.3599\n",
      "Epoch 29/72, Train Loss: 1.0105, Test Loss: 1.3514\n",
      "Epoch 30/72, Train Loss: 1.0258, Test Loss: 1.3486\n",
      "Epoch 31/72, Train Loss: 1.0529, Test Loss: 1.3494\n",
      "Epoch 32/72, Train Loss: 1.0464, Test Loss: 1.3545\n",
      "Epoch 33/72, Train Loss: 1.0279, Test Loss: 1.3595\n",
      "Epoch 34/72, Train Loss: 1.0342, Test Loss: 1.3531\n",
      "Epoch 35/72, Train Loss: 1.0480, Test Loss: 1.3541\n",
      "Epoch 36/72, Train Loss: 1.0237, Test Loss: 1.3559\n",
      "Epoch 37/72, Train Loss: 1.0269, Test Loss: 1.3553\n",
      "Epoch 38/72, Train Loss: 0.9969, Test Loss: 1.3671\n",
      "Epoch 39/72, Train Loss: 1.0284, Test Loss: 1.3689\n",
      "Epoch 40/72, Train Loss: 1.0536, Test Loss: 1.3644\n",
      "Epoch 41/72, Train Loss: 1.0094, Test Loss: 1.3603\n",
      "Epoch 42/72, Train Loss: 1.0230, Test Loss: 1.3535\n",
      "Epoch 43/72, Train Loss: 1.0218, Test Loss: 1.3489\n",
      "Epoch 44/72, Train Loss: 1.0218, Test Loss: 1.3466\n",
      "Epoch 45/72, Train Loss: 0.9942, Test Loss: 1.3525\n",
      "Epoch 46/72, Train Loss: 0.9900, Test Loss: 1.3564\n",
      "Epoch 47/72, Train Loss: 0.9857, Test Loss: 1.3497\n",
      "Epoch 48/72, Train Loss: 1.0206, Test Loss: 1.3582\n",
      "Epoch 49/72, Train Loss: 1.0035, Test Loss: 1.3566\n",
      "Epoch 50/72, Train Loss: 0.9935, Test Loss: 1.3485\n",
      "Epoch 51/72, Train Loss: 0.9847, Test Loss: 1.3477\n",
      "Epoch 52/72, Train Loss: 1.0187, Test Loss: 1.3532\n",
      "Epoch 53/72, Train Loss: 1.0057, Test Loss: 1.3498\n",
      "Epoch 54/72, Train Loss: 1.0085, Test Loss: 1.3545\n",
      "Epoch 55/72, Train Loss: 1.0070, Test Loss: 1.3440\n",
      "Epoch 56/72, Train Loss: 0.9929, Test Loss: 1.3491\n",
      "Epoch 57/72, Train Loss: 1.0090, Test Loss: 1.3416\n",
      "Epoch 58/72, Train Loss: 0.9935, Test Loss: 1.3371\n",
      "Epoch 59/72, Train Loss: 0.9987, Test Loss: 1.3338\n",
      "Epoch 60/72, Train Loss: 1.0057, Test Loss: 1.3305\n",
      "Epoch 61/72, Train Loss: 1.0126, Test Loss: 1.3342\n",
      "Epoch 62/72, Train Loss: 1.0069, Test Loss: 1.3373\n",
      "Epoch 63/72, Train Loss: 0.9914, Test Loss: 1.3412\n",
      "Epoch 64/72, Train Loss: 0.9742, Test Loss: 1.3386\n",
      "Epoch 65/72, Train Loss: 0.9907, Test Loss: 1.3384\n",
      "Epoch 66/72, Train Loss: 0.9907, Test Loss: 1.3427\n",
      "Epoch 67/72, Train Loss: 0.9882, Test Loss: 1.3404\n",
      "Epoch 68/72, Train Loss: 1.0142, Test Loss: 1.3425\n",
      "Epoch 69/72, Train Loss: 0.9980, Test Loss: 1.3532\n",
      "Epoch 70/72, Train Loss: 0.9995, Test Loss: 1.3587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:13:05,954] Trial 215 finished with value: 1.3585064113140106 and parameters: {'num_hidden_layers': 14, 'layer_0_size': 212, 'layer_1_size': 137, 'layer_2_size': 148, 'layer_3_size': 145, 'layer_4_size': 98, 'layer_5_size': 205, 'layer_6_size': 181, 'layer_7_size': 103, 'layer_8_size': 96, 'layer_9_size': 170, 'layer_10_size': 228, 'layer_11_size': 185, 'layer_12_size': 37, 'layer_13_size': 164, 'dropout_rate': 0.3027686811915888, 'learning_rate': 0.0001826072220463768, 'batch_size': 64, 'epochs': 72}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/72, Train Loss: 0.9868, Test Loss: 1.3582\n",
      "Epoch 72/72, Train Loss: 1.0023, Test Loss: 1.3585\n",
      "Epoch 1/55, Train Loss: 1.3260, Test Loss: 0.8886\n",
      "Epoch 2/55, Train Loss: 1.1991, Test Loss: 0.8875\n",
      "Epoch 3/55, Train Loss: 1.2185, Test Loss: 0.8865\n",
      "Epoch 4/55, Train Loss: 1.2315, Test Loss: 0.8867\n",
      "Epoch 5/55, Train Loss: 1.1807, Test Loss: 0.8892\n",
      "Epoch 6/55, Train Loss: 1.1621, Test Loss: 0.8958\n",
      "Epoch 7/55, Train Loss: 1.2014, Test Loss: 0.9058\n",
      "Epoch 8/55, Train Loss: 1.1932, Test Loss: 0.9099\n",
      "Epoch 9/55, Train Loss: 1.1293, Test Loss: 0.9047\n",
      "Epoch 10/55, Train Loss: 1.1681, Test Loss: 0.9078\n",
      "Epoch 11/55, Train Loss: 1.1428, Test Loss: 0.8989\n",
      "Epoch 12/55, Train Loss: 1.1626, Test Loss: 0.8922\n",
      "Epoch 13/55, Train Loss: 1.1248, Test Loss: 0.8878\n",
      "Epoch 14/55, Train Loss: 1.1426, Test Loss: 0.8866\n",
      "Epoch 15/55, Train Loss: 1.1237, Test Loss: 0.8852\n",
      "Epoch 16/55, Train Loss: 1.1546, Test Loss: 0.8843\n",
      "Epoch 17/55, Train Loss: 1.1503, Test Loss: 0.8842\n",
      "Epoch 18/55, Train Loss: 1.1418, Test Loss: 0.8906\n",
      "Epoch 19/55, Train Loss: 1.1517, Test Loss: 0.8943\n",
      "Epoch 20/55, Train Loss: 1.1547, Test Loss: 0.8982\n",
      "Epoch 21/55, Train Loss: 1.1336, Test Loss: 0.8985\n",
      "Epoch 22/55, Train Loss: 1.1253, Test Loss: 0.9004\n",
      "Epoch 23/55, Train Loss: 1.1650, Test Loss: 0.9004\n",
      "Epoch 24/55, Train Loss: 1.1329, Test Loss: 0.8991\n",
      "Epoch 25/55, Train Loss: 1.1238, Test Loss: 0.9026\n",
      "Epoch 26/55, Train Loss: 1.0932, Test Loss: 0.9030\n",
      "Epoch 27/55, Train Loss: 1.1460, Test Loss: 0.9059\n",
      "Epoch 28/55, Train Loss: 1.1407, Test Loss: 0.9068\n",
      "Epoch 29/55, Train Loss: 1.1099, Test Loss: 0.9095\n",
      "Epoch 30/55, Train Loss: 1.1177, Test Loss: 0.9152\n",
      "Epoch 31/55, Train Loss: 1.1167, Test Loss: 0.9133\n",
      "Epoch 32/55, Train Loss: 1.1269, Test Loss: 0.9107\n",
      "Epoch 33/55, Train Loss: 1.1287, Test Loss: 0.9154\n",
      "Epoch 34/55, Train Loss: 1.1125, Test Loss: 0.9126\n",
      "Epoch 35/55, Train Loss: 1.1174, Test Loss: 0.9097\n",
      "Epoch 36/55, Train Loss: 1.1234, Test Loss: 0.9036\n",
      "Epoch 37/55, Train Loss: 1.0911, Test Loss: 0.9006\n",
      "Epoch 38/55, Train Loss: 1.1109, Test Loss: 0.9040\n",
      "Epoch 39/55, Train Loss: 1.1003, Test Loss: 0.9068\n",
      "Epoch 40/55, Train Loss: 1.1027, Test Loss: 0.9114\n",
      "Epoch 41/55, Train Loss: 1.0865, Test Loss: 0.9141\n",
      "Epoch 42/55, Train Loss: 1.1238, Test Loss: 0.9153\n",
      "Epoch 43/55, Train Loss: 1.1178, Test Loss: 0.9101\n",
      "Epoch 44/55, Train Loss: 1.1434, Test Loss: 0.9014\n",
      "Epoch 45/55, Train Loss: 1.0948, Test Loss: 0.8944\n",
      "Epoch 46/55, Train Loss: 1.0864, Test Loss: 0.8917\n",
      "Epoch 47/55, Train Loss: 1.0907, Test Loss: 0.8905\n",
      "Epoch 48/55, Train Loss: 1.1192, Test Loss: 0.8897\n",
      "Epoch 49/55, Train Loss: 1.0963, Test Loss: 0.8918\n",
      "Epoch 50/55, Train Loss: 1.1080, Test Loss: 0.8947\n",
      "Epoch 51/55, Train Loss: 1.1056, Test Loss: 0.8956\n",
      "Epoch 52/55, Train Loss: 1.0910, Test Loss: 0.8940\n",
      "Epoch 53/55, Train Loss: 1.1075, Test Loss: 0.8932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:13:09,767] Trial 216 finished with value: 0.8943122029304504 and parameters: {'num_hidden_layers': 15, 'layer_0_size': 76, 'layer_1_size': 223, 'layer_2_size': 239, 'layer_3_size': 237, 'layer_4_size': 112, 'layer_5_size': 187, 'layer_6_size': 96, 'layer_7_size': 146, 'layer_8_size': 169, 'layer_9_size': 228, 'layer_10_size': 236, 'layer_11_size': 162, 'layer_12_size': 96, 'layer_13_size': 177, 'layer_14_size': 140, 'dropout_rate': 0.20448184761033067, 'learning_rate': 0.0004490890687339715, 'batch_size': 256, 'epochs': 55}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/55, Train Loss: 1.1075, Test Loss: 0.8948\n",
      "Epoch 55/55, Train Loss: 1.1094, Test Loss: 0.8943\n",
      "Epoch 1/32, Train Loss: 1.2162, Test Loss: 1.0642\n",
      "Epoch 2/32, Train Loss: 1.2088, Test Loss: 1.0627\n",
      "Epoch 3/32, Train Loss: 1.1896, Test Loss: 1.0585\n",
      "Epoch 4/32, Train Loss: 1.2154, Test Loss: 1.0657\n",
      "Epoch 5/32, Train Loss: 1.1221, Test Loss: 1.0663\n",
      "Epoch 6/32, Train Loss: 1.1283, Test Loss: 1.0730\n",
      "Epoch 7/32, Train Loss: 1.2060, Test Loss: 1.0596\n",
      "Epoch 8/32, Train Loss: 1.1576, Test Loss: 1.0556\n",
      "Epoch 9/32, Train Loss: 1.1292, Test Loss: 1.0525\n",
      "Epoch 10/32, Train Loss: 1.1547, Test Loss: 1.0584\n",
      "Epoch 11/32, Train Loss: 1.1296, Test Loss: 1.0554\n",
      "Epoch 12/32, Train Loss: 1.1329, Test Loss: 1.0553\n",
      "Epoch 13/32, Train Loss: 1.1484, Test Loss: 1.0539\n",
      "Epoch 14/32, Train Loss: 1.1158, Test Loss: 1.0608\n",
      "Epoch 15/32, Train Loss: 1.1049, Test Loss: 1.0604\n",
      "Epoch 16/32, Train Loss: 1.1103, Test Loss: 1.0578\n",
      "Epoch 17/32, Train Loss: 1.1141, Test Loss: 1.0555\n",
      "Epoch 18/32, Train Loss: 1.0979, Test Loss: 1.0518\n",
      "Epoch 19/32, Train Loss: 1.1065, Test Loss: 1.0547\n",
      "Epoch 20/32, Train Loss: 1.1272, Test Loss: 1.0559\n",
      "Epoch 21/32, Train Loss: 1.0876, Test Loss: 1.0573\n",
      "Epoch 22/32, Train Loss: 1.0831, Test Loss: 1.0561\n",
      "Epoch 23/32, Train Loss: 1.0624, Test Loss: 1.0606\n",
      "Epoch 24/32, Train Loss: 1.0473, Test Loss: 1.0634\n",
      "Epoch 25/32, Train Loss: 1.0893, Test Loss: 1.0594\n",
      "Epoch 26/32, Train Loss: 1.1071, Test Loss: 1.0555\n",
      "Epoch 27/32, Train Loss: 1.1043, Test Loss: 1.0569\n",
      "Epoch 28/32, Train Loss: 1.0901, Test Loss: 1.0614\n",
      "Epoch 29/32, Train Loss: 1.0978, Test Loss: 1.0622\n",
      "Epoch 30/32, Train Loss: 1.0754, Test Loss: 1.0570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:13:15,649] Trial 217 finished with value: 1.059972039290837 and parameters: {'num_hidden_layers': 13, 'layer_0_size': 160, 'layer_1_size': 209, 'layer_2_size': 139, 'layer_3_size': 127, 'layer_4_size': 58, 'layer_5_size': 200, 'layer_6_size': 62, 'layer_7_size': 64, 'layer_8_size': 160, 'layer_9_size': 241, 'layer_10_size': 143, 'layer_11_size': 195, 'layer_12_size': 175, 'dropout_rate': 0.3414382556439354, 'learning_rate': 0.0001448851515906217, 'batch_size': 32, 'epochs': 32}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/32, Train Loss: 1.0863, Test Loss: 1.0594\n",
      "Epoch 32/32, Train Loss: 1.0949, Test Loss: 1.0600\n",
      "Epoch 1/53, Train Loss: 1.1425, Test Loss: 0.7969\n",
      "Epoch 2/53, Train Loss: 1.0740, Test Loss: 0.7959\n",
      "Epoch 3/53, Train Loss: 1.1229, Test Loss: 0.8025\n",
      "Epoch 4/53, Train Loss: 1.1124, Test Loss: 0.8034\n",
      "Epoch 5/53, Train Loss: 1.1073, Test Loss: 0.7999\n",
      "Epoch 6/53, Train Loss: 1.0964, Test Loss: 0.7990\n",
      "Epoch 7/53, Train Loss: 1.0772, Test Loss: 0.8084\n",
      "Epoch 8/53, Train Loss: 1.0924, Test Loss: 0.8128\n",
      "Epoch 9/53, Train Loss: 1.0801, Test Loss: 0.8130\n",
      "Epoch 10/53, Train Loss: 1.0710, Test Loss: 0.8181\n",
      "Epoch 11/53, Train Loss: 1.0650, Test Loss: 0.8111\n",
      "Epoch 12/53, Train Loss: 1.0611, Test Loss: 0.8199\n",
      "Epoch 13/53, Train Loss: 1.1175, Test Loss: 0.8144\n",
      "Epoch 14/53, Train Loss: 1.0682, Test Loss: 0.8051\n",
      "Epoch 15/53, Train Loss: 1.0735, Test Loss: 0.8165\n",
      "Epoch 16/53, Train Loss: 1.0618, Test Loss: 0.8360\n",
      "Epoch 17/53, Train Loss: 1.0413, Test Loss: 0.8321\n",
      "Epoch 18/53, Train Loss: 1.0779, Test Loss: 0.8282\n",
      "Epoch 19/53, Train Loss: 1.0689, Test Loss: 0.8179\n",
      "Epoch 20/53, Train Loss: 1.0559, Test Loss: 0.8110\n",
      "Epoch 21/53, Train Loss: 1.0824, Test Loss: 0.8075\n",
      "Epoch 22/53, Train Loss: 1.0556, Test Loss: 0.8106\n",
      "Epoch 23/53, Train Loss: 1.0427, Test Loss: 0.8099\n",
      "Epoch 24/53, Train Loss: 1.0208, Test Loss: 0.8209\n",
      "Epoch 25/53, Train Loss: 1.0208, Test Loss: 0.8236\n",
      "Epoch 26/53, Train Loss: 1.0340, Test Loss: 0.8136\n",
      "Epoch 27/53, Train Loss: 1.0406, Test Loss: 0.8147\n",
      "Epoch 28/53, Train Loss: 1.0476, Test Loss: 0.8173\n",
      "Epoch 29/53, Train Loss: 1.0564, Test Loss: 0.8158\n",
      "Epoch 30/53, Train Loss: 1.0344, Test Loss: 0.8136\n",
      "Epoch 31/53, Train Loss: 1.0210, Test Loss: 0.8214\n",
      "Epoch 32/53, Train Loss: 1.0233, Test Loss: 0.8218\n",
      "Epoch 33/53, Train Loss: 1.0374, Test Loss: 0.8161\n",
      "Epoch 34/53, Train Loss: 1.0354, Test Loss: 0.8195\n",
      "Epoch 35/53, Train Loss: 1.0471, Test Loss: 0.8170\n",
      "Epoch 36/53, Train Loss: 1.0473, Test Loss: 0.8122\n",
      "Epoch 37/53, Train Loss: 1.0398, Test Loss: 0.8174\n",
      "Epoch 38/53, Train Loss: 1.0341, Test Loss: 0.8184\n",
      "Epoch 39/53, Train Loss: 1.0239, Test Loss: 0.8165\n",
      "Epoch 40/53, Train Loss: 1.0345, Test Loss: 0.8141\n",
      "Epoch 41/53, Train Loss: 1.0421, Test Loss: 0.8070\n",
      "Epoch 42/53, Train Loss: 1.0268, Test Loss: 0.8133\n",
      "Epoch 43/53, Train Loss: 1.0511, Test Loss: 0.8080\n",
      "Epoch 44/53, Train Loss: 1.0335, Test Loss: 0.8133\n",
      "Epoch 45/53, Train Loss: 1.0255, Test Loss: 0.8070\n",
      "Epoch 46/53, Train Loss: 1.0714, Test Loss: 0.8119\n",
      "Epoch 47/53, Train Loss: 1.0460, Test Loss: 0.8122\n",
      "Epoch 48/53, Train Loss: 1.0265, Test Loss: 0.8128\n",
      "Epoch 49/53, Train Loss: 1.0194, Test Loss: 0.8203\n",
      "Epoch 50/53, Train Loss: 1.0335, Test Loss: 0.8165\n",
      "Epoch 51/53, Train Loss: 1.0325, Test Loss: 0.8143\n",
      "Epoch 52/53, Train Loss: 1.0187, Test Loss: 0.8100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:13:23,745] Trial 218 finished with value: 0.8160977704184396 and parameters: {'num_hidden_layers': 9, 'layer_0_size': 231, 'layer_1_size': 89, 'layer_2_size': 186, 'layer_3_size': 151, 'layer_4_size': 93, 'layer_5_size': 136, 'layer_6_size': 43, 'layer_7_size': 153, 'layer_8_size': 62, 'dropout_rate': 0.2903637841063548, 'learning_rate': 0.0003064488902864672, 'batch_size': 32, 'epochs': 53}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/53, Train Loss: 1.0159, Test Loss: 0.8161\n",
      "Epoch 1/79, Train Loss: 1.1655, Test Loss: 0.9368\n",
      "Epoch 2/79, Train Loss: 1.1287, Test Loss: 0.9525\n",
      "Epoch 3/79, Train Loss: 1.1286, Test Loss: 0.9389\n",
      "Epoch 4/79, Train Loss: 1.1007, Test Loss: 0.9422\n",
      "Epoch 5/79, Train Loss: 1.0697, Test Loss: 0.9404\n",
      "Epoch 6/79, Train Loss: 1.0502, Test Loss: 0.9342\n",
      "Epoch 7/79, Train Loss: 1.0680, Test Loss: 0.9338\n",
      "Epoch 8/79, Train Loss: 1.0475, Test Loss: 0.9308\n",
      "Epoch 9/79, Train Loss: 1.0607, Test Loss: 0.9294\n",
      "Epoch 10/79, Train Loss: 1.0533, Test Loss: 0.9293\n",
      "Epoch 11/79, Train Loss: 1.0534, Test Loss: 0.9371\n",
      "Epoch 12/79, Train Loss: 1.0375, Test Loss: 0.9339\n",
      "Epoch 13/79, Train Loss: 1.0231, Test Loss: 0.9361\n",
      "Epoch 14/79, Train Loss: 1.0354, Test Loss: 0.9293\n",
      "Epoch 15/79, Train Loss: 1.0374, Test Loss: 0.9331\n",
      "Epoch 16/79, Train Loss: 1.0437, Test Loss: 0.9300\n",
      "Epoch 17/79, Train Loss: 1.0563, Test Loss: 0.9318\n",
      "Epoch 18/79, Train Loss: 1.0887, Test Loss: 0.9310\n",
      "Epoch 19/79, Train Loss: 1.0477, Test Loss: 0.9315\n",
      "Epoch 20/79, Train Loss: 1.0339, Test Loss: 0.9349\n",
      "Epoch 21/79, Train Loss: 1.0495, Test Loss: 0.9315\n",
      "Epoch 22/79, Train Loss: 1.0104, Test Loss: 0.9288\n",
      "Epoch 23/79, Train Loss: 1.0835, Test Loss: 0.9241\n",
      "Epoch 24/79, Train Loss: 1.0534, Test Loss: 0.9260\n",
      "Epoch 25/79, Train Loss: 1.0508, Test Loss: 0.9280\n",
      "Epoch 26/79, Train Loss: 1.0239, Test Loss: 0.9219\n",
      "Epoch 27/79, Train Loss: 1.0655, Test Loss: 0.9175\n",
      "Epoch 28/79, Train Loss: 1.0263, Test Loss: 0.9234\n",
      "Epoch 29/79, Train Loss: 1.0316, Test Loss: 0.9216\n",
      "Epoch 30/79, Train Loss: 1.0439, Test Loss: 0.9231\n",
      "Epoch 31/79, Train Loss: 1.0323, Test Loss: 0.9243\n",
      "Epoch 32/79, Train Loss: 1.0336, Test Loss: 0.9279\n",
      "Epoch 33/79, Train Loss: 1.0240, Test Loss: 0.9252\n",
      "Epoch 34/79, Train Loss: 1.0258, Test Loss: 0.9287\n",
      "Epoch 35/79, Train Loss: 1.0126, Test Loss: 0.9291\n",
      "Epoch 36/79, Train Loss: 1.0492, Test Loss: 0.9255\n",
      "Epoch 37/79, Train Loss: 1.0348, Test Loss: 0.9322\n",
      "Epoch 38/79, Train Loss: 1.0202, Test Loss: 0.9299\n",
      "Epoch 39/79, Train Loss: 1.0276, Test Loss: 0.9251\n",
      "Epoch 40/79, Train Loss: 1.0188, Test Loss: 0.9309\n",
      "Epoch 41/79, Train Loss: 1.0052, Test Loss: 0.9368\n",
      "Epoch 42/79, Train Loss: 0.9988, Test Loss: 0.9300\n",
      "Epoch 43/79, Train Loss: 1.0151, Test Loss: 0.9315\n",
      "Epoch 44/79, Train Loss: 0.9956, Test Loss: 0.9323\n",
      "Epoch 45/79, Train Loss: 1.0197, Test Loss: 0.9317\n",
      "Epoch 46/79, Train Loss: 1.0195, Test Loss: 0.9327\n",
      "Epoch 47/79, Train Loss: 0.9921, Test Loss: 0.9294\n",
      "Epoch 48/79, Train Loss: 1.0155, Test Loss: 0.9243\n",
      "Epoch 49/79, Train Loss: 0.9741, Test Loss: 0.9317\n",
      "Epoch 50/79, Train Loss: 0.9911, Test Loss: 0.9327\n",
      "Epoch 51/79, Train Loss: 1.0044, Test Loss: 0.9327\n",
      "Epoch 52/79, Train Loss: 0.9682, Test Loss: 0.9389\n",
      "Epoch 53/79, Train Loss: 1.0015, Test Loss: 0.9379\n",
      "Epoch 54/79, Train Loss: 1.0163, Test Loss: 0.9422\n",
      "Epoch 55/79, Train Loss: 1.0080, Test Loss: 0.9415\n",
      "Epoch 56/79, Train Loss: 0.9646, Test Loss: 0.9402\n",
      "Epoch 57/79, Train Loss: 0.9600, Test Loss: 0.9436\n",
      "Epoch 58/79, Train Loss: 0.9356, Test Loss: 0.9514\n",
      "Epoch 59/79, Train Loss: 0.9524, Test Loss: 0.9706\n",
      "Epoch 60/79, Train Loss: 0.9323, Test Loss: 0.9648\n",
      "Epoch 61/79, Train Loss: 0.9543, Test Loss: 0.9645\n",
      "Epoch 62/79, Train Loss: 0.9452, Test Loss: 0.9536\n",
      "Epoch 63/79, Train Loss: 0.9420, Test Loss: 0.9663\n",
      "Epoch 64/79, Train Loss: 0.9671, Test Loss: 0.9595\n",
      "Epoch 65/79, Train Loss: 0.9131, Test Loss: 0.9598\n",
      "Epoch 66/79, Train Loss: 0.9261, Test Loss: 0.9677\n",
      "Epoch 67/79, Train Loss: 0.9706, Test Loss: 0.9626\n",
      "Epoch 68/79, Train Loss: 0.9300, Test Loss: 0.9789\n",
      "Epoch 69/79, Train Loss: 0.8992, Test Loss: 0.9822\n",
      "Epoch 70/79, Train Loss: 0.9035, Test Loss: 0.9877\n",
      "Epoch 71/79, Train Loss: 0.8942, Test Loss: 0.9966\n",
      "Epoch 72/79, Train Loss: 0.8939, Test Loss: 0.9981\n",
      "Epoch 73/79, Train Loss: 0.8912, Test Loss: 0.9931\n",
      "Epoch 74/79, Train Loss: 0.9029, Test Loss: 1.0080\n",
      "Epoch 75/79, Train Loss: 0.8941, Test Loss: 1.0137\n",
      "Epoch 76/79, Train Loss: 0.8695, Test Loss: 0.9945\n",
      "Epoch 77/79, Train Loss: 0.8718, Test Loss: 1.0078\n",
      "Epoch 78/79, Train Loss: 0.8505, Test Loss: 1.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:13:35,200] Trial 219 finished with value: 1.029134247984205 and parameters: {'num_hidden_layers': 9, 'layer_0_size': 230, 'layer_1_size': 71, 'layer_2_size': 187, 'layer_3_size': 57, 'layer_4_size': 90, 'layer_5_size': 135, 'layer_6_size': 35, 'layer_7_size': 158, 'layer_8_size': 48, 'dropout_rate': 0.24345941067745927, 'learning_rate': 0.00033288499586675146, 'batch_size': 32, 'epochs': 79}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/79, Train Loss: 0.8405, Test Loss: 1.0291\n",
      "Epoch 1/48, Train Loss: 1.2566, Test Loss: 0.9861\n",
      "Epoch 2/48, Train Loss: 1.2840, Test Loss: 0.9905\n",
      "Epoch 3/48, Train Loss: 1.2290, Test Loss: 0.9953\n",
      "Epoch 4/48, Train Loss: 1.2200, Test Loss: 0.9886\n",
      "Epoch 5/48, Train Loss: 1.1693, Test Loss: 0.9865\n",
      "Epoch 6/48, Train Loss: 1.2002, Test Loss: 0.9917\n",
      "Epoch 7/48, Train Loss: 1.1680, Test Loss: 0.9914\n",
      "Epoch 8/48, Train Loss: 1.1528, Test Loss: 0.9991\n",
      "Epoch 9/48, Train Loss: 1.1653, Test Loss: 1.0123\n",
      "Epoch 10/48, Train Loss: 1.2184, Test Loss: 0.9961\n",
      "Epoch 11/48, Train Loss: 1.1821, Test Loss: 0.9947\n",
      "Epoch 12/48, Train Loss: 1.1883, Test Loss: 1.0226\n",
      "Epoch 13/48, Train Loss: 1.2533, Test Loss: 1.0084\n",
      "Epoch 14/48, Train Loss: 1.2057, Test Loss: 1.0130\n",
      "Epoch 15/48, Train Loss: 1.1598, Test Loss: 1.0111\n",
      "Epoch 16/48, Train Loss: 1.1287, Test Loss: 1.0176\n",
      "Epoch 17/48, Train Loss: 1.1868, Test Loss: 1.0088\n",
      "Epoch 18/48, Train Loss: 1.1653, Test Loss: 1.0138\n",
      "Epoch 19/48, Train Loss: 1.1495, Test Loss: 1.0157\n",
      "Epoch 20/48, Train Loss: 1.1705, Test Loss: 1.0081\n",
      "Epoch 21/48, Train Loss: 1.1612, Test Loss: 1.0152\n",
      "Epoch 22/48, Train Loss: 1.1076, Test Loss: 1.0071\n",
      "Epoch 23/48, Train Loss: 1.1397, Test Loss: 1.0267\n",
      "Epoch 24/48, Train Loss: 1.1280, Test Loss: 1.0256\n",
      "Epoch 25/48, Train Loss: 1.1606, Test Loss: 1.0132\n",
      "Epoch 26/48, Train Loss: 1.1597, Test Loss: 1.0175\n",
      "Epoch 27/48, Train Loss: 1.1547, Test Loss: 0.9977\n",
      "Epoch 28/48, Train Loss: 1.1417, Test Loss: 1.0019\n",
      "Epoch 29/48, Train Loss: 1.1657, Test Loss: 1.0086\n",
      "Epoch 30/48, Train Loss: 1.1439, Test Loss: 1.0215\n",
      "Epoch 31/48, Train Loss: 1.1524, Test Loss: 1.0059\n",
      "Epoch 32/48, Train Loss: 1.1360, Test Loss: 1.0128\n",
      "Epoch 33/48, Train Loss: 1.0858, Test Loss: 1.0104\n",
      "Epoch 34/48, Train Loss: 1.1447, Test Loss: 1.0151\n",
      "Epoch 35/48, Train Loss: 1.1041, Test Loss: 1.0147\n",
      "Epoch 36/48, Train Loss: 1.1749, Test Loss: 1.0202\n",
      "Epoch 37/48, Train Loss: 1.1296, Test Loss: 1.0159\n",
      "Epoch 38/48, Train Loss: 1.1198, Test Loss: 1.0109\n",
      "Epoch 39/48, Train Loss: 1.1372, Test Loss: 1.0180\n",
      "Epoch 40/48, Train Loss: 1.0920, Test Loss: 1.0304\n",
      "Epoch 41/48, Train Loss: 1.1241, Test Loss: 1.0103\n",
      "Epoch 42/48, Train Loss: 1.0858, Test Loss: 1.0253\n",
      "Epoch 43/48, Train Loss: 1.1298, Test Loss: 1.0145\n",
      "Epoch 44/48, Train Loss: 1.1426, Test Loss: 1.0188\n",
      "Epoch 45/48, Train Loss: 1.1667, Test Loss: 1.0235\n",
      "Epoch 46/48, Train Loss: 1.1080, Test Loss: 1.0194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:13:41,074] Trial 220 finished with value: 1.018245586327144 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 232, 'layer_1_size': 79, 'layer_2_size': 107, 'layer_3_size': 156, 'layer_4_size': 101, 'layer_5_size': 131, 'layer_6_size': 41, 'layer_7_size': 153, 'dropout_rate': 0.22636106805729955, 'learning_rate': 1.741886603667967e-05, 'batch_size': 32, 'epochs': 48}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/48, Train Loss: 1.1008, Test Loss: 1.0157\n",
      "Epoch 48/48, Train Loss: 1.1248, Test Loss: 1.0182\n",
      "Epoch 1/51, Train Loss: 1.1501, Test Loss: 0.9687\n",
      "Epoch 2/51, Train Loss: 1.1318, Test Loss: 0.9769\n",
      "Epoch 3/51, Train Loss: 1.0604, Test Loss: 0.9866\n",
      "Epoch 4/51, Train Loss: 1.0919, Test Loss: 0.9763\n",
      "Epoch 5/51, Train Loss: 1.0524, Test Loss: 0.9791\n",
      "Epoch 6/51, Train Loss: 1.0516, Test Loss: 0.9741\n",
      "Epoch 7/51, Train Loss: 1.0727, Test Loss: 0.9750\n",
      "Epoch 8/51, Train Loss: 1.0505, Test Loss: 0.9847\n",
      "Epoch 9/51, Train Loss: 1.0841, Test Loss: 0.9938\n",
      "Epoch 10/51, Train Loss: 1.0517, Test Loss: 0.9889\n",
      "Epoch 11/51, Train Loss: 1.0462, Test Loss: 0.9797\n",
      "Epoch 12/51, Train Loss: 1.0500, Test Loss: 0.9736\n",
      "Epoch 13/51, Train Loss: 1.0201, Test Loss: 0.9617\n",
      "Epoch 14/51, Train Loss: 1.0801, Test Loss: 0.9608\n",
      "Epoch 15/51, Train Loss: 1.0256, Test Loss: 0.9703\n",
      "Epoch 16/51, Train Loss: 1.0037, Test Loss: 0.9653\n",
      "Epoch 17/51, Train Loss: 1.0307, Test Loss: 0.9656\n",
      "Epoch 18/51, Train Loss: 1.0367, Test Loss: 0.9726\n",
      "Epoch 19/51, Train Loss: 1.0543, Test Loss: 0.9866\n",
      "Epoch 20/51, Train Loss: 1.0006, Test Loss: 0.9774\n",
      "Epoch 21/51, Train Loss: 1.0390, Test Loss: 0.9877\n",
      "Epoch 22/51, Train Loss: 1.0156, Test Loss: 0.9729\n",
      "Epoch 23/51, Train Loss: 1.0207, Test Loss: 0.9722\n",
      "Epoch 24/51, Train Loss: 0.9656, Test Loss: 0.9763\n",
      "Epoch 25/51, Train Loss: 0.9860, Test Loss: 0.9766\n",
      "Epoch 26/51, Train Loss: 0.9682, Test Loss: 0.9952\n",
      "Epoch 27/51, Train Loss: 1.0171, Test Loss: 0.9817\n",
      "Epoch 28/51, Train Loss: 1.0016, Test Loss: 0.9717\n",
      "Epoch 29/51, Train Loss: 0.9923, Test Loss: 0.9724\n",
      "Epoch 30/51, Train Loss: 1.0187, Test Loss: 0.9760\n",
      "Epoch 31/51, Train Loss: 1.0273, Test Loss: 0.9830\n",
      "Epoch 32/51, Train Loss: 1.0063, Test Loss: 0.9761\n",
      "Epoch 33/51, Train Loss: 1.0181, Test Loss: 0.9749\n",
      "Epoch 34/51, Train Loss: 1.0112, Test Loss: 0.9731\n",
      "Epoch 35/51, Train Loss: 1.0169, Test Loss: 0.9713\n",
      "Epoch 36/51, Train Loss: 0.9662, Test Loss: 0.9631\n",
      "Epoch 37/51, Train Loss: 0.9977, Test Loss: 0.9605\n",
      "Epoch 38/51, Train Loss: 1.0074, Test Loss: 0.9646\n",
      "Epoch 39/51, Train Loss: 0.9836, Test Loss: 0.9645\n",
      "Epoch 40/51, Train Loss: 0.9992, Test Loss: 0.9619\n",
      "Epoch 41/51, Train Loss: 0.9798, Test Loss: 0.9563\n",
      "Epoch 42/51, Train Loss: 0.9969, Test Loss: 0.9552\n",
      "Epoch 43/51, Train Loss: 0.9913, Test Loss: 0.9519\n",
      "Epoch 44/51, Train Loss: 0.9927, Test Loss: 0.9541\n",
      "Epoch 45/51, Train Loss: 0.9913, Test Loss: 0.9574\n",
      "Epoch 46/51, Train Loss: 0.9975, Test Loss: 0.9555\n",
      "Epoch 47/51, Train Loss: 0.9753, Test Loss: 0.9605\n",
      "Epoch 48/51, Train Loss: 1.0050, Test Loss: 0.9568\n",
      "Epoch 49/51, Train Loss: 1.0057, Test Loss: 0.9576\n",
      "Epoch 50/51, Train Loss: 0.9635, Test Loss: 0.9458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:13:46,873] Trial 221 finished with value: 0.9502240589686802 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 224, 'layer_1_size': 88, 'layer_2_size': 183, 'layer_3_size': 149, 'layer_4_size': 94, 'layer_5_size': 91, 'layer_6_size': 48, 'layer_7_size': 161, 'dropout_rate': 0.2850050121924145, 'learning_rate': 0.0002877116510269812, 'batch_size': 32, 'epochs': 51}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/51, Train Loss: 0.9971, Test Loss: 0.9502\n",
      "Epoch 1/54, Train Loss: 1.1320, Test Loss: 1.0715\n",
      "Epoch 2/54, Train Loss: 1.0899, Test Loss: 1.0680\n",
      "Epoch 3/54, Train Loss: 1.0857, Test Loss: 1.0676\n",
      "Epoch 4/54, Train Loss: 1.0637, Test Loss: 1.0681\n",
      "Epoch 5/54, Train Loss: 1.0631, Test Loss: 1.0596\n",
      "Epoch 6/54, Train Loss: 1.0545, Test Loss: 1.0667\n",
      "Epoch 7/54, Train Loss: 1.0575, Test Loss: 1.0698\n",
      "Epoch 8/54, Train Loss: 1.0500, Test Loss: 1.0599\n",
      "Epoch 9/54, Train Loss: 1.0357, Test Loss: 1.0573\n",
      "Epoch 10/54, Train Loss: 1.0188, Test Loss: 1.0607\n",
      "Epoch 11/54, Train Loss: 1.0344, Test Loss: 1.0685\n",
      "Epoch 12/54, Train Loss: 1.0268, Test Loss: 1.0630\n",
      "Epoch 13/54, Train Loss: 1.0370, Test Loss: 1.0542\n",
      "Epoch 14/54, Train Loss: 1.0077, Test Loss: 1.0598\n",
      "Epoch 15/54, Train Loss: 1.0717, Test Loss: 1.0659\n",
      "Epoch 16/54, Train Loss: 1.0312, Test Loss: 1.0646\n",
      "Epoch 17/54, Train Loss: 1.0095, Test Loss: 1.0559\n",
      "Epoch 18/54, Train Loss: 0.9912, Test Loss: 1.0601\n",
      "Epoch 19/54, Train Loss: 0.9903, Test Loss: 1.0580\n",
      "Epoch 20/54, Train Loss: 0.9960, Test Loss: 1.0512\n",
      "Epoch 21/54, Train Loss: 1.0337, Test Loss: 1.0521\n",
      "Epoch 22/54, Train Loss: 0.9951, Test Loss: 1.0489\n",
      "Epoch 23/54, Train Loss: 1.0209, Test Loss: 1.0498\n",
      "Epoch 24/54, Train Loss: 1.0097, Test Loss: 1.0509\n",
      "Epoch 25/54, Train Loss: 0.9811, Test Loss: 1.0539\n",
      "Epoch 26/54, Train Loss: 1.0116, Test Loss: 1.0566\n",
      "Epoch 27/54, Train Loss: 0.9724, Test Loss: 1.0494\n",
      "Epoch 28/54, Train Loss: 1.0071, Test Loss: 1.0524\n",
      "Epoch 29/54, Train Loss: 0.9518, Test Loss: 1.0525\n",
      "Epoch 30/54, Train Loss: 0.9664, Test Loss: 1.0502\n",
      "Epoch 31/54, Train Loss: 1.0180, Test Loss: 1.0507\n",
      "Epoch 32/54, Train Loss: 0.9935, Test Loss: 1.0510\n",
      "Epoch 33/54, Train Loss: 0.9667, Test Loss: 1.0527\n",
      "Epoch 34/54, Train Loss: 0.9876, Test Loss: 1.0504\n",
      "Epoch 35/54, Train Loss: 0.9974, Test Loss: 1.0484\n",
      "Epoch 36/54, Train Loss: 0.9712, Test Loss: 1.0466\n",
      "Epoch 37/54, Train Loss: 0.9695, Test Loss: 1.0516\n",
      "Epoch 38/54, Train Loss: 1.0006, Test Loss: 1.0510\n",
      "Epoch 39/54, Train Loss: 0.9889, Test Loss: 1.0536\n",
      "Epoch 40/54, Train Loss: 1.0192, Test Loss: 1.0507\n",
      "Epoch 41/54, Train Loss: 0.9750, Test Loss: 1.0554\n",
      "Epoch 42/54, Train Loss: 0.9885, Test Loss: 1.0535\n",
      "Epoch 43/54, Train Loss: 0.9743, Test Loss: 1.0537\n",
      "Epoch 44/54, Train Loss: 0.9696, Test Loss: 1.0515\n",
      "Epoch 45/54, Train Loss: 0.9869, Test Loss: 1.0533\n",
      "Epoch 46/54, Train Loss: 0.9918, Test Loss: 1.0529\n",
      "Epoch 47/54, Train Loss: 0.9811, Test Loss: 1.0542\n",
      "Epoch 48/54, Train Loss: 0.9436, Test Loss: 1.0529\n",
      "Epoch 49/54, Train Loss: 0.9944, Test Loss: 1.0516\n",
      "Epoch 50/54, Train Loss: 0.9744, Test Loss: 1.0533\n",
      "Epoch 51/54, Train Loss: 0.9847, Test Loss: 1.0515\n",
      "Epoch 52/54, Train Loss: 0.9745, Test Loss: 1.0532\n",
      "Epoch 53/54, Train Loss: 0.9731, Test Loss: 1.0542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:13:54,125] Trial 222 finished with value: 1.053607506411416 and parameters: {'num_hidden_layers': 10, 'layer_0_size': 234, 'layer_1_size': 91, 'layer_2_size': 151, 'layer_3_size': 145, 'layer_4_size': 107, 'layer_5_size': 126, 'layer_6_size': 38, 'layer_7_size': 209, 'layer_8_size': 64, 'layer_9_size': 49, 'dropout_rate': 0.2976305152252774, 'learning_rate': 0.0002513814801691115, 'batch_size': 32, 'epochs': 54}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/54, Train Loss: 0.9809, Test Loss: 1.0536\n",
      "Epoch 1/64, Train Loss: 1.1752, Test Loss: 0.8886\n",
      "Epoch 2/64, Train Loss: 1.1583, Test Loss: 0.8687\n",
      "Epoch 3/64, Train Loss: 1.0985, Test Loss: 0.8691\n",
      "Epoch 4/64, Train Loss: 1.1600, Test Loss: 0.8668\n",
      "Epoch 5/64, Train Loss: 1.0256, Test Loss: 0.8741\n",
      "Epoch 6/64, Train Loss: 1.1102, Test Loss: 0.8685\n",
      "Epoch 7/64, Train Loss: 1.0889, Test Loss: 0.8732\n",
      "Epoch 8/64, Train Loss: 1.0663, Test Loss: 0.8860\n",
      "Epoch 9/64, Train Loss: 1.0470, Test Loss: 0.8891\n",
      "Epoch 10/64, Train Loss: 1.0536, Test Loss: 0.8692\n",
      "Epoch 11/64, Train Loss: 0.9974, Test Loss: 0.8696\n",
      "Epoch 12/64, Train Loss: 1.0667, Test Loss: 0.8711\n",
      "Epoch 13/64, Train Loss: 1.0363, Test Loss: 0.8785\n",
      "Epoch 14/64, Train Loss: 1.0555, Test Loss: 0.8767\n",
      "Epoch 15/64, Train Loss: 1.0625, Test Loss: 0.8771\n",
      "Epoch 16/64, Train Loss: 1.0771, Test Loss: 0.8678\n",
      "Epoch 17/64, Train Loss: 1.0461, Test Loss: 0.8679\n",
      "Epoch 18/64, Train Loss: 1.0754, Test Loss: 0.8706\n",
      "Epoch 19/64, Train Loss: 1.0476, Test Loss: 0.8643\n",
      "Epoch 20/64, Train Loss: 1.0530, Test Loss: 0.8669\n",
      "Epoch 21/64, Train Loss: 1.0505, Test Loss: 0.8737\n",
      "Epoch 22/64, Train Loss: 1.0317, Test Loss: 0.8713\n",
      "Epoch 23/64, Train Loss: 1.0365, Test Loss: 0.8649\n",
      "Epoch 24/64, Train Loss: 1.0158, Test Loss: 0.8668\n",
      "Epoch 25/64, Train Loss: 1.0457, Test Loss: 0.8696\n",
      "Epoch 26/64, Train Loss: 1.0261, Test Loss: 0.8796\n",
      "Epoch 27/64, Train Loss: 1.0056, Test Loss: 0.8723\n",
      "Epoch 28/64, Train Loss: 1.0261, Test Loss: 0.8730\n",
      "Epoch 29/64, Train Loss: 1.0238, Test Loss: 0.8674\n",
      "Epoch 30/64, Train Loss: 1.0197, Test Loss: 0.8681\n",
      "Epoch 31/64, Train Loss: 1.0359, Test Loss: 0.8721\n",
      "Epoch 32/64, Train Loss: 1.0057, Test Loss: 0.8826\n",
      "Epoch 33/64, Train Loss: 1.0161, Test Loss: 0.8890\n",
      "Epoch 34/64, Train Loss: 1.0171, Test Loss: 0.8872\n",
      "Epoch 35/64, Train Loss: 1.0174, Test Loss: 0.8775\n",
      "Epoch 36/64, Train Loss: 1.0096, Test Loss: 0.8726\n",
      "Epoch 37/64, Train Loss: 0.9810, Test Loss: 0.8705\n",
      "Epoch 38/64, Train Loss: 1.0010, Test Loss: 0.8754\n",
      "Epoch 39/64, Train Loss: 1.0140, Test Loss: 0.8716\n",
      "Epoch 40/64, Train Loss: 0.9972, Test Loss: 0.8672\n",
      "Epoch 41/64, Train Loss: 0.9925, Test Loss: 0.8736\n",
      "Epoch 42/64, Train Loss: 1.0084, Test Loss: 0.8722\n",
      "Epoch 43/64, Train Loss: 1.0062, Test Loss: 0.8631\n",
      "Epoch 44/64, Train Loss: 1.0029, Test Loss: 0.8644\n",
      "Epoch 45/64, Train Loss: 0.9825, Test Loss: 0.8660\n",
      "Epoch 46/64, Train Loss: 0.9900, Test Loss: 0.8708\n",
      "Epoch 47/64, Train Loss: 1.0258, Test Loss: 0.8690\n",
      "Epoch 48/64, Train Loss: 0.9768, Test Loss: 0.8741\n",
      "Epoch 49/64, Train Loss: 0.9903, Test Loss: 0.8633\n",
      "Epoch 50/64, Train Loss: 0.9677, Test Loss: 0.8615\n",
      "Epoch 51/64, Train Loss: 0.9930, Test Loss: 0.8640\n",
      "Epoch 52/64, Train Loss: 1.0030, Test Loss: 0.8563\n",
      "Epoch 53/64, Train Loss: 0.9643, Test Loss: 0.8541\n",
      "Epoch 54/64, Train Loss: 0.9865, Test Loss: 0.8589\n",
      "Epoch 55/64, Train Loss: 0.9446, Test Loss: 0.8613\n",
      "Epoch 56/64, Train Loss: 0.9261, Test Loss: 0.8540\n",
      "Epoch 57/64, Train Loss: 0.9413, Test Loss: 0.8558\n",
      "Epoch 58/64, Train Loss: 0.9453, Test Loss: 0.8584\n",
      "Epoch 59/64, Train Loss: 0.9264, Test Loss: 0.8525\n",
      "Epoch 60/64, Train Loss: 0.9084, Test Loss: 0.8520\n",
      "Epoch 61/64, Train Loss: 0.9727, Test Loss: 0.8579\n",
      "Epoch 62/64, Train Loss: 0.9194, Test Loss: 0.8574\n",
      "Epoch 63/64, Train Loss: 0.9134, Test Loss: 0.8582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:14:03,562] Trial 223 finished with value: 0.8634502036230904 and parameters: {'num_hidden_layers': 9, 'layer_0_size': 239, 'layer_1_size': 99, 'layer_2_size': 194, 'layer_3_size': 133, 'layer_4_size': 76, 'layer_5_size': 137, 'layer_6_size': 212, 'layer_7_size': 217, 'layer_8_size': 63, 'dropout_rate': 0.28993858389730837, 'learning_rate': 0.000378198110707968, 'batch_size': 32, 'epochs': 64}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/64, Train Loss: 0.8849, Test Loss: 0.8635\n",
      "Epoch 1/63, Train Loss: 1.2093, Test Loss: 0.8295\n",
      "Epoch 2/63, Train Loss: 1.1958, Test Loss: 0.8159\n",
      "Epoch 3/63, Train Loss: 1.1880, Test Loss: 0.8165\n",
      "Epoch 4/63, Train Loss: 1.1696, Test Loss: 0.8207\n",
      "Epoch 5/63, Train Loss: 1.1299, Test Loss: 0.8224\n",
      "Epoch 6/63, Train Loss: 1.1046, Test Loss: 0.8317\n",
      "Epoch 7/63, Train Loss: 1.1483, Test Loss: 0.8336\n",
      "Epoch 8/63, Train Loss: 1.0984, Test Loss: 0.8261\n",
      "Epoch 9/63, Train Loss: 1.1267, Test Loss: 0.8310\n",
      "Epoch 10/63, Train Loss: 1.1114, Test Loss: 0.8427\n",
      "Epoch 11/63, Train Loss: 1.1208, Test Loss: 0.8257\n",
      "Epoch 12/63, Train Loss: 1.0929, Test Loss: 0.8140\n",
      "Epoch 13/63, Train Loss: 1.1003, Test Loss: 0.8169\n",
      "Epoch 14/63, Train Loss: 1.0942, Test Loss: 0.8111\n",
      "Epoch 15/63, Train Loss: 1.1060, Test Loss: 0.8142\n",
      "Epoch 16/63, Train Loss: 1.0816, Test Loss: 0.8178\n",
      "Epoch 17/63, Train Loss: 1.0925, Test Loss: 0.8178\n",
      "Epoch 18/63, Train Loss: 1.0941, Test Loss: 0.8230\n",
      "Epoch 19/63, Train Loss: 1.1210, Test Loss: 0.8200\n",
      "Epoch 20/63, Train Loss: 1.0395, Test Loss: 0.8221\n",
      "Epoch 21/63, Train Loss: 1.0528, Test Loss: 0.8223\n",
      "Epoch 22/63, Train Loss: 1.0725, Test Loss: 0.8415\n",
      "Epoch 23/63, Train Loss: 1.0357, Test Loss: 0.8371\n",
      "Epoch 24/63, Train Loss: 1.1129, Test Loss: 0.8305\n",
      "Epoch 25/63, Train Loss: 1.0451, Test Loss: 0.8385\n",
      "Epoch 26/63, Train Loss: 1.0322, Test Loss: 0.8422\n",
      "Epoch 27/63, Train Loss: 1.0700, Test Loss: 0.8483\n",
      "Epoch 28/63, Train Loss: 1.0773, Test Loss: 0.8525\n",
      "Epoch 29/63, Train Loss: 1.0454, Test Loss: 0.8343\n",
      "Epoch 30/63, Train Loss: 1.0742, Test Loss: 0.8278\n",
      "Epoch 31/63, Train Loss: 1.0415, Test Loss: 0.8249\n",
      "Epoch 32/63, Train Loss: 1.0314, Test Loss: 0.8320\n",
      "Epoch 33/63, Train Loss: 1.0454, Test Loss: 0.8342\n",
      "Epoch 34/63, Train Loss: 1.0484, Test Loss: 0.8409\n",
      "Epoch 35/63, Train Loss: 1.0526, Test Loss: 0.8552\n",
      "Epoch 36/63, Train Loss: 1.0354, Test Loss: 0.8616\n",
      "Epoch 37/63, Train Loss: 1.0844, Test Loss: 0.8532\n",
      "Epoch 38/63, Train Loss: 1.0893, Test Loss: 0.8520\n",
      "Epoch 39/63, Train Loss: 1.0480, Test Loss: 0.8476\n",
      "Epoch 40/63, Train Loss: 1.0598, Test Loss: 0.8426\n",
      "Epoch 41/63, Train Loss: 1.0538, Test Loss: 0.8376\n",
      "Epoch 42/63, Train Loss: 1.0334, Test Loss: 0.8353\n",
      "Epoch 43/63, Train Loss: 1.0288, Test Loss: 0.8375\n",
      "Epoch 44/63, Train Loss: 1.0191, Test Loss: 0.8365\n",
      "Epoch 45/63, Train Loss: 1.0340, Test Loss: 0.8409\n",
      "Epoch 46/63, Train Loss: 1.0342, Test Loss: 0.8353\n",
      "Epoch 47/63, Train Loss: 1.0390, Test Loss: 0.8378\n",
      "Epoch 48/63, Train Loss: 1.0256, Test Loss: 0.8365\n",
      "Epoch 49/63, Train Loss: 1.0126, Test Loss: 0.8421\n",
      "Epoch 50/63, Train Loss: 1.0348, Test Loss: 0.8392\n",
      "Epoch 51/63, Train Loss: 1.0343, Test Loss: 0.8339\n",
      "Epoch 52/63, Train Loss: 1.0019, Test Loss: 0.8311\n",
      "Epoch 53/63, Train Loss: 1.0115, Test Loss: 0.8355\n",
      "Epoch 54/63, Train Loss: 1.0371, Test Loss: 0.8378\n",
      "Epoch 55/63, Train Loss: 1.0243, Test Loss: 0.8421\n",
      "Epoch 56/63, Train Loss: 1.0075, Test Loss: 0.8342\n",
      "Epoch 57/63, Train Loss: 1.0385, Test Loss: 0.8432\n",
      "Epoch 58/63, Train Loss: 1.0273, Test Loss: 0.8553\n",
      "Epoch 59/63, Train Loss: 1.0210, Test Loss: 0.8619\n",
      "Epoch 60/63, Train Loss: 0.9943, Test Loss: 0.8663\n",
      "Epoch 61/63, Train Loss: 1.0125, Test Loss: 0.8598\n",
      "Epoch 62/63, Train Loss: 1.0072, Test Loss: 0.8626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:14:11,898] Trial 224 finished with value: 0.8665679778371539 and parameters: {'num_hidden_layers': 9, 'layer_0_size': 237, 'layer_1_size': 105, 'layer_2_size': 190, 'layer_3_size': 152, 'layer_4_size': 82, 'layer_5_size': 137, 'layer_6_size': 215, 'layer_7_size': 195, 'layer_8_size': 63, 'dropout_rate': 0.2815638782137514, 'learning_rate': 0.0003728193162083617, 'batch_size': 32, 'epochs': 63}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/63, Train Loss: 0.9891, Test Loss: 0.8666\n",
      "Epoch 1/64, Train Loss: 1.2002, Test Loss: 1.1388\n",
      "Epoch 2/64, Train Loss: 1.1236, Test Loss: 1.1335\n",
      "Epoch 3/64, Train Loss: 1.0727, Test Loss: 1.1421\n",
      "Epoch 4/64, Train Loss: 1.0600, Test Loss: 1.1448\n",
      "Epoch 5/64, Train Loss: 1.0264, Test Loss: 1.1129\n",
      "Epoch 6/64, Train Loss: 1.0544, Test Loss: 1.0996\n",
      "Epoch 7/64, Train Loss: 1.0476, Test Loss: 1.0922\n",
      "Epoch 8/64, Train Loss: 1.0082, Test Loss: 1.0916\n",
      "Epoch 9/64, Train Loss: 1.0289, Test Loss: 1.0842\n",
      "Epoch 10/64, Train Loss: 1.0103, Test Loss: 1.1014\n",
      "Epoch 11/64, Train Loss: 1.0291, Test Loss: 1.0942\n",
      "Epoch 12/64, Train Loss: 1.0104, Test Loss: 1.1011\n",
      "Epoch 13/64, Train Loss: 1.0210, Test Loss: 1.0987\n",
      "Epoch 14/64, Train Loss: 1.0423, Test Loss: 1.1039\n",
      "Epoch 15/64, Train Loss: 1.0060, Test Loss: 1.1048\n",
      "Epoch 16/64, Train Loss: 0.9759, Test Loss: 1.1087\n",
      "Epoch 17/64, Train Loss: 1.0090, Test Loss: 1.1063\n",
      "Epoch 18/64, Train Loss: 0.9899, Test Loss: 1.1086\n",
      "Epoch 19/64, Train Loss: 0.9818, Test Loss: 1.1036\n",
      "Epoch 20/64, Train Loss: 1.0043, Test Loss: 1.1058\n",
      "Epoch 21/64, Train Loss: 0.9891, Test Loss: 1.0998\n",
      "Epoch 22/64, Train Loss: 0.9932, Test Loss: 1.0918\n",
      "Epoch 23/64, Train Loss: 1.0072, Test Loss: 1.0881\n",
      "Epoch 24/64, Train Loss: 0.9886, Test Loss: 1.0847\n",
      "Epoch 25/64, Train Loss: 0.9583, Test Loss: 1.0872\n",
      "Epoch 26/64, Train Loss: 0.9898, Test Loss: 1.0836\n",
      "Epoch 27/64, Train Loss: 0.9940, Test Loss: 1.0823\n",
      "Epoch 28/64, Train Loss: 0.9823, Test Loss: 1.0921\n",
      "Epoch 29/64, Train Loss: 0.9742, Test Loss: 1.0918\n",
      "Epoch 30/64, Train Loss: 0.9648, Test Loss: 1.0883\n",
      "Epoch 31/64, Train Loss: 0.9628, Test Loss: 1.0788\n",
      "Epoch 32/64, Train Loss: 0.9489, Test Loss: 1.0859\n",
      "Epoch 33/64, Train Loss: 0.9609, Test Loss: 1.0830\n",
      "Epoch 34/64, Train Loss: 0.9499, Test Loss: 1.0856\n",
      "Epoch 35/64, Train Loss: 0.9720, Test Loss: 1.0844\n",
      "Epoch 36/64, Train Loss: 0.9580, Test Loss: 1.0875\n",
      "Epoch 37/64, Train Loss: 0.9865, Test Loss: 1.0852\n",
      "Epoch 38/64, Train Loss: 0.9437, Test Loss: 1.0860\n",
      "Epoch 39/64, Train Loss: 0.9815, Test Loss: 1.0891\n",
      "Epoch 40/64, Train Loss: 0.9675, Test Loss: 1.0911\n",
      "Epoch 41/64, Train Loss: 0.9583, Test Loss: 1.0967\n",
      "Epoch 42/64, Train Loss: 0.9481, Test Loss: 1.0968\n",
      "Epoch 43/64, Train Loss: 0.9534, Test Loss: 1.1074\n",
      "Epoch 44/64, Train Loss: 0.9292, Test Loss: 1.1094\n",
      "Epoch 45/64, Train Loss: 0.9287, Test Loss: 1.1107\n",
      "Epoch 46/64, Train Loss: 0.9218, Test Loss: 1.1149\n",
      "Epoch 47/64, Train Loss: 0.9265, Test Loss: 1.1126\n",
      "Epoch 48/64, Train Loss: 0.9556, Test Loss: 1.1155\n",
      "Epoch 49/64, Train Loss: 0.9202, Test Loss: 1.1100\n",
      "Epoch 50/64, Train Loss: 0.9144, Test Loss: 1.1184\n",
      "Epoch 51/64, Train Loss: 0.9426, Test Loss: 1.1239\n",
      "Epoch 52/64, Train Loss: 0.9084, Test Loss: 1.1457\n",
      "Epoch 53/64, Train Loss: 0.9159, Test Loss: 1.1359\n",
      "Epoch 54/64, Train Loss: 0.8810, Test Loss: 1.1363\n",
      "Epoch 55/64, Train Loss: 0.8854, Test Loss: 1.1354\n",
      "Epoch 56/64, Train Loss: 0.9319, Test Loss: 1.1496\n",
      "Epoch 57/64, Train Loss: 0.8469, Test Loss: 1.1325\n",
      "Epoch 58/64, Train Loss: 0.8406, Test Loss: 1.1726\n",
      "Epoch 59/64, Train Loss: 0.8684, Test Loss: 1.1703\n",
      "Epoch 60/64, Train Loss: 0.8456, Test Loss: 1.1486\n",
      "Epoch 61/64, Train Loss: 0.8498, Test Loss: 1.1704\n",
      "Epoch 62/64, Train Loss: 0.8358, Test Loss: 1.1611\n",
      "Epoch 63/64, Train Loss: 0.8010, Test Loss: 1.1744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:14:20,238] Trial 225 finished with value: 1.1838209799357824 and parameters: {'num_hidden_layers': 9, 'layer_0_size': 240, 'layer_1_size': 100, 'layer_2_size': 189, 'layer_3_size': 134, 'layer_4_size': 85, 'layer_5_size': 137, 'layer_6_size': 221, 'layer_7_size': 196, 'layer_8_size': 59, 'dropout_rate': 0.27445336529167796, 'learning_rate': 0.00040914321309409065, 'batch_size': 32, 'epochs': 64}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/64, Train Loss: 0.8684, Test Loss: 1.1838\n",
      "Epoch 1/62, Train Loss: 1.1721, Test Loss: 0.8905\n",
      "Epoch 2/62, Train Loss: 1.0996, Test Loss: 0.8524\n",
      "Epoch 3/62, Train Loss: 1.0827, Test Loss: 0.8577\n",
      "Epoch 4/62, Train Loss: 1.0560, Test Loss: 0.8750\n",
      "Epoch 5/62, Train Loss: 1.0770, Test Loss: 0.8787\n",
      "Epoch 6/62, Train Loss: 1.0602, Test Loss: 0.8640\n",
      "Epoch 7/62, Train Loss: 1.0590, Test Loss: 0.8668\n",
      "Epoch 8/62, Train Loss: 1.0424, Test Loss: 0.8643\n",
      "Epoch 9/62, Train Loss: 1.0533, Test Loss: 0.8499\n",
      "Epoch 10/62, Train Loss: 1.0560, Test Loss: 0.8446\n",
      "Epoch 11/62, Train Loss: 1.0463, Test Loss: 0.8453\n",
      "Epoch 12/62, Train Loss: 1.0278, Test Loss: 0.8449\n",
      "Epoch 13/62, Train Loss: 1.0046, Test Loss: 0.8529\n",
      "Epoch 14/62, Train Loss: 1.0183, Test Loss: 0.8517\n",
      "Epoch 15/62, Train Loss: 1.0405, Test Loss: 0.8574\n",
      "Epoch 16/62, Train Loss: 0.9976, Test Loss: 0.8553\n",
      "Epoch 17/62, Train Loss: 1.0199, Test Loss: 0.8508\n",
      "Epoch 18/62, Train Loss: 1.0344, Test Loss: 0.8496\n",
      "Epoch 19/62, Train Loss: 1.0221, Test Loss: 0.8537\n",
      "Epoch 20/62, Train Loss: 1.0125, Test Loss: 0.8526\n",
      "Epoch 21/62, Train Loss: 1.0161, Test Loss: 0.8547\n",
      "Epoch 22/62, Train Loss: 1.0018, Test Loss: 0.8509\n",
      "Epoch 23/62, Train Loss: 1.0011, Test Loss: 0.8511\n",
      "Epoch 24/62, Train Loss: 1.0251, Test Loss: 0.8517\n",
      "Epoch 25/62, Train Loss: 1.0181, Test Loss: 0.8464\n",
      "Epoch 26/62, Train Loss: 0.9750, Test Loss: 0.8466\n",
      "Epoch 27/62, Train Loss: 1.0327, Test Loss: 0.8501\n",
      "Epoch 28/62, Train Loss: 1.0158, Test Loss: 0.8478\n",
      "Epoch 29/62, Train Loss: 0.9981, Test Loss: 0.8496\n",
      "Epoch 30/62, Train Loss: 0.9927, Test Loss: 0.8461\n",
      "Epoch 31/62, Train Loss: 0.9910, Test Loss: 0.8498\n",
      "Epoch 32/62, Train Loss: 0.9908, Test Loss: 0.8520\n",
      "Epoch 33/62, Train Loss: 0.9896, Test Loss: 0.8533\n",
      "Epoch 34/62, Train Loss: 0.9869, Test Loss: 0.8498\n",
      "Epoch 35/62, Train Loss: 0.9848, Test Loss: 0.8501\n",
      "Epoch 36/62, Train Loss: 0.9952, Test Loss: 0.8519\n",
      "Epoch 37/62, Train Loss: 0.9679, Test Loss: 0.8530\n",
      "Epoch 38/62, Train Loss: 0.9800, Test Loss: 0.8588\n",
      "Epoch 39/62, Train Loss: 0.9851, Test Loss: 0.8590\n",
      "Epoch 40/62, Train Loss: 0.9796, Test Loss: 0.8639\n",
      "Epoch 41/62, Train Loss: 0.9682, Test Loss: 0.8826\n",
      "Epoch 42/62, Train Loss: 0.9562, Test Loss: 0.8741\n",
      "Epoch 43/62, Train Loss: 0.9641, Test Loss: 0.8707\n",
      "Epoch 44/62, Train Loss: 0.9809, Test Loss: 0.8761\n",
      "Epoch 45/62, Train Loss: 0.9326, Test Loss: 0.8783\n",
      "Epoch 46/62, Train Loss: 0.9372, Test Loss: 0.8820\n",
      "Epoch 47/62, Train Loss: 0.9571, Test Loss: 0.8747\n",
      "Epoch 48/62, Train Loss: 0.9365, Test Loss: 0.8752\n",
      "Epoch 49/62, Train Loss: 0.9158, Test Loss: 0.8800\n",
      "Epoch 50/62, Train Loss: 0.9312, Test Loss: 0.8804\n",
      "Epoch 51/62, Train Loss: 0.9271, Test Loss: 0.8763\n",
      "Epoch 52/62, Train Loss: 0.8927, Test Loss: 0.8832\n",
      "Epoch 53/62, Train Loss: 0.9093, Test Loss: 0.8865\n",
      "Epoch 54/62, Train Loss: 0.8971, Test Loss: 0.8813\n",
      "Epoch 55/62, Train Loss: 0.9212, Test Loss: 0.8956\n",
      "Epoch 56/62, Train Loss: 0.8642, Test Loss: 0.9122\n",
      "Epoch 57/62, Train Loss: 0.8633, Test Loss: 0.9101\n",
      "Epoch 58/62, Train Loss: 0.8622, Test Loss: 0.9378\n",
      "Epoch 59/62, Train Loss: 0.8613, Test Loss: 0.9379\n",
      "Epoch 60/62, Train Loss: 0.8444, Test Loss: 0.9373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:14:31,694] Trial 226 finished with value: 0.9464090083326612 and parameters: {'num_hidden_layers': 9, 'layer_0_size': 239, 'layer_1_size': 101, 'layer_2_size': 192, 'layer_3_size': 190, 'layer_4_size': 71, 'layer_5_size': 143, 'layer_6_size': 213, 'layer_7_size': 203, 'layer_8_size': 54, 'dropout_rate': 0.2820731845883067, 'learning_rate': 0.0003777337971173677, 'batch_size': 32, 'epochs': 62}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/62, Train Loss: 0.8513, Test Loss: 0.9348\n",
      "Epoch 62/62, Train Loss: 0.8523, Test Loss: 0.9464\n",
      "Epoch 1/66, Train Loss: 1.4461, Test Loss: 1.1189\n",
      "Epoch 2/66, Train Loss: 1.1756, Test Loss: 1.1549\n",
      "Epoch 3/66, Train Loss: 1.1209, Test Loss: 1.0940\n",
      "Epoch 4/66, Train Loss: 1.0437, Test Loss: 1.0650\n",
      "Epoch 5/66, Train Loss: 1.0304, Test Loss: 1.0619\n",
      "Epoch 6/66, Train Loss: 1.0489, Test Loss: 1.0589\n",
      "Epoch 7/66, Train Loss: 1.0214, Test Loss: 1.0535\n",
      "Epoch 8/66, Train Loss: 1.0340, Test Loss: 1.0552\n",
      "Epoch 9/66, Train Loss: 1.0214, Test Loss: 1.0532\n",
      "Epoch 10/66, Train Loss: 0.9765, Test Loss: 1.0682\n",
      "Epoch 11/66, Train Loss: 1.0095, Test Loss: 1.0657\n",
      "Epoch 12/66, Train Loss: 1.0210, Test Loss: 1.0645\n",
      "Epoch 13/66, Train Loss: 1.0094, Test Loss: 1.0501\n",
      "Epoch 14/66, Train Loss: 1.0163, Test Loss: 1.0625\n",
      "Epoch 15/66, Train Loss: 1.0034, Test Loss: 1.0581\n",
      "Epoch 16/66, Train Loss: 0.9936, Test Loss: 1.0539\n",
      "Epoch 17/66, Train Loss: 0.9775, Test Loss: 1.0594\n",
      "Epoch 18/66, Train Loss: 0.9552, Test Loss: 1.0649\n",
      "Epoch 19/66, Train Loss: 0.9815, Test Loss: 1.0641\n",
      "Epoch 20/66, Train Loss: 0.9755, Test Loss: 1.0711\n",
      "Epoch 21/66, Train Loss: 0.9599, Test Loss: 1.0672\n",
      "Epoch 22/66, Train Loss: 0.9632, Test Loss: 1.0623\n",
      "Epoch 23/66, Train Loss: 0.9906, Test Loss: 1.0601\n",
      "Epoch 24/66, Train Loss: 0.9778, Test Loss: 1.0603\n",
      "Epoch 25/66, Train Loss: 0.9793, Test Loss: 1.0677\n",
      "Epoch 26/66, Train Loss: 0.9609, Test Loss: 1.0674\n",
      "Epoch 27/66, Train Loss: 0.9630, Test Loss: 1.0775\n",
      "Epoch 28/66, Train Loss: 0.9397, Test Loss: 1.0723\n",
      "Epoch 29/66, Train Loss: 0.9793, Test Loss: 1.0698\n",
      "Epoch 30/66, Train Loss: 0.9620, Test Loss: 1.0626\n",
      "Epoch 31/66, Train Loss: 0.9454, Test Loss: 1.0612\n",
      "Epoch 32/66, Train Loss: 0.9531, Test Loss: 1.0668\n",
      "Epoch 33/66, Train Loss: 0.9618, Test Loss: 1.0631\n",
      "Epoch 34/66, Train Loss: 0.9756, Test Loss: 1.0572\n",
      "Epoch 35/66, Train Loss: 0.9783, Test Loss: 1.0566\n",
      "Epoch 36/66, Train Loss: 0.9534, Test Loss: 1.0565\n",
      "Epoch 37/66, Train Loss: 0.9413, Test Loss: 1.0568\n",
      "Epoch 38/66, Train Loss: 0.9637, Test Loss: 1.0641\n",
      "Epoch 39/66, Train Loss: 0.9690, Test Loss: 1.0611\n",
      "Epoch 40/66, Train Loss: 0.9461, Test Loss: 1.0646\n",
      "Epoch 41/66, Train Loss: 0.9470, Test Loss: 1.0675\n",
      "Epoch 42/66, Train Loss: 0.9750, Test Loss: 1.0755\n",
      "Epoch 43/66, Train Loss: 0.9586, Test Loss: 1.0696\n",
      "Epoch 44/66, Train Loss: 0.9529, Test Loss: 1.0727\n",
      "Epoch 45/66, Train Loss: 0.9480, Test Loss: 1.0745\n",
      "Epoch 46/66, Train Loss: 0.9518, Test Loss: 1.0767\n",
      "Epoch 47/66, Train Loss: 0.9516, Test Loss: 1.0700\n",
      "Epoch 48/66, Train Loss: 0.9591, Test Loss: 1.0743\n",
      "Epoch 49/66, Train Loss: 0.9559, Test Loss: 1.0681\n",
      "Epoch 50/66, Train Loss: 0.9284, Test Loss: 1.0692\n",
      "Epoch 51/66, Train Loss: 0.9670, Test Loss: 1.0706\n",
      "Epoch 52/66, Train Loss: 0.9481, Test Loss: 1.0708\n",
      "Epoch 53/66, Train Loss: 0.9646, Test Loss: 1.0714\n",
      "Epoch 54/66, Train Loss: 0.9413, Test Loss: 1.0659\n",
      "Epoch 55/66, Train Loss: 0.9675, Test Loss: 1.0651\n",
      "Epoch 56/66, Train Loss: 0.9331, Test Loss: 1.0632\n",
      "Epoch 57/66, Train Loss: 0.9547, Test Loss: 1.0593\n",
      "Epoch 58/66, Train Loss: 0.9434, Test Loss: 1.0598\n",
      "Epoch 59/66, Train Loss: 0.9274, Test Loss: 1.0591\n",
      "Epoch 60/66, Train Loss: 0.9383, Test Loss: 1.0631\n",
      "Epoch 61/66, Train Loss: 0.9465, Test Loss: 1.0600\n",
      "Epoch 62/66, Train Loss: 0.9332, Test Loss: 1.0565\n",
      "Epoch 63/66, Train Loss: 0.9381, Test Loss: 1.0538\n",
      "Epoch 64/66, Train Loss: 0.9238, Test Loss: 1.0577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:14:43,554] Trial 227 finished with value: 1.051335837159838 and parameters: {'num_hidden_layers': 10, 'layer_0_size': 249, 'layer_1_size': 116, 'layer_2_size': 178, 'layer_3_size': 100, 'layer_4_size': 79, 'layer_5_size': 147, 'layer_6_size': 215, 'layer_7_size': 191, 'layer_8_size': 63, 'layer_9_size': 45, 'dropout_rate': 0.26548703421356307, 'learning_rate': 0.0004308118141596547, 'batch_size': 32, 'epochs': 66}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/66, Train Loss: 0.9371, Test Loss: 1.0542\n",
      "Epoch 66/66, Train Loss: 0.9136, Test Loss: 1.0513\n",
      "Epoch 1/65, Train Loss: 1.1130, Test Loss: 1.1758\n",
      "Epoch 2/65, Train Loss: 1.0939, Test Loss: 1.1821\n",
      "Epoch 3/65, Train Loss: 1.1034, Test Loss: 1.2152\n",
      "Epoch 4/65, Train Loss: 1.0422, Test Loss: 1.1812\n",
      "Epoch 5/65, Train Loss: 1.0638, Test Loss: 1.1793\n",
      "Epoch 6/65, Train Loss: 1.0518, Test Loss: 1.1632\n",
      "Epoch 7/65, Train Loss: 1.0960, Test Loss: 1.1701\n",
      "Epoch 8/65, Train Loss: 1.0048, Test Loss: 1.1736\n",
      "Epoch 9/65, Train Loss: 1.0103, Test Loss: 1.1726\n",
      "Epoch 10/65, Train Loss: 1.0636, Test Loss: 1.1764\n",
      "Epoch 11/65, Train Loss: 1.0269, Test Loss: 1.1609\n",
      "Epoch 12/65, Train Loss: 1.0025, Test Loss: 1.1853\n",
      "Epoch 13/65, Train Loss: 1.0359, Test Loss: 1.1850\n",
      "Epoch 14/65, Train Loss: 1.0538, Test Loss: 1.1785\n",
      "Epoch 15/65, Train Loss: 1.0340, Test Loss: 1.1787\n",
      "Epoch 16/65, Train Loss: 1.0271, Test Loss: 1.1740\n",
      "Epoch 17/65, Train Loss: 0.9874, Test Loss: 1.1674\n",
      "Epoch 18/65, Train Loss: 0.9969, Test Loss: 1.1624\n",
      "Epoch 19/65, Train Loss: 1.0186, Test Loss: 1.1551\n",
      "Epoch 20/65, Train Loss: 1.0277, Test Loss: 1.1646\n",
      "Epoch 21/65, Train Loss: 0.9986, Test Loss: 1.1644\n",
      "Epoch 22/65, Train Loss: 0.9966, Test Loss: 1.1638\n",
      "Epoch 23/65, Train Loss: 1.0150, Test Loss: 1.1764\n",
      "Epoch 24/65, Train Loss: 0.9912, Test Loss: 1.1762\n",
      "Epoch 25/65, Train Loss: 0.9815, Test Loss: 1.1804\n",
      "Epoch 26/65, Train Loss: 1.0208, Test Loss: 1.1817\n",
      "Epoch 27/65, Train Loss: 0.9986, Test Loss: 1.1765\n",
      "Epoch 28/65, Train Loss: 0.9725, Test Loss: 1.1744\n",
      "Epoch 29/65, Train Loss: 0.9584, Test Loss: 1.1719\n",
      "Epoch 30/65, Train Loss: 0.9940, Test Loss: 1.1721\n",
      "Epoch 31/65, Train Loss: 0.9822, Test Loss: 1.1793\n",
      "Epoch 32/65, Train Loss: 0.9739, Test Loss: 1.1824\n",
      "Epoch 33/65, Train Loss: 0.9790, Test Loss: 1.1779\n",
      "Epoch 34/65, Train Loss: 1.0246, Test Loss: 1.1872\n",
      "Epoch 35/65, Train Loss: 0.9615, Test Loss: 1.1925\n",
      "Epoch 36/65, Train Loss: 0.9484, Test Loss: 1.1952\n",
      "Epoch 37/65, Train Loss: 0.9399, Test Loss: 1.2092\n",
      "Epoch 38/65, Train Loss: 0.9530, Test Loss: 1.2036\n",
      "Epoch 39/65, Train Loss: 0.9534, Test Loss: 1.2016\n",
      "Epoch 40/65, Train Loss: 0.9531, Test Loss: 1.1913\n",
      "Epoch 41/65, Train Loss: 0.9346, Test Loss: 1.2069\n",
      "Epoch 42/65, Train Loss: 0.9384, Test Loss: 1.2153\n",
      "Epoch 43/65, Train Loss: 0.9552, Test Loss: 1.2052\n",
      "Epoch 44/65, Train Loss: 0.9550, Test Loss: 1.2067\n",
      "Epoch 45/65, Train Loss: 0.9463, Test Loss: 1.2034\n",
      "Epoch 46/65, Train Loss: 0.9137, Test Loss: 1.2157\n",
      "Epoch 47/65, Train Loss: 0.9231, Test Loss: 1.2191\n",
      "Epoch 48/65, Train Loss: 0.9390, Test Loss: 1.2188\n",
      "Epoch 49/65, Train Loss: 0.8957, Test Loss: 1.2356\n",
      "Epoch 50/65, Train Loss: 0.9028, Test Loss: 1.2470\n",
      "Epoch 51/65, Train Loss: 0.9251, Test Loss: 1.2561\n",
      "Epoch 52/65, Train Loss: 0.9276, Test Loss: 1.2631\n",
      "Epoch 53/65, Train Loss: 0.9101, Test Loss: 1.2376\n",
      "Epoch 54/65, Train Loss: 0.9018, Test Loss: 1.2616\n",
      "Epoch 55/65, Train Loss: 0.9001, Test Loss: 1.2772\n",
      "Epoch 56/65, Train Loss: 0.8808, Test Loss: 1.2642\n",
      "Epoch 57/65, Train Loss: 0.8232, Test Loss: 1.2810\n",
      "Epoch 58/65, Train Loss: 0.8633, Test Loss: 1.2657\n",
      "Epoch 59/65, Train Loss: 0.8498, Test Loss: 1.2802\n",
      "Epoch 60/65, Train Loss: 0.8311, Test Loss: 1.2698\n",
      "Epoch 61/65, Train Loss: 0.8363, Test Loss: 1.2529\n",
      "Epoch 62/65, Train Loss: 0.8293, Test Loss: 1.2684\n",
      "Epoch 63/65, Train Loss: 0.8709, Test Loss: 1.2676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:14:53,701] Trial 228 finished with value: 1.2819907580103194 and parameters: {'num_hidden_layers': 9, 'layer_0_size': 229, 'layer_1_size': 111, 'layer_2_size': 195, 'layer_3_size': 121, 'layer_4_size': 90, 'layer_5_size': 136, 'layer_6_size': 225, 'layer_7_size': 187, 'layer_8_size': 72, 'dropout_rate': 0.256045803833059, 'learning_rate': 0.00035541461785542554, 'batch_size': 32, 'epochs': 65}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/65, Train Loss: 0.8164, Test Loss: 1.2945\n",
      "Epoch 65/65, Train Loss: 0.8098, Test Loss: 1.2820\n",
      "Epoch 1/63, Train Loss: 1.2506, Test Loss: 1.0491\n",
      "Epoch 2/63, Train Loss: 1.1614, Test Loss: 1.0908\n",
      "Epoch 3/63, Train Loss: 1.1630, Test Loss: 1.0468\n",
      "Epoch 4/63, Train Loss: 1.1206, Test Loss: 1.0656\n",
      "Epoch 5/63, Train Loss: 1.1860, Test Loss: 1.0667\n",
      "Epoch 6/63, Train Loss: 1.1225, Test Loss: 1.0888\n",
      "Epoch 7/63, Train Loss: 1.0939, Test Loss: 1.0512\n",
      "Epoch 8/63, Train Loss: 1.0748, Test Loss: 1.0412\n",
      "Epoch 9/63, Train Loss: 1.0558, Test Loss: 1.0530\n",
      "Epoch 10/63, Train Loss: 1.0705, Test Loss: 1.0455\n",
      "Epoch 11/63, Train Loss: 1.0195, Test Loss: 1.1118\n",
      "Epoch 12/63, Train Loss: 1.0370, Test Loss: 1.0854\n",
      "Epoch 13/63, Train Loss: 0.9796, Test Loss: 1.0801\n",
      "Epoch 14/63, Train Loss: 0.9773, Test Loss: 1.1131\n",
      "Epoch 15/63, Train Loss: 0.9778, Test Loss: 1.0849\n",
      "Epoch 16/63, Train Loss: 0.9141, Test Loss: 1.0908\n",
      "Epoch 17/63, Train Loss: 0.8888, Test Loss: 1.0675\n",
      "Epoch 18/63, Train Loss: 0.8494, Test Loss: 1.1005\n",
      "Epoch 19/63, Train Loss: 0.8578, Test Loss: 1.1037\n",
      "Epoch 20/63, Train Loss: 0.8496, Test Loss: 1.0937\n",
      "Epoch 21/63, Train Loss: 0.8059, Test Loss: 1.0938\n",
      "Epoch 22/63, Train Loss: 0.7887, Test Loss: 1.1591\n",
      "Epoch 23/63, Train Loss: 0.7256, Test Loss: 1.1775\n",
      "Epoch 24/63, Train Loss: 0.6838, Test Loss: 1.1481\n",
      "Epoch 25/63, Train Loss: 0.6867, Test Loss: 1.1769\n",
      "Epoch 26/63, Train Loss: 0.6219, Test Loss: 1.2100\n",
      "Epoch 27/63, Train Loss: 0.6242, Test Loss: 1.2124\n",
      "Epoch 28/63, Train Loss: 0.6116, Test Loss: 1.2170\n",
      "Epoch 29/63, Train Loss: 0.6104, Test Loss: 1.2215\n",
      "Epoch 30/63, Train Loss: 0.5989, Test Loss: 1.1448\n",
      "Epoch 31/63, Train Loss: 0.6067, Test Loss: 1.1265\n",
      "Epoch 32/63, Train Loss: 0.5605, Test Loss: 1.1301\n",
      "Epoch 33/63, Train Loss: 0.5651, Test Loss: 1.2321\n",
      "Epoch 34/63, Train Loss: 0.5584, Test Loss: 1.2039\n",
      "Epoch 35/63, Train Loss: 0.6069, Test Loss: 1.1852\n",
      "Epoch 36/63, Train Loss: 0.5466, Test Loss: 1.2236\n",
      "Epoch 37/63, Train Loss: 0.4812, Test Loss: 1.2423\n",
      "Epoch 38/63, Train Loss: 0.5043, Test Loss: 1.2368\n",
      "Epoch 39/63, Train Loss: 0.5322, Test Loss: 1.2576\n",
      "Epoch 40/63, Train Loss: 0.4691, Test Loss: 1.3299\n",
      "Epoch 41/63, Train Loss: 0.4337, Test Loss: 1.2740\n",
      "Epoch 42/63, Train Loss: 0.4631, Test Loss: 1.2873\n",
      "Epoch 43/63, Train Loss: 0.4565, Test Loss: 1.2695\n",
      "Epoch 44/63, Train Loss: 0.4701, Test Loss: 1.2966\n",
      "Epoch 45/63, Train Loss: 0.4747, Test Loss: 1.2521\n",
      "Epoch 46/63, Train Loss: 0.4478, Test Loss: 1.1942\n",
      "Epoch 47/63, Train Loss: 0.4195, Test Loss: 1.2152\n",
      "Epoch 48/63, Train Loss: 0.4363, Test Loss: 1.2345\n",
      "Epoch 49/63, Train Loss: 0.4621, Test Loss: 1.2469\n",
      "Epoch 50/63, Train Loss: 0.3906, Test Loss: 1.2246\n",
      "Epoch 51/63, Train Loss: 0.3860, Test Loss: 1.2662\n",
      "Epoch 52/63, Train Loss: 0.4209, Test Loss: 1.2141\n",
      "Epoch 53/63, Train Loss: 0.4327, Test Loss: 1.2792\n",
      "Epoch 54/63, Train Loss: 0.3842, Test Loss: 1.3220\n",
      "Epoch 55/63, Train Loss: 0.3519, Test Loss: 1.2184\n",
      "Epoch 56/63, Train Loss: 0.4131, Test Loss: 1.2456\n",
      "Epoch 57/63, Train Loss: 0.3863, Test Loss: 1.2812\n",
      "Epoch 58/63, Train Loss: 0.3828, Test Loss: 1.2816\n",
      "Epoch 59/63, Train Loss: 0.3516, Test Loss: 1.2476\n",
      "Epoch 60/63, Train Loss: 0.3619, Test Loss: 1.2652\n",
      "Epoch 61/63, Train Loss: 0.3881, Test Loss: 1.1843\n",
      "Epoch 62/63, Train Loss: 0.4018, Test Loss: 1.2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-26 12:15:01,887] Trial 229 finished with value: 1.2306502205984933 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 218, 'layer_1_size': 104, 'layer_2_size': 185, 'layer_3_size': 151, 'layer_4_size': 82, 'layer_5_size': 130, 'layer_6_size': 235, 'layer_7_size': 198, 'dropout_rate': 0.12886100853846166, 'learning_rate': 0.0005135808065726896, 'batch_size': 32, 'epochs': 63}. Best is trial 65 with value: 0.6746163879122052.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/63, Train Loss: 0.3448, Test Loss: 1.2307\n",
      "Best trial:\n",
      "  Value: 0.6746163879122052\n",
      "  Params:\n",
      "    num_hidden_layers: 12\n",
      "    layer_0_size: 164\n",
      "    layer_1_size: 191\n",
      "    layer_2_size: 224\n",
      "    layer_3_size: 114\n",
      "    layer_4_size: 164\n",
      "    layer_5_size: 90\n",
      "    layer_6_size: 182\n",
      "    layer_7_size: 208\n",
      "    layer_8_size: 72\n",
      "    layer_9_size: 43\n",
      "    layer_10_size: 248\n",
      "    layer_11_size: 76\n",
      "    dropout_rate: 0.2542002795679332\n",
      "    learning_rate: 0.00016799883676835427\n",
      "    batch_size: 32\n",
      "    epochs: 57\n"
     ]
    }
   ],
   "source": [
    "# Creating function for training and evaluating the model\n",
    "def train_and_evaluate_model(model, train_loader, test_loader, criterion, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0.0\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item()\n",
    "            test_loss /= len(test_loader)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
    "    \n",
    "    return test_loss\n",
    "\n",
    "# Creating function for selection of best hyperparameters \n",
    "def objective(trial):\n",
    "    # Defining hyperparameters\n",
    "    num_hidden_layers = trial.suggest_int('num_hidden_layers', 1, 20)\n",
    "    hidden_layer_sizes = [\n",
    "        trial.suggest_int(f'layer_{i}_size', 32, 256)\n",
    "        for i in range(num_hidden_layers)\n",
    "    ]\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    epochs = trial.suggest_int('epochs', 5, 100)\n",
    "    input_size = 59\n",
    "\n",
    "    # Creating a model and defining of optimizer\n",
    "    model = FlexibleRegressionNN(input_size, hidden_layer_sizes, dropout_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Dataloader\n",
    "    train_data = torch.randn(1000, input_size), torch.randn(1000, 1)\n",
    "    test_data = torch.randn(200, input_size), torch.randn(200, 1)\n",
    "    train_loader = DataLoader(TensorDataset(*train_data), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(TensorDataset(*test_data), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Training launch and return the test loss\n",
    "    return train_and_evaluate_model(model, train_loader, test_loader, criterion, optimizer, epochs)\n",
    "\n",
    "# Settings of searching by Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=230)\n",
    "\n",
    "# Getting the best hyperparameters\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
