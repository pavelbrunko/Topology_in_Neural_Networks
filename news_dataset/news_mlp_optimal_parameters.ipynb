{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting optimal MLP's hiperparameters for news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "pd.set_option('display.max_columns', 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank( timedelta)</th>\n",
       "      <th>rank( n_tokens_title)</th>\n",
       "      <th>rank( n_tokens_content)</th>\n",
       "      <th>rank( n_unique_tokens)</th>\n",
       "      <th>rank( n_non_stop_words)</th>\n",
       "      <th>rank( n_non_stop_unique_tokens)</th>\n",
       "      <th>rank( num_hrefs)</th>\n",
       "      <th>rank( num_self_hrefs)</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>rank( average_token_length)</th>\n",
       "      <th>rank( num_keywords)</th>\n",
       "      <th>data_channel_is_lifestyle</th>\n",
       "      <th>data_channel_is_entertainment</th>\n",
       "      <th>data_channel_is_bus</th>\n",
       "      <th>data_channel_is_socmed</th>\n",
       "      <th>data_channel_is_tech</th>\n",
       "      <th>data_channel_is_world</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>rank( kw_max_min)</th>\n",
       "      <th>rank( kw_avg_min)</th>\n",
       "      <th>kw_min_max</th>\n",
       "      <th>kw_max_max</th>\n",
       "      <th>rank( kw_avg_max)</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>rank( kw_max_avg)</th>\n",
       "      <th>rank( kw_avg_avg)</th>\n",
       "      <th>rank( self_reference_min_shares)</th>\n",
       "      <th>rank( self_reference_max_shares)</th>\n",
       "      <th>rank( self_reference_avg_sharess)</th>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <th>weekday_is_saturday</th>\n",
       "      <th>weekday_is_sunday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>rank( LDA_00)</th>\n",
       "      <th>rank( LDA_01)</th>\n",
       "      <th>rank( LDA_02)</th>\n",
       "      <th>rank( LDA_03)</th>\n",
       "      <th>rank( LDA_04)</th>\n",
       "      <th>rank( global_subjectivity)</th>\n",
       "      <th>rank( global_sentiment_polarity)</th>\n",
       "      <th>rank( global_rate_positive_words)</th>\n",
       "      <th>rank( global_rate_negative_words)</th>\n",
       "      <th>rank( rate_positive_words)</th>\n",
       "      <th>rank( rate_negative_words)</th>\n",
       "      <th>rank( avg_positive_polarity)</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>rank( avg_negative_polarity)</th>\n",
       "      <th>rank( min_negative_polarity)</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.091313</td>\n",
       "      <td>0.746312</td>\n",
       "      <td>-0.830751</td>\n",
       "      <td>1.171875</td>\n",
       "      <td>-0.859076</td>\n",
       "      <td>1.250329</td>\n",
       "      <td>-0.773700</td>\n",
       "      <td>-0.247319</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060832</td>\n",
       "      <td>-1.077791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.314162</td>\n",
       "      <td>-2.013451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-0.809729</td>\n",
       "      <td>-0.897243</td>\n",
       "      <td>-0.895353</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.011874</td>\n",
       "      <td>1.107189</td>\n",
       "      <td>0.003762</td>\n",
       "      <td>0.133906</td>\n",
       "      <td>-0.019697</td>\n",
       "      <td>0.834551</td>\n",
       "      <td>-0.299737</td>\n",
       "      <td>0.407371</td>\n",
       "      <td>-0.186943</td>\n",
       "      <td>0.425334</td>\n",
       "      <td>-0.344559</td>\n",
       "      <td>0.257404</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.840613</td>\n",
       "      <td>-0.309042</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.091313</td>\n",
       "      <td>-0.627749</td>\n",
       "      <td>-0.629945</td>\n",
       "      <td>0.636396</td>\n",
       "      <td>-0.682170</td>\n",
       "      <td>1.037852</td>\n",
       "      <td>-1.082209</td>\n",
       "      <td>-0.770293</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.874264</td>\n",
       "      <td>-1.654145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.314162</td>\n",
       "      <td>-2.013451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-1.337803</td>\n",
       "      <td>-1.337726</td>\n",
       "      <td>-1.337803</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.506066</td>\n",
       "      <td>0.481113</td>\n",
       "      <td>0.164789</td>\n",
       "      <td>0.215657</td>\n",
       "      <td>0.071541</td>\n",
       "      <td>-1.206170</td>\n",
       "      <td>0.345029</td>\n",
       "      <td>0.255150</td>\n",
       "      <td>0.038674</td>\n",
       "      <td>0.167738</td>\n",
       "      <td>-0.092096</td>\n",
       "      <td>-0.900984</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.303623</td>\n",
       "      <td>1.355812</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.091313</td>\n",
       "      <td>-0.627749</td>\n",
       "      <td>-0.877419</td>\n",
       "      <td>0.346405</td>\n",
       "      <td>-0.977397</td>\n",
       "      <td>-0.284000</td>\n",
       "      <td>-1.082209</td>\n",
       "      <td>-0.770293</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.983430</td>\n",
       "      <td>-0.561454</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.314162</td>\n",
       "      <td>-2.013451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-0.314418</td>\n",
       "      <td>-0.753138</td>\n",
       "      <td>-0.722589</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.628018</td>\n",
       "      <td>-0.102896</td>\n",
       "      <td>-0.194219</td>\n",
       "      <td>-0.319837</td>\n",
       "      <td>1.102533</td>\n",
       "      <td>2.439542</td>\n",
       "      <td>2.017790</td>\n",
       "      <td>1.034661</td>\n",
       "      <td>-0.690612</td>\n",
       "      <td>1.106431</td>\n",
       "      <td>-0.977397</td>\n",
       "      <td>1.564511</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-1.637609</td>\n",
       "      <td>-0.868165</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.091313</td>\n",
       "      <td>-0.627749</td>\n",
       "      <td>0.309539</td>\n",
       "      <td>-0.344123</td>\n",
       "      <td>0.321434</td>\n",
       "      <td>-0.264207</td>\n",
       "      <td>0.207090</td>\n",
       "      <td>-1.494860</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.947209</td>\n",
       "      <td>-0.086922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.314162</td>\n",
       "      <td>-2.013451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-1.337803</td>\n",
       "      <td>-1.337726</td>\n",
       "      <td>-1.337803</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.437130</td>\n",
       "      <td>1.172315</td>\n",
       "      <td>0.875191</td>\n",
       "      <td>-0.392816</td>\n",
       "      <td>-0.811837</td>\n",
       "      <td>-0.291810</td>\n",
       "      <td>-0.208737</td>\n",
       "      <td>0.150161</td>\n",
       "      <td>0.574307</td>\n",
       "      <td>-0.284132</td>\n",
       "      <td>0.363172</td>\n",
       "      <td>0.351546</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-0.984764</td>\n",
       "      <td>-0.309042</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.091313</td>\n",
       "      <td>1.191898</td>\n",
       "      <td>1.253929</td>\n",
       "      <td>-1.200952</td>\n",
       "      <td>1.406604</td>\n",
       "      <td>-1.358432</td>\n",
       "      <td>1.046996</td>\n",
       "      <td>2.268357</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.069322</td>\n",
       "      <td>-0.086922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.314162</td>\n",
       "      <td>-2.013451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-3.091313</td>\n",
       "      <td>-0.768720</td>\n",
       "      <td>1.144183</td>\n",
       "      <td>0.294747</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.314651</td>\n",
       "      <td>-0.220448</td>\n",
       "      <td>-0.531720</td>\n",
       "      <td>-0.626094</td>\n",
       "      <td>1.680026</td>\n",
       "      <td>0.738234</td>\n",
       "      <td>1.686806</td>\n",
       "      <td>1.934743</td>\n",
       "      <td>-0.367261</td>\n",
       "      <td>1.143818</td>\n",
       "      <td>-1.009924</td>\n",
       "      <td>0.671991</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.327633</td>\n",
       "      <td>0.032505</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39639</th>\n",
       "      <td>-3.716757</td>\n",
       "      <td>0.299307</td>\n",
       "      <td>-0.208059</td>\n",
       "      <td>-0.100354</td>\n",
       "      <td>-0.394045</td>\n",
       "      <td>-0.060357</td>\n",
       "      <td>0.207090</td>\n",
       "      <td>1.378593</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.507837</td>\n",
       "      <td>0.343989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.024409</td>\n",
       "      <td>-0.424676</td>\n",
       "      <td>26900.0</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>0.911184</td>\n",
       "      <td>2514.742857</td>\n",
       "      <td>-0.200666</td>\n",
       "      <td>0.179677</td>\n",
       "      <td>1.547643</td>\n",
       "      <td>1.792045</td>\n",
       "      <td>1.960439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.686325</td>\n",
       "      <td>-0.822431</td>\n",
       "      <td>0.344962</td>\n",
       "      <td>-0.966295</td>\n",
       "      <td>1.300303</td>\n",
       "      <td>0.369968</td>\n",
       "      <td>0.265549</td>\n",
       "      <td>-0.088382</td>\n",
       "      <td>-0.099273</td>\n",
       "      <td>0.086256</td>\n",
       "      <td>-0.011160</td>\n",
       "      <td>-0.331371</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.056493</td>\n",
       "      <td>0.032505</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39640</th>\n",
       "      <td>-3.716757</td>\n",
       "      <td>0.746312</td>\n",
       "      <td>-0.278442</td>\n",
       "      <td>1.442174</td>\n",
       "      <td>-0.472542</td>\n",
       "      <td>1.915979</td>\n",
       "      <td>0.207090</td>\n",
       "      <td>1.378593</td>\n",
       "      <td>3.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>-0.944538</td>\n",
       "      <td>-0.086922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.111863</td>\n",
       "      <td>-0.343553</td>\n",
       "      <td>6500.0</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>-0.463864</td>\n",
       "      <td>1664.267857</td>\n",
       "      <td>0.457255</td>\n",
       "      <td>0.522204</td>\n",
       "      <td>0.512085</td>\n",
       "      <td>-0.200537</td>\n",
       "      <td>-0.045952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.229267</td>\n",
       "      <td>-0.398422</td>\n",
       "      <td>0.508340</td>\n",
       "      <td>1.064995</td>\n",
       "      <td>-0.650512</td>\n",
       "      <td>1.306810</td>\n",
       "      <td>0.854330</td>\n",
       "      <td>0.039433</td>\n",
       "      <td>-0.730368</td>\n",
       "      <td>0.757048</td>\n",
       "      <td>-0.660623</td>\n",
       "      <td>0.203796</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.416663</td>\n",
       "      <td>0.394831</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39641</th>\n",
       "      <td>-3.716757</td>\n",
       "      <td>-0.158415</td>\n",
       "      <td>0.091302</td>\n",
       "      <td>-0.222587</td>\n",
       "      <td>0.147508</td>\n",
       "      <td>-0.491646</td>\n",
       "      <td>1.335872</td>\n",
       "      <td>-0.770293</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.415071</td>\n",
       "      <td>0.343989</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.072048</td>\n",
       "      <td>-0.460873</td>\n",
       "      <td>6200.0</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>0.404934</td>\n",
       "      <td>1753.882353</td>\n",
       "      <td>0.855287</td>\n",
       "      <td>1.068122</td>\n",
       "      <td>0.159376</td>\n",
       "      <td>-0.483671</td>\n",
       "      <td>-0.385722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.505177</td>\n",
       "      <td>-0.640777</td>\n",
       "      <td>-0.697652</td>\n",
       "      <td>1.015044</td>\n",
       "      <td>0.250808</td>\n",
       "      <td>0.697813</td>\n",
       "      <td>-0.997733</td>\n",
       "      <td>-0.324699</td>\n",
       "      <td>0.953765</td>\n",
       "      <td>-0.792776</td>\n",
       "      <td>0.900083</td>\n",
       "      <td>-0.660859</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.892713</td>\n",
       "      <td>-0.868165</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39642</th>\n",
       "      <td>-3.716757</td>\n",
       "      <td>-2.153192</td>\n",
       "      <td>0.617082</td>\n",
       "      <td>0.002434</td>\n",
       "      <td>0.705942</td>\n",
       "      <td>0.023808</td>\n",
       "      <td>0.329067</td>\n",
       "      <td>-0.770293</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.082436</td>\n",
       "      <td>-1.077791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-2.314162</td>\n",
       "      <td>-2.375909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>0.081465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.198484</td>\n",
       "      <td>-1.659634</td>\n",
       "      <td>-0.842009</td>\n",
       "      <td>-0.902979</td>\n",
       "      <td>-0.902124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.193188</td>\n",
       "      <td>0.241684</td>\n",
       "      <td>1.515967</td>\n",
       "      <td>0.021404</td>\n",
       "      <td>-0.064950</td>\n",
       "      <td>-1.048091</td>\n",
       "      <td>-1.469914</td>\n",
       "      <td>-1.181859</td>\n",
       "      <td>0.827942</td>\n",
       "      <td>-1.331563</td>\n",
       "      <td>1.541598</td>\n",
       "      <td>-1.397816</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.475867</td>\n",
       "      <td>0.032505</td>\n",
       "      <td>-0.012500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39643</th>\n",
       "      <td>-3.716757</td>\n",
       "      <td>-0.158415</td>\n",
       "      <td>-1.245233</td>\n",
       "      <td>1.489854</td>\n",
       "      <td>-1.299567</td>\n",
       "      <td>1.540148</td>\n",
       "      <td>-1.696325</td>\n",
       "      <td>-0.770293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.700679</td>\n",
       "      <td>-1.654145</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.622922</td>\n",
       "      <td>-1.749738</td>\n",
       "      <td>205600.0</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>0.859487</td>\n",
       "      <td>3035.080555</td>\n",
       "      <td>-0.473319</td>\n",
       "      <td>0.424711</td>\n",
       "      <td>0.512085</td>\n",
       "      <td>-0.200537</td>\n",
       "      <td>-0.045952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.309937</td>\n",
       "      <td>1.816184</td>\n",
       "      <td>0.115936</td>\n",
       "      <td>0.239210</td>\n",
       "      <td>0.067294</td>\n",
       "      <td>0.792819</td>\n",
       "      <td>-0.159920</td>\n",
       "      <td>1.407029</td>\n",
       "      <td>-0.296564</td>\n",
       "      <td>0.920030</td>\n",
       "      <td>-0.810958</td>\n",
       "      <td>-1.304289</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.532339</td>\n",
       "      <td>0.988570</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>1300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39644 rows Ã— 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rank( timedelta)  rank( n_tokens_title)  rank( n_tokens_content)  \\\n",
       "0              3.091313               0.746312                -0.830751   \n",
       "1              3.091313              -0.627749                -0.629945   \n",
       "2              3.091313              -0.627749                -0.877419   \n",
       "3              3.091313              -0.627749                 0.309539   \n",
       "4              3.091313               1.191898                 1.253929   \n",
       "...                 ...                    ...                      ...   \n",
       "39639         -3.716757               0.299307                -0.208059   \n",
       "39640         -3.716757               0.746312                -0.278442   \n",
       "39641         -3.716757              -0.158415                 0.091302   \n",
       "39642         -3.716757              -2.153192                 0.617082   \n",
       "39643         -3.716757              -0.158415                -1.245233   \n",
       "\n",
       "       rank( n_unique_tokens)  rank( n_non_stop_words)  \\\n",
       "0                    1.171875                -0.859076   \n",
       "1                    0.636396                -0.682170   \n",
       "2                    0.346405                -0.977397   \n",
       "3                   -0.344123                 0.321434   \n",
       "4                   -1.200952                 1.406604   \n",
       "...                       ...                      ...   \n",
       "39639               -0.100354                -0.394045   \n",
       "39640                1.442174                -0.472542   \n",
       "39641               -0.222587                 0.147508   \n",
       "39642                0.002434                 0.705942   \n",
       "39643                1.489854                -1.299567   \n",
       "\n",
       "       rank( n_non_stop_unique_tokens)  rank( num_hrefs)  \\\n",
       "0                             1.250329         -0.773700   \n",
       "1                             1.037852         -1.082209   \n",
       "2                            -0.284000         -1.082209   \n",
       "3                            -0.264207          0.207090   \n",
       "4                            -1.358432          1.046996   \n",
       "...                                ...               ...   \n",
       "39639                        -0.060357          0.207090   \n",
       "39640                         1.915979          0.207090   \n",
       "39641                        -0.491646          1.335872   \n",
       "39642                         0.023808          0.329067   \n",
       "39643                         1.540148         -1.696325   \n",
       "\n",
       "       rank( num_self_hrefs)   num_imgs   num_videos  \\\n",
       "0                  -0.247319        1.0          0.0   \n",
       "1                  -0.770293        1.0          0.0   \n",
       "2                  -0.770293        1.0          0.0   \n",
       "3                  -1.494860        1.0          0.0   \n",
       "4                   2.268357       20.0          0.0   \n",
       "...                      ...        ...          ...   \n",
       "39639               1.378593        1.0          1.0   \n",
       "39640               1.378593        3.0         48.0   \n",
       "39641              -0.770293       12.0          1.0   \n",
       "39642              -0.770293        1.0          0.0   \n",
       "39643              -0.770293        0.0          2.0   \n",
       "\n",
       "       rank( average_token_length)  rank( num_keywords)  \\\n",
       "0                         0.060832            -1.077791   \n",
       "1                         0.874264            -1.654145   \n",
       "2                        -0.983430            -0.561454   \n",
       "3                        -0.947209            -0.086922   \n",
       "4                         0.069322            -0.086922   \n",
       "...                            ...                  ...   \n",
       "39639                    -0.507837             0.343989   \n",
       "39640                    -0.944538            -0.086922   \n",
       "39641                     1.415071             0.343989   \n",
       "39642                     1.082436            -1.077791   \n",
       "39643                    -0.700679            -1.654145   \n",
       "\n",
       "        data_channel_is_lifestyle   data_channel_is_entertainment  \\\n",
       "0                             0.0                             1.0   \n",
       "1                             0.0                             0.0   \n",
       "2                             0.0                             0.0   \n",
       "3                             0.0                             1.0   \n",
       "4                             0.0                             0.0   \n",
       "...                           ...                             ...   \n",
       "39639                         0.0                             0.0   \n",
       "39640                         0.0                             0.0   \n",
       "39641                         0.0                             0.0   \n",
       "39642                         0.0                             0.0   \n",
       "39643                         0.0                             1.0   \n",
       "\n",
       "        data_channel_is_bus   data_channel_is_socmed   data_channel_is_tech  \\\n",
       "0                       0.0                      0.0                    0.0   \n",
       "1                       1.0                      0.0                    0.0   \n",
       "2                       1.0                      0.0                    0.0   \n",
       "3                       0.0                      0.0                    0.0   \n",
       "4                       0.0                      0.0                    1.0   \n",
       "...                     ...                      ...                    ...   \n",
       "39639                   0.0                      0.0                    1.0   \n",
       "39640                   0.0                      1.0                    0.0   \n",
       "39641                   0.0                      0.0                    0.0   \n",
       "39642                   0.0                      0.0                    0.0   \n",
       "39643                   0.0                      0.0                    0.0   \n",
       "\n",
       "        data_channel_is_world   kw_min_min  rank( kw_max_min)  \\\n",
       "0                         0.0          0.0          -2.314162   \n",
       "1                         0.0          0.0          -2.314162   \n",
       "2                         0.0          0.0          -2.314162   \n",
       "3                         0.0          0.0          -2.314162   \n",
       "4                         0.0          0.0          -2.314162   \n",
       "...                       ...          ...                ...   \n",
       "39639                     0.0         -1.0           0.024409   \n",
       "39640                     0.0         -1.0          -0.111863   \n",
       "39641                     0.0         -1.0           0.072048   \n",
       "39642                     1.0         -1.0          -2.314162   \n",
       "39643                     0.0         -1.0          -1.622922   \n",
       "\n",
       "       rank( kw_avg_min)   kw_min_max   kw_max_max  rank( kw_avg_max)  \\\n",
       "0              -2.013451          0.0          0.0          -3.091313   \n",
       "1              -2.013451          0.0          0.0          -3.091313   \n",
       "2              -2.013451          0.0          0.0          -3.091313   \n",
       "3              -2.013451          0.0          0.0          -3.091313   \n",
       "4              -2.013451          0.0          0.0          -3.091313   \n",
       "...                  ...          ...          ...                ...   \n",
       "39639          -0.424676      26900.0     843300.0           0.911184   \n",
       "39640          -0.343553       6500.0     843300.0          -0.463864   \n",
       "39641          -0.460873       6200.0     843300.0           0.404934   \n",
       "39642          -2.375909          0.0     843300.0           0.081465   \n",
       "39643          -1.749738     205600.0     843300.0           0.859487   \n",
       "\n",
       "        kw_min_avg  rank( kw_max_avg)  rank( kw_avg_avg)  \\\n",
       "0         0.000000          -3.091313          -3.091313   \n",
       "1         0.000000          -3.091313          -3.091313   \n",
       "2         0.000000          -3.091313          -3.091313   \n",
       "3         0.000000          -3.091313          -3.091313   \n",
       "4         0.000000          -3.091313          -3.091313   \n",
       "...            ...                ...                ...   \n",
       "39639  2514.742857          -0.200666           0.179677   \n",
       "39640  1664.267857           0.457255           0.522204   \n",
       "39641  1753.882353           0.855287           1.068122   \n",
       "39642     0.000000          -1.198484          -1.659634   \n",
       "39643  3035.080555          -0.473319           0.424711   \n",
       "\n",
       "       rank( self_reference_min_shares)  rank( self_reference_max_shares)  \\\n",
       "0                             -0.809729                         -0.897243   \n",
       "1                             -1.337803                         -1.337726   \n",
       "2                             -0.314418                         -0.753138   \n",
       "3                             -1.337803                         -1.337726   \n",
       "4                             -0.768720                          1.144183   \n",
       "...                                 ...                               ...   \n",
       "39639                          1.547643                          1.792045   \n",
       "39640                          0.512085                         -0.200537   \n",
       "39641                          0.159376                         -0.483671   \n",
       "39642                         -0.842009                         -0.902979   \n",
       "39643                          0.512085                         -0.200537   \n",
       "\n",
       "       rank( self_reference_avg_sharess)   weekday_is_monday  \\\n",
       "0                              -0.895353                 1.0   \n",
       "1                              -1.337803                 1.0   \n",
       "2                              -0.722589                 1.0   \n",
       "3                              -1.337803                 1.0   \n",
       "4                               0.294747                 1.0   \n",
       "...                                  ...                 ...   \n",
       "39639                           1.960439                 0.0   \n",
       "39640                          -0.045952                 0.0   \n",
       "39641                          -0.385722                 0.0   \n",
       "39642                          -0.902124                 0.0   \n",
       "39643                          -0.045952                 0.0   \n",
       "\n",
       "        weekday_is_tuesday   weekday_is_wednesday   weekday_is_thursday  \\\n",
       "0                      0.0                    0.0                   0.0   \n",
       "1                      0.0                    0.0                   0.0   \n",
       "2                      0.0                    0.0                   0.0   \n",
       "3                      0.0                    0.0                   0.0   \n",
       "4                      0.0                    0.0                   0.0   \n",
       "...                    ...                    ...                   ...   \n",
       "39639                  0.0                    1.0                   0.0   \n",
       "39640                  0.0                    1.0                   0.0   \n",
       "39641                  0.0                    1.0                   0.0   \n",
       "39642                  0.0                    1.0                   0.0   \n",
       "39643                  0.0                    1.0                   0.0   \n",
       "\n",
       "        weekday_is_friday   weekday_is_saturday   weekday_is_sunday  \\\n",
       "0                     0.0                   0.0                 0.0   \n",
       "1                     0.0                   0.0                 0.0   \n",
       "2                     0.0                   0.0                 0.0   \n",
       "3                     0.0                   0.0                 0.0   \n",
       "4                     0.0                   0.0                 0.0   \n",
       "...                   ...                   ...                 ...   \n",
       "39639                 0.0                   0.0                 0.0   \n",
       "39640                 0.0                   0.0                 0.0   \n",
       "39641                 0.0                   0.0                 0.0   \n",
       "39642                 0.0                   0.0                 0.0   \n",
       "39643                 0.0                   0.0                 0.0   \n",
       "\n",
       "        is_weekend  rank( LDA_00)  rank( LDA_01)  rank( LDA_02)  \\\n",
       "0              0.0       1.011874       1.107189       0.003762   \n",
       "1              0.0       1.506066       0.481113       0.164789   \n",
       "2              0.0       0.628018      -0.102896      -0.194219   \n",
       "3              0.0      -0.437130       1.172315       0.875191   \n",
       "4              0.0      -0.314651      -0.220448      -0.531720   \n",
       "...            ...            ...            ...            ...   \n",
       "39639          0.0      -0.686325      -0.822431       0.344962   \n",
       "39640          0.0      -0.229267      -0.398422       0.508340   \n",
       "39641          0.0       0.505177      -0.640777      -0.697652   \n",
       "39642          0.0       0.193188       0.241684       1.515967   \n",
       "39643          0.0       0.309937       1.816184       0.115936   \n",
       "\n",
       "       rank( LDA_03)  rank( LDA_04)  rank( global_subjectivity)  \\\n",
       "0           0.133906      -0.019697                    0.834551   \n",
       "1           0.215657       0.071541                   -1.206170   \n",
       "2          -0.319837       1.102533                    2.439542   \n",
       "3          -0.392816      -0.811837                   -0.291810   \n",
       "4          -0.626094       1.680026                    0.738234   \n",
       "...              ...            ...                         ...   \n",
       "39639      -0.966295       1.300303                    0.369968   \n",
       "39640       1.064995      -0.650512                    1.306810   \n",
       "39641       1.015044       0.250808                    0.697813   \n",
       "39642       0.021404      -0.064950                   -1.048091   \n",
       "39643       0.239210       0.067294                    0.792819   \n",
       "\n",
       "       rank( global_sentiment_polarity)  rank( global_rate_positive_words)  \\\n",
       "0                             -0.299737                           0.407371   \n",
       "1                              0.345029                           0.255150   \n",
       "2                              2.017790                           1.034661   \n",
       "3                             -0.208737                           0.150161   \n",
       "4                              1.686806                           1.934743   \n",
       "...                                 ...                                ...   \n",
       "39639                          0.265549                          -0.088382   \n",
       "39640                          0.854330                           0.039433   \n",
       "39641                         -0.997733                          -0.324699   \n",
       "39642                         -1.469914                          -1.181859   \n",
       "39643                         -0.159920                           1.407029   \n",
       "\n",
       "       rank( global_rate_negative_words)  rank( rate_positive_words)  \\\n",
       "0                              -0.186943                    0.425334   \n",
       "1                               0.038674                    0.167738   \n",
       "2                              -0.690612                    1.106431   \n",
       "3                               0.574307                   -0.284132   \n",
       "4                              -0.367261                    1.143818   \n",
       "...                                  ...                         ...   \n",
       "39639                          -0.099273                    0.086256   \n",
       "39640                          -0.730368                    0.757048   \n",
       "39641                           0.953765                   -0.792776   \n",
       "39642                           0.827942                   -1.331563   \n",
       "39643                          -0.296564                    0.920030   \n",
       "\n",
       "       rank( rate_negative_words)  rank( avg_positive_polarity)  \\\n",
       "0                       -0.344559                      0.257404   \n",
       "1                       -0.092096                     -0.900984   \n",
       "2                       -0.977397                      1.564511   \n",
       "3                        0.363172                      0.351546   \n",
       "4                       -1.009924                      0.671991   \n",
       "...                           ...                           ...   \n",
       "39639                   -0.011160                     -0.331371   \n",
       "39640                   -0.660623                      0.203796   \n",
       "39641                    0.900083                     -0.660859   \n",
       "39642                    1.541598                     -1.397816   \n",
       "39643                   -0.810958                     -1.304289   \n",
       "\n",
       "        min_positive_polarity   max_positive_polarity  \\\n",
       "0                    0.100000                    0.70   \n",
       "1                    0.033333                    0.70   \n",
       "2                    0.100000                    1.00   \n",
       "3                    0.136364                    0.80   \n",
       "4                    0.033333                    1.00   \n",
       "...                       ...                     ...   \n",
       "39639                0.100000                    0.75   \n",
       "39640                0.136364                    0.70   \n",
       "39641                0.136364                    0.50   \n",
       "39642                0.062500                    0.50   \n",
       "39643                0.100000                    0.50   \n",
       "\n",
       "       rank( avg_negative_polarity)  rank( min_negative_polarity)  \\\n",
       "0                         -0.840613                     -0.309042   \n",
       "1                          1.303623                      1.355812   \n",
       "2                         -1.637609                     -0.868165   \n",
       "3                         -0.984764                     -0.309042   \n",
       "4                          0.327633                      0.032505   \n",
       "...                             ...                           ...   \n",
       "39639                     -0.056493                      0.032505   \n",
       "39640                      0.416663                      0.394831   \n",
       "39641                     -0.892713                     -0.868165   \n",
       "39642                      0.475867                      0.032505   \n",
       "39643                      0.532339                      0.988570   \n",
       "\n",
       "        max_negative_polarity   title_subjectivity   title_sentiment_polarity  \\\n",
       "0                   -0.200000             0.500000                  -0.187500   \n",
       "1                   -0.100000             0.000000                   0.000000   \n",
       "2                   -0.133333             0.000000                   0.000000   \n",
       "3                   -0.166667             0.000000                   0.000000   \n",
       "4                   -0.050000             0.454545                   0.136364   \n",
       "...                       ...                  ...                        ...   \n",
       "39639               -0.125000             0.100000                   0.000000   \n",
       "39640               -0.100000             0.300000                   1.000000   \n",
       "39641               -0.166667             0.454545                   0.136364   \n",
       "39642               -0.012500             0.000000                   0.000000   \n",
       "39643               -0.200000             0.333333                   0.250000   \n",
       "\n",
       "        abs_title_subjectivity   abs_title_sentiment_polarity   shares  \n",
       "0                     0.000000                       0.187500      593  \n",
       "1                     0.500000                       0.000000      711  \n",
       "2                     0.500000                       0.000000     1500  \n",
       "3                     0.500000                       0.000000     1200  \n",
       "4                     0.045455                       0.136364      505  \n",
       "...                        ...                            ...      ...  \n",
       "39639                 0.400000                       0.000000     1800  \n",
       "39640                 0.200000                       1.000000     1900  \n",
       "39641                 0.045455                       0.136364     1900  \n",
       "39642                 0.500000                       0.000000     1100  \n",
       "39643                 0.166667                       0.250000     1300  \n",
       "\n",
       "[39644 rows x 60 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read preprocessed news dataset\n",
    "news_df = pd.read_csv('./prep_news_data.csv')\n",
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Neural Network class with flexible settings\n",
    "class FlexibleRegressionNN(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_layers_sizes: list, dropout_rate: float) -> None:\n",
    "        super(FlexibleRegressionNN, self).__init__()\n",
    "        layers = list()\n",
    "\n",
    "        # Define first and n-1 hidden layers\n",
    "        neuron_num = input_size\n",
    "        for size in hidden_layers_sizes:\n",
    "            layers.append(nn.Linear(neuron_num, size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            neuron_num = size\n",
    "\n",
    "        layers.append(nn.Linear(neuron_num, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    # Define forward loop for NN\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for calculation RMSE\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    mse = torch.mean((y_true - y_pred) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "    return rmse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 20:28:14,080] A new study created in memory with name: no-name-cc16965a-188b-4a75-8ef0-82e5362e9e03\n",
      "/var/folders/nd/fsmv2lz93k3_cjwz_1lx75j80000gn/T/ipykernel_96862/2814934376.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
      "/var/folders/nd/fsmv2lz93k3_cjwz_1lx75j80000gn/T/ipykernel_96862/2814934376.py:37: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/45, Train Loss: 0.0005, Test Loss: 0.0002\n",
      "Epoch 2/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 3/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 4/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 5/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 6/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 7/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 8/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 9/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 10/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 11/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 12/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 13/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 14/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 15/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 16/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 17/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 18/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 19/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 20/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 21/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 22/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 23/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 24/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 25/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 26/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 27/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 28/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 29/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 30/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 31/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 32/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 33/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 34/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 35/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 36/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 37/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 38/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 39/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 40/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 41/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 42/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 43/45, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 44/45, Train Loss: 0.0002, Test Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-13 20:28:37,044] Trial 0 finished with value: 0.0001666661719116658 and parameters: {'num_hidden_layers': 2, 'layer_0_size': 186, 'layer_1_size': 199, 'dropout_rate': 0.4259986028873003, 'learning_rate': 0.0008468771666289129, 'batch_size': 64, 'epochs': 45}. Best is trial 0 with value: 0.0001666661719116658.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/45, Train Loss: 0.0002, Test Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nd/fsmv2lz93k3_cjwz_1lx75j80000gn/T/ipykernel_96862/2814934376.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
      "/var/folders/nd/fsmv2lz93k3_cjwz_1lx75j80000gn/T/ipykernel_96862/2814934376.py:37: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12, Train Loss: 0.0003, Test Loss: 0.0002\n",
      "Epoch 2/12, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 3/12, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 4/12, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 5/12, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 6/12, Train Loss: 0.0002, Test Loss: 0.0002\n",
      "Epoch 7/12, Train Loss: 0.0002, Test Loss: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-13 20:28:49,183] Trial 1 failed with parameters: {'num_hidden_layers': 17, 'layer_0_size': 86, 'layer_1_size': 229, 'layer_2_size': 176, 'layer_3_size': 256, 'layer_4_size': 65, 'layer_5_size': 213, 'layer_6_size': 64, 'layer_7_size': 52, 'layer_8_size': 237, 'layer_9_size': 236, 'layer_10_size': 152, 'layer_11_size': 172, 'layer_12_size': 197, 'layer_13_size': 240, 'layer_14_size': 77, 'layer_15_size': 158, 'layer_16_size': 143, 'dropout_rate': 0.22521904371576032, 'learning_rate': 0.00015444564504123095, 'batch_size': 256, 'epochs': 12} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/var/folders/nd/fsmv2lz93k3_cjwz_1lx75j80000gn/T/ipykernel_96862/2814934376.py\", line 77, in objective\n",
      "    return train_and_evaluate_model(model, train_loader, test_loader, criterion, optimizer, epochs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/nd/fsmv2lz93k3_cjwz_1lx75j80000gn/T/ipykernel_96862/2814934376.py\", line 11, in train_and_evaluate_model\n",
      "    optimizer.step()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 487, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 91, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/adam.py\", line 223, in step\n",
      "    adam(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/optimizer.py\", line 154, in maybe_fallback\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/adam.py\", line 784, in adam\n",
      "    func(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/adam.py\", line 430, in _single_tensor_adam\n",
      "    denom = (exp_avg_sq.sqrt() / bias_correction2_sqrt).add_(eps)\n",
      "             ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-03-13 20:28:49,188] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 81\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# Settings of searching by Optuna\u001b[39;00m\n\u001b[1;32m     80\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 81\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Getting the best hyperparameters\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[12], line 77\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     74\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Training launch and return the test loss\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 11\u001b[0m, in \u001b[0;36mtrain_and_evaluate_model\u001b[0;34m(model, train_loader, test_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m      9\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     10\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 11\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     13\u001b[0m train_loss \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    214\u001b[0m         group,\n\u001b[1;32m    215\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m         state_steps,\n\u001b[1;32m    221\u001b[0m     )\n\u001b[0;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/optim/adam.py:430\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    428\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m    432\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Creating function for training and evaluating the model\n",
    "def train_and_evaluate_model(model, train_loader, test_loader, criterion, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0.0\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item()\n",
    "            test_loss /= len(test_loader)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
    "    \n",
    "    return test_loss\n",
    "\n",
    "# Creating function for selection of best hyperparameters \n",
    "def objective(trial):\n",
    "    # Defining hyperparameters\n",
    "    num_hidden_layers = trial.suggest_int('num_hidden_layers', 1, 20)\n",
    "    hidden_layer_sizes = [\n",
    "        trial.suggest_int(f'layer_{i}_size', 32, 256)\n",
    "        for i in range(num_hidden_layers)\n",
    "    ]\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    epochs = trial.suggest_int('epochs', 5, 100)\n",
    "    input_size = 59\n",
    "\n",
    "    news_df = pd.read_csv('./prep_news_data.csv')\n",
    "\n",
    "    X = news_df.iloc[:, :-1].values\n",
    "    y = news_df.iloc[:, -1].values\n",
    "    y = y.reshape(-1, 1)\n",
    "\n",
    "    # Data normalization\n",
    "    scaler_X = MinMaxScaler()\n",
    "    X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "    scaler_y = MinMaxScaler()\n",
    "    y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "    # Splitting data into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Convertation data to PyTorch tensors\n",
    "    train_features = torch.tensor(X_train, dtype=torch.float32)\n",
    "    test_features = torch.tensor(X_test, dtype=torch.float32)\n",
    "    train_targets = torch.tensor(y_train, dtype=torch.float32)\n",
    "    test_targets = torch.tensor(y_test, dtype=torch.float32)    \n",
    "\n",
    "    # Creating DataLoader\n",
    "    train_data = TensorDataset(train_features, train_targets)\n",
    "    test_data = TensorDataset(test_features, test_targets)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Creating a model and defining of optimizer\n",
    "    model = FlexibleRegressionNN(input_size, hidden_layer_sizes, dropout_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Training launch and return the test loss\n",
    "    return train_and_evaluate_model(model, train_loader, test_loader, criterion, optimizer, epochs)\n",
    "\n",
    "# Settings of searching by Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=200)\n",
    "\n",
    "# Getting the best hyperparameters\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
