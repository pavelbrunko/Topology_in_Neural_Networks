{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting optimal MLP's hiperparameters for bike dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import optuna\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank(season)</th>\n",
       "      <th>yr</th>\n",
       "      <th>rank(mnth)</th>\n",
       "      <th>rank(hr)</th>\n",
       "      <th>holiday</th>\n",
       "      <th>rank(weekday)</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>rank(temp)</th>\n",
       "      <th>rank(atemp)</th>\n",
       "      <th>rank(hum)</th>\n",
       "      <th>rank(windspeed)</th>\n",
       "      <th>rank(casual)</th>\n",
       "      <th>rank(registered)</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.16483</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.737916</td>\n",
       "      <td>-2.035759</td>\n",
       "      <td>0</td>\n",
       "      <td>1.459083</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.276942</td>\n",
       "      <td>-0.946560</td>\n",
       "      <td>0.770810</td>\n",
       "      <td>-1.532339</td>\n",
       "      <td>-0.774893</td>\n",
       "      <td>-1.037755</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.16483</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.737916</td>\n",
       "      <td>-1.533273</td>\n",
       "      <td>0</td>\n",
       "      <td>1.459083</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.448913</td>\n",
       "      <td>-1.045073</td>\n",
       "      <td>0.734244</td>\n",
       "      <td>-1.532339</td>\n",
       "      <td>-0.356451</td>\n",
       "      <td>-0.699187</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.16483</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.737916</td>\n",
       "      <td>-1.259057</td>\n",
       "      <td>0</td>\n",
       "      <td>1.459083</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.448913</td>\n",
       "      <td>-1.045073</td>\n",
       "      <td>0.734244</td>\n",
       "      <td>-1.532339</td>\n",
       "      <td>-0.564583</td>\n",
       "      <td>-0.766063</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.16483</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.737916</td>\n",
       "      <td>-1.059753</td>\n",
       "      <td>0</td>\n",
       "      <td>1.459083</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.276942</td>\n",
       "      <td>-0.946560</td>\n",
       "      <td>0.509186</td>\n",
       "      <td>-1.532339</td>\n",
       "      <td>-0.774893</td>\n",
       "      <td>-1.126601</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.16483</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.737916</td>\n",
       "      <td>-0.897469</td>\n",
       "      <td>0</td>\n",
       "      <td>1.459083</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.276942</td>\n",
       "      <td>-0.946560</td>\n",
       "      <td>0.509186</td>\n",
       "      <td>-1.532339</td>\n",
       "      <td>-1.690293</td>\n",
       "      <td>-2.448942</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17374</th>\n",
       "      <td>-1.16483</td>\n",
       "      <td>1</td>\n",
       "      <td>1.720548</td>\n",
       "      <td>0.883425</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.788204</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.117398</td>\n",
       "      <td>-1.152974</td>\n",
       "      <td>-0.121890</td>\n",
       "      <td>-0.127559</td>\n",
       "      <td>-0.204644</td>\n",
       "      <td>-0.057581</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17375</th>\n",
       "      <td>-1.16483</td>\n",
       "      <td>1</td>\n",
       "      <td>1.720548</td>\n",
       "      <td>1.051068</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.788204</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.117398</td>\n",
       "      <td>-1.152974</td>\n",
       "      <td>-0.121890</td>\n",
       "      <td>-0.127559</td>\n",
       "      <td>-0.356451</td>\n",
       "      <td>-0.265968</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17376</th>\n",
       "      <td>-1.16483</td>\n",
       "      <td>1</td>\n",
       "      <td>1.720548</td>\n",
       "      <td>1.255084</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.788204</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.117398</td>\n",
       "      <td>-1.152974</td>\n",
       "      <td>-0.121890</td>\n",
       "      <td>-0.127559</td>\n",
       "      <td>-0.417236</td>\n",
       "      <td>-0.250236</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17377</th>\n",
       "      <td>-1.16483</td>\n",
       "      <td>1</td>\n",
       "      <td>1.720548</td>\n",
       "      <td>1.531407</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.788204</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.117398</td>\n",
       "      <td>-1.045073</td>\n",
       "      <td>-0.276442</td>\n",
       "      <td>-0.384024</td>\n",
       "      <td>-0.119057</td>\n",
       "      <td>-0.537970</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17378</th>\n",
       "      <td>-1.16483</td>\n",
       "      <td>1</td>\n",
       "      <td>1.720548</td>\n",
       "      <td>2.034615</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.788204</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.117398</td>\n",
       "      <td>-1.045073</td>\n",
       "      <td>0.076084</td>\n",
       "      <td>-0.384024</td>\n",
       "      <td>-0.159620</td>\n",
       "      <td>-0.643619</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17379 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rank(season)  yr  rank(mnth)  rank(hr)  holiday  rank(weekday)  \\\n",
       "0          -1.16483   0   -1.737916 -2.035759        0       1.459083   \n",
       "1          -1.16483   0   -1.737916 -1.533273        0       1.459083   \n",
       "2          -1.16483   0   -1.737916 -1.259057        0       1.459083   \n",
       "3          -1.16483   0   -1.737916 -1.059753        0       1.459083   \n",
       "4          -1.16483   0   -1.737916 -0.897469        0       1.459083   \n",
       "...             ...  ..         ...       ...      ...            ...   \n",
       "17374      -1.16483   1    1.720548  0.883425        0      -0.788204   \n",
       "17375      -1.16483   1    1.720548  1.051068        0      -0.788204   \n",
       "17376      -1.16483   1    1.720548  1.255084        0      -0.788204   \n",
       "17377      -1.16483   1    1.720548  1.531407        0      -0.788204   \n",
       "17378      -1.16483   1    1.720548  2.034615        0      -0.788204   \n",
       "\n",
       "       workingday  weathersit  rank(temp)  rank(atemp)  rank(hum)  \\\n",
       "0               0           1   -1.276942    -0.946560   0.770810   \n",
       "1               0           1   -1.448913    -1.045073   0.734244   \n",
       "2               0           1   -1.448913    -1.045073   0.734244   \n",
       "3               0           1   -1.276942    -0.946560   0.509186   \n",
       "4               0           1   -1.276942    -0.946560   0.509186   \n",
       "...           ...         ...         ...          ...        ...   \n",
       "17374           1           2   -1.117398    -1.152974  -0.121890   \n",
       "17375           1           2   -1.117398    -1.152974  -0.121890   \n",
       "17376           1           1   -1.117398    -1.152974  -0.121890   \n",
       "17377           1           1   -1.117398    -1.045073  -0.276442   \n",
       "17378           1           1   -1.117398    -1.045073   0.076084   \n",
       "\n",
       "       rank(windspeed)  rank(casual)  rank(registered)  cnt  \n",
       "0            -1.532339     -0.774893         -1.037755   16  \n",
       "1            -1.532339     -0.356451         -0.699187   40  \n",
       "2            -1.532339     -0.564583         -0.766063   32  \n",
       "3            -1.532339     -0.774893         -1.126601   13  \n",
       "4            -1.532339     -1.690293         -2.448942    1  \n",
       "...                ...           ...               ...  ...  \n",
       "17374        -0.127559     -0.204644         -0.057581  119  \n",
       "17375        -0.127559     -0.356451         -0.265968   89  \n",
       "17376        -0.127559     -0.417236         -0.250236   90  \n",
       "17377        -0.384024     -0.119057         -0.537970   61  \n",
       "17378        -0.384024     -0.159620         -0.643619   49  \n",
       "\n",
       "[17379 rows x 15 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read preprocessed bike dataset\n",
    "bike_df = pd.read_csv('./prep_bike_data.csv')\n",
    "bike_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data into features and targets\n",
    "X = bike_df.iloc[:, :-1].values\n",
    "y = bike_df.iloc[:, -1].values\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Data normalization\n",
    "scaler_X = MinMaxScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "scaler_y = MinMaxScaler()\n",
    "y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "# Splitting data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.1, random_state=42)\n",
    "\n",
    "# Convertation data to PyTorch tensors\n",
    "train_features = torch.tensor(X_train, dtype=torch.float32)\n",
    "test_features = torch.tensor(X_test, dtype=torch.float32)\n",
    "train_targets = torch.tensor(y_train, dtype=torch.float32)\n",
    "test_targets = torch.tensor(y_test, dtype=torch.float32)    \n",
    "\n",
    "# Creating DataLoader\n",
    "train_data = TensorDataset(train_features, train_targets)\n",
    "test_data = TensorDataset(test_features, test_targets)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Neural Network class with flexible settings\n",
    "class FlexibleRegressionNN(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_layers_sizes: list, dropout_rate: float) -> None:\n",
    "        super(FlexibleRegressionNN, self).__init__()\n",
    "        layers = list()\n",
    "\n",
    "        # Define first and n-1 hidden layers\n",
    "        neuron_num = input_size\n",
    "        for size in hidden_layers_sizes:\n",
    "            layers.append(nn.Linear(neuron_num, size))\n",
    "            layers.append(nn.BatchNorm1d(size))\n",
    "            layers.append(nn.LeakyReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            neuron_num = size\n",
    "\n",
    "        layers.append(nn.Linear(neuron_num, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    # Define forward loop for NN\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for calculation RMSE\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    mse = torch.mean((y_true - y_pred) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "    return rmse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:19:26,161] A new study created in memory with name: no-name-ac2130e2-85eb-4162-b2c6-ecf66212d007\n",
      "/var/folders/qq/gt96f0sx7nn6wsr9d8n13r2w0000gn/T/ipykernel_75395/355664794.py:36: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
      "/var/folders/qq/gt96f0sx7nn6wsr9d8n13r2w0000gn/T/ipykernel_75395/355664794.py:37: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/28, Train Loss: 1.1940, Test Loss: 0.9746\n",
      "Epoch 2/28, Train Loss: 1.1643, Test Loss: 0.9689\n",
      "Epoch 3/28, Train Loss: 1.0901, Test Loss: 0.9567\n",
      "Epoch 4/28, Train Loss: 1.1472, Test Loss: 0.9652\n",
      "Epoch 5/28, Train Loss: 1.1082, Test Loss: 0.9660\n",
      "Epoch 6/28, Train Loss: 1.0822, Test Loss: 0.9656\n",
      "Epoch 7/28, Train Loss: 1.1038, Test Loss: 0.9675\n",
      "Epoch 8/28, Train Loss: 1.0927, Test Loss: 0.9720\n",
      "Epoch 9/28, Train Loss: 1.0901, Test Loss: 0.9757\n",
      "Epoch 10/28, Train Loss: 1.0947, Test Loss: 0.9691\n",
      "Epoch 11/28, Train Loss: 1.0956, Test Loss: 0.9669\n",
      "Epoch 12/28, Train Loss: 1.0626, Test Loss: 0.9673\n",
      "Epoch 13/28, Train Loss: 1.0591, Test Loss: 0.9669\n",
      "Epoch 14/28, Train Loss: 1.1217, Test Loss: 0.9678\n",
      "Epoch 15/28, Train Loss: 1.0437, Test Loss: 0.9703\n",
      "Epoch 16/28, Train Loss: 1.1080, Test Loss: 0.9781\n",
      "Epoch 17/28, Train Loss: 1.0720, Test Loss: 0.9725\n",
      "Epoch 18/28, Train Loss: 1.0879, Test Loss: 0.9723\n",
      "Epoch 19/28, Train Loss: 1.0672, Test Loss: 0.9682\n",
      "Epoch 20/28, Train Loss: 1.0814, Test Loss: 0.9754\n",
      "Epoch 21/28, Train Loss: 1.0400, Test Loss: 0.9714\n",
      "Epoch 22/28, Train Loss: 1.0219, Test Loss: 0.9714\n",
      "Epoch 23/28, Train Loss: 1.0470, Test Loss: 0.9696\n",
      "Epoch 24/28, Train Loss: 1.0534, Test Loss: 0.9702\n",
      "Epoch 25/28, Train Loss: 1.0474, Test Loss: 0.9681\n",
      "Epoch 26/28, Train Loss: 1.0638, Test Loss: 0.9663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:19:30,666] Trial 0 finished with value: 0.9749857783317566 and parameters: {'num_hidden_layers': 10, 'layer_0_size': 80, 'layer_1_size': 77, 'layer_2_size': 170, 'layer_3_size': 57, 'layer_4_size': 130, 'layer_5_size': 227, 'layer_6_size': 142, 'layer_7_size': 252, 'layer_8_size': 198, 'layer_9_size': 210, 'dropout_rate': 0.3109885954578083, 'learning_rate': 0.00040078470478524845, 'batch_size': 64, 'epochs': 28}. Best is trial 0 with value: 0.9749857783317566.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/28, Train Loss: 1.0472, Test Loss: 0.9831\n",
      "Epoch 28/28, Train Loss: 1.0578, Test Loss: 0.9750\n",
      "Epoch 1/46, Train Loss: 1.3499, Test Loss: 1.0102\n",
      "Epoch 2/46, Train Loss: 1.2096, Test Loss: 1.0040\n",
      "Epoch 3/46, Train Loss: 1.2793, Test Loss: 0.9982\n",
      "Epoch 4/46, Train Loss: 1.2223, Test Loss: 0.9897\n",
      "Epoch 5/46, Train Loss: 1.2735, Test Loss: 0.9872\n",
      "Epoch 6/46, Train Loss: 1.1963, Test Loss: 0.9896\n",
      "Epoch 7/46, Train Loss: 1.2237, Test Loss: 0.9912\n",
      "Epoch 8/46, Train Loss: 1.1875, Test Loss: 0.9907\n",
      "Epoch 9/46, Train Loss: 1.1138, Test Loss: 0.9916\n",
      "Epoch 10/46, Train Loss: 1.1934, Test Loss: 0.9869\n",
      "Epoch 11/46, Train Loss: 1.1899, Test Loss: 0.9921\n",
      "Epoch 12/46, Train Loss: 1.1377, Test Loss: 0.9925\n",
      "Epoch 13/46, Train Loss: 1.1007, Test Loss: 0.9941\n",
      "Epoch 14/46, Train Loss: 1.1316, Test Loss: 1.0003\n",
      "Epoch 15/46, Train Loss: 1.1229, Test Loss: 0.9906\n",
      "Epoch 16/46, Train Loss: 1.1273, Test Loss: 0.9934\n",
      "Epoch 17/46, Train Loss: 1.1028, Test Loss: 0.9939\n",
      "Epoch 18/46, Train Loss: 1.1624, Test Loss: 1.0002\n",
      "Epoch 19/46, Train Loss: 1.1090, Test Loss: 1.0018\n",
      "Epoch 20/46, Train Loss: 1.1317, Test Loss: 0.9963\n",
      "Epoch 21/46, Train Loss: 1.1527, Test Loss: 1.0007\n",
      "Epoch 22/46, Train Loss: 1.1734, Test Loss: 0.9994\n",
      "Epoch 23/46, Train Loss: 1.1018, Test Loss: 1.0062\n",
      "Epoch 24/46, Train Loss: 1.0908, Test Loss: 0.9993\n",
      "Epoch 25/46, Train Loss: 1.1561, Test Loss: 0.9981\n",
      "Epoch 26/46, Train Loss: 1.1246, Test Loss: 1.0098\n",
      "Epoch 27/46, Train Loss: 1.1308, Test Loss: 1.0056\n",
      "Epoch 28/46, Train Loss: 1.1094, Test Loss: 1.0015\n",
      "Epoch 29/46, Train Loss: 1.1127, Test Loss: 1.0022\n",
      "Epoch 30/46, Train Loss: 1.0868, Test Loss: 1.0059\n",
      "Epoch 31/46, Train Loss: 1.1309, Test Loss: 1.0050\n",
      "Epoch 32/46, Train Loss: 1.1390, Test Loss: 1.0046\n",
      "Epoch 33/46, Train Loss: 1.1335, Test Loss: 1.0018\n",
      "Epoch 34/46, Train Loss: 1.0794, Test Loss: 0.9992\n",
      "Epoch 35/46, Train Loss: 1.0972, Test Loss: 0.9970\n",
      "Epoch 36/46, Train Loss: 1.0923, Test Loss: 0.9989\n",
      "Epoch 37/46, Train Loss: 1.1264, Test Loss: 0.9984\n",
      "Epoch 38/46, Train Loss: 1.0894, Test Loss: 0.9987\n",
      "Epoch 39/46, Train Loss: 1.0838, Test Loss: 1.0047\n",
      "Epoch 40/46, Train Loss: 1.0593, Test Loss: 1.0014\n",
      "Epoch 41/46, Train Loss: 1.1197, Test Loss: 1.0036\n",
      "Epoch 42/46, Train Loss: 1.1445, Test Loss: 1.0038\n",
      "Epoch 43/46, Train Loss: 1.1266, Test Loss: 0.9946\n",
      "Epoch 44/46, Train Loss: 1.0796, Test Loss: 0.9979\n",
      "Epoch 45/46, Train Loss: 1.1160, Test Loss: 0.9925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:19:36,132] Trial 1 finished with value: 0.9977798972811017 and parameters: {'num_hidden_layers': 5, 'layer_0_size': 156, 'layer_1_size': 59, 'layer_2_size': 220, 'layer_3_size': 199, 'layer_4_size': 229, 'dropout_rate': 0.39268877513854294, 'learning_rate': 9.41677046519703e-05, 'batch_size': 32, 'epochs': 46}. Best is trial 0 with value: 0.9749857783317566.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/46, Train Loss: 1.1104, Test Loss: 0.9978\n",
      "Epoch 1/47, Train Loss: 1.1964, Test Loss: 1.1585\n",
      "Epoch 2/47, Train Loss: 1.1142, Test Loss: 1.1699\n",
      "Epoch 3/47, Train Loss: 1.1329, Test Loss: 1.1698\n",
      "Epoch 4/47, Train Loss: 1.1309, Test Loss: 1.1602\n",
      "Epoch 5/47, Train Loss: 1.0552, Test Loss: 1.1552\n",
      "Epoch 6/47, Train Loss: 1.0617, Test Loss: 1.1560\n",
      "Epoch 7/47, Train Loss: 1.0909, Test Loss: 1.1523\n",
      "Epoch 8/47, Train Loss: 1.0547, Test Loss: 1.1551\n",
      "Epoch 9/47, Train Loss: 1.0703, Test Loss: 1.1524\n",
      "Epoch 10/47, Train Loss: 1.0759, Test Loss: 1.1504\n",
      "Epoch 11/47, Train Loss: 1.0753, Test Loss: 1.1546\n",
      "Epoch 12/47, Train Loss: 1.0628, Test Loss: 1.1574\n",
      "Epoch 13/47, Train Loss: 1.0466, Test Loss: 1.1572\n",
      "Epoch 14/47, Train Loss: 1.1027, Test Loss: 1.1564\n",
      "Epoch 15/47, Train Loss: 1.0484, Test Loss: 1.1534\n",
      "Epoch 16/47, Train Loss: 1.0481, Test Loss: 1.1488\n",
      "Epoch 17/47, Train Loss: 1.0355, Test Loss: 1.1487\n",
      "Epoch 18/47, Train Loss: 1.0508, Test Loss: 1.1484\n",
      "Epoch 19/47, Train Loss: 1.0554, Test Loss: 1.1506\n",
      "Epoch 20/47, Train Loss: 1.0515, Test Loss: 1.1562\n",
      "Epoch 21/47, Train Loss: 1.0439, Test Loss: 1.1549\n",
      "Epoch 22/47, Train Loss: 1.0420, Test Loss: 1.1544\n",
      "Epoch 23/47, Train Loss: 1.0121, Test Loss: 1.1551\n",
      "Epoch 24/47, Train Loss: 1.0595, Test Loss: 1.1566\n",
      "Epoch 25/47, Train Loss: 1.0197, Test Loss: 1.1508\n",
      "Epoch 26/47, Train Loss: 1.0182, Test Loss: 1.1472\n",
      "Epoch 27/47, Train Loss: 1.0692, Test Loss: 1.1507\n",
      "Epoch 28/47, Train Loss: 1.0354, Test Loss: 1.1525\n",
      "Epoch 29/47, Train Loss: 1.0329, Test Loss: 1.1564\n",
      "Epoch 30/47, Train Loss: 1.0140, Test Loss: 1.1573\n",
      "Epoch 31/47, Train Loss: 1.0496, Test Loss: 1.1597\n",
      "Epoch 32/47, Train Loss: 1.0355, Test Loss: 1.1618\n",
      "Epoch 33/47, Train Loss: 1.0398, Test Loss: 1.1665\n",
      "Epoch 34/47, Train Loss: 1.0097, Test Loss: 1.1686\n",
      "Epoch 35/47, Train Loss: 1.0141, Test Loss: 1.1747\n",
      "Epoch 36/47, Train Loss: 1.0016, Test Loss: 1.1776\n",
      "Epoch 37/47, Train Loss: 1.0268, Test Loss: 1.1771\n",
      "Epoch 38/47, Train Loss: 1.0189, Test Loss: 1.1776\n",
      "Epoch 39/47, Train Loss: 1.0196, Test Loss: 1.1783\n",
      "Epoch 40/47, Train Loss: 0.9939, Test Loss: 1.1814\n",
      "Epoch 41/47, Train Loss: 0.9951, Test Loss: 1.1838\n",
      "Epoch 42/47, Train Loss: 1.0214, Test Loss: 1.1862\n",
      "Epoch 43/47, Train Loss: 1.0279, Test Loss: 1.1857\n",
      "Epoch 44/47, Train Loss: 0.9919, Test Loss: 1.1880\n",
      "Epoch 45/47, Train Loss: 1.0051, Test Loss: 1.1939\n",
      "Epoch 46/47, Train Loss: 0.9664, Test Loss: 1.1932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:19:39,822] Trial 2 finished with value: 1.1931042075157166 and parameters: {'num_hidden_layers': 9, 'layer_0_size': 229, 'layer_1_size': 212, 'layer_2_size': 240, 'layer_3_size': 147, 'layer_4_size': 155, 'layer_5_size': 55, 'layer_6_size': 69, 'layer_7_size': 109, 'layer_8_size': 247, 'dropout_rate': 0.17751415726925523, 'learning_rate': 0.0002864650301833832, 'batch_size': 128, 'epochs': 47}. Best is trial 0 with value: 0.9749857783317566.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/47, Train Loss: 0.9851, Test Loss: 1.1931\n",
      "Epoch 1/68, Train Loss: 1.1729, Test Loss: 0.9289\n",
      "Epoch 2/68, Train Loss: 1.0583, Test Loss: 0.9258\n",
      "Epoch 3/68, Train Loss: 0.9811, Test Loss: 0.9187\n",
      "Epoch 4/68, Train Loss: 0.9904, Test Loss: 0.9330\n",
      "Epoch 5/68, Train Loss: 0.9447, Test Loss: 0.9477\n",
      "Epoch 6/68, Train Loss: 0.9387, Test Loss: 0.9352\n",
      "Epoch 7/68, Train Loss: 0.9297, Test Loss: 0.9272\n",
      "Epoch 8/68, Train Loss: 0.9282, Test Loss: 0.9217\n",
      "Epoch 9/68, Train Loss: 0.9214, Test Loss: 0.9305\n",
      "Epoch 10/68, Train Loss: 0.9073, Test Loss: 0.9468\n",
      "Epoch 11/68, Train Loss: 0.9087, Test Loss: 0.9538\n",
      "Epoch 12/68, Train Loss: 0.9176, Test Loss: 0.9491\n",
      "Epoch 13/68, Train Loss: 0.8891, Test Loss: 0.9620\n",
      "Epoch 14/68, Train Loss: 0.9156, Test Loss: 0.9594\n",
      "Epoch 15/68, Train Loss: 0.9053, Test Loss: 0.9433\n",
      "Epoch 16/68, Train Loss: 0.8791, Test Loss: 0.9472\n",
      "Epoch 17/68, Train Loss: 0.8621, Test Loss: 0.9536\n",
      "Epoch 18/68, Train Loss: 0.8566, Test Loss: 0.9634\n",
      "Epoch 19/68, Train Loss: 0.8499, Test Loss: 0.9532\n",
      "Epoch 20/68, Train Loss: 0.8577, Test Loss: 0.9680\n",
      "Epoch 21/68, Train Loss: 0.8463, Test Loss: 0.9929\n",
      "Epoch 22/68, Train Loss: 0.8375, Test Loss: 0.9877\n",
      "Epoch 23/68, Train Loss: 0.8459, Test Loss: 0.9904\n",
      "Epoch 24/68, Train Loss: 0.8454, Test Loss: 0.9974\n",
      "Epoch 25/68, Train Loss: 0.8295, Test Loss: 1.0041\n",
      "Epoch 26/68, Train Loss: 0.7945, Test Loss: 1.0111\n",
      "Epoch 27/68, Train Loss: 0.8150, Test Loss: 1.0178\n",
      "Epoch 28/68, Train Loss: 0.8111, Test Loss: 1.0279\n",
      "Epoch 29/68, Train Loss: 0.8223, Test Loss: 1.0269\n",
      "Epoch 30/68, Train Loss: 0.8097, Test Loss: 1.0053\n",
      "Epoch 31/68, Train Loss: 0.8354, Test Loss: 1.0003\n",
      "Epoch 32/68, Train Loss: 0.7892, Test Loss: 1.0080\n",
      "Epoch 33/68, Train Loss: 0.7864, Test Loss: 1.0094\n",
      "Epoch 34/68, Train Loss: 0.7937, Test Loss: 1.0165\n",
      "Epoch 35/68, Train Loss: 0.7994, Test Loss: 1.0305\n",
      "Epoch 36/68, Train Loss: 0.7771, Test Loss: 1.0350\n",
      "Epoch 37/68, Train Loss: 0.8003, Test Loss: 1.0363\n",
      "Epoch 38/68, Train Loss: 0.7752, Test Loss: 1.0377\n",
      "Epoch 39/68, Train Loss: 0.7555, Test Loss: 1.0408\n",
      "Epoch 40/68, Train Loss: 0.7746, Test Loss: 1.0449\n",
      "Epoch 41/68, Train Loss: 0.7491, Test Loss: 1.0800\n",
      "Epoch 42/68, Train Loss: 0.7428, Test Loss: 1.0909\n",
      "Epoch 43/68, Train Loss: 0.7930, Test Loss: 1.0719\n",
      "Epoch 44/68, Train Loss: 0.7401, Test Loss: 1.0592\n",
      "Epoch 45/68, Train Loss: 0.7236, Test Loss: 1.0547\n",
      "Epoch 46/68, Train Loss: 0.7323, Test Loss: 1.0675\n",
      "Epoch 47/68, Train Loss: 0.7544, Test Loss: 1.0631\n",
      "Epoch 48/68, Train Loss: 0.7082, Test Loss: 1.0860\n",
      "Epoch 49/68, Train Loss: 0.7417, Test Loss: 1.0784\n",
      "Epoch 50/68, Train Loss: 0.6954, Test Loss: 1.0884\n",
      "Epoch 51/68, Train Loss: 0.6928, Test Loss: 1.0810\n",
      "Epoch 52/68, Train Loss: 0.6987, Test Loss: 1.0868\n",
      "Epoch 53/68, Train Loss: 0.6750, Test Loss: 1.0912\n",
      "Epoch 54/68, Train Loss: 0.7025, Test Loss: 1.1262\n",
      "Epoch 55/68, Train Loss: 0.6880, Test Loss: 1.1089\n",
      "Epoch 56/68, Train Loss: 0.6749, Test Loss: 1.1037\n",
      "Epoch 57/68, Train Loss: 0.6774, Test Loss: 1.0895\n",
      "Epoch 58/68, Train Loss: 0.6860, Test Loss: 1.0945\n",
      "Epoch 59/68, Train Loss: 0.6380, Test Loss: 1.1036\n",
      "Epoch 60/68, Train Loss: 0.6809, Test Loss: 1.1004\n",
      "Epoch 61/68, Train Loss: 0.7035, Test Loss: 1.0818\n",
      "Epoch 62/68, Train Loss: 0.6984, Test Loss: 1.0780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:19:41,246] Trial 3 finished with value: 1.113232672214508 and parameters: {'num_hidden_layers': 3, 'layer_0_size': 247, 'layer_1_size': 34, 'layer_2_size': 124, 'dropout_rate': 0.25763627508012427, 'learning_rate': 0.0014336981741889085, 'batch_size': 128, 'epochs': 68}. Best is trial 0 with value: 0.9749857783317566.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/68, Train Loss: 0.6979, Test Loss: 1.0853\n",
      "Epoch 64/68, Train Loss: 0.6356, Test Loss: 1.0792\n",
      "Epoch 65/68, Train Loss: 0.6411, Test Loss: 1.0799\n",
      "Epoch 66/68, Train Loss: 0.6340, Test Loss: 1.0904\n",
      "Epoch 67/68, Train Loss: 0.6674, Test Loss: 1.0952\n",
      "Epoch 68/68, Train Loss: 0.6372, Test Loss: 1.1132\n",
      "Epoch 1/99, Train Loss: 1.1613, Test Loss: 0.9634\n",
      "Epoch 2/99, Train Loss: 1.1282, Test Loss: 0.9656\n",
      "Epoch 3/99, Train Loss: 1.1285, Test Loss: 0.9649\n",
      "Epoch 4/99, Train Loss: 1.1330, Test Loss: 0.9651\n",
      "Epoch 5/99, Train Loss: 1.1150, Test Loss: 0.9642\n",
      "Epoch 6/99, Train Loss: 1.1251, Test Loss: 0.9607\n",
      "Epoch 7/99, Train Loss: 1.0997, Test Loss: 0.9679\n",
      "Epoch 8/99, Train Loss: 1.0737, Test Loss: 0.9662\n",
      "Epoch 9/99, Train Loss: 1.0737, Test Loss: 0.9647\n",
      "Epoch 10/99, Train Loss: 1.0582, Test Loss: 0.9604\n",
      "Epoch 11/99, Train Loss: 1.0494, Test Loss: 0.9624\n",
      "Epoch 12/99, Train Loss: 1.0967, Test Loss: 0.9632\n",
      "Epoch 13/99, Train Loss: 1.0974, Test Loss: 0.9615\n",
      "Epoch 14/99, Train Loss: 1.0803, Test Loss: 0.9608\n",
      "Epoch 15/99, Train Loss: 1.0811, Test Loss: 0.9605\n",
      "Epoch 16/99, Train Loss: 1.0668, Test Loss: 0.9567\n",
      "Epoch 17/99, Train Loss: 1.0606, Test Loss: 0.9638\n",
      "Epoch 18/99, Train Loss: 1.0649, Test Loss: 0.9623\n",
      "Epoch 19/99, Train Loss: 1.0573, Test Loss: 0.9615\n",
      "Epoch 20/99, Train Loss: 1.0430, Test Loss: 0.9618\n",
      "Epoch 21/99, Train Loss: 1.0491, Test Loss: 0.9613\n",
      "Epoch 22/99, Train Loss: 1.0495, Test Loss: 0.9626\n",
      "Epoch 23/99, Train Loss: 1.0413, Test Loss: 0.9637\n",
      "Epoch 24/99, Train Loss: 1.0350, Test Loss: 0.9625\n",
      "Epoch 25/99, Train Loss: 1.0330, Test Loss: 0.9614\n",
      "Epoch 26/99, Train Loss: 1.0295, Test Loss: 0.9588\n",
      "Epoch 27/99, Train Loss: 1.0388, Test Loss: 0.9614\n",
      "Epoch 28/99, Train Loss: 1.0582, Test Loss: 0.9584\n",
      "Epoch 29/99, Train Loss: 1.0410, Test Loss: 0.9582\n",
      "Epoch 30/99, Train Loss: 1.0470, Test Loss: 0.9591\n",
      "Epoch 31/99, Train Loss: 1.0140, Test Loss: 0.9628\n",
      "Epoch 32/99, Train Loss: 1.0395, Test Loss: 0.9668\n",
      "Epoch 33/99, Train Loss: 1.0067, Test Loss: 0.9659\n",
      "Epoch 34/99, Train Loss: 1.0324, Test Loss: 0.9676\n",
      "Epoch 35/99, Train Loss: 1.0027, Test Loss: 0.9655\n",
      "Epoch 36/99, Train Loss: 1.0196, Test Loss: 0.9653\n",
      "Epoch 37/99, Train Loss: 1.0328, Test Loss: 0.9665\n",
      "Epoch 38/99, Train Loss: 1.0288, Test Loss: 0.9688\n",
      "Epoch 39/99, Train Loss: 1.0318, Test Loss: 0.9686\n",
      "Epoch 40/99, Train Loss: 1.0380, Test Loss: 0.9668\n",
      "Epoch 41/99, Train Loss: 1.0354, Test Loss: 0.9679\n",
      "Epoch 42/99, Train Loss: 1.0207, Test Loss: 0.9678\n",
      "Epoch 43/99, Train Loss: 1.0222, Test Loss: 0.9702\n",
      "Epoch 44/99, Train Loss: 1.0107, Test Loss: 0.9703\n",
      "Epoch 45/99, Train Loss: 1.0125, Test Loss: 0.9730\n",
      "Epoch 46/99, Train Loss: 1.0128, Test Loss: 0.9776\n",
      "Epoch 47/99, Train Loss: 1.0179, Test Loss: 0.9797\n",
      "Epoch 48/99, Train Loss: 1.0318, Test Loss: 0.9766\n",
      "Epoch 49/99, Train Loss: 0.9923, Test Loss: 0.9726\n",
      "Epoch 50/99, Train Loss: 1.0227, Test Loss: 0.9741\n",
      "Epoch 51/99, Train Loss: 1.0167, Test Loss: 0.9719\n",
      "Epoch 52/99, Train Loss: 1.0173, Test Loss: 0.9714\n",
      "Epoch 53/99, Train Loss: 1.0112, Test Loss: 0.9700\n",
      "Epoch 54/99, Train Loss: 1.0187, Test Loss: 0.9719\n",
      "Epoch 55/99, Train Loss: 1.0209, Test Loss: 0.9717\n",
      "Epoch 56/99, Train Loss: 0.9792, Test Loss: 0.9741\n",
      "Epoch 57/99, Train Loss: 0.9960, Test Loss: 0.9726\n",
      "Epoch 58/99, Train Loss: 1.0245, Test Loss: 0.9763\n",
      "Epoch 59/99, Train Loss: 1.0139, Test Loss: 0.9709\n",
      "Epoch 60/99, Train Loss: 1.0083, Test Loss: 0.9717\n",
      "Epoch 61/99, Train Loss: 0.9985, Test Loss: 0.9711\n",
      "Epoch 62/99, Train Loss: 1.0032, Test Loss: 0.9739\n",
      "Epoch 63/99, Train Loss: 0.9977, Test Loss: 0.9781\n",
      "Epoch 64/99, Train Loss: 1.0122, Test Loss: 0.9800\n",
      "Epoch 65/99, Train Loss: 1.0081, Test Loss: 0.9812\n",
      "Epoch 66/99, Train Loss: 0.9964, Test Loss: 0.9800\n",
      "Epoch 67/99, Train Loss: 1.0018, Test Loss: 0.9785\n",
      "Epoch 68/99, Train Loss: 0.9861, Test Loss: 0.9825\n",
      "Epoch 69/99, Train Loss: 1.0101, Test Loss: 0.9828\n",
      "Epoch 70/99, Train Loss: 1.0058, Test Loss: 0.9839\n",
      "Epoch 71/99, Train Loss: 0.9934, Test Loss: 0.9821\n",
      "Epoch 72/99, Train Loss: 1.0018, Test Loss: 0.9855\n",
      "Epoch 73/99, Train Loss: 1.0058, Test Loss: 0.9868\n",
      "Epoch 74/99, Train Loss: 0.9934, Test Loss: 0.9874\n",
      "Epoch 75/99, Train Loss: 1.0022, Test Loss: 0.9880\n",
      "Epoch 76/99, Train Loss: 1.0083, Test Loss: 0.9907\n",
      "Epoch 77/99, Train Loss: 0.9977, Test Loss: 0.9907\n",
      "Epoch 78/99, Train Loss: 0.9911, Test Loss: 0.9951\n",
      "Epoch 79/99, Train Loss: 0.9864, Test Loss: 0.9975\n",
      "Epoch 80/99, Train Loss: 0.9834, Test Loss: 1.0010\n",
      "Epoch 81/99, Train Loss: 0.9896, Test Loss: 1.0005\n",
      "Epoch 82/99, Train Loss: 1.0000, Test Loss: 0.9993\n",
      "Epoch 83/99, Train Loss: 0.9839, Test Loss: 1.0024\n",
      "Epoch 84/99, Train Loss: 0.9943, Test Loss: 1.0010\n",
      "Epoch 85/99, Train Loss: 0.9970, Test Loss: 1.0058\n",
      "Epoch 86/99, Train Loss: 0.9746, Test Loss: 1.0069\n",
      "Epoch 87/99, Train Loss: 0.9821, Test Loss: 1.0113\n",
      "Epoch 88/99, Train Loss: 1.0221, Test Loss: 1.0095\n",
      "Epoch 89/99, Train Loss: 0.9932, Test Loss: 1.0065\n",
      "Epoch 90/99, Train Loss: 0.9990, Test Loss: 1.0086\n",
      "Epoch 91/99, Train Loss: 0.9824, Test Loss: 1.0080\n",
      "Epoch 92/99, Train Loss: 0.9851, Test Loss: 1.0106\n",
      "Epoch 93/99, Train Loss: 0.9842, Test Loss: 1.0087\n",
      "Epoch 94/99, Train Loss: 0.9815, Test Loss: 1.0125\n",
      "Epoch 95/99, Train Loss: 0.9804, Test Loss: 1.0113\n",
      "Epoch 96/99, Train Loss: 0.9921, Test Loss: 1.0139\n",
      "Epoch 97/99, Train Loss: 0.9891, Test Loss: 1.0134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:19:50,060] Trial 4 finished with value: 1.0096525847911835 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 102, 'layer_1_size': 168, 'layer_2_size': 172, 'layer_3_size': 98, 'layer_4_size': 212, 'layer_5_size': 169, 'layer_6_size': 119, 'layer_7_size': 59, 'dropout_rate': 0.349883372125423, 'learning_rate': 0.0002648844435315349, 'batch_size': 64, 'epochs': 99}. Best is trial 0 with value: 0.9749857783317566.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/99, Train Loss: 0.9676, Test Loss: 1.0102\n",
      "Epoch 99/99, Train Loss: 0.9775, Test Loss: 1.0097\n",
      "Epoch 1/81, Train Loss: 1.3160, Test Loss: 0.9593\n",
      "Epoch 2/81, Train Loss: 1.2316, Test Loss: 0.9618\n",
      "Epoch 3/81, Train Loss: 1.2044, Test Loss: 0.9671\n",
      "Epoch 4/81, Train Loss: 1.1526, Test Loss: 0.9720\n",
      "Epoch 5/81, Train Loss: 1.1715, Test Loss: 0.9773\n",
      "Epoch 6/81, Train Loss: 1.1353, Test Loss: 0.9808\n",
      "Epoch 7/81, Train Loss: 1.1521, Test Loss: 0.9812\n",
      "Epoch 8/81, Train Loss: 1.1001, Test Loss: 0.9834\n",
      "Epoch 9/81, Train Loss: 1.1292, Test Loss: 0.9867\n",
      "Epoch 10/81, Train Loss: 1.1386, Test Loss: 0.9867\n",
      "Epoch 11/81, Train Loss: 1.1072, Test Loss: 0.9867\n",
      "Epoch 12/81, Train Loss: 1.0970, Test Loss: 0.9886\n",
      "Epoch 13/81, Train Loss: 1.0794, Test Loss: 0.9877\n",
      "Epoch 14/81, Train Loss: 1.1077, Test Loss: 0.9874\n",
      "Epoch 15/81, Train Loss: 1.0742, Test Loss: 0.9862\n",
      "Epoch 16/81, Train Loss: 1.0902, Test Loss: 0.9852\n",
      "Epoch 17/81, Train Loss: 1.0684, Test Loss: 0.9853\n",
      "Epoch 18/81, Train Loss: 1.0662, Test Loss: 0.9845\n",
      "Epoch 19/81, Train Loss: 1.0766, Test Loss: 0.9845\n",
      "Epoch 20/81, Train Loss: 1.0674, Test Loss: 0.9844\n",
      "Epoch 21/81, Train Loss: 1.0752, Test Loss: 0.9862\n",
      "Epoch 22/81, Train Loss: 1.0404, Test Loss: 0.9895\n",
      "Epoch 23/81, Train Loss: 1.0532, Test Loss: 0.9890\n",
      "Epoch 24/81, Train Loss: 1.0577, Test Loss: 0.9884\n",
      "Epoch 25/81, Train Loss: 1.0560, Test Loss: 0.9868\n",
      "Epoch 26/81, Train Loss: 1.0645, Test Loss: 0.9859\n",
      "Epoch 27/81, Train Loss: 1.0689, Test Loss: 0.9871\n",
      "Epoch 28/81, Train Loss: 1.0393, Test Loss: 0.9863\n",
      "Epoch 29/81, Train Loss: 1.0313, Test Loss: 0.9859\n",
      "Epoch 30/81, Train Loss: 1.0337, Test Loss: 0.9877\n",
      "Epoch 31/81, Train Loss: 1.0231, Test Loss: 0.9884\n",
      "Epoch 32/81, Train Loss: 1.0244, Test Loss: 0.9879\n",
      "Epoch 33/81, Train Loss: 1.0161, Test Loss: 0.9897\n",
      "Epoch 34/81, Train Loss: 1.0251, Test Loss: 0.9899\n",
      "Epoch 35/81, Train Loss: 1.0259, Test Loss: 0.9903\n",
      "Epoch 36/81, Train Loss: 1.0496, Test Loss: 0.9899\n",
      "Epoch 37/81, Train Loss: 1.0236, Test Loss: 0.9907\n",
      "Epoch 38/81, Train Loss: 1.0283, Test Loss: 0.9899\n",
      "Epoch 39/81, Train Loss: 1.0248, Test Loss: 0.9895\n",
      "Epoch 40/81, Train Loss: 1.0041, Test Loss: 0.9911\n",
      "Epoch 41/81, Train Loss: 1.0301, Test Loss: 0.9897\n",
      "Epoch 42/81, Train Loss: 1.0312, Test Loss: 0.9908\n",
      "Epoch 43/81, Train Loss: 1.0417, Test Loss: 0.9926\n",
      "Epoch 44/81, Train Loss: 1.0026, Test Loss: 0.9935\n",
      "Epoch 45/81, Train Loss: 1.0167, Test Loss: 0.9951\n",
      "Epoch 46/81, Train Loss: 1.0232, Test Loss: 0.9942\n",
      "Epoch 47/81, Train Loss: 1.0108, Test Loss: 0.9936\n",
      "Epoch 48/81, Train Loss: 1.0160, Test Loss: 0.9941\n",
      "Epoch 49/81, Train Loss: 1.0191, Test Loss: 0.9944\n",
      "Epoch 50/81, Train Loss: 1.0047, Test Loss: 0.9931\n",
      "Epoch 51/81, Train Loss: 0.9946, Test Loss: 0.9946\n",
      "Epoch 52/81, Train Loss: 0.9811, Test Loss: 0.9930\n",
      "Epoch 53/81, Train Loss: 1.0143, Test Loss: 0.9915\n",
      "Epoch 54/81, Train Loss: 1.0069, Test Loss: 0.9918\n",
      "Epoch 55/81, Train Loss: 0.9967, Test Loss: 0.9943\n",
      "Epoch 56/81, Train Loss: 1.0039, Test Loss: 0.9956\n",
      "Epoch 57/81, Train Loss: 1.0036, Test Loss: 0.9970\n",
      "Epoch 58/81, Train Loss: 0.9977, Test Loss: 0.9982\n",
      "Epoch 59/81, Train Loss: 0.9922, Test Loss: 0.9987\n",
      "Epoch 60/81, Train Loss: 0.9985, Test Loss: 0.9998\n",
      "Epoch 61/81, Train Loss: 1.0207, Test Loss: 1.0017\n",
      "Epoch 62/81, Train Loss: 0.9960, Test Loss: 1.0019\n",
      "Epoch 63/81, Train Loss: 1.0124, Test Loss: 1.0006\n",
      "Epoch 64/81, Train Loss: 0.9950, Test Loss: 1.0009\n",
      "Epoch 65/81, Train Loss: 0.9813, Test Loss: 1.0015\n",
      "Epoch 66/81, Train Loss: 0.9765, Test Loss: 1.0041\n",
      "Epoch 67/81, Train Loss: 0.9803, Test Loss: 1.0022\n",
      "Epoch 68/81, Train Loss: 0.9841, Test Loss: 1.0054\n",
      "Epoch 69/81, Train Loss: 1.0050, Test Loss: 1.0030\n",
      "Epoch 70/81, Train Loss: 0.9994, Test Loss: 1.0022\n",
      "Epoch 71/81, Train Loss: 0.9883, Test Loss: 0.9996\n",
      "Epoch 72/81, Train Loss: 1.0055, Test Loss: 0.9979\n",
      "Epoch 73/81, Train Loss: 0.9755, Test Loss: 0.9972\n",
      "Epoch 74/81, Train Loss: 0.9780, Test Loss: 1.0002\n",
      "Epoch 75/81, Train Loss: 0.9912, Test Loss: 0.9983\n",
      "Epoch 76/81, Train Loss: 1.0009, Test Loss: 1.0000\n",
      "Epoch 77/81, Train Loss: 0.9815, Test Loss: 1.0012\n",
      "Epoch 78/81, Train Loss: 0.9498, Test Loss: 1.0041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:19:50,888] Trial 5 finished with value: 1.0069734454154968 and parameters: {'num_hidden_layers': 1, 'layer_0_size': 98, 'dropout_rate': 0.2956155660489025, 'learning_rate': 0.0006857384998344561, 'batch_size': 128, 'epochs': 81}. Best is trial 0 with value: 0.9749857783317566.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/81, Train Loss: 0.9884, Test Loss: 1.0051\n",
      "Epoch 80/81, Train Loss: 0.9860, Test Loss: 1.0055\n",
      "Epoch 81/81, Train Loss: 0.9720, Test Loss: 1.0070\n",
      "Epoch 1/13, Train Loss: 1.1590, Test Loss: 0.8195\n",
      "Epoch 2/13, Train Loss: 1.1178, Test Loss: 0.8359\n",
      "Epoch 3/13, Train Loss: 1.0582, Test Loss: 0.8202\n",
      "Epoch 4/13, Train Loss: 1.0412, Test Loss: 0.8318\n",
      "Epoch 5/13, Train Loss: 1.0012, Test Loss: 0.8478\n",
      "Epoch 6/13, Train Loss: 0.9603, Test Loss: 0.8685\n",
      "Epoch 7/13, Train Loss: 0.9429, Test Loss: 0.8881\n",
      "Epoch 8/13, Train Loss: 0.9059, Test Loss: 0.8885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:19:51,461] Trial 6 finished with value: 0.8935349583625793 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 137, 'layer_1_size': 86, 'layer_2_size': 84, 'layer_3_size': 99, 'layer_4_size': 76, 'layer_5_size': 163, 'layer_6_size': 226, 'dropout_rate': 0.10375644065439192, 'learning_rate': 0.0016328855111063267, 'batch_size': 128, 'epochs': 13}. Best is trial 6 with value: 0.8935349583625793.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/13, Train Loss: 0.9279, Test Loss: 0.9098\n",
      "Epoch 10/13, Train Loss: 0.8936, Test Loss: 0.9413\n",
      "Epoch 11/13, Train Loss: 0.8284, Test Loss: 0.9654\n",
      "Epoch 12/13, Train Loss: 0.8598, Test Loss: 0.9173\n",
      "Epoch 13/13, Train Loss: 0.8274, Test Loss: 0.8935\n",
      "Epoch 1/9, Train Loss: 1.4273, Test Loss: 1.0529\n",
      "Epoch 2/9, Train Loss: 1.4002, Test Loss: 1.0513\n",
      "Epoch 3/9, Train Loss: 1.3869, Test Loss: 1.0521\n",
      "Epoch 4/9, Train Loss: 1.3595, Test Loss: 1.0537\n",
      "Epoch 5/9, Train Loss: 1.3557, Test Loss: 1.0553\n",
      "Epoch 6/9, Train Loss: 1.3800, Test Loss: 1.0555\n",
      "Epoch 7/9, Train Loss: 1.3735, Test Loss: 1.0550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:19:51,804] Trial 7 finished with value: 1.0563913583755493 and parameters: {'num_hidden_layers': 4, 'layer_0_size': 133, 'layer_1_size': 228, 'layer_2_size': 244, 'layer_3_size': 196, 'dropout_rate': 0.49778302702375055, 'learning_rate': 1.452674175151942e-05, 'batch_size': 128, 'epochs': 9}. Best is trial 6 with value: 0.8935349583625793.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/9, Train Loss: 1.3539, Test Loss: 1.0551\n",
      "Epoch 9/9, Train Loss: 1.3707, Test Loss: 1.0564\n",
      "Epoch 1/92, Train Loss: 1.2322, Test Loss: 0.8119\n",
      "Epoch 2/92, Train Loss: 1.1132, Test Loss: 0.7968\n",
      "Epoch 3/92, Train Loss: 1.0914, Test Loss: 0.8061\n",
      "Epoch 4/92, Train Loss: 1.0517, Test Loss: 0.8042\n",
      "Epoch 5/92, Train Loss: 1.0887, Test Loss: 0.8275\n",
      "Epoch 6/92, Train Loss: 1.0586, Test Loss: 0.8078\n",
      "Epoch 7/92, Train Loss: 1.0311, Test Loss: 0.7924\n",
      "Epoch 8/92, Train Loss: 1.0912, Test Loss: 0.8003\n",
      "Epoch 9/92, Train Loss: 1.0704, Test Loss: 0.8475\n",
      "Epoch 10/92, Train Loss: 1.0472, Test Loss: 0.7860\n",
      "Epoch 11/92, Train Loss: 1.0598, Test Loss: 0.7986\n",
      "Epoch 12/92, Train Loss: 1.0464, Test Loss: 0.7999\n",
      "Epoch 13/92, Train Loss: 1.0536, Test Loss: 0.8082\n",
      "Epoch 14/92, Train Loss: 1.0272, Test Loss: 0.8116\n",
      "Epoch 15/92, Train Loss: 1.0440, Test Loss: 0.8124\n",
      "Epoch 16/92, Train Loss: 1.0134, Test Loss: 0.8332\n",
      "Epoch 17/92, Train Loss: 1.0443, Test Loss: 0.8036\n",
      "Epoch 18/92, Train Loss: 1.0156, Test Loss: 0.8109\n",
      "Epoch 19/92, Train Loss: 1.0283, Test Loss: 0.8110\n",
      "Epoch 20/92, Train Loss: 1.0156, Test Loss: 0.8286\n",
      "Epoch 21/92, Train Loss: 1.0185, Test Loss: 0.8202\n",
      "Epoch 22/92, Train Loss: 1.0357, Test Loss: 0.8121\n",
      "Epoch 23/92, Train Loss: 1.0138, Test Loss: 0.8122\n",
      "Epoch 24/92, Train Loss: 1.0125, Test Loss: 0.8239\n",
      "Epoch 25/92, Train Loss: 0.9799, Test Loss: 0.8630\n",
      "Epoch 26/92, Train Loss: 1.0038, Test Loss: 0.8254\n",
      "Epoch 27/92, Train Loss: 1.0279, Test Loss: 0.8084\n",
      "Epoch 28/92, Train Loss: 1.0126, Test Loss: 0.8265\n",
      "Epoch 29/92, Train Loss: 1.0555, Test Loss: 0.7904\n",
      "Epoch 30/92, Train Loss: 0.9948, Test Loss: 0.8009\n",
      "Epoch 31/92, Train Loss: 0.9938, Test Loss: 0.8300\n",
      "Epoch 32/92, Train Loss: 0.9925, Test Loss: 0.7990\n",
      "Epoch 33/92, Train Loss: 1.0260, Test Loss: 0.8507\n",
      "Epoch 34/92, Train Loss: 1.0527, Test Loss: 0.8093\n",
      "Epoch 35/92, Train Loss: 0.9699, Test Loss: 0.8111\n",
      "Epoch 36/92, Train Loss: 1.0010, Test Loss: 0.8442\n",
      "Epoch 37/92, Train Loss: 0.9909, Test Loss: 0.8204\n",
      "Epoch 38/92, Train Loss: 0.9956, Test Loss: 0.8112\n",
      "Epoch 39/92, Train Loss: 0.9880, Test Loss: 0.8460\n",
      "Epoch 40/92, Train Loss: 0.9859, Test Loss: 0.8414\n",
      "Epoch 41/92, Train Loss: 0.9790, Test Loss: 0.8459\n",
      "Epoch 42/92, Train Loss: 0.9635, Test Loss: 0.8318\n",
      "Epoch 43/92, Train Loss: 0.9962, Test Loss: 0.8580\n",
      "Epoch 44/92, Train Loss: 0.9870, Test Loss: 0.8272\n",
      "Epoch 45/92, Train Loss: 0.9694, Test Loss: 0.8431\n",
      "Epoch 46/92, Train Loss: 0.9327, Test Loss: 0.9032\n",
      "Epoch 47/92, Train Loss: 0.9711, Test Loss: 0.8027\n",
      "Epoch 48/92, Train Loss: 0.9844, Test Loss: 0.8266\n",
      "Epoch 49/92, Train Loss: 0.9882, Test Loss: 0.8651\n",
      "Epoch 50/92, Train Loss: 0.9885, Test Loss: 0.8171\n",
      "Epoch 51/92, Train Loss: 0.9987, Test Loss: 0.8278\n",
      "Epoch 52/92, Train Loss: 0.9637, Test Loss: 0.8840\n",
      "Epoch 53/92, Train Loss: 0.9963, Test Loss: 0.8241\n",
      "Epoch 54/92, Train Loss: 0.9817, Test Loss: 0.8630\n",
      "Epoch 55/92, Train Loss: 0.9903, Test Loss: 0.8687\n",
      "Epoch 56/92, Train Loss: 0.9909, Test Loss: 0.8534\n",
      "Epoch 57/92, Train Loss: 0.9933, Test Loss: 0.8403\n",
      "Epoch 58/92, Train Loss: 0.9824, Test Loss: 0.8789\n",
      "Epoch 59/92, Train Loss: 0.9357, Test Loss: 0.8746\n",
      "Epoch 60/92, Train Loss: 0.9725, Test Loss: 0.8789\n",
      "Epoch 61/92, Train Loss: 0.9249, Test Loss: 0.8902\n",
      "Epoch 62/92, Train Loss: 0.9458, Test Loss: 0.8540\n",
      "Epoch 63/92, Train Loss: 0.9571, Test Loss: 0.8420\n",
      "Epoch 64/92, Train Loss: 0.9633, Test Loss: 0.8916\n",
      "Epoch 65/92, Train Loss: 0.9354, Test Loss: 0.9241\n",
      "Epoch 66/92, Train Loss: 0.9650, Test Loss: 0.8461\n",
      "Epoch 67/92, Train Loss: 0.9225, Test Loss: 0.8697\n",
      "Epoch 68/92, Train Loss: 0.9406, Test Loss: 0.8993\n",
      "Epoch 69/92, Train Loss: 1.0005, Test Loss: 0.8428\n",
      "Epoch 70/92, Train Loss: 0.9422, Test Loss: 0.8433\n",
      "Epoch 71/92, Train Loss: 0.9650, Test Loss: 0.8630\n",
      "Epoch 72/92, Train Loss: 0.9441, Test Loss: 0.8868\n",
      "Epoch 73/92, Train Loss: 0.9334, Test Loss: 0.8303\n",
      "Epoch 74/92, Train Loss: 0.9298, Test Loss: 0.8914\n",
      "Epoch 75/92, Train Loss: 0.9239, Test Loss: 0.8685\n",
      "Epoch 76/92, Train Loss: 0.9201, Test Loss: 0.8737\n",
      "Epoch 77/92, Train Loss: 0.8892, Test Loss: 0.8897\n",
      "Epoch 78/92, Train Loss: 0.9225, Test Loss: 0.8541\n",
      "Epoch 79/92, Train Loss: 0.9450, Test Loss: 0.8703\n",
      "Epoch 80/92, Train Loss: 0.9299, Test Loss: 0.8278\n",
      "Epoch 81/92, Train Loss: 0.9041, Test Loss: 0.8346\n",
      "Epoch 82/92, Train Loss: 0.9175, Test Loss: 0.8485\n",
      "Epoch 83/92, Train Loss: 0.9013, Test Loss: 0.9574\n",
      "Epoch 84/92, Train Loss: 0.9067, Test Loss: 0.8499\n",
      "Epoch 85/92, Train Loss: 0.9158, Test Loss: 0.8960\n",
      "Epoch 86/92, Train Loss: 0.9660, Test Loss: 0.8296\n",
      "Epoch 87/92, Train Loss: 0.9193, Test Loss: 0.8873\n",
      "Epoch 88/92, Train Loss: 0.9124, Test Loss: 0.8961\n",
      "Epoch 89/92, Train Loss: 0.9265, Test Loss: 0.8633\n",
      "Epoch 90/92, Train Loss: 0.9066, Test Loss: 0.8678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:20:06,024] Trial 8 finished with value: 0.8738630967480796 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 194, 'layer_1_size': 221, 'layer_2_size': 174, 'layer_3_size': 153, 'layer_4_size': 238, 'layer_5_size': 165, 'layer_6_size': 98, 'dropout_rate': 0.48947121713814845, 'learning_rate': 0.005698451723467807, 'batch_size': 32, 'epochs': 92}. Best is trial 8 with value: 0.8738630967480796.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/92, Train Loss: 0.9169, Test Loss: 0.9071\n",
      "Epoch 92/92, Train Loss: 0.9194, Test Loss: 0.8739\n",
      "Epoch 1/7, Train Loss: 1.5055, Test Loss: 0.9381\n",
      "Epoch 2/7, Train Loss: 1.2527, Test Loss: 0.9524\n",
      "Epoch 3/7, Train Loss: 1.2186, Test Loss: 0.9124\n",
      "Epoch 4/7, Train Loss: 1.1689, Test Loss: 0.9013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:20:06,450] Trial 9 finished with value: 0.9066389203071594 and parameters: {'num_hidden_layers': 9, 'layer_0_size': 40, 'layer_1_size': 151, 'layer_2_size': 218, 'layer_3_size': 32, 'layer_4_size': 77, 'layer_5_size': 149, 'layer_6_size': 194, 'layer_7_size': 241, 'layer_8_size': 54, 'dropout_rate': 0.44495285022407405, 'learning_rate': 0.001701184190210542, 'batch_size': 128, 'epochs': 7}. Best is trial 8 with value: 0.8738630967480796.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/7, Train Loss: 1.1626, Test Loss: 0.9062\n",
      "Epoch 6/7, Train Loss: 1.1356, Test Loss: 0.9107\n",
      "Epoch 7/7, Train Loss: 1.0791, Test Loss: 0.9066\n",
      "Epoch 1/99, Train Loss: 1.1802, Test Loss: 1.0171\n",
      "Epoch 2/99, Train Loss: 1.1387, Test Loss: 0.9735\n",
      "Epoch 3/99, Train Loss: 1.0957, Test Loss: 0.9896\n",
      "Epoch 4/99, Train Loss: 1.1044, Test Loss: 0.9742\n",
      "Epoch 5/99, Train Loss: 1.0866, Test Loss: 0.9724\n",
      "Epoch 6/99, Train Loss: 1.1248, Test Loss: 0.9850\n",
      "Epoch 7/99, Train Loss: 1.0752, Test Loss: 1.0097\n",
      "Epoch 8/99, Train Loss: 1.1099, Test Loss: 0.9783\n",
      "Epoch 9/99, Train Loss: 1.0766, Test Loss: 0.9877\n",
      "Epoch 10/99, Train Loss: 1.1090, Test Loss: 0.9945\n",
      "Epoch 11/99, Train Loss: 1.0817, Test Loss: 0.9905\n",
      "Epoch 12/99, Train Loss: 1.0594, Test Loss: 0.9924\n",
      "Epoch 13/99, Train Loss: 1.0487, Test Loss: 1.0024\n",
      "Epoch 14/99, Train Loss: 1.0974, Test Loss: 0.9896\n",
      "Epoch 15/99, Train Loss: 1.0569, Test Loss: 1.0116\n",
      "Epoch 16/99, Train Loss: 1.0462, Test Loss: 1.0161\n",
      "Epoch 17/99, Train Loss: 1.0633, Test Loss: 1.0069\n",
      "Epoch 18/99, Train Loss: 1.0689, Test Loss: 0.9975\n",
      "Epoch 19/99, Train Loss: 1.0552, Test Loss: 0.9879\n",
      "Epoch 20/99, Train Loss: 1.0302, Test Loss: 1.0238\n",
      "Epoch 21/99, Train Loss: 1.0291, Test Loss: 1.0128\n",
      "Epoch 22/99, Train Loss: 1.0450, Test Loss: 1.0118\n",
      "Epoch 23/99, Train Loss: 1.0184, Test Loss: 1.0327\n",
      "Epoch 24/99, Train Loss: 1.0510, Test Loss: 1.0073\n",
      "Epoch 25/99, Train Loss: 1.0429, Test Loss: 1.0224\n",
      "Epoch 26/99, Train Loss: 1.0404, Test Loss: 1.0182\n",
      "Epoch 27/99, Train Loss: 1.0461, Test Loss: 0.9918\n",
      "Epoch 28/99, Train Loss: 1.0216, Test Loss: 1.0313\n",
      "Epoch 29/99, Train Loss: 1.0707, Test Loss: 1.0241\n",
      "Epoch 30/99, Train Loss: 1.0145, Test Loss: 1.0142\n",
      "Epoch 31/99, Train Loss: 1.0218, Test Loss: 1.0079\n",
      "Epoch 32/99, Train Loss: 1.0275, Test Loss: 1.0292\n",
      "Epoch 33/99, Train Loss: 1.0450, Test Loss: 1.0376\n",
      "Epoch 34/99, Train Loss: 1.0133, Test Loss: 1.0131\n",
      "Epoch 35/99, Train Loss: 1.0209, Test Loss: 1.0381\n",
      "Epoch 36/99, Train Loss: 1.0529, Test Loss: 1.0146\n",
      "Epoch 37/99, Train Loss: 0.9975, Test Loss: 1.0185\n",
      "Epoch 38/99, Train Loss: 1.0460, Test Loss: 1.0350\n",
      "Epoch 39/99, Train Loss: 1.0220, Test Loss: 1.0346\n",
      "Epoch 40/99, Train Loss: 1.0167, Test Loss: 1.0647\n",
      "Epoch 41/99, Train Loss: 1.0359, Test Loss: 1.0143\n",
      "Epoch 42/99, Train Loss: 1.0270, Test Loss: 1.0533\n",
      "Epoch 43/99, Train Loss: 1.0016, Test Loss: 1.0860\n",
      "Epoch 44/99, Train Loss: 1.0372, Test Loss: 1.0672\n",
      "Epoch 45/99, Train Loss: 1.0147, Test Loss: 1.0022\n",
      "Epoch 46/99, Train Loss: 1.0311, Test Loss: 1.0210\n",
      "Epoch 47/99, Train Loss: 0.9887, Test Loss: 1.0143\n",
      "Epoch 48/99, Train Loss: 0.9848, Test Loss: 1.0574\n",
      "Epoch 49/99, Train Loss: 0.9890, Test Loss: 1.0271\n",
      "Epoch 50/99, Train Loss: 1.0090, Test Loss: 1.0121\n",
      "Epoch 51/99, Train Loss: 1.0160, Test Loss: 1.0478\n",
      "Epoch 52/99, Train Loss: 1.0162, Test Loss: 1.0238\n",
      "Epoch 53/99, Train Loss: 1.0287, Test Loss: 1.0428\n",
      "Epoch 54/99, Train Loss: 0.9874, Test Loss: 1.0984\n",
      "Epoch 55/99, Train Loss: 0.9748, Test Loss: 1.0675\n",
      "Epoch 56/99, Train Loss: 1.0055, Test Loss: 1.0290\n",
      "Epoch 57/99, Train Loss: 0.9931, Test Loss: 1.0179\n",
      "Epoch 58/99, Train Loss: 1.0049, Test Loss: 1.0258\n",
      "Epoch 59/99, Train Loss: 1.0145, Test Loss: 1.0511\n",
      "Epoch 60/99, Train Loss: 1.0111, Test Loss: 1.0383\n",
      "Epoch 61/99, Train Loss: 0.9996, Test Loss: 1.0592\n",
      "Epoch 62/99, Train Loss: 0.9867, Test Loss: 1.0850\n",
      "Epoch 63/99, Train Loss: 0.9635, Test Loss: 1.1546\n",
      "Epoch 64/99, Train Loss: 1.0278, Test Loss: 1.0385\n",
      "Epoch 65/99, Train Loss: 0.9996, Test Loss: 1.0560\n",
      "Epoch 66/99, Train Loss: 1.0300, Test Loss: 1.0461\n",
      "Epoch 67/99, Train Loss: 1.0008, Test Loss: 1.0555\n",
      "Epoch 68/99, Train Loss: 0.9865, Test Loss: 1.1299\n",
      "Epoch 69/99, Train Loss: 0.9776, Test Loss: 1.0939\n",
      "Epoch 70/99, Train Loss: 0.9857, Test Loss: 1.0084\n",
      "Epoch 71/99, Train Loss: 0.9632, Test Loss: 1.0642\n",
      "Epoch 72/99, Train Loss: 0.9513, Test Loss: 1.0202\n",
      "Epoch 73/99, Train Loss: 0.9874, Test Loss: 1.0383\n",
      "Epoch 74/99, Train Loss: 0.9759, Test Loss: 1.0830\n",
      "Epoch 75/99, Train Loss: 0.9564, Test Loss: 1.0753\n",
      "Epoch 76/99, Train Loss: 0.9713, Test Loss: 1.0789\n",
      "Epoch 77/99, Train Loss: 0.9715, Test Loss: 1.0698\n",
      "Epoch 78/99, Train Loss: 0.9681, Test Loss: 1.1009\n",
      "Epoch 79/99, Train Loss: 0.9660, Test Loss: 1.0633\n",
      "Epoch 80/99, Train Loss: 1.0118, Test Loss: 1.0729\n",
      "Epoch 81/99, Train Loss: 0.9917, Test Loss: 1.0612\n",
      "Epoch 82/99, Train Loss: 0.9793, Test Loss: 1.0340\n",
      "Epoch 83/99, Train Loss: 0.9948, Test Loss: 1.0262\n",
      "Epoch 84/99, Train Loss: 0.9554, Test Loss: 1.0963\n",
      "Epoch 85/99, Train Loss: 0.9585, Test Loss: 1.0868\n",
      "Epoch 86/99, Train Loss: 0.9844, Test Loss: 1.0724\n",
      "Epoch 87/99, Train Loss: 0.9849, Test Loss: 1.0936\n",
      "Epoch 88/99, Train Loss: 0.9527, Test Loss: 1.0972\n",
      "Epoch 89/99, Train Loss: 0.9310, Test Loss: 1.2419\n",
      "Epoch 90/99, Train Loss: 0.9285, Test Loss: 1.0978\n",
      "Epoch 91/99, Train Loss: 0.9326, Test Loss: 1.0871\n",
      "Epoch 92/99, Train Loss: 0.9401, Test Loss: 1.0837\n",
      "Epoch 93/99, Train Loss: 0.9528, Test Loss: 1.0859\n",
      "Epoch 94/99, Train Loss: 1.0052, Test Loss: 1.0179\n",
      "Epoch 95/99, Train Loss: 0.9134, Test Loss: 1.1618\n",
      "Epoch 96/99, Train Loss: 0.9390, Test Loss: 1.1430\n",
      "Epoch 97/99, Train Loss: 0.9087, Test Loss: 1.0806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:20:18,091] Trial 10 finished with value: 1.0483316779136658 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 199, 'layer_1_size': 253, 'layer_2_size': 41, 'layer_3_size': 241, 'layer_4_size': 252, 'layer_5_size': 66, 'dropout_rate': 0.4876499857009356, 'learning_rate': 0.009301838792939193, 'batch_size': 32, 'epochs': 99}. Best is trial 8 with value: 0.8738630967480796.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/99, Train Loss: 0.9056, Test Loss: 1.1216\n",
      "Epoch 99/99, Train Loss: 0.9351, Test Loss: 1.0483\n",
      "Epoch 1/29, Train Loss: 1.5553, Test Loss: 1.1535\n",
      "Epoch 2/29, Train Loss: 1.0913, Test Loss: 1.1594\n",
      "Epoch 3/29, Train Loss: 0.9625, Test Loss: 1.1564\n",
      "Epoch 4/29, Train Loss: 0.9345, Test Loss: 1.1597\n",
      "Epoch 5/29, Train Loss: 0.8995, Test Loss: 1.1921\n",
      "Epoch 6/29, Train Loss: 0.9041, Test Loss: 1.1931\n",
      "Epoch 7/29, Train Loss: 0.8847, Test Loss: 1.2473\n",
      "Epoch 8/29, Train Loss: 0.8586, Test Loss: 1.2781\n",
      "Epoch 9/29, Train Loss: 0.8680, Test Loss: 1.3257\n",
      "Epoch 10/29, Train Loss: 0.8379, Test Loss: 1.2366\n",
      "Epoch 11/29, Train Loss: 0.8240, Test Loss: 1.2718\n",
      "Epoch 12/29, Train Loss: 0.8202, Test Loss: 1.3225\n",
      "Epoch 13/29, Train Loss: 0.7813, Test Loss: 1.3380\n",
      "Epoch 14/29, Train Loss: 0.7442, Test Loss: 1.3370\n",
      "Epoch 15/29, Train Loss: 0.7469, Test Loss: 1.3334\n",
      "Epoch 16/29, Train Loss: 0.7211, Test Loss: 1.3426\n",
      "Epoch 17/29, Train Loss: 0.7243, Test Loss: 1.3644\n",
      "Epoch 18/29, Train Loss: 0.7113, Test Loss: 1.3753\n",
      "Epoch 19/29, Train Loss: 0.6738, Test Loss: 1.4651\n",
      "Epoch 20/29, Train Loss: 0.6599, Test Loss: 1.4606\n",
      "Epoch 21/29, Train Loss: 0.5969, Test Loss: 1.5257\n",
      "Epoch 22/29, Train Loss: 0.6265, Test Loss: 1.5683\n",
      "Epoch 23/29, Train Loss: 0.6472, Test Loss: 1.4697\n",
      "Epoch 24/29, Train Loss: 0.6499, Test Loss: 1.4982\n",
      "Epoch 25/29, Train Loss: 0.5644, Test Loss: 1.5502\n",
      "Epoch 26/29, Train Loss: 0.5574, Test Loss: 1.6308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:20:19,136] Trial 11 finished with value: 1.4760217666625977 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 178, 'layer_1_size': 100, 'layer_2_size': 96, 'layer_3_size': 127, 'layer_4_size': 32, 'layer_5_size': 202, 'layer_6_size': 255, 'dropout_rate': 0.14161199958015017, 'learning_rate': 0.00798339073909845, 'batch_size': 256, 'epochs': 29}. Best is trial 8 with value: 0.8738630967480796.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/29, Train Loss: 0.5594, Test Loss: 1.5215\n",
      "Epoch 28/29, Train Loss: 0.5595, Test Loss: 1.5379\n",
      "Epoch 29/29, Train Loss: 0.5354, Test Loss: 1.4760\n",
      "Epoch 1/70, Train Loss: 1.0451, Test Loss: 0.8999\n",
      "Epoch 2/70, Train Loss: 0.9567, Test Loss: 0.9135\n",
      "Epoch 3/70, Train Loss: 0.9565, Test Loss: 0.9231\n",
      "Epoch 4/70, Train Loss: 0.9477, Test Loss: 0.8812\n",
      "Epoch 5/70, Train Loss: 0.9426, Test Loss: 0.9117\n",
      "Epoch 6/70, Train Loss: 0.9312, Test Loss: 0.9014\n",
      "Epoch 7/70, Train Loss: 0.9244, Test Loss: 0.9175\n",
      "Epoch 8/70, Train Loss: 0.9156, Test Loss: 0.9264\n",
      "Epoch 9/70, Train Loss: 0.9078, Test Loss: 0.9051\n",
      "Epoch 10/70, Train Loss: 0.9151, Test Loss: 0.9425\n",
      "Epoch 11/70, Train Loss: 0.8862, Test Loss: 0.9445\n",
      "Epoch 12/70, Train Loss: 0.8660, Test Loss: 0.9188\n",
      "Epoch 13/70, Train Loss: 0.8910, Test Loss: 0.8935\n",
      "Epoch 14/70, Train Loss: 0.8756, Test Loss: 0.8896\n",
      "Epoch 15/70, Train Loss: 0.8687, Test Loss: 0.9149\n",
      "Epoch 16/70, Train Loss: 0.8484, Test Loss: 0.9554\n",
      "Epoch 17/70, Train Loss: 0.8493, Test Loss: 0.9658\n",
      "Epoch 18/70, Train Loss: 0.8788, Test Loss: 0.9206\n",
      "Epoch 19/70, Train Loss: 0.8091, Test Loss: 0.9824\n",
      "Epoch 20/70, Train Loss: 0.8344, Test Loss: 0.9370\n",
      "Epoch 21/70, Train Loss: 0.8062, Test Loss: 0.9558\n",
      "Epoch 22/70, Train Loss: 0.8290, Test Loss: 0.9726\n",
      "Epoch 23/70, Train Loss: 0.8060, Test Loss: 1.0020\n",
      "Epoch 24/70, Train Loss: 0.8309, Test Loss: 0.9589\n",
      "Epoch 25/70, Train Loss: 0.7821, Test Loss: 0.9910\n",
      "Epoch 26/70, Train Loss: 0.8089, Test Loss: 1.0103\n",
      "Epoch 27/70, Train Loss: 0.7752, Test Loss: 0.9688\n",
      "Epoch 28/70, Train Loss: 0.8320, Test Loss: 0.9431\n",
      "Epoch 29/70, Train Loss: 0.7523, Test Loss: 1.0286\n",
      "Epoch 30/70, Train Loss: 0.8038, Test Loss: 1.0185\n",
      "Epoch 31/70, Train Loss: 0.7396, Test Loss: 0.9789\n",
      "Epoch 32/70, Train Loss: 0.7743, Test Loss: 0.9931\n",
      "Epoch 33/70, Train Loss: 0.7161, Test Loss: 1.0168\n",
      "Epoch 34/70, Train Loss: 0.7300, Test Loss: 1.0587\n",
      "Epoch 35/70, Train Loss: 0.7527, Test Loss: 1.0274\n",
      "Epoch 36/70, Train Loss: 0.7167, Test Loss: 1.0452\n",
      "Epoch 37/70, Train Loss: 0.7284, Test Loss: 1.0093\n",
      "Epoch 38/70, Train Loss: 0.6702, Test Loss: 1.0890\n",
      "Epoch 39/70, Train Loss: 0.7131, Test Loss: 0.9808\n",
      "Epoch 40/70, Train Loss: 0.6477, Test Loss: 1.0836\n",
      "Epoch 41/70, Train Loss: 0.6891, Test Loss: 1.0300\n",
      "Epoch 42/70, Train Loss: 0.7037, Test Loss: 1.0351\n",
      "Epoch 43/70, Train Loss: 0.6489, Test Loss: 1.1108\n",
      "Epoch 44/70, Train Loss: 0.6528, Test Loss: 1.0332\n",
      "Epoch 45/70, Train Loss: 0.6825, Test Loss: 1.1567\n",
      "Epoch 46/70, Train Loss: 0.6931, Test Loss: 1.0086\n",
      "Epoch 47/70, Train Loss: 0.6681, Test Loss: 0.9929\n",
      "Epoch 48/70, Train Loss: 0.6606, Test Loss: 1.0453\n",
      "Epoch 49/70, Train Loss: 0.6938, Test Loss: 1.0698\n",
      "Epoch 50/70, Train Loss: 0.6755, Test Loss: 1.1194\n",
      "Epoch 51/70, Train Loss: 0.6753, Test Loss: 1.0368\n",
      "Epoch 52/70, Train Loss: 0.6380, Test Loss: 1.0796\n",
      "Epoch 53/70, Train Loss: 0.6416, Test Loss: 1.0149\n",
      "Epoch 54/70, Train Loss: 0.6522, Test Loss: 1.1730\n",
      "Epoch 55/70, Train Loss: 0.6016, Test Loss: 1.0477\n",
      "Epoch 56/70, Train Loss: 0.6034, Test Loss: 1.0687\n",
      "Epoch 57/70, Train Loss: 0.6518, Test Loss: 1.0250\n",
      "Epoch 58/70, Train Loss: 0.6020, Test Loss: 1.0849\n",
      "Epoch 59/70, Train Loss: 0.6162, Test Loss: 1.0768\n",
      "Epoch 60/70, Train Loss: 0.5744, Test Loss: 1.1676\n",
      "Epoch 61/70, Train Loss: 0.5696, Test Loss: 1.0716\n",
      "Epoch 62/70, Train Loss: 0.6278, Test Loss: 1.0535\n",
      "Epoch 63/70, Train Loss: 0.5600, Test Loss: 1.1701\n",
      "Epoch 64/70, Train Loss: 0.5968, Test Loss: 1.0887\n",
      "Epoch 65/70, Train Loss: 0.5766, Test Loss: 1.1186\n",
      "Epoch 66/70, Train Loss: 0.5863, Test Loss: 1.0776\n",
      "Epoch 67/70, Train Loss: 0.5891, Test Loss: 1.1228\n",
      "Epoch 68/70, Train Loss: 0.5906, Test Loss: 1.1093\n",
      "Epoch 69/70, Train Loss: 0.5827, Test Loss: 1.1207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:20:28,562] Trial 12 finished with value: 0.9959243706294468 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 206, 'layer_1_size': 116, 'layer_2_size': 75, 'layer_3_size': 96, 'layer_4_size': 161, 'layer_5_size': 111, 'layer_6_size': 58, 'dropout_rate': 0.21422750427292242, 'learning_rate': 0.003181361295536872, 'batch_size': 32, 'epochs': 70}. Best is trial 8 with value: 0.8738630967480796.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/70, Train Loss: 0.6061, Test Loss: 0.9959\n",
      "Epoch 1/27, Train Loss: 1.2321, Test Loss: 1.1974\n",
      "Epoch 2/27, Train Loss: 1.1062, Test Loss: 1.2243\n",
      "Epoch 3/27, Train Loss: 1.0368, Test Loss: 1.2526\n",
      "Epoch 4/27, Train Loss: 1.0145, Test Loss: 1.2489\n",
      "Epoch 5/27, Train Loss: 0.9317, Test Loss: 1.2605\n",
      "Epoch 6/27, Train Loss: 0.9125, Test Loss: 1.2920\n",
      "Epoch 7/27, Train Loss: 0.8508, Test Loss: 1.3329\n",
      "Epoch 8/27, Train Loss: 0.7896, Test Loss: 1.3789\n",
      "Epoch 9/27, Train Loss: 0.7711, Test Loss: 1.3935\n",
      "Epoch 10/27, Train Loss: 0.7176, Test Loss: 1.4039\n",
      "Epoch 11/27, Train Loss: 0.6640, Test Loss: 1.3783\n",
      "Epoch 12/27, Train Loss: 0.6600, Test Loss: 1.4199\n",
      "Epoch 13/27, Train Loss: 0.5989, Test Loss: 1.4157\n",
      "Epoch 14/27, Train Loss: 0.5838, Test Loss: 1.5092\n",
      "Epoch 15/27, Train Loss: 0.5627, Test Loss: 1.4844\n",
      "Epoch 16/27, Train Loss: 0.5379, Test Loss: 1.3980\n",
      "Epoch 17/27, Train Loss: 0.5606, Test Loss: 1.3733\n",
      "Epoch 18/27, Train Loss: 0.5322, Test Loss: 1.4918\n",
      "Epoch 19/27, Train Loss: 0.4975, Test Loss: 1.5112\n",
      "Epoch 20/27, Train Loss: 0.4688, Test Loss: 1.4899\n",
      "Epoch 21/27, Train Loss: 0.4548, Test Loss: 1.4980\n",
      "Epoch 22/27, Train Loss: 0.4282, Test Loss: 1.5712\n",
      "Epoch 23/27, Train Loss: 0.4113, Test Loss: 1.5847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:20:29,563] Trial 13 finished with value: 1.5223040580749512 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 142, 'layer_1_size': 186, 'layer_2_size': 145, 'layer_3_size': 158, 'layer_4_size': 95, 'layer_5_size': 116, 'dropout_rate': 0.10834359466617369, 'learning_rate': 0.004154431753719337, 'batch_size': 256, 'epochs': 27}. Best is trial 8 with value: 0.8738630967480796.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/27, Train Loss: 0.4139, Test Loss: 1.4619\n",
      "Epoch 25/27, Train Loss: 0.3989, Test Loss: 1.4723\n",
      "Epoch 26/27, Train Loss: 0.3894, Test Loss: 1.5422\n",
      "Epoch 27/27, Train Loss: 0.3908, Test Loss: 1.5223\n",
      "Epoch 1/81, Train Loss: 1.2228, Test Loss: 0.9424\n",
      "Epoch 2/81, Train Loss: 1.1052, Test Loss: 0.9775\n",
      "Epoch 3/81, Train Loss: 1.0399, Test Loss: 0.9679\n",
      "Epoch 4/81, Train Loss: 1.0508, Test Loss: 0.9599\n",
      "Epoch 5/81, Train Loss: 1.0670, Test Loss: 0.9624\n",
      "Epoch 6/81, Train Loss: 1.0069, Test Loss: 0.9683\n",
      "Epoch 7/81, Train Loss: 1.0008, Test Loss: 0.9684\n",
      "Epoch 8/81, Train Loss: 0.9821, Test Loss: 1.0012\n",
      "Epoch 9/81, Train Loss: 1.0050, Test Loss: 0.9807\n",
      "Epoch 10/81, Train Loss: 1.0192, Test Loss: 0.9918\n",
      "Epoch 11/81, Train Loss: 1.0111, Test Loss: 0.9810\n",
      "Epoch 12/81, Train Loss: 0.9862, Test Loss: 0.9721\n",
      "Epoch 13/81, Train Loss: 0.9898, Test Loss: 0.9631\n",
      "Epoch 14/81, Train Loss: 0.9844, Test Loss: 0.9543\n",
      "Epoch 15/81, Train Loss: 0.9866, Test Loss: 0.9491\n",
      "Epoch 16/81, Train Loss: 0.9720, Test Loss: 0.9541\n",
      "Epoch 17/81, Train Loss: 0.9967, Test Loss: 0.9604\n",
      "Epoch 18/81, Train Loss: 0.9861, Test Loss: 0.9600\n",
      "Epoch 19/81, Train Loss: 0.9603, Test Loss: 0.9617\n",
      "Epoch 20/81, Train Loss: 0.9915, Test Loss: 0.9680\n",
      "Epoch 21/81, Train Loss: 0.9948, Test Loss: 0.9935\n",
      "Epoch 22/81, Train Loss: 0.9841, Test Loss: 0.9517\n",
      "Epoch 23/81, Train Loss: 0.9894, Test Loss: 0.9477\n",
      "Epoch 24/81, Train Loss: 0.9897, Test Loss: 0.9447\n",
      "Epoch 25/81, Train Loss: 0.9941, Test Loss: 0.9471\n",
      "Epoch 26/81, Train Loss: 0.9972, Test Loss: 0.9602\n",
      "Epoch 27/81, Train Loss: 0.9947, Test Loss: 0.9505\n",
      "Epoch 28/81, Train Loss: 1.0105, Test Loss: 0.9508\n",
      "Epoch 29/81, Train Loss: 0.9736, Test Loss: 0.9652\n",
      "Epoch 30/81, Train Loss: 0.9935, Test Loss: 0.9523\n",
      "Epoch 31/81, Train Loss: 0.9961, Test Loss: 0.9574\n",
      "Epoch 32/81, Train Loss: 0.9825, Test Loss: 0.9566\n",
      "Epoch 33/81, Train Loss: 0.9931, Test Loss: 0.9563\n",
      "Epoch 34/81, Train Loss: 0.9883, Test Loss: 0.9508\n",
      "Epoch 35/81, Train Loss: 0.9935, Test Loss: 0.9585\n",
      "Epoch 36/81, Train Loss: 1.0070, Test Loss: 0.9499\n",
      "Epoch 37/81, Train Loss: 0.9805, Test Loss: 0.9679\n",
      "Epoch 38/81, Train Loss: 0.9782, Test Loss: 0.9524\n",
      "Epoch 39/81, Train Loss: 0.9728, Test Loss: 0.9651\n",
      "Epoch 40/81, Train Loss: 0.9526, Test Loss: 0.9492\n",
      "Epoch 41/81, Train Loss: 0.9802, Test Loss: 0.9515\n",
      "Epoch 42/81, Train Loss: 0.9949, Test Loss: 0.9555\n",
      "Epoch 43/81, Train Loss: 0.9630, Test Loss: 0.9505\n",
      "Epoch 44/81, Train Loss: 0.9773, Test Loss: 0.9682\n",
      "Epoch 45/81, Train Loss: 0.9894, Test Loss: 0.9490\n",
      "Epoch 46/81, Train Loss: 0.9728, Test Loss: 0.9444\n",
      "Epoch 47/81, Train Loss: 0.9572, Test Loss: 0.9479\n",
      "Epoch 48/81, Train Loss: 0.9585, Test Loss: 0.9639\n",
      "Epoch 49/81, Train Loss: 0.9618, Test Loss: 0.9482\n",
      "Epoch 50/81, Train Loss: 0.9560, Test Loss: 0.9572\n",
      "Epoch 51/81, Train Loss: 0.9800, Test Loss: 0.9517\n",
      "Epoch 52/81, Train Loss: 0.9438, Test Loss: 0.9584\n",
      "Epoch 53/81, Train Loss: 0.9445, Test Loss: 0.9676\n",
      "Epoch 54/81, Train Loss: 0.9553, Test Loss: 0.9717\n",
      "Epoch 55/81, Train Loss: 0.9489, Test Loss: 0.9756\n",
      "Epoch 56/81, Train Loss: 0.9364, Test Loss: 0.9699\n",
      "Epoch 57/81, Train Loss: 0.9479, Test Loss: 0.9530\n",
      "Epoch 58/81, Train Loss: 0.9312, Test Loss: 0.9830\n",
      "Epoch 59/81, Train Loss: 0.9611, Test Loss: 0.9856\n",
      "Epoch 60/81, Train Loss: 0.9726, Test Loss: 0.9560\n",
      "Epoch 61/81, Train Loss: 0.9454, Test Loss: 0.9954\n",
      "Epoch 62/81, Train Loss: 0.9354, Test Loss: 0.9639\n",
      "Epoch 63/81, Train Loss: 0.9485, Test Loss: 0.9521\n",
      "Epoch 64/81, Train Loss: 0.9380, Test Loss: 0.9627\n",
      "Epoch 65/81, Train Loss: 0.9251, Test Loss: 0.9817\n",
      "Epoch 66/81, Train Loss: 0.9388, Test Loss: 0.9529\n",
      "Epoch 67/81, Train Loss: 0.9392, Test Loss: 0.9759\n",
      "Epoch 68/81, Train Loss: 0.9302, Test Loss: 0.9522\n",
      "Epoch 69/81, Train Loss: 0.9265, Test Loss: 0.9762\n",
      "Epoch 70/81, Train Loss: 0.9122, Test Loss: 0.9938\n",
      "Epoch 71/81, Train Loss: 0.9462, Test Loss: 0.9611\n",
      "Epoch 72/81, Train Loss: 0.9107, Test Loss: 0.9681\n",
      "Epoch 73/81, Train Loss: 0.9022, Test Loss: 0.9894\n",
      "Epoch 74/81, Train Loss: 0.9289, Test Loss: 0.9753\n",
      "Epoch 75/81, Train Loss: 0.9494, Test Loss: 0.9675\n",
      "Epoch 76/81, Train Loss: 0.9272, Test Loss: 0.9651\n",
      "Epoch 77/81, Train Loss: 0.9273, Test Loss: 0.9904\n",
      "Epoch 78/81, Train Loss: 0.8968, Test Loss: 0.9874\n",
      "Epoch 79/81, Train Loss: 0.9105, Test Loss: 1.0231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:20:42,435] Trial 14 finished with value: 1.010168697152819 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 171, 'layer_1_size': 122, 'layer_2_size': 35, 'layer_3_size': 93, 'layer_4_size': 197, 'layer_5_size': 185, 'layer_6_size': 224, 'layer_7_size': 169, 'dropout_rate': 0.40581245769083985, 'learning_rate': 0.0014038090260456294, 'batch_size': 32, 'epochs': 81}. Best is trial 8 with value: 0.8738630967480796.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/81, Train Loss: 0.9246, Test Loss: 0.9916\n",
      "Epoch 81/81, Train Loss: 0.9009, Test Loss: 1.0102\n",
      "Epoch 1/59, Train Loss: 1.2053, Test Loss: 1.0223\n",
      "Epoch 2/59, Train Loss: 1.0595, Test Loss: 1.0895\n",
      "Epoch 3/59, Train Loss: 1.0418, Test Loss: 1.0047\n",
      "Epoch 4/59, Train Loss: 1.0217, Test Loss: 1.0447\n",
      "Epoch 5/59, Train Loss: 0.9950, Test Loss: 1.0272\n",
      "Epoch 6/59, Train Loss: 0.9768, Test Loss: 1.0684\n",
      "Epoch 7/59, Train Loss: 1.0455, Test Loss: 1.0177\n",
      "Epoch 8/59, Train Loss: 0.9542, Test Loss: 1.0541\n",
      "Epoch 9/59, Train Loss: 0.9382, Test Loss: 1.0505\n",
      "Epoch 10/59, Train Loss: 0.9907, Test Loss: 1.0509\n",
      "Epoch 11/59, Train Loss: 0.9443, Test Loss: 1.0592\n",
      "Epoch 12/59, Train Loss: 0.9897, Test Loss: 1.0736\n",
      "Epoch 13/59, Train Loss: 0.9321, Test Loss: 1.0397\n",
      "Epoch 14/59, Train Loss: 0.9269, Test Loss: 1.0146\n",
      "Epoch 15/59, Train Loss: 0.8852, Test Loss: 1.0724\n",
      "Epoch 16/59, Train Loss: 0.8615, Test Loss: 1.0642\n",
      "Epoch 17/59, Train Loss: 0.8785, Test Loss: 1.1482\n",
      "Epoch 18/59, Train Loss: 0.8595, Test Loss: 1.0648\n",
      "Epoch 19/59, Train Loss: 0.9075, Test Loss: 1.1773\n",
      "Epoch 20/59, Train Loss: 0.8545, Test Loss: 1.0693\n",
      "Epoch 21/59, Train Loss: 0.8530, Test Loss: 1.1255\n",
      "Epoch 22/59, Train Loss: 0.8062, Test Loss: 1.1000\n",
      "Epoch 23/59, Train Loss: 0.8158, Test Loss: 1.1273\n",
      "Epoch 24/59, Train Loss: 0.8554, Test Loss: 1.1128\n",
      "Epoch 25/59, Train Loss: 0.7920, Test Loss: 1.1829\n",
      "Epoch 26/59, Train Loss: 0.8235, Test Loss: 1.1655\n",
      "Epoch 27/59, Train Loss: 0.7533, Test Loss: 1.1817\n",
      "Epoch 28/59, Train Loss: 0.8090, Test Loss: 1.1407\n",
      "Epoch 29/59, Train Loss: 0.8102, Test Loss: 1.2915\n",
      "Epoch 30/59, Train Loss: 0.7475, Test Loss: 1.1476\n",
      "Epoch 31/59, Train Loss: 0.7629, Test Loss: 1.1461\n",
      "Epoch 32/59, Train Loss: 0.6986, Test Loss: 1.2296\n",
      "Epoch 33/59, Train Loss: 0.7517, Test Loss: 1.1564\n",
      "Epoch 34/59, Train Loss: 0.7197, Test Loss: 1.1305\n",
      "Epoch 35/59, Train Loss: 0.7254, Test Loss: 1.1632\n",
      "Epoch 36/59, Train Loss: 0.7043, Test Loss: 1.1866\n",
      "Epoch 37/59, Train Loss: 0.7050, Test Loss: 1.2189\n",
      "Epoch 38/59, Train Loss: 0.7030, Test Loss: 1.2062\n",
      "Epoch 39/59, Train Loss: 0.6889, Test Loss: 1.1834\n",
      "Epoch 40/59, Train Loss: 0.6849, Test Loss: 1.1891\n",
      "Epoch 41/59, Train Loss: 0.6756, Test Loss: 1.2346\n",
      "Epoch 42/59, Train Loss: 0.6606, Test Loss: 1.2303\n",
      "Epoch 43/59, Train Loss: 0.6190, Test Loss: 1.1676\n",
      "Epoch 44/59, Train Loss: 0.6394, Test Loss: 1.1451\n",
      "Epoch 45/59, Train Loss: 0.6639, Test Loss: 1.1816\n",
      "Epoch 46/59, Train Loss: 0.6316, Test Loss: 1.1920\n",
      "Epoch 47/59, Train Loss: 0.6780, Test Loss: 1.2414\n",
      "Epoch 48/59, Train Loss: 0.6246, Test Loss: 1.2291\n",
      "Epoch 49/59, Train Loss: 0.6237, Test Loss: 1.1894\n",
      "Epoch 50/59, Train Loss: 0.6413, Test Loss: 1.2307\n",
      "Epoch 51/59, Train Loss: 0.6184, Test Loss: 1.2994\n",
      "Epoch 52/59, Train Loss: 0.6124, Test Loss: 1.2753\n",
      "Epoch 53/59, Train Loss: 0.6164, Test Loss: 1.1894\n",
      "Epoch 54/59, Train Loss: 0.6192, Test Loss: 1.2068\n",
      "Epoch 55/59, Train Loss: 0.5957, Test Loss: 1.1908\n",
      "Epoch 56/59, Train Loss: 0.6263, Test Loss: 1.2317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:20:46,503] Trial 15 finished with value: 1.2990570153508867 and parameters: {'num_hidden_layers': 4, 'layer_0_size': 128, 'layer_1_size': 199, 'layer_2_size': 95, 'layer_3_size': 183, 'dropout_rate': 0.22415991390622592, 'learning_rate': 0.003390128208332348, 'batch_size': 32, 'epochs': 59}. Best is trial 8 with value: 0.8738630967480796.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/59, Train Loss: 0.6086, Test Loss: 1.2878\n",
      "Epoch 58/59, Train Loss: 0.6023, Test Loss: 1.2087\n",
      "Epoch 59/59, Train Loss: 0.6153, Test Loss: 1.2991\n",
      "Epoch 1/38, Train Loss: 1.1716, Test Loss: 0.9596\n",
      "Epoch 2/38, Train Loss: 1.1244, Test Loss: 0.9717\n",
      "Epoch 3/38, Train Loss: 1.1613, Test Loss: 0.9750\n",
      "Epoch 4/38, Train Loss: 1.1282, Test Loss: 0.9737\n",
      "Epoch 5/38, Train Loss: 1.0887, Test Loss: 0.9623\n",
      "Epoch 6/38, Train Loss: 1.0777, Test Loss: 0.9581\n",
      "Epoch 7/38, Train Loss: 1.1056, Test Loss: 0.9597\n",
      "Epoch 8/38, Train Loss: 1.0693, Test Loss: 0.9614\n",
      "Epoch 9/38, Train Loss: 1.0865, Test Loss: 0.9577\n",
      "Epoch 10/38, Train Loss: 1.0976, Test Loss: 0.9582\n",
      "Epoch 11/38, Train Loss: 1.0982, Test Loss: 0.9636\n",
      "Epoch 12/38, Train Loss: 1.0743, Test Loss: 0.9709\n",
      "Epoch 13/38, Train Loss: 1.0971, Test Loss: 0.9706\n",
      "Epoch 14/38, Train Loss: 1.0635, Test Loss: 0.9640\n",
      "Epoch 15/38, Train Loss: 1.0830, Test Loss: 0.9644\n",
      "Epoch 16/38, Train Loss: 1.0732, Test Loss: 0.9681\n",
      "Epoch 17/38, Train Loss: 1.0540, Test Loss: 0.9630\n",
      "Epoch 18/38, Train Loss: 1.0297, Test Loss: 0.9650\n",
      "Epoch 19/38, Train Loss: 1.0842, Test Loss: 0.9635\n",
      "Epoch 20/38, Train Loss: 1.0551, Test Loss: 0.9640\n",
      "Epoch 21/38, Train Loss: 1.0933, Test Loss: 0.9668\n",
      "Epoch 22/38, Train Loss: 1.0471, Test Loss: 0.9640\n",
      "Epoch 23/38, Train Loss: 1.0708, Test Loss: 0.9679\n",
      "Epoch 24/38, Train Loss: 1.0523, Test Loss: 0.9715\n",
      "Epoch 25/38, Train Loss: 1.0696, Test Loss: 0.9694\n",
      "Epoch 26/38, Train Loss: 1.0453, Test Loss: 0.9699\n",
      "Epoch 27/38, Train Loss: 1.0839, Test Loss: 0.9743\n",
      "Epoch 28/38, Train Loss: 1.0390, Test Loss: 0.9710\n",
      "Epoch 29/38, Train Loss: 1.0323, Test Loss: 0.9737\n",
      "Epoch 30/38, Train Loss: 1.0919, Test Loss: 0.9696\n",
      "Epoch 31/38, Train Loss: 1.0479, Test Loss: 0.9682\n",
      "Epoch 32/38, Train Loss: 1.0380, Test Loss: 0.9693\n",
      "Epoch 33/38, Train Loss: 1.0300, Test Loss: 0.9688\n",
      "Epoch 34/38, Train Loss: 1.0676, Test Loss: 0.9687\n",
      "Epoch 35/38, Train Loss: 1.0469, Test Loss: 0.9749\n",
      "Epoch 36/38, Train Loss: 1.0218, Test Loss: 0.9724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:20:49,722] Trial 16 finished with value: 0.9763868078589439 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 200, 'layer_1_size': 142, 'layer_2_size': 183, 'layer_3_size': 118, 'layer_4_size': 40, 'layer_5_size': 244, 'layer_6_size': 97, 'dropout_rate': 0.3487755520542122, 'learning_rate': 7.438173922762102e-05, 'batch_size': 64, 'epochs': 38}. Best is trial 8 with value: 0.8738630967480796.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/38, Train Loss: 1.0189, Test Loss: 0.9662\n",
      "Epoch 38/38, Train Loss: 1.0494, Test Loss: 0.9764\n",
      "Epoch 1/20, Train Loss: 1.1283, Test Loss: 0.9913\n",
      "Epoch 2/20, Train Loss: 1.0788, Test Loss: 0.9848\n",
      "Epoch 3/20, Train Loss: 1.0238, Test Loss: 0.9798\n",
      "Epoch 4/20, Train Loss: 1.0180, Test Loss: 0.9746\n",
      "Epoch 5/20, Train Loss: 1.0229, Test Loss: 0.9760\n",
      "Epoch 6/20, Train Loss: 1.0219, Test Loss: 0.9826\n",
      "Epoch 7/20, Train Loss: 1.0228, Test Loss: 0.9932\n",
      "Epoch 8/20, Train Loss: 1.0007, Test Loss: 0.9982\n",
      "Epoch 9/20, Train Loss: 1.0219, Test Loss: 1.0000\n",
      "Epoch 10/20, Train Loss: 0.9683, Test Loss: 1.0011\n",
      "Epoch 11/20, Train Loss: 0.9603, Test Loss: 1.0014\n",
      "Epoch 12/20, Train Loss: 0.9599, Test Loss: 1.0099\n",
      "Epoch 13/20, Train Loss: 0.9179, Test Loss: 1.0220\n",
      "Epoch 14/20, Train Loss: 0.9345, Test Loss: 1.0372\n",
      "Epoch 15/20, Train Loss: 0.9393, Test Loss: 1.0639\n",
      "Epoch 16/20, Train Loss: 0.9200, Test Loss: 1.0857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:20:50,391] Trial 17 finished with value: 1.1395856142044067 and parameters: {'num_hidden_layers': 5, 'layer_0_size': 256, 'layer_1_size': 81, 'layer_2_size': 134, 'layer_3_size': 70, 'layer_4_size': 111, 'dropout_rate': 0.16617909898775735, 'learning_rate': 0.000832326620982985, 'batch_size': 256, 'epochs': 20}. Best is trial 8 with value: 0.8738630967480796.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Train Loss: 0.9148, Test Loss: 1.1012\n",
      "Epoch 18/20, Train Loss: 0.8991, Test Loss: 1.1186\n",
      "Epoch 19/20, Train Loss: 0.9128, Test Loss: 1.1315\n",
      "Epoch 20/20, Train Loss: 0.8617, Test Loss: 1.1396\n",
      "Epoch 1/59, Train Loss: 1.1895, Test Loss: 0.9876\n",
      "Epoch 2/59, Train Loss: 1.0191, Test Loss: 0.9779\n",
      "Epoch 3/59, Train Loss: 0.9859, Test Loss: 1.0152\n",
      "Epoch 4/59, Train Loss: 0.9270, Test Loss: 1.0213\n",
      "Epoch 5/59, Train Loss: 0.9484, Test Loss: 1.0071\n",
      "Epoch 6/59, Train Loss: 0.9481, Test Loss: 1.0154\n",
      "Epoch 7/59, Train Loss: 0.9195, Test Loss: 1.1173\n",
      "Epoch 8/59, Train Loss: 0.8843, Test Loss: 1.1046\n",
      "Epoch 9/59, Train Loss: 0.8828, Test Loss: 1.0528\n",
      "Epoch 10/59, Train Loss: 0.8722, Test Loss: 1.0444\n",
      "Epoch 11/59, Train Loss: 0.8362, Test Loss: 1.0483\n",
      "Epoch 12/59, Train Loss: 0.8367, Test Loss: 1.0207\n",
      "Epoch 13/59, Train Loss: 0.8000, Test Loss: 1.0704\n",
      "Epoch 14/59, Train Loss: 0.8082, Test Loss: 1.1065\n",
      "Epoch 15/59, Train Loss: 0.8295, Test Loss: 1.1233\n",
      "Epoch 16/59, Train Loss: 0.8289, Test Loss: 1.0508\n",
      "Epoch 17/59, Train Loss: 0.7853, Test Loss: 1.1343\n",
      "Epoch 18/59, Train Loss: 0.7505, Test Loss: 1.1374\n",
      "Epoch 19/59, Train Loss: 0.7281, Test Loss: 1.1428\n",
      "Epoch 20/59, Train Loss: 0.7279, Test Loss: 1.1753\n",
      "Epoch 21/59, Train Loss: 0.7547, Test Loss: 1.1425\n",
      "Epoch 22/59, Train Loss: 0.7313, Test Loss: 1.1431\n",
      "Epoch 23/59, Train Loss: 0.6772, Test Loss: 1.1644\n",
      "Epoch 24/59, Train Loss: 0.7099, Test Loss: 1.1193\n",
      "Epoch 25/59, Train Loss: 0.7040, Test Loss: 1.2245\n",
      "Epoch 26/59, Train Loss: 0.6597, Test Loss: 1.2248\n",
      "Epoch 27/59, Train Loss: 0.7699, Test Loss: 1.2583\n",
      "Epoch 28/59, Train Loss: 0.7080, Test Loss: 1.1393\n",
      "Epoch 29/59, Train Loss: 0.6497, Test Loss: 1.2098\n",
      "Epoch 30/59, Train Loss: 0.6588, Test Loss: 1.1653\n",
      "Epoch 31/59, Train Loss: 0.6732, Test Loss: 1.2373\n",
      "Epoch 32/59, Train Loss: 0.6728, Test Loss: 1.1792\n",
      "Epoch 33/59, Train Loss: 0.6412, Test Loss: 1.1954\n",
      "Epoch 34/59, Train Loss: 0.6687, Test Loss: 1.1838\n",
      "Epoch 35/59, Train Loss: 0.6184, Test Loss: 1.2544\n",
      "Epoch 36/59, Train Loss: 0.6605, Test Loss: 1.1793\n",
      "Epoch 37/59, Train Loss: 0.5883, Test Loss: 1.1926\n",
      "Epoch 38/59, Train Loss: 0.6207, Test Loss: 1.2147\n",
      "Epoch 39/59, Train Loss: 0.6185, Test Loss: 1.2405\n",
      "Epoch 40/59, Train Loss: 0.6043, Test Loss: 1.1786\n",
      "Epoch 41/59, Train Loss: 0.6337, Test Loss: 1.1693\n",
      "Epoch 42/59, Train Loss: 0.5804, Test Loss: 1.2352\n",
      "Epoch 43/59, Train Loss: 0.6045, Test Loss: 1.2403\n",
      "Epoch 44/59, Train Loss: 0.5492, Test Loss: 1.2384\n",
      "Epoch 45/59, Train Loss: 0.5811, Test Loss: 1.1844\n",
      "Epoch 46/59, Train Loss: 0.5923, Test Loss: 1.2348\n",
      "Epoch 47/59, Train Loss: 0.5350, Test Loss: 1.2342\n",
      "Epoch 48/59, Train Loss: 0.5809, Test Loss: 1.2113\n",
      "Epoch 49/59, Train Loss: 0.5668, Test Loss: 1.2458\n",
      "Epoch 50/59, Train Loss: 0.5954, Test Loss: 1.2247\n",
      "Epoch 51/59, Train Loss: 0.5685, Test Loss: 1.1744\n",
      "Epoch 52/59, Train Loss: 0.6041, Test Loss: 1.1665\n",
      "Epoch 53/59, Train Loss: 0.5523, Test Loss: 1.2301\n",
      "Epoch 54/59, Train Loss: 0.5489, Test Loss: 1.2314\n",
      "Epoch 55/59, Train Loss: 0.5090, Test Loss: 1.2449\n",
      "Epoch 56/59, Train Loss: 0.5383, Test Loss: 1.2958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:20:52,878] Trial 18 finished with value: 1.2567092861448015 and parameters: {'num_hidden_layers': 2, 'layer_0_size': 47, 'layer_1_size': 255, 'dropout_rate': 0.10152937419673627, 'learning_rate': 0.0039505385376093095, 'batch_size': 32, 'epochs': 59}. Best is trial 8 with value: 0.8738630967480796.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/59, Train Loss: 0.5452, Test Loss: 1.2292\n",
      "Epoch 58/59, Train Loss: 0.5520, Test Loss: 1.2626\n",
      "Epoch 59/59, Train Loss: 0.5641, Test Loss: 1.2567\n",
      "Epoch 1/17, Train Loss: 1.3929, Test Loss: 0.9539\n",
      "Epoch 2/17, Train Loss: 1.4183, Test Loss: 0.9539\n",
      "Epoch 3/17, Train Loss: 1.4341, Test Loss: 0.9548\n",
      "Epoch 4/17, Train Loss: 1.3103, Test Loss: 0.9566\n",
      "Epoch 5/17, Train Loss: 1.3454, Test Loss: 0.9581\n",
      "Epoch 6/17, Train Loss: 1.2801, Test Loss: 0.9594\n",
      "Epoch 7/17, Train Loss: 1.3036, Test Loss: 0.9611\n",
      "Epoch 8/17, Train Loss: 1.2974, Test Loss: 0.9615\n",
      "Epoch 9/17, Train Loss: 1.2656, Test Loss: 0.9627\n",
      "Epoch 10/17, Train Loss: 1.3304, Test Loss: 0.9637\n",
      "Epoch 11/17, Train Loss: 1.2810, Test Loss: 0.9624\n",
      "Epoch 12/17, Train Loss: 1.2578, Test Loss: 0.9633\n",
      "Epoch 13/17, Train Loss: 1.2153, Test Loss: 0.9626\n",
      "Epoch 14/17, Train Loss: 1.2562, Test Loss: 0.9627\n",
      "Epoch 15/17, Train Loss: 1.2085, Test Loss: 0.9607\n",
      "Epoch 16/17, Train Loss: 1.2439, Test Loss: 0.9636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:20:54,202] Trial 19 finished with value: 0.9630425870418549 and parameters: {'num_hidden_layers': 10, 'layer_0_size': 180, 'layer_1_size': 47, 'layer_2_size': 64, 'layer_3_size': 165, 'layer_4_size': 184, 'layer_5_size': 117, 'layer_6_size': 170, 'layer_7_size': 36, 'layer_8_size': 82, 'layer_9_size': 37, 'dropout_rate': 0.44446849440001124, 'learning_rate': 0.00010737706357374742, 'batch_size': 128, 'epochs': 17}. Best is trial 8 with value: 0.8738630967480796.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/17, Train Loss: 1.2200, Test Loss: 0.9630\n",
      "Epoch 1/87, Train Loss: 1.1627, Test Loss: 1.0990\n",
      "Epoch 2/87, Train Loss: 1.2267, Test Loss: 1.0963\n",
      "Epoch 3/87, Train Loss: 1.1688, Test Loss: 1.0962\n",
      "Epoch 4/87, Train Loss: 1.1641, Test Loss: 1.1044\n",
      "Epoch 5/87, Train Loss: 1.1248, Test Loss: 1.0942\n",
      "Epoch 6/87, Train Loss: 1.1508, Test Loss: 1.1005\n",
      "Epoch 7/87, Train Loss: 1.1381, Test Loss: 1.0908\n",
      "Epoch 8/87, Train Loss: 1.1025, Test Loss: 1.0893\n",
      "Epoch 9/87, Train Loss: 1.1476, Test Loss: 1.0966\n",
      "Epoch 10/87, Train Loss: 1.1161, Test Loss: 1.0956\n",
      "Epoch 11/87, Train Loss: 1.1501, Test Loss: 1.0928\n",
      "Epoch 12/87, Train Loss: 1.1658, Test Loss: 1.0865\n",
      "Epoch 13/87, Train Loss: 1.1169, Test Loss: 1.0844\n",
      "Epoch 14/87, Train Loss: 1.1389, Test Loss: 1.0843\n",
      "Epoch 15/87, Train Loss: 1.1462, Test Loss: 1.0843\n",
      "Epoch 16/87, Train Loss: 1.1964, Test Loss: 1.0919\n",
      "Epoch 17/87, Train Loss: 1.1713, Test Loss: 1.0917\n",
      "Epoch 18/87, Train Loss: 1.1597, Test Loss: 1.0863\n",
      "Epoch 19/87, Train Loss: 1.1459, Test Loss: 1.0904\n",
      "Epoch 20/87, Train Loss: 1.1225, Test Loss: 1.0879\n",
      "Epoch 21/87, Train Loss: 1.1700, Test Loss: 1.0872\n",
      "Epoch 22/87, Train Loss: 1.1149, Test Loss: 1.0900\n",
      "Epoch 23/87, Train Loss: 1.1461, Test Loss: 1.0912\n",
      "Epoch 24/87, Train Loss: 1.1022, Test Loss: 1.0918\n",
      "Epoch 25/87, Train Loss: 1.1313, Test Loss: 1.0844\n",
      "Epoch 26/87, Train Loss: 1.1878, Test Loss: 1.0942\n",
      "Epoch 27/87, Train Loss: 1.1196, Test Loss: 1.0901\n",
      "Epoch 28/87, Train Loss: 1.1036, Test Loss: 1.0892\n",
      "Epoch 29/87, Train Loss: 1.1147, Test Loss: 1.0924\n",
      "Epoch 30/87, Train Loss: 1.1423, Test Loss: 1.0818\n",
      "Epoch 31/87, Train Loss: 1.0671, Test Loss: 1.0941\n",
      "Epoch 32/87, Train Loss: 1.1130, Test Loss: 1.0884\n",
      "Epoch 33/87, Train Loss: 1.1115, Test Loss: 1.0838\n",
      "Epoch 34/87, Train Loss: 1.1010, Test Loss: 1.0930\n",
      "Epoch 35/87, Train Loss: 1.1021, Test Loss: 1.0892\n",
      "Epoch 36/87, Train Loss: 1.0899, Test Loss: 1.0911\n",
      "Epoch 37/87, Train Loss: 1.1274, Test Loss: 1.0859\n",
      "Epoch 38/87, Train Loss: 1.1268, Test Loss: 1.0863\n",
      "Epoch 39/87, Train Loss: 1.0871, Test Loss: 1.0832\n",
      "Epoch 40/87, Train Loss: 1.1138, Test Loss: 1.0835\n",
      "Epoch 41/87, Train Loss: 1.1247, Test Loss: 1.0888\n",
      "Epoch 42/87, Train Loss: 1.0923, Test Loss: 1.0874\n",
      "Epoch 43/87, Train Loss: 1.0867, Test Loss: 1.0881\n",
      "Epoch 44/87, Train Loss: 1.1246, Test Loss: 1.0875\n",
      "Epoch 45/87, Train Loss: 1.1252, Test Loss: 1.0873\n",
      "Epoch 46/87, Train Loss: 1.0965, Test Loss: 1.0963\n",
      "Epoch 47/87, Train Loss: 1.1001, Test Loss: 1.0883\n",
      "Epoch 48/87, Train Loss: 1.0923, Test Loss: 1.0939\n",
      "Epoch 49/87, Train Loss: 1.1128, Test Loss: 1.0883\n",
      "Epoch 50/87, Train Loss: 1.0514, Test Loss: 1.0922\n",
      "Epoch 51/87, Train Loss: 1.0845, Test Loss: 1.0912\n",
      "Epoch 52/87, Train Loss: 1.1105, Test Loss: 1.0846\n",
      "Epoch 53/87, Train Loss: 1.0420, Test Loss: 1.0837\n",
      "Epoch 54/87, Train Loss: 1.0943, Test Loss: 1.0922\n",
      "Epoch 55/87, Train Loss: 1.1281, Test Loss: 1.0914\n",
      "Epoch 56/87, Train Loss: 1.0827, Test Loss: 1.0822\n",
      "Epoch 57/87, Train Loss: 1.0673, Test Loss: 1.0923\n",
      "Epoch 58/87, Train Loss: 1.0744, Test Loss: 1.0840\n",
      "Epoch 59/87, Train Loss: 1.0788, Test Loss: 1.0883\n",
      "Epoch 60/87, Train Loss: 1.0916, Test Loss: 1.0847\n",
      "Epoch 61/87, Train Loss: 1.0727, Test Loss: 1.0940\n",
      "Epoch 62/87, Train Loss: 1.0709, Test Loss: 1.0990\n",
      "Epoch 63/87, Train Loss: 1.1350, Test Loss: 1.0880\n",
      "Epoch 64/87, Train Loss: 1.0959, Test Loss: 1.0920\n",
      "Epoch 65/87, Train Loss: 1.0880, Test Loss: 1.0803\n",
      "Epoch 66/87, Train Loss: 1.0994, Test Loss: 1.0831\n",
      "Epoch 67/87, Train Loss: 1.1008, Test Loss: 1.0909\n",
      "Epoch 68/87, Train Loss: 1.0819, Test Loss: 1.0848\n",
      "Epoch 69/87, Train Loss: 1.0443, Test Loss: 1.0835\n",
      "Epoch 70/87, Train Loss: 1.0845, Test Loss: 1.0862\n",
      "Epoch 71/87, Train Loss: 1.0934, Test Loss: 1.0873\n",
      "Epoch 72/87, Train Loss: 1.0662, Test Loss: 1.0861\n",
      "Epoch 73/87, Train Loss: 1.0929, Test Loss: 1.0955\n",
      "Epoch 74/87, Train Loss: 1.1266, Test Loss: 1.0834\n",
      "Epoch 75/87, Train Loss: 1.0730, Test Loss: 1.0920\n",
      "Epoch 76/87, Train Loss: 1.1262, Test Loss: 1.0951\n",
      "Epoch 77/87, Train Loss: 1.0829, Test Loss: 1.0928\n",
      "Epoch 78/87, Train Loss: 1.1078, Test Loss: 1.0912\n",
      "Epoch 79/87, Train Loss: 1.0411, Test Loss: 1.0877\n",
      "Epoch 80/87, Train Loss: 1.0909, Test Loss: 1.0888\n",
      "Epoch 81/87, Train Loss: 1.1049, Test Loss: 1.0907\n",
      "Epoch 82/87, Train Loss: 1.0554, Test Loss: 1.0860\n",
      "Epoch 83/87, Train Loss: 1.0945, Test Loss: 1.0902\n",
      "Epoch 84/87, Train Loss: 1.0900, Test Loss: 1.0868\n",
      "Epoch 85/87, Train Loss: 1.0951, Test Loss: 1.0909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:21:06,343] Trial 20 finished with value: 1.0895724637167794 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 113, 'layer_1_size': 169, 'layer_2_size': 113, 'layer_3_size': 229, 'layer_4_size': 69, 'layer_5_size': 151, 'layer_6_size': 35, 'layer_7_size': 167, 'dropout_rate': 0.2813304419707377, 'learning_rate': 1.035717728567469e-05, 'batch_size': 32, 'epochs': 87}. Best is trial 8 with value: 0.8738630967480796.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/87, Train Loss: 1.0643, Test Loss: 1.0871\n",
      "Epoch 87/87, Train Loss: 1.0545, Test Loss: 1.0896\n",
      "Epoch 1/11, Train Loss: 1.2349, Test Loss: 1.0197\n",
      "Epoch 2/11, Train Loss: 1.1483, Test Loss: 1.0104\n",
      "Epoch 3/11, Train Loss: 1.1647, Test Loss: 1.0140\n",
      "Epoch 4/11, Train Loss: 1.0850, Test Loss: 1.0109\n",
      "Epoch 5/11, Train Loss: 1.0643, Test Loss: 1.0125\n",
      "Epoch 6/11, Train Loss: 1.0690, Test Loss: 1.0151\n",
      "Epoch 7/11, Train Loss: 1.0561, Test Loss: 1.0129\n",
      "Epoch 8/11, Train Loss: 1.0603, Test Loss: 1.0099\n",
      "Epoch 9/11, Train Loss: 1.0349, Test Loss: 1.0099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:21:06,982] Trial 21 finished with value: 1.0102915167808533 and parameters: {'num_hidden_layers': 9, 'layer_0_size': 42, 'layer_1_size': 142, 'layer_2_size': 205, 'layer_3_size': 34, 'layer_4_size': 77, 'layer_5_size': 145, 'layer_6_size': 195, 'layer_7_size': 256, 'layer_8_size': 41, 'dropout_rate': 0.4462569010640485, 'learning_rate': 0.002042762160112753, 'batch_size': 128, 'epochs': 11}. Best is trial 8 with value: 0.8738630967480796.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/11, Train Loss: 1.0467, Test Loss: 1.0100\n",
      "Epoch 11/11, Train Loss: 1.0281, Test Loss: 1.0103\n",
      "Epoch 1/8, Train Loss: 1.2831, Test Loss: 0.9011\n",
      "Epoch 2/8, Train Loss: 1.1454, Test Loss: 0.9071\n",
      "Epoch 3/8, Train Loss: 1.0813, Test Loss: 0.9000\n",
      "Epoch 4/8, Train Loss: 1.0536, Test Loss: 0.8982\n",
      "Epoch 5/8, Train Loss: 1.0291, Test Loss: 0.8961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:21:07,504] Trial 22 finished with value: 0.8986360132694244 and parameters: {'num_hidden_layers': 9, 'layer_0_size': 64, 'layer_1_size': 170, 'layer_2_size': 208, 'layer_3_size': 46, 'layer_4_size': 68, 'layer_5_size': 199, 'layer_6_size': 205, 'layer_7_size': 209, 'layer_8_size': 117, 'dropout_rate': 0.4500385355889575, 'learning_rate': 0.00615270058260755, 'batch_size': 128, 'epochs': 8}. Best is trial 8 with value: 0.8738630967480796.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/8, Train Loss: 1.0258, Test Loss: 0.9125\n",
      "Epoch 7/8, Train Loss: 1.0157, Test Loss: 0.8972\n",
      "Epoch 8/8, Train Loss: 1.0100, Test Loss: 0.8986\n",
      "Epoch 1/36, Train Loss: 1.5524, Test Loss: 0.8748\n",
      "Epoch 2/36, Train Loss: 1.3215, Test Loss: 0.8872\n",
      "Epoch 3/36, Train Loss: 1.1196, Test Loss: 0.8879\n",
      "Epoch 4/36, Train Loss: 1.0963, Test Loss: 0.8624\n",
      "Epoch 5/36, Train Loss: 1.1187, Test Loss: 0.8938\n",
      "Epoch 6/36, Train Loss: 1.1041, Test Loss: 0.8955\n",
      "Epoch 7/36, Train Loss: 1.0970, Test Loss: 0.8967\n",
      "Epoch 8/36, Train Loss: 1.0885, Test Loss: 0.8866\n",
      "Epoch 9/36, Train Loss: 1.0883, Test Loss: 0.9018\n",
      "Epoch 10/36, Train Loss: 1.0590, Test Loss: 0.9066\n",
      "Epoch 11/36, Train Loss: 1.0703, Test Loss: 0.9189\n",
      "Epoch 12/36, Train Loss: 1.1213, Test Loss: 0.8924\n",
      "Epoch 13/36, Train Loss: 1.0664, Test Loss: 0.8910\n",
      "Epoch 14/36, Train Loss: 1.0636, Test Loss: 0.9073\n",
      "Epoch 15/36, Train Loss: 1.0602, Test Loss: 0.9163\n",
      "Epoch 16/36, Train Loss: 1.0934, Test Loss: 0.9039\n",
      "Epoch 17/36, Train Loss: 1.0799, Test Loss: 0.9144\n",
      "Epoch 18/36, Train Loss: 1.0706, Test Loss: 0.9203\n",
      "Epoch 19/36, Train Loss: 1.0539, Test Loss: 0.9295\n",
      "Epoch 20/36, Train Loss: 1.0623, Test Loss: 0.9503\n",
      "Epoch 21/36, Train Loss: 1.0498, Test Loss: 0.9242\n",
      "Epoch 22/36, Train Loss: 1.0469, Test Loss: 0.9465\n",
      "Epoch 23/36, Train Loss: 1.0611, Test Loss: 0.9052\n",
      "Epoch 24/36, Train Loss: 1.0493, Test Loss: 0.9209\n",
      "Epoch 25/36, Train Loss: 1.0765, Test Loss: 0.9395\n",
      "Epoch 26/36, Train Loss: 1.0510, Test Loss: 0.9381\n",
      "Epoch 27/36, Train Loss: 1.0331, Test Loss: 0.9467\n",
      "Epoch 28/36, Train Loss: 1.0357, Test Loss: 0.9394\n",
      "Epoch 29/36, Train Loss: 1.0420, Test Loss: 0.9549\n",
      "Epoch 30/36, Train Loss: 1.0437, Test Loss: 0.9243\n",
      "Epoch 31/36, Train Loss: 1.0149, Test Loss: 0.9818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:21:09,434] Trial 23 finished with value: 0.9617009460926056 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 71, 'layer_1_size': 226, 'layer_2_size': 152, 'layer_3_size': 68, 'layer_4_size': 56, 'layer_5_size': 209, 'layer_6_size': 254, 'dropout_rate': 0.382530626786132, 'learning_rate': 0.006395210466841712, 'batch_size': 128, 'epochs': 36}. Best is trial 8 with value: 0.8738630967480796.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/36, Train Loss: 1.0168, Test Loss: 0.9372\n",
      "Epoch 33/36, Train Loss: 1.0333, Test Loss: 0.9412\n",
      "Epoch 34/36, Train Loss: 1.0447, Test Loss: 0.9307\n",
      "Epoch 35/36, Train Loss: 1.0156, Test Loss: 0.9587\n",
      "Epoch 36/36, Train Loss: 1.0124, Test Loss: 0.9617\n",
      "Epoch 1/19, Train Loss: 1.4577, Test Loss: 0.8949\n",
      "Epoch 2/19, Train Loss: 1.3487, Test Loss: 0.9001\n",
      "Epoch 3/19, Train Loss: 1.2212, Test Loss: 0.9009\n",
      "Epoch 4/19, Train Loss: 1.2206, Test Loss: 0.9407\n",
      "Epoch 5/19, Train Loss: 1.2160, Test Loss: 0.9163\n",
      "Epoch 6/19, Train Loss: 1.1616, Test Loss: 0.9137\n",
      "Epoch 7/19, Train Loss: 1.1772, Test Loss: 0.9011\n",
      "Epoch 8/19, Train Loss: 1.1909, Test Loss: 0.9069\n",
      "Epoch 9/19, Train Loss: 1.1579, Test Loss: 0.9061\n",
      "Epoch 10/19, Train Loss: 1.1571, Test Loss: 0.9414\n",
      "Epoch 11/19, Train Loss: 1.1819, Test Loss: 0.8986\n",
      "Epoch 12/19, Train Loss: 1.2065, Test Loss: 0.8992\n",
      "Epoch 13/19, Train Loss: 1.1872, Test Loss: 0.8938\n",
      "Epoch 14/19, Train Loss: 1.1688, Test Loss: 0.8970\n",
      "Epoch 15/19, Train Loss: 1.1803, Test Loss: 0.9479\n",
      "Epoch 16/19, Train Loss: 1.1736, Test Loss: 0.9066\n",
      "Epoch 17/19, Train Loss: 1.1777, Test Loss: 0.9076\n",
      "Epoch 18/19, Train Loss: 1.1591, Test Loss: 0.8994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:21:10,764] Trial 24 finished with value: 0.9115764498710632 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 68, 'layer_1_size': 185, 'layer_2_size': 197, 'layer_3_size': 130, 'layer_4_size': 113, 'layer_5_size': 179, 'layer_6_size': 208, 'layer_7_size': 205, 'dropout_rate': 0.46224880985157324, 'learning_rate': 0.005564732149622325, 'batch_size': 128, 'epochs': 19}. Best is trial 8 with value: 0.8738630967480796.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/19, Train Loss: 1.1568, Test Loss: 0.9116\n",
      "Epoch 1/15, Train Loss: 1.2260, Test Loss: 1.0419\n",
      "Epoch 2/15, Train Loss: 1.1795, Test Loss: 1.0433\n",
      "Epoch 3/15, Train Loss: 1.1104, Test Loss: 1.0394\n",
      "Epoch 4/15, Train Loss: 1.1303, Test Loss: 1.0477\n",
      "Epoch 5/15, Train Loss: 1.1027, Test Loss: 1.0406\n",
      "Epoch 6/15, Train Loss: 1.0464, Test Loss: 1.0388\n",
      "Epoch 7/15, Train Loss: 1.0432, Test Loss: 1.0410\n",
      "Epoch 8/15, Train Loss: 1.0391, Test Loss: 1.0422\n",
      "Epoch 9/15, Train Loss: 1.0425, Test Loss: 1.0496\n",
      "Epoch 10/15, Train Loss: 1.0247, Test Loss: 1.0518\n",
      "Epoch 11/15, Train Loss: 1.0067, Test Loss: 1.0454\n",
      "Epoch 12/15, Train Loss: 1.0328, Test Loss: 1.0406\n",
      "Epoch 13/15, Train Loss: 1.0269, Test Loss: 1.0407\n",
      "Epoch 14/15, Train Loss: 1.0443, Test Loss: 1.0417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:21:11,668] Trial 25 finished with value: 1.04107266664505 and parameters: {'num_hidden_layers': 9, 'layer_0_size': 157, 'layer_1_size': 231, 'layer_2_size': 165, 'layer_3_size': 56, 'layer_4_size': 98, 'layer_5_size': 202, 'layer_6_size': 172, 'layer_7_size': 108, 'layer_8_size': 143, 'dropout_rate': 0.4147936556426528, 'learning_rate': 0.0026610079910022455, 'batch_size': 128, 'epochs': 15}. Best is trial 8 with value: 0.8738630967480796.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15, Train Loss: 1.0318, Test Loss: 1.0411\n",
      "Epoch 1/38, Train Loss: 1.1765, Test Loss: 0.8885\n",
      "Epoch 2/38, Train Loss: 1.1188, Test Loss: 0.8886\n",
      "Epoch 3/38, Train Loss: 1.1828, Test Loss: 0.8905\n",
      "Epoch 4/38, Train Loss: 1.1227, Test Loss: 0.8926\n",
      "Epoch 5/38, Train Loss: 1.1259, Test Loss: 0.8958\n",
      "Epoch 6/38, Train Loss: 1.0882, Test Loss: 0.8987\n",
      "Epoch 7/38, Train Loss: 1.1031, Test Loss: 0.8981\n",
      "Epoch 8/38, Train Loss: 1.0838, Test Loss: 0.8975\n",
      "Epoch 9/38, Train Loss: 1.0941, Test Loss: 0.8984\n",
      "Epoch 10/38, Train Loss: 1.0629, Test Loss: 0.8988\n",
      "Epoch 11/38, Train Loss: 1.0963, Test Loss: 0.8973\n",
      "Epoch 12/38, Train Loss: 1.0866, Test Loss: 0.8970\n",
      "Epoch 13/38, Train Loss: 1.0343, Test Loss: 0.8988\n",
      "Epoch 14/38, Train Loss: 1.0310, Test Loss: 0.9012\n",
      "Epoch 15/38, Train Loss: 1.0349, Test Loss: 0.9022\n",
      "Epoch 16/38, Train Loss: 1.0520, Test Loss: 0.9036\n",
      "Epoch 17/38, Train Loss: 1.0533, Test Loss: 0.9032\n",
      "Epoch 18/38, Train Loss: 1.0565, Test Loss: 0.9036\n",
      "Epoch 19/38, Train Loss: 1.0273, Test Loss: 0.9025\n",
      "Epoch 20/38, Train Loss: 1.0620, Test Loss: 0.9003\n",
      "Epoch 21/38, Train Loss: 1.0465, Test Loss: 0.8973\n",
      "Epoch 22/38, Train Loss: 1.0379, Test Loss: 0.8955\n",
      "Epoch 23/38, Train Loss: 1.0327, Test Loss: 0.8955\n",
      "Epoch 24/38, Train Loss: 1.0280, Test Loss: 0.8975\n",
      "Epoch 25/38, Train Loss: 1.0127, Test Loss: 0.8995\n",
      "Epoch 26/38, Train Loss: 1.0147, Test Loss: 0.9017\n",
      "Epoch 27/38, Train Loss: 1.0331, Test Loss: 0.9038\n",
      "Epoch 28/38, Train Loss: 1.0127, Test Loss: 0.9069\n",
      "Epoch 29/38, Train Loss: 1.0177, Test Loss: 0.9097\n",
      "Epoch 30/38, Train Loss: 1.0362, Test Loss: 0.9118\n",
      "Epoch 31/38, Train Loss: 0.9923, Test Loss: 0.9127\n",
      "Epoch 32/38, Train Loss: 0.9948, Test Loss: 0.9112\n",
      "Epoch 33/38, Train Loss: 1.0017, Test Loss: 0.9073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:21:12,866] Trial 26 finished with value: 0.8998379707336426 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 222, 'layer_1_size': 115, 'layer_2_size': 193, 'layer_3_size': 114, 'layer_4_size': 135, 'layer_5_size': 166, 'dropout_rate': 0.3485404414845212, 'learning_rate': 0.0009110589965031471, 'batch_size': 256, 'epochs': 38}. Best is trial 8 with value: 0.8738630967480796.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/38, Train Loss: 1.0056, Test Loss: 0.9026\n",
      "Epoch 35/38, Train Loss: 0.9855, Test Loss: 0.8992\n",
      "Epoch 36/38, Train Loss: 1.0077, Test Loss: 0.8996\n",
      "Epoch 37/38, Train Loss: 0.9928, Test Loss: 0.8989\n",
      "Epoch 38/38, Train Loss: 0.9986, Test Loss: 0.8998\n",
      "Epoch 1/90, Train Loss: 1.2433, Test Loss: 0.9045\n",
      "Epoch 2/90, Train Loss: 1.1125, Test Loss: 0.9095\n",
      "Epoch 3/90, Train Loss: 1.0741, Test Loss: 0.8967\n",
      "Epoch 4/90, Train Loss: 1.0617, Test Loss: 0.8983\n",
      "Epoch 5/90, Train Loss: 1.0728, Test Loss: 0.9029\n",
      "Epoch 6/90, Train Loss: 1.0668, Test Loss: 0.9293\n",
      "Epoch 7/90, Train Loss: 1.0504, Test Loss: 0.9033\n",
      "Epoch 8/90, Train Loss: 1.0522, Test Loss: 0.9173\n",
      "Epoch 9/90, Train Loss: 1.0293, Test Loss: 0.9286\n",
      "Epoch 10/90, Train Loss: 1.0457, Test Loss: 0.9185\n",
      "Epoch 11/90, Train Loss: 1.0442, Test Loss: 0.9147\n",
      "Epoch 12/90, Train Loss: 1.0552, Test Loss: 0.9174\n",
      "Epoch 13/90, Train Loss: 1.0502, Test Loss: 0.9378\n",
      "Epoch 14/90, Train Loss: 1.0313, Test Loss: 0.9064\n",
      "Epoch 15/90, Train Loss: 1.0232, Test Loss: 0.9178\n",
      "Epoch 16/90, Train Loss: 1.0463, Test Loss: 0.8980\n",
      "Epoch 17/90, Train Loss: 1.0398, Test Loss: 0.9182\n",
      "Epoch 18/90, Train Loss: 1.0229, Test Loss: 0.8907\n",
      "Epoch 19/90, Train Loss: 1.0277, Test Loss: 0.9282\n",
      "Epoch 20/90, Train Loss: 1.0363, Test Loss: 0.9006\n",
      "Epoch 21/90, Train Loss: 1.0268, Test Loss: 0.9284\n",
      "Epoch 22/90, Train Loss: 1.0114, Test Loss: 0.9164\n",
      "Epoch 23/90, Train Loss: 1.0215, Test Loss: 0.8817\n",
      "Epoch 24/90, Train Loss: 1.0201, Test Loss: 0.8785\n",
      "Epoch 25/90, Train Loss: 1.0347, Test Loss: 0.9105\n",
      "Epoch 26/90, Train Loss: 1.0219, Test Loss: 0.9104\n",
      "Epoch 27/90, Train Loss: 1.0294, Test Loss: 0.9120\n",
      "Epoch 28/90, Train Loss: 1.0244, Test Loss: 0.9199\n",
      "Epoch 29/90, Train Loss: 0.9950, Test Loss: 0.9137\n",
      "Epoch 30/90, Train Loss: 1.0193, Test Loss: 0.9158\n",
      "Epoch 31/90, Train Loss: 1.0265, Test Loss: 0.8968\n",
      "Epoch 32/90, Train Loss: 1.0051, Test Loss: 0.9051\n",
      "Epoch 33/90, Train Loss: 0.9931, Test Loss: 0.9061\n",
      "Epoch 34/90, Train Loss: 1.0462, Test Loss: 0.8956\n",
      "Epoch 35/90, Train Loss: 1.0008, Test Loss: 0.9188\n",
      "Epoch 36/90, Train Loss: 0.9938, Test Loss: 0.9062\n",
      "Epoch 37/90, Train Loss: 1.0063, Test Loss: 0.9041\n",
      "Epoch 38/90, Train Loss: 0.9959, Test Loss: 0.8910\n",
      "Epoch 39/90, Train Loss: 1.0184, Test Loss: 0.9147\n",
      "Epoch 40/90, Train Loss: 0.9998, Test Loss: 0.9008\n",
      "Epoch 41/90, Train Loss: 0.9942, Test Loss: 0.9018\n",
      "Epoch 42/90, Train Loss: 1.0057, Test Loss: 0.9334\n",
      "Epoch 43/90, Train Loss: 1.0011, Test Loss: 0.9099\n",
      "Epoch 44/90, Train Loss: 1.0059, Test Loss: 0.9112\n",
      "Epoch 45/90, Train Loss: 1.0103, Test Loss: 0.9210\n",
      "Epoch 46/90, Train Loss: 0.9935, Test Loss: 0.9512\n",
      "Epoch 47/90, Train Loss: 0.9787, Test Loss: 0.9042\n",
      "Epoch 48/90, Train Loss: 0.9956, Test Loss: 0.9295\n",
      "Epoch 49/90, Train Loss: 0.9930, Test Loss: 0.9175\n",
      "Epoch 50/90, Train Loss: 0.9886, Test Loss: 0.9207\n",
      "Epoch 51/90, Train Loss: 0.9973, Test Loss: 0.8985\n",
      "Epoch 52/90, Train Loss: 0.9734, Test Loss: 0.9157\n",
      "Epoch 53/90, Train Loss: 0.9953, Test Loss: 0.8794\n",
      "Epoch 54/90, Train Loss: 0.9890, Test Loss: 0.8928\n",
      "Epoch 55/90, Train Loss: 0.9850, Test Loss: 0.9037\n",
      "Epoch 56/90, Train Loss: 0.9964, Test Loss: 0.9121\n",
      "Epoch 57/90, Train Loss: 0.9834, Test Loss: 0.9273\n",
      "Epoch 58/90, Train Loss: 0.9883, Test Loss: 0.8950\n",
      "Epoch 59/90, Train Loss: 0.9668, Test Loss: 0.9429\n",
      "Epoch 60/90, Train Loss: 0.9706, Test Loss: 0.9036\n",
      "Epoch 61/90, Train Loss: 0.9762, Test Loss: 0.9490\n",
      "Epoch 62/90, Train Loss: 0.9692, Test Loss: 0.9211\n",
      "Epoch 63/90, Train Loss: 0.9890, Test Loss: 0.9476\n",
      "Epoch 64/90, Train Loss: 0.9765, Test Loss: 0.9142\n",
      "Epoch 65/90, Train Loss: 0.9789, Test Loss: 0.9044\n",
      "Epoch 66/90, Train Loss: 0.9564, Test Loss: 0.9127\n",
      "Epoch 67/90, Train Loss: 0.9658, Test Loss: 0.9152\n",
      "Epoch 68/90, Train Loss: 0.9555, Test Loss: 0.9302\n",
      "Epoch 69/90, Train Loss: 0.9565, Test Loss: 0.9401\n",
      "Epoch 70/90, Train Loss: 0.9434, Test Loss: 0.9425\n",
      "Epoch 71/90, Train Loss: 0.9740, Test Loss: 0.9284\n",
      "Epoch 72/90, Train Loss: 0.9709, Test Loss: 0.9479\n",
      "Epoch 73/90, Train Loss: 0.9674, Test Loss: 0.9239\n",
      "Epoch 74/90, Train Loss: 0.9388, Test Loss: 0.9632\n",
      "Epoch 75/90, Train Loss: 0.9471, Test Loss: 0.9351\n",
      "Epoch 76/90, Train Loss: 0.9477, Test Loss: 0.9189\n",
      "Epoch 77/90, Train Loss: 0.9416, Test Loss: 0.9130\n",
      "Epoch 78/90, Train Loss: 0.9696, Test Loss: 0.9481\n",
      "Epoch 79/90, Train Loss: 0.9751, Test Loss: 0.9301\n",
      "Epoch 80/90, Train Loss: 0.9466, Test Loss: 1.0000\n",
      "Epoch 81/90, Train Loss: 0.9610, Test Loss: 0.9160\n",
      "Epoch 82/90, Train Loss: 0.9647, Test Loss: 0.9438\n",
      "Epoch 83/90, Train Loss: 0.9522, Test Loss: 0.9069\n",
      "Epoch 84/90, Train Loss: 0.9470, Test Loss: 0.9616\n",
      "Epoch 85/90, Train Loss: 0.9531, Test Loss: 0.9265\n",
      "Epoch 86/90, Train Loss: 0.9646, Test Loss: 0.9549\n",
      "Epoch 87/90, Train Loss: 0.9302, Test Loss: 0.9183\n",
      "Epoch 88/90, Train Loss: 0.9547, Test Loss: 0.9458\n",
      "Epoch 89/90, Train Loss: 0.9320, Test Loss: 0.9088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:21:23,512] Trial 27 finished with value: 0.8701657056808472 and parameters: {'num_hidden_layers': 10, 'layer_0_size': 113, 'layer_1_size': 156, 'layer_2_size': 221, 'layer_3_size': 79, 'layer_4_size': 256, 'layer_5_size': 229, 'layer_6_size': 228, 'layer_7_size': 200, 'layer_8_size': 123, 'layer_9_size': 89, 'dropout_rate': 0.4651722717820232, 'learning_rate': 0.008984965667218929, 'batch_size': 64, 'epochs': 90}. Best is trial 27 with value: 0.8701657056808472.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/90, Train Loss: 0.9405, Test Loss: 0.8702\n",
      "Epoch 1/91, Train Loss: 1.2774, Test Loss: 1.0086\n",
      "Epoch 2/91, Train Loss: 1.0467, Test Loss: 1.0048\n",
      "Epoch 3/91, Train Loss: 1.0271, Test Loss: 1.0104\n",
      "Epoch 4/91, Train Loss: 1.0430, Test Loss: 1.0073\n",
      "Epoch 5/91, Train Loss: 1.0314, Test Loss: 1.0058\n",
      "Epoch 6/91, Train Loss: 1.0326, Test Loss: 1.0110\n",
      "Epoch 7/91, Train Loss: 1.0196, Test Loss: 1.0081\n",
      "Epoch 8/91, Train Loss: 1.0247, Test Loss: 1.0057\n",
      "Epoch 9/91, Train Loss: 1.0361, Test Loss: 1.0134\n",
      "Epoch 10/91, Train Loss: 1.0133, Test Loss: 1.0056\n",
      "Epoch 11/91, Train Loss: 1.0071, Test Loss: 1.0094\n",
      "Epoch 12/91, Train Loss: 1.0276, Test Loss: 1.0065\n",
      "Epoch 13/91, Train Loss: 1.0130, Test Loss: 1.0051\n",
      "Epoch 14/91, Train Loss: 1.0072, Test Loss: 1.0088\n",
      "Epoch 15/91, Train Loss: 1.0096, Test Loss: 1.0095\n",
      "Epoch 16/91, Train Loss: 1.0285, Test Loss: 1.0094\n",
      "Epoch 17/91, Train Loss: 1.0136, Test Loss: 1.0115\n",
      "Epoch 18/91, Train Loss: 1.0054, Test Loss: 1.0078\n",
      "Epoch 19/91, Train Loss: 1.0321, Test Loss: 1.0074\n",
      "Epoch 20/91, Train Loss: 1.0102, Test Loss: 1.0076\n",
      "Epoch 21/91, Train Loss: 1.0044, Test Loss: 1.0064\n",
      "Epoch 22/91, Train Loss: 1.0148, Test Loss: 1.0060\n",
      "Epoch 23/91, Train Loss: 1.0189, Test Loss: 1.0087\n",
      "Epoch 24/91, Train Loss: 1.0222, Test Loss: 1.0078\n",
      "Epoch 25/91, Train Loss: 1.0040, Test Loss: 1.0078\n",
      "Epoch 26/91, Train Loss: 0.9973, Test Loss: 1.0080\n",
      "Epoch 27/91, Train Loss: 1.0077, Test Loss: 1.0077\n",
      "Epoch 28/91, Train Loss: 1.0049, Test Loss: 1.0061\n",
      "Epoch 29/91, Train Loss: 1.0187, Test Loss: 1.0047\n",
      "Epoch 30/91, Train Loss: 1.0076, Test Loss: 1.0063\n",
      "Epoch 31/91, Train Loss: 1.0057, Test Loss: 1.0059\n",
      "Epoch 32/91, Train Loss: 1.0015, Test Loss: 1.0056\n",
      "Epoch 33/91, Train Loss: 1.0068, Test Loss: 1.0047\n",
      "Epoch 34/91, Train Loss: 1.0051, Test Loss: 1.0047\n",
      "Epoch 35/91, Train Loss: 1.0083, Test Loss: 1.0012\n",
      "Epoch 36/91, Train Loss: 1.0021, Test Loss: 1.0080\n",
      "Epoch 37/91, Train Loss: 1.0155, Test Loss: 1.0066\n",
      "Epoch 38/91, Train Loss: 1.0046, Test Loss: 1.0043\n",
      "Epoch 39/91, Train Loss: 1.0004, Test Loss: 1.0041\n",
      "Epoch 40/91, Train Loss: 1.0013, Test Loss: 1.0040\n",
      "Epoch 41/91, Train Loss: 0.9910, Test Loss: 1.0053\n",
      "Epoch 42/91, Train Loss: 0.9955, Test Loss: 1.0051\n",
      "Epoch 43/91, Train Loss: 1.0246, Test Loss: 1.0052\n",
      "Epoch 44/91, Train Loss: 0.9945, Test Loss: 1.0042\n",
      "Epoch 45/91, Train Loss: 0.9956, Test Loss: 1.0057\n",
      "Epoch 46/91, Train Loss: 0.9872, Test Loss: 1.0060\n",
      "Epoch 47/91, Train Loss: 1.0002, Test Loss: 1.0051\n",
      "Epoch 48/91, Train Loss: 1.0062, Test Loss: 1.0026\n",
      "Epoch 49/91, Train Loss: 0.9984, Test Loss: 1.0051\n",
      "Epoch 50/91, Train Loss: 0.9941, Test Loss: 1.0077\n",
      "Epoch 51/91, Train Loss: 0.9976, Test Loss: 1.0057\n",
      "Epoch 52/91, Train Loss: 0.9977, Test Loss: 1.0069\n",
      "Epoch 53/91, Train Loss: 1.0006, Test Loss: 1.0064\n",
      "Epoch 54/91, Train Loss: 0.9995, Test Loss: 1.0058\n",
      "Epoch 55/91, Train Loss: 1.0015, Test Loss: 1.0058\n",
      "Epoch 56/91, Train Loss: 0.9987, Test Loss: 1.0059\n",
      "Epoch 57/91, Train Loss: 0.9953, Test Loss: 1.0064\n",
      "Epoch 58/91, Train Loss: 1.0005, Test Loss: 1.0072\n",
      "Epoch 59/91, Train Loss: 0.9838, Test Loss: 1.0057\n",
      "Epoch 60/91, Train Loss: 0.9863, Test Loss: 1.0070\n",
      "Epoch 61/91, Train Loss: 0.9808, Test Loss: 1.0088\n",
      "Epoch 62/91, Train Loss: 1.0009, Test Loss: 1.0051\n",
      "Epoch 63/91, Train Loss: 0.9907, Test Loss: 1.0068\n",
      "Epoch 64/91, Train Loss: 0.9980, Test Loss: 1.0063\n",
      "Epoch 65/91, Train Loss: 0.9885, Test Loss: 1.0062\n",
      "Epoch 66/91, Train Loss: 0.9864, Test Loss: 1.0105\n",
      "Epoch 67/91, Train Loss: 1.0133, Test Loss: 1.0086\n",
      "Epoch 68/91, Train Loss: 1.0078, Test Loss: 1.0078\n",
      "Epoch 69/91, Train Loss: 0.9912, Test Loss: 1.0079\n",
      "Epoch 70/91, Train Loss: 0.9930, Test Loss: 1.0073\n",
      "Epoch 71/91, Train Loss: 0.9979, Test Loss: 1.0082\n",
      "Epoch 72/91, Train Loss: 0.9952, Test Loss: 1.0073\n",
      "Epoch 73/91, Train Loss: 0.9896, Test Loss: 1.0093\n",
      "Epoch 74/91, Train Loss: 0.9954, Test Loss: 1.0079\n",
      "Epoch 75/91, Train Loss: 0.9872, Test Loss: 1.0102\n",
      "Epoch 76/91, Train Loss: 0.9886, Test Loss: 1.0129\n",
      "Epoch 77/91, Train Loss: 0.9961, Test Loss: 1.0085\n",
      "Epoch 78/91, Train Loss: 0.9970, Test Loss: 1.0107\n",
      "Epoch 79/91, Train Loss: 0.9955, Test Loss: 1.0103\n",
      "Epoch 80/91, Train Loss: 0.9980, Test Loss: 1.0100\n",
      "Epoch 81/91, Train Loss: 1.0038, Test Loss: 1.0088\n",
      "Epoch 82/91, Train Loss: 0.9975, Test Loss: 1.0103\n",
      "Epoch 83/91, Train Loss: 1.0044, Test Loss: 1.0092\n",
      "Epoch 84/91, Train Loss: 1.0026, Test Loss: 1.0102\n",
      "Epoch 85/91, Train Loss: 1.0001, Test Loss: 1.0091\n",
      "Epoch 86/91, Train Loss: 0.9983, Test Loss: 1.0082\n",
      "Epoch 87/91, Train Loss: 0.9936, Test Loss: 1.0086\n",
      "Epoch 88/91, Train Loss: 0.9982, Test Loss: 1.0102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:21:34,121] Trial 28 finished with value: 1.0090029835700989 and parameters: {'num_hidden_layers': 10, 'layer_0_size': 111, 'layer_1_size': 93, 'layer_2_size': 231, 'layer_3_size': 78, 'layer_4_size': 243, 'layer_5_size': 106, 'layer_6_size': 231, 'layer_7_size': 127, 'layer_8_size': 191, 'layer_9_size': 73, 'dropout_rate': 0.4892505770769967, 'learning_rate': 0.009333903639678088, 'batch_size': 64, 'epochs': 91}. Best is trial 27 with value: 0.8701657056808472.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/91, Train Loss: 0.9880, Test Loss: 1.0114\n",
      "Epoch 90/91, Train Loss: 0.9922, Test Loss: 1.0109\n",
      "Epoch 91/91, Train Loss: 0.9907, Test Loss: 1.0090\n",
      "Epoch 1/72, Train Loss: 1.2978, Test Loss: 0.9355\n",
      "Epoch 2/72, Train Loss: 1.2057, Test Loss: 0.9367\n",
      "Epoch 3/72, Train Loss: 1.1821, Test Loss: 0.9307\n",
      "Epoch 4/72, Train Loss: 1.1729, Test Loss: 0.9338\n",
      "Epoch 5/72, Train Loss: 1.1695, Test Loss: 0.9264\n",
      "Epoch 6/72, Train Loss: 1.1986, Test Loss: 0.9270\n",
      "Epoch 7/72, Train Loss: 1.1759, Test Loss: 0.9280\n",
      "Epoch 8/72, Train Loss: 1.1453, Test Loss: 0.9267\n",
      "Epoch 9/72, Train Loss: 1.1136, Test Loss: 0.9299\n",
      "Epoch 10/72, Train Loss: 1.1399, Test Loss: 0.9352\n",
      "Epoch 11/72, Train Loss: 1.1387, Test Loss: 0.9320\n",
      "Epoch 12/72, Train Loss: 1.1356, Test Loss: 0.9360\n",
      "Epoch 13/72, Train Loss: 1.1143, Test Loss: 0.9315\n",
      "Epoch 14/72, Train Loss: 1.1528, Test Loss: 0.9287\n",
      "Epoch 15/72, Train Loss: 1.1253, Test Loss: 0.9313\n",
      "Epoch 16/72, Train Loss: 1.1003, Test Loss: 0.9346\n",
      "Epoch 17/72, Train Loss: 1.1173, Test Loss: 0.9363\n",
      "Epoch 18/72, Train Loss: 1.1182, Test Loss: 0.9457\n",
      "Epoch 19/72, Train Loss: 1.1095, Test Loss: 0.9611\n",
      "Epoch 20/72, Train Loss: 1.1066, Test Loss: 0.9548\n",
      "Epoch 21/72, Train Loss: 1.1059, Test Loss: 0.9536\n",
      "Epoch 22/72, Train Loss: 1.1181, Test Loss: 0.9510\n",
      "Epoch 23/72, Train Loss: 1.1266, Test Loss: 0.9482\n",
      "Epoch 24/72, Train Loss: 1.1056, Test Loss: 0.9441\n",
      "Epoch 25/72, Train Loss: 1.1017, Test Loss: 0.9416\n",
      "Epoch 26/72, Train Loss: 1.0960, Test Loss: 0.9360\n",
      "Epoch 27/72, Train Loss: 1.0814, Test Loss: 0.9367\n",
      "Epoch 28/72, Train Loss: 1.0933, Test Loss: 0.9348\n",
      "Epoch 29/72, Train Loss: 1.0836, Test Loss: 0.9344\n",
      "Epoch 30/72, Train Loss: 1.0847, Test Loss: 0.9360\n",
      "Epoch 31/72, Train Loss: 1.0983, Test Loss: 0.9323\n",
      "Epoch 32/72, Train Loss: 1.0876, Test Loss: 0.9329\n",
      "Epoch 33/72, Train Loss: 1.0885, Test Loss: 0.9359\n",
      "Epoch 34/72, Train Loss: 1.1026, Test Loss: 0.9342\n",
      "Epoch 35/72, Train Loss: 1.0996, Test Loss: 0.9359\n",
      "Epoch 36/72, Train Loss: 1.0863, Test Loss: 0.9359\n",
      "Epoch 37/72, Train Loss: 1.0780, Test Loss: 0.9368\n",
      "Epoch 38/72, Train Loss: 1.0941, Test Loss: 0.9349\n",
      "Epoch 39/72, Train Loss: 1.0795, Test Loss: 0.9343\n",
      "Epoch 40/72, Train Loss: 1.0815, Test Loss: 0.9388\n",
      "Epoch 41/72, Train Loss: 1.0756, Test Loss: 0.9346\n",
      "Epoch 42/72, Train Loss: 1.0776, Test Loss: 0.9313\n",
      "Epoch 43/72, Train Loss: 1.0727, Test Loss: 0.9326\n",
      "Epoch 44/72, Train Loss: 1.0655, Test Loss: 0.9297\n",
      "Epoch 45/72, Train Loss: 1.0706, Test Loss: 0.9296\n",
      "Epoch 46/72, Train Loss: 1.0918, Test Loss: 0.9299\n",
      "Epoch 47/72, Train Loss: 1.0780, Test Loss: 0.9285\n",
      "Epoch 48/72, Train Loss: 1.0698, Test Loss: 0.9306\n",
      "Epoch 49/72, Train Loss: 1.0645, Test Loss: 0.9320\n",
      "Epoch 50/72, Train Loss: 1.0722, Test Loss: 0.9310\n",
      "Epoch 51/72, Train Loss: 1.0791, Test Loss: 0.9318\n",
      "Epoch 52/72, Train Loss: 1.0760, Test Loss: 0.9333\n",
      "Epoch 53/72, Train Loss: 1.0772, Test Loss: 0.9344\n",
      "Epoch 54/72, Train Loss: 1.0678, Test Loss: 0.9328\n",
      "Epoch 55/72, Train Loss: 1.0717, Test Loss: 0.9325\n",
      "Epoch 56/72, Train Loss: 1.0777, Test Loss: 0.9359\n",
      "Epoch 57/72, Train Loss: 1.0708, Test Loss: 0.9322\n",
      "Epoch 58/72, Train Loss: 1.0838, Test Loss: 0.9318\n",
      "Epoch 59/72, Train Loss: 1.0673, Test Loss: 0.9302\n",
      "Epoch 60/72, Train Loss: 1.0765, Test Loss: 0.9311\n",
      "Epoch 61/72, Train Loss: 1.0673, Test Loss: 0.9312\n",
      "Epoch 62/72, Train Loss: 1.0637, Test Loss: 0.9307\n",
      "Epoch 63/72, Train Loss: 1.0670, Test Loss: 0.9313\n",
      "Epoch 64/72, Train Loss: 1.0706, Test Loss: 0.9312\n",
      "Epoch 65/72, Train Loss: 1.0686, Test Loss: 0.9293\n",
      "Epoch 66/72, Train Loss: 1.0704, Test Loss: 0.9300\n",
      "Epoch 67/72, Train Loss: 1.0630, Test Loss: 0.9305\n",
      "Epoch 68/72, Train Loss: 1.0809, Test Loss: 0.9323\n",
      "Epoch 69/72, Train Loss: 1.0712, Test Loss: 0.9288\n",
      "Epoch 70/72, Train Loss: 1.0634, Test Loss: 0.9297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:21:41,383] Trial 29 finished with value: 0.927614763379097 and parameters: {'num_hidden_layers': 10, 'layer_0_size': 92, 'layer_1_size': 130, 'layer_2_size': 56, 'layer_3_size': 107, 'layer_4_size': 225, 'layer_5_size': 244, 'layer_6_size': 120, 'layer_7_size': 206, 'layer_8_size': 106, 'layer_9_size': 139, 'dropout_rate': 0.3171211261386181, 'learning_rate': 0.0004716936533188833, 'batch_size': 64, 'epochs': 72}. Best is trial 27 with value: 0.8701657056808472.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/72, Train Loss: 1.0800, Test Loss: 0.9296\n",
      "Epoch 72/72, Train Loss: 1.0791, Test Loss: 0.9276\n",
      "Epoch 1/92, Train Loss: 1.2382, Test Loss: 0.9600\n",
      "Epoch 2/92, Train Loss: 1.0952, Test Loss: 0.9494\n",
      "Epoch 3/92, Train Loss: 1.0720, Test Loss: 0.9844\n",
      "Epoch 4/92, Train Loss: 1.0336, Test Loss: 0.9550\n",
      "Epoch 5/92, Train Loss: 1.0035, Test Loss: 0.9531\n",
      "Epoch 6/92, Train Loss: 1.0480, Test Loss: 1.0118\n",
      "Epoch 7/92, Train Loss: 0.9752, Test Loss: 0.9779\n",
      "Epoch 8/92, Train Loss: 0.9616, Test Loss: 1.0489\n",
      "Epoch 9/92, Train Loss: 0.9753, Test Loss: 1.0547\n",
      "Epoch 10/92, Train Loss: 0.9616, Test Loss: 1.0512\n",
      "Epoch 11/92, Train Loss: 0.9380, Test Loss: 1.0492\n",
      "Epoch 12/92, Train Loss: 0.9452, Test Loss: 1.1078\n",
      "Epoch 13/92, Train Loss: 0.9528, Test Loss: 1.0524\n",
      "Epoch 14/92, Train Loss: 0.9171, Test Loss: 1.0740\n",
      "Epoch 15/92, Train Loss: 0.9323, Test Loss: 1.0909\n",
      "Epoch 16/92, Train Loss: 0.9255, Test Loss: 1.1649\n",
      "Epoch 17/92, Train Loss: 0.9072, Test Loss: 1.0565\n",
      "Epoch 18/92, Train Loss: 0.9113, Test Loss: 1.0590\n",
      "Epoch 19/92, Train Loss: 0.8896, Test Loss: 1.1092\n",
      "Epoch 20/92, Train Loss: 0.8545, Test Loss: 1.2552\n",
      "Epoch 21/92, Train Loss: 0.8729, Test Loss: 1.2253\n",
      "Epoch 22/92, Train Loss: 0.8734, Test Loss: 1.1778\n",
      "Epoch 23/92, Train Loss: 0.8436, Test Loss: 1.1643\n",
      "Epoch 24/92, Train Loss: 0.8702, Test Loss: 1.1488\n",
      "Epoch 25/92, Train Loss: 0.8129, Test Loss: 1.1858\n",
      "Epoch 26/92, Train Loss: 0.8203, Test Loss: 1.1849\n",
      "Epoch 27/92, Train Loss: 0.8347, Test Loss: 1.0478\n",
      "Epoch 28/92, Train Loss: 0.8269, Test Loss: 1.0846\n",
      "Epoch 29/92, Train Loss: 0.7835, Test Loss: 1.1824\n",
      "Epoch 30/92, Train Loss: 0.7811, Test Loss: 1.1624\n",
      "Epoch 31/92, Train Loss: 0.8018, Test Loss: 1.1756\n",
      "Epoch 32/92, Train Loss: 0.8183, Test Loss: 1.1906\n",
      "Epoch 33/92, Train Loss: 0.8004, Test Loss: 1.2193\n",
      "Epoch 34/92, Train Loss: 0.7465, Test Loss: 1.2796\n",
      "Epoch 35/92, Train Loss: 0.7879, Test Loss: 1.3100\n",
      "Epoch 36/92, Train Loss: 0.7633, Test Loss: 1.2587\n",
      "Epoch 37/92, Train Loss: 0.7521, Test Loss: 1.1470\n",
      "Epoch 38/92, Train Loss: 0.7494, Test Loss: 1.2734\n",
      "Epoch 39/92, Train Loss: 0.7712, Test Loss: 1.2154\n",
      "Epoch 40/92, Train Loss: 0.7911, Test Loss: 1.3250\n",
      "Epoch 41/92, Train Loss: 0.6900, Test Loss: 1.2713\n",
      "Epoch 42/92, Train Loss: 0.7286, Test Loss: 1.2815\n",
      "Epoch 43/92, Train Loss: 0.7470, Test Loss: 1.3649\n",
      "Epoch 44/92, Train Loss: 0.6870, Test Loss: 1.2922\n",
      "Epoch 45/92, Train Loss: 0.7245, Test Loss: 1.3065\n",
      "Epoch 46/92, Train Loss: 0.6900, Test Loss: 1.2931\n",
      "Epoch 47/92, Train Loss: 0.6804, Test Loss: 1.3578\n",
      "Epoch 48/92, Train Loss: 0.6712, Test Loss: 1.4092\n",
      "Epoch 49/92, Train Loss: 0.6957, Test Loss: 1.1894\n",
      "Epoch 50/92, Train Loss: 0.6859, Test Loss: 1.1649\n",
      "Epoch 51/92, Train Loss: 0.6809, Test Loss: 1.2871\n",
      "Epoch 52/92, Train Loss: 0.6560, Test Loss: 1.2689\n",
      "Epoch 53/92, Train Loss: 0.6908, Test Loss: 1.3617\n",
      "Epoch 54/92, Train Loss: 0.6758, Test Loss: 1.3507\n",
      "Epoch 55/92, Train Loss: 0.6741, Test Loss: 1.3760\n",
      "Epoch 56/92, Train Loss: 0.6340, Test Loss: 1.3472\n",
      "Epoch 57/92, Train Loss: 0.6726, Test Loss: 1.4003\n",
      "Epoch 58/92, Train Loss: 0.6011, Test Loss: 1.4442\n",
      "Epoch 59/92, Train Loss: 0.6053, Test Loss: 1.4952\n",
      "Epoch 60/92, Train Loss: 0.6343, Test Loss: 1.4229\n",
      "Epoch 61/92, Train Loss: 0.6283, Test Loss: 1.4145\n",
      "Epoch 62/92, Train Loss: 0.6541, Test Loss: 1.4803\n",
      "Epoch 63/92, Train Loss: 0.6065, Test Loss: 1.4498\n",
      "Epoch 64/92, Train Loss: 0.6427, Test Loss: 1.3726\n",
      "Epoch 65/92, Train Loss: 0.5972, Test Loss: 1.4505\n",
      "Epoch 66/92, Train Loss: 0.6031, Test Loss: 1.4427\n",
      "Epoch 67/92, Train Loss: 0.6629, Test Loss: 1.4373\n",
      "Epoch 68/92, Train Loss: 0.5856, Test Loss: 1.4440\n",
      "Epoch 69/92, Train Loss: 0.6390, Test Loss: 1.3734\n",
      "Epoch 70/92, Train Loss: 0.6225, Test Loss: 1.4079\n",
      "Epoch 71/92, Train Loss: 0.5789, Test Loss: 1.4914\n",
      "Epoch 72/92, Train Loss: 0.6018, Test Loss: 1.4383\n",
      "Epoch 73/92, Train Loss: 0.6012, Test Loss: 1.5467\n",
      "Epoch 74/92, Train Loss: 0.5639, Test Loss: 1.4384\n",
      "Epoch 75/92, Train Loss: 0.5568, Test Loss: 1.4516\n",
      "Epoch 76/92, Train Loss: 0.5816, Test Loss: 1.3859\n",
      "Epoch 77/92, Train Loss: 0.5820, Test Loss: 1.3610\n",
      "Epoch 78/92, Train Loss: 0.5623, Test Loss: 1.4189\n",
      "Epoch 79/92, Train Loss: 0.6037, Test Loss: 1.3532\n",
      "Epoch 80/92, Train Loss: 0.5825, Test Loss: 1.4632\n",
      "Epoch 81/92, Train Loss: 0.5447, Test Loss: 1.4190\n",
      "Epoch 82/92, Train Loss: 0.5516, Test Loss: 1.4137\n",
      "Epoch 83/92, Train Loss: 0.5507, Test Loss: 1.4102\n",
      "Epoch 84/92, Train Loss: 0.5542, Test Loss: 1.4318\n",
      "Epoch 85/92, Train Loss: 0.5680, Test Loss: 1.3315\n",
      "Epoch 86/92, Train Loss: 0.5673, Test Loss: 1.4944\n",
      "Epoch 87/92, Train Loss: 0.5304, Test Loss: 1.4782\n",
      "Epoch 88/92, Train Loss: 0.5275, Test Loss: 1.5183\n",
      "Epoch 89/92, Train Loss: 0.5507, Test Loss: 1.4504\n",
      "Epoch 90/92, Train Loss: 0.5475, Test Loss: 1.4175\n",
      "Epoch 91/92, Train Loss: 0.5474, Test Loss: 1.5162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:21:45,103] Trial 30 finished with value: 1.4203618466854095 and parameters: {'num_hidden_layers': 4, 'layer_0_size': 158, 'layer_1_size': 65, 'layer_2_size': 252, 'layer_3_size': 86, 'dropout_rate': 0.25738696065415845, 'learning_rate': 0.002399039982531997, 'batch_size': 64, 'epochs': 92}. Best is trial 27 with value: 0.8701657056808472.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/92, Train Loss: 0.5472, Test Loss: 1.4204\n",
      "Epoch 1/78, Train Loss: 1.2153, Test Loss: 0.9953\n",
      "Epoch 2/78, Train Loss: 1.1111, Test Loss: 0.9744\n",
      "Epoch 3/78, Train Loss: 1.0267, Test Loss: 0.9880\n",
      "Epoch 4/78, Train Loss: 0.9889, Test Loss: 0.9747\n",
      "Epoch 5/78, Train Loss: 1.0121, Test Loss: 0.9779\n",
      "Epoch 6/78, Train Loss: 1.0013, Test Loss: 0.9701\n",
      "Epoch 7/78, Train Loss: 0.9871, Test Loss: 0.9805\n",
      "Epoch 8/78, Train Loss: 1.0122, Test Loss: 0.9540\n",
      "Epoch 9/78, Train Loss: 1.0046, Test Loss: 0.9931\n",
      "Epoch 10/78, Train Loss: 1.0122, Test Loss: 0.9640\n",
      "Epoch 11/78, Train Loss: 0.9784, Test Loss: 0.9825\n",
      "Epoch 12/78, Train Loss: 0.9891, Test Loss: 0.9656\n",
      "Epoch 13/78, Train Loss: 0.9901, Test Loss: 0.9633\n",
      "Epoch 14/78, Train Loss: 0.9879, Test Loss: 0.9711\n",
      "Epoch 15/78, Train Loss: 0.9994, Test Loss: 0.9705\n",
      "Epoch 16/78, Train Loss: 1.0004, Test Loss: 0.9687\n",
      "Epoch 17/78, Train Loss: 0.9821, Test Loss: 0.9635\n",
      "Epoch 18/78, Train Loss: 0.9789, Test Loss: 0.9639\n",
      "Epoch 19/78, Train Loss: 0.9782, Test Loss: 0.9766\n",
      "Epoch 20/78, Train Loss: 0.9793, Test Loss: 0.9571\n",
      "Epoch 21/78, Train Loss: 0.9876, Test Loss: 0.9686\n",
      "Epoch 22/78, Train Loss: 1.0017, Test Loss: 0.9722\n",
      "Epoch 23/78, Train Loss: 0.9758, Test Loss: 0.9652\n",
      "Epoch 24/78, Train Loss: 0.9736, Test Loss: 0.9905\n",
      "Epoch 25/78, Train Loss: 0.9855, Test Loss: 0.9758\n",
      "Epoch 26/78, Train Loss: 0.9718, Test Loss: 0.9747\n",
      "Epoch 27/78, Train Loss: 0.9777, Test Loss: 0.9668\n",
      "Epoch 28/78, Train Loss: 0.9797, Test Loss: 0.9609\n",
      "Epoch 29/78, Train Loss: 0.9847, Test Loss: 0.9624\n",
      "Epoch 30/78, Train Loss: 0.9856, Test Loss: 0.9744\n",
      "Epoch 31/78, Train Loss: 0.9818, Test Loss: 0.9669\n",
      "Epoch 32/78, Train Loss: 0.9713, Test Loss: 0.9610\n",
      "Epoch 33/78, Train Loss: 0.9878, Test Loss: 0.9630\n",
      "Epoch 34/78, Train Loss: 0.9716, Test Loss: 0.9616\n",
      "Epoch 35/78, Train Loss: 0.9713, Test Loss: 0.9596\n",
      "Epoch 36/78, Train Loss: 0.9886, Test Loss: 0.9592\n",
      "Epoch 37/78, Train Loss: 0.9940, Test Loss: 0.9625\n",
      "Epoch 38/78, Train Loss: 0.9834, Test Loss: 0.9533\n",
      "Epoch 39/78, Train Loss: 0.9717, Test Loss: 0.9624\n",
      "Epoch 40/78, Train Loss: 0.9697, Test Loss: 0.9692\n",
      "Epoch 41/78, Train Loss: 0.9824, Test Loss: 0.9550\n",
      "Epoch 42/78, Train Loss: 0.9606, Test Loss: 0.9524\n",
      "Epoch 43/78, Train Loss: 0.9661, Test Loss: 0.9536\n",
      "Epoch 44/78, Train Loss: 0.9744, Test Loss: 0.9633\n",
      "Epoch 45/78, Train Loss: 0.9835, Test Loss: 0.9456\n",
      "Epoch 46/78, Train Loss: 0.9745, Test Loss: 0.9542\n",
      "Epoch 47/78, Train Loss: 0.9641, Test Loss: 0.9452\n",
      "Epoch 48/78, Train Loss: 0.9686, Test Loss: 0.9600\n",
      "Epoch 49/78, Train Loss: 0.9669, Test Loss: 0.9437\n",
      "Epoch 50/78, Train Loss: 0.9610, Test Loss: 0.9520\n",
      "Epoch 51/78, Train Loss: 0.9778, Test Loss: 0.9518\n",
      "Epoch 52/78, Train Loss: 0.9751, Test Loss: 0.9440\n",
      "Epoch 53/78, Train Loss: 0.9700, Test Loss: 0.9475\n",
      "Epoch 54/78, Train Loss: 0.9675, Test Loss: 0.9243\n",
      "Epoch 55/78, Train Loss: 0.9597, Test Loss: 0.9536\n",
      "Epoch 56/78, Train Loss: 0.9649, Test Loss: 0.9663\n",
      "Epoch 57/78, Train Loss: 0.9510, Test Loss: 0.9478\n",
      "Epoch 58/78, Train Loss: 0.9611, Test Loss: 0.9514\n",
      "Epoch 59/78, Train Loss: 0.9610, Test Loss: 0.9528\n",
      "Epoch 60/78, Train Loss: 0.9621, Test Loss: 0.9572\n",
      "Epoch 61/78, Train Loss: 0.9755, Test Loss: 0.9525\n",
      "Epoch 62/78, Train Loss: 0.9576, Test Loss: 0.9483\n",
      "Epoch 63/78, Train Loss: 0.9674, Test Loss: 0.9491\n",
      "Epoch 64/78, Train Loss: 0.9620, Test Loss: 0.9416\n",
      "Epoch 65/78, Train Loss: 0.9800, Test Loss: 0.9458\n",
      "Epoch 66/78, Train Loss: 0.9601, Test Loss: 0.9493\n",
      "Epoch 67/78, Train Loss: 0.9559, Test Loss: 0.9649\n",
      "Epoch 68/78, Train Loss: 0.9597, Test Loss: 0.9523\n",
      "Epoch 69/78, Train Loss: 0.9460, Test Loss: 0.9405\n",
      "Epoch 70/78, Train Loss: 0.9724, Test Loss: 0.9479\n",
      "Epoch 71/78, Train Loss: 0.9613, Test Loss: 0.9438\n",
      "Epoch 72/78, Train Loss: 0.9516, Test Loss: 0.9528\n",
      "Epoch 73/78, Train Loss: 0.9583, Test Loss: 0.9467\n",
      "Epoch 74/78, Train Loss: 0.9454, Test Loss: 0.9486\n",
      "Epoch 75/78, Train Loss: 0.9574, Test Loss: 0.9590\n",
      "Epoch 76/78, Train Loss: 0.9449, Test Loss: 0.9548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:21:55,133] Trial 31 finished with value: 0.968924880027771 and parameters: {'num_hidden_layers': 10, 'layer_0_size': 60, 'layer_1_size': 162, 'layer_2_size': 211, 'layer_3_size': 50, 'layer_4_size': 253, 'layer_5_size': 222, 'layer_6_size': 235, 'layer_7_size': 207, 'layer_8_size': 127, 'layer_9_size': 122, 'dropout_rate': 0.4594817292564838, 'learning_rate': 0.005236431097937834, 'batch_size': 64, 'epochs': 78}. Best is trial 27 with value: 0.8701657056808472.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/78, Train Loss: 0.9389, Test Loss: 0.9510\n",
      "Epoch 78/78, Train Loss: 0.9512, Test Loss: 0.9689\n",
      "Epoch 1/24, Train Loss: 1.2462, Test Loss: 1.0669\n",
      "Epoch 2/24, Train Loss: 1.1205, Test Loss: 1.0888\n",
      "Epoch 3/24, Train Loss: 1.0724, Test Loss: 1.0883\n",
      "Epoch 4/24, Train Loss: 1.0481, Test Loss: 1.0563\n",
      "Epoch 5/24, Train Loss: 1.0533, Test Loss: 1.0749\n",
      "Epoch 6/24, Train Loss: 1.0694, Test Loss: 1.0649\n",
      "Epoch 7/24, Train Loss: 1.0840, Test Loss: 1.0620\n",
      "Epoch 8/24, Train Loss: 1.0429, Test Loss: 1.0637\n",
      "Epoch 9/24, Train Loss: 1.0595, Test Loss: 1.0636\n",
      "Epoch 10/24, Train Loss: 1.0400, Test Loss: 1.0576\n",
      "Epoch 11/24, Train Loss: 1.0391, Test Loss: 1.0626\n",
      "Epoch 12/24, Train Loss: 1.0290, Test Loss: 1.0816\n",
      "Epoch 13/24, Train Loss: 1.0353, Test Loss: 1.0460\n",
      "Epoch 14/24, Train Loss: 1.0522, Test Loss: 1.0474\n",
      "Epoch 15/24, Train Loss: 1.0547, Test Loss: 1.0636\n",
      "Epoch 16/24, Train Loss: 1.0197, Test Loss: 1.0559\n",
      "Epoch 17/24, Train Loss: 1.0416, Test Loss: 1.0740\n",
      "Epoch 18/24, Train Loss: 1.0361, Test Loss: 1.0562\n",
      "Epoch 19/24, Train Loss: 1.0269, Test Loss: 1.0507\n",
      "Epoch 20/24, Train Loss: 1.0380, Test Loss: 1.0656\n",
      "Epoch 21/24, Train Loss: 1.0325, Test Loss: 1.0565\n",
      "Epoch 22/24, Train Loss: 1.0303, Test Loss: 1.0634\n",
      "Epoch 23/24, Train Loss: 1.0355, Test Loss: 1.0670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:21:57,680] Trial 32 finished with value: 1.063623622059822 and parameters: {'num_hidden_layers': 9, 'layer_0_size': 83, 'layer_1_size': 186, 'layer_2_size': 181, 'layer_3_size': 140, 'layer_4_size': 184, 'layer_5_size': 189, 'layer_6_size': 176, 'layer_7_size': 174, 'layer_8_size': 165, 'dropout_rate': 0.4186115332768205, 'learning_rate': 0.005951065457840974, 'batch_size': 64, 'epochs': 24}. Best is trial 27 with value: 0.8701657056808472.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/24, Train Loss: 1.0375, Test Loss: 1.0636\n",
      "Epoch 1/54, Train Loss: 1.6085, Test Loss: 0.9294\n",
      "Epoch 2/54, Train Loss: 1.2285, Test Loss: 1.0862\n",
      "Epoch 3/54, Train Loss: 1.2397, Test Loss: 0.9426\n",
      "Epoch 4/54, Train Loss: 1.2068, Test Loss: 0.9299\n",
      "Epoch 5/54, Train Loss: 1.1494, Test Loss: 0.9277\n",
      "Epoch 6/54, Train Loss: 1.1832, Test Loss: 0.9682\n",
      "Epoch 7/54, Train Loss: 1.1614, Test Loss: 0.9235\n",
      "Epoch 8/54, Train Loss: 1.1392, Test Loss: 0.9264\n",
      "Epoch 9/54, Train Loss: 1.1335, Test Loss: 0.9303\n",
      "Epoch 10/54, Train Loss: 1.1836, Test Loss: 0.9304\n",
      "Epoch 11/54, Train Loss: 1.1517, Test Loss: 0.9325\n",
      "Epoch 12/54, Train Loss: 1.1388, Test Loss: 0.9289\n",
      "Epoch 13/54, Train Loss: 1.1469, Test Loss: 0.9285\n",
      "Epoch 14/54, Train Loss: 1.1426, Test Loss: 0.9299\n",
      "Epoch 15/54, Train Loss: 1.1717, Test Loss: 0.9392\n",
      "Epoch 16/54, Train Loss: 1.1376, Test Loss: 0.9345\n",
      "Epoch 17/54, Train Loss: 1.1631, Test Loss: 0.9397\n",
      "Epoch 18/54, Train Loss: 1.1553, Test Loss: 0.9321\n",
      "Epoch 19/54, Train Loss: 1.1670, Test Loss: 0.9422\n",
      "Epoch 20/54, Train Loss: 1.1372, Test Loss: 0.9356\n",
      "Epoch 21/54, Train Loss: 1.1399, Test Loss: 0.9391\n",
      "Epoch 22/54, Train Loss: 1.1572, Test Loss: 0.9329\n",
      "Epoch 23/54, Train Loss: 1.1427, Test Loss: 0.9327\n",
      "Epoch 24/54, Train Loss: 1.1260, Test Loss: 0.9438\n",
      "Epoch 25/54, Train Loss: 1.1408, Test Loss: 0.9336\n",
      "Epoch 26/54, Train Loss: 1.1261, Test Loss: 0.9329\n",
      "Epoch 27/54, Train Loss: 1.1362, Test Loss: 0.9362\n",
      "Epoch 28/54, Train Loss: 1.1273, Test Loss: 0.9313\n",
      "Epoch 29/54, Train Loss: 1.1174, Test Loss: 0.9395\n",
      "Epoch 30/54, Train Loss: 1.1053, Test Loss: 0.9368\n",
      "Epoch 31/54, Train Loss: 1.1149, Test Loss: 0.9310\n",
      "Epoch 32/54, Train Loss: 1.1189, Test Loss: 0.9405\n",
      "Epoch 33/54, Train Loss: 1.1057, Test Loss: 0.9471\n",
      "Epoch 34/54, Train Loss: 1.1070, Test Loss: 0.9422\n",
      "Epoch 35/54, Train Loss: 1.1042, Test Loss: 0.9465\n",
      "Epoch 36/54, Train Loss: 1.0924, Test Loss: 0.9484\n",
      "Epoch 37/54, Train Loss: 1.0944, Test Loss: 0.9366\n",
      "Epoch 38/54, Train Loss: 1.1107, Test Loss: 0.9374\n",
      "Epoch 39/54, Train Loss: 1.0959, Test Loss: 0.9621\n",
      "Epoch 40/54, Train Loss: 1.1007, Test Loss: 0.9530\n",
      "Epoch 41/54, Train Loss: 1.1123, Test Loss: 0.9609\n",
      "Epoch 42/54, Train Loss: 1.0870, Test Loss: 0.9525\n",
      "Epoch 43/54, Train Loss: 1.0960, Test Loss: 0.9457\n",
      "Epoch 44/54, Train Loss: 1.0871, Test Loss: 0.9497\n",
      "Epoch 45/54, Train Loss: 1.1125, Test Loss: 0.9467\n",
      "Epoch 46/54, Train Loss: 1.0960, Test Loss: 0.9490\n",
      "Epoch 47/54, Train Loss: 1.0939, Test Loss: 0.9478\n",
      "Epoch 48/54, Train Loss: 1.0932, Test Loss: 0.9870\n",
      "Epoch 49/54, Train Loss: 1.0916, Test Loss: 0.9371\n",
      "Epoch 50/54, Train Loss: 1.0742, Test Loss: 0.9401\n",
      "Epoch 51/54, Train Loss: 1.0789, Test Loss: 0.9402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:22:01,499] Trial 33 finished with value: 0.9401988983154297 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 121, 'layer_1_size': 200, 'layer_2_size': 226, 'layer_3_size': 48, 'layer_4_size': 233, 'layer_5_size': 218, 'layer_6_size': 222, 'layer_7_size': 220, 'dropout_rate': 0.47305045682615676, 'learning_rate': 0.00977541930675325, 'batch_size': 128, 'epochs': 54}. Best is trial 27 with value: 0.8701657056808472.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/54, Train Loss: 1.0679, Test Loss: 0.9406\n",
      "Epoch 53/54, Train Loss: 1.0767, Test Loss: 0.9377\n",
      "Epoch 54/54, Train Loss: 1.0712, Test Loss: 0.9402\n",
      "Epoch 1/63, Train Loss: 1.1300, Test Loss: 0.9423\n",
      "Epoch 2/63, Train Loss: 1.0755, Test Loss: 0.9285\n",
      "Epoch 3/63, Train Loss: 1.0366, Test Loss: 0.9299\n",
      "Epoch 4/63, Train Loss: 0.9930, Test Loss: 0.9216\n",
      "Epoch 5/63, Train Loss: 0.9961, Test Loss: 0.9116\n",
      "Epoch 6/63, Train Loss: 1.0049, Test Loss: 0.9149\n",
      "Epoch 7/63, Train Loss: 1.0056, Test Loss: 0.9190\n",
      "Epoch 8/63, Train Loss: 0.9798, Test Loss: 0.9271\n",
      "Epoch 9/63, Train Loss: 0.9667, Test Loss: 0.9295\n",
      "Epoch 10/63, Train Loss: 1.0118, Test Loss: 0.9241\n",
      "Epoch 11/63, Train Loss: 1.0034, Test Loss: 0.9225\n",
      "Epoch 12/63, Train Loss: 0.9661, Test Loss: 0.9278\n",
      "Epoch 13/63, Train Loss: 0.9575, Test Loss: 0.9291\n",
      "Epoch 14/63, Train Loss: 0.9394, Test Loss: 0.9387\n",
      "Epoch 15/63, Train Loss: 0.9588, Test Loss: 0.9384\n",
      "Epoch 16/63, Train Loss: 0.9579, Test Loss: 0.9409\n",
      "Epoch 17/63, Train Loss: 0.9455, Test Loss: 0.9435\n",
      "Epoch 18/63, Train Loss: 0.9394, Test Loss: 0.9480\n",
      "Epoch 19/63, Train Loss: 0.9486, Test Loss: 0.9453\n",
      "Epoch 20/63, Train Loss: 0.9630, Test Loss: 0.9490\n",
      "Epoch 21/63, Train Loss: 0.9698, Test Loss: 0.9438\n",
      "Epoch 22/63, Train Loss: 0.9158, Test Loss: 0.9450\n",
      "Epoch 23/63, Train Loss: 0.9430, Test Loss: 0.9372\n",
      "Epoch 24/63, Train Loss: 0.9176, Test Loss: 0.9291\n",
      "Epoch 25/63, Train Loss: 0.9635, Test Loss: 0.9421\n",
      "Epoch 26/63, Train Loss: 0.9245, Test Loss: 0.9353\n",
      "Epoch 27/63, Train Loss: 0.9400, Test Loss: 0.9351\n",
      "Epoch 28/63, Train Loss: 0.9121, Test Loss: 0.9421\n",
      "Epoch 29/63, Train Loss: 0.9357, Test Loss: 0.9402\n",
      "Epoch 30/63, Train Loss: 0.9205, Test Loss: 0.9515\n",
      "Epoch 31/63, Train Loss: 0.9312, Test Loss: 0.9625\n",
      "Epoch 32/63, Train Loss: 0.9391, Test Loss: 0.9620\n",
      "Epoch 33/63, Train Loss: 0.9353, Test Loss: 0.9430\n",
      "Epoch 34/63, Train Loss: 0.9192, Test Loss: 0.9505\n",
      "Epoch 35/63, Train Loss: 0.9165, Test Loss: 0.9560\n",
      "Epoch 36/63, Train Loss: 0.8884, Test Loss: 0.9741\n",
      "Epoch 37/63, Train Loss: 0.9094, Test Loss: 0.9651\n",
      "Epoch 38/63, Train Loss: 0.9029, Test Loss: 0.9580\n",
      "Epoch 39/63, Train Loss: 0.9338, Test Loss: 0.9461\n",
      "Epoch 40/63, Train Loss: 0.9232, Test Loss: 0.9621\n",
      "Epoch 41/63, Train Loss: 0.9080, Test Loss: 0.9541\n",
      "Epoch 42/63, Train Loss: 0.9040, Test Loss: 0.9841\n",
      "Epoch 43/63, Train Loss: 0.8931, Test Loss: 0.9515\n",
      "Epoch 44/63, Train Loss: 0.9129, Test Loss: 0.9496\n",
      "Epoch 45/63, Train Loss: 0.8866, Test Loss: 0.9605\n",
      "Epoch 46/63, Train Loss: 0.9059, Test Loss: 0.9704\n",
      "Epoch 47/63, Train Loss: 0.8531, Test Loss: 0.9961\n",
      "Epoch 48/63, Train Loss: 0.8984, Test Loss: 0.9839\n",
      "Epoch 49/63, Train Loss: 0.8960, Test Loss: 0.9833\n",
      "Epoch 50/63, Train Loss: 0.8776, Test Loss: 0.9677\n",
      "Epoch 51/63, Train Loss: 0.8927, Test Loss: 0.9687\n",
      "Epoch 52/63, Train Loss: 0.8856, Test Loss: 0.9567\n",
      "Epoch 53/63, Train Loss: 0.8876, Test Loss: 0.9856\n",
      "Epoch 54/63, Train Loss: 0.8690, Test Loss: 0.9902\n",
      "Epoch 55/63, Train Loss: 0.9016, Test Loss: 0.9684\n",
      "Epoch 56/63, Train Loss: 0.8787, Test Loss: 0.9853\n",
      "Epoch 57/63, Train Loss: 0.8686, Test Loss: 0.9744\n",
      "Epoch 58/63, Train Loss: 0.8694, Test Loss: 0.9884\n",
      "Epoch 59/63, Train Loss: 0.8719, Test Loss: 0.9922\n",
      "Epoch 60/63, Train Loss: 0.8909, Test Loss: 0.9797\n",
      "Epoch 61/63, Train Loss: 0.8880, Test Loss: 0.9751\n",
      "Epoch 62/63, Train Loss: 0.8697, Test Loss: 1.0073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:22:06,593] Trial 34 finished with value: 0.990111095564706 and parameters: {'num_hidden_layers': 5, 'layer_0_size': 140, 'layer_1_size': 158, 'layer_2_size': 157, 'layer_3_size': 66, 'layer_4_size': 60, 'dropout_rate': 0.38628015476565125, 'learning_rate': 0.001175880537819524, 'batch_size': 32, 'epochs': 63}. Best is trial 27 with value: 0.8701657056808472.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/63, Train Loss: 0.8684, Test Loss: 0.9901\n",
      "Epoch 1/49, Train Loss: 1.2179, Test Loss: 1.1409\n",
      "Epoch 2/49, Train Loss: 1.0965, Test Loss: 1.1418\n",
      "Epoch 3/49, Train Loss: 1.0966, Test Loss: 1.1404\n",
      "Epoch 4/49, Train Loss: 1.0474, Test Loss: 1.1608\n",
      "Epoch 5/49, Train Loss: 1.0241, Test Loss: 1.1426\n",
      "Epoch 6/49, Train Loss: 1.0196, Test Loss: 1.1530\n",
      "Epoch 7/49, Train Loss: 1.0146, Test Loss: 1.1546\n",
      "Epoch 8/49, Train Loss: 1.0084, Test Loss: 1.1531\n",
      "Epoch 9/49, Train Loss: 1.0264, Test Loss: 1.1451\n",
      "Epoch 10/49, Train Loss: 1.0145, Test Loss: 1.1540\n",
      "Epoch 11/49, Train Loss: 1.0014, Test Loss: 1.1491\n",
      "Epoch 12/49, Train Loss: 1.0127, Test Loss: 1.1384\n",
      "Epoch 13/49, Train Loss: 1.0041, Test Loss: 1.1386\n",
      "Epoch 14/49, Train Loss: 0.9848, Test Loss: 1.1619\n",
      "Epoch 15/49, Train Loss: 1.0047, Test Loss: 1.1535\n",
      "Epoch 16/49, Train Loss: 1.0005, Test Loss: 1.1419\n",
      "Epoch 17/49, Train Loss: 1.0096, Test Loss: 1.1407\n",
      "Epoch 18/49, Train Loss: 0.9958, Test Loss: 1.1471\n",
      "Epoch 19/49, Train Loss: 0.9983, Test Loss: 1.1605\n",
      "Epoch 20/49, Train Loss: 1.0004, Test Loss: 1.1667\n",
      "Epoch 21/49, Train Loss: 1.0044, Test Loss: 1.1614\n",
      "Epoch 22/49, Train Loss: 0.9935, Test Loss: 1.1667\n",
      "Epoch 23/49, Train Loss: 0.9931, Test Loss: 1.1877\n",
      "Epoch 24/49, Train Loss: 1.0047, Test Loss: 1.1729\n",
      "Epoch 25/49, Train Loss: 0.9998, Test Loss: 1.1704\n",
      "Epoch 26/49, Train Loss: 1.0013, Test Loss: 1.1582\n",
      "Epoch 27/49, Train Loss: 0.9957, Test Loss: 1.1464\n",
      "Epoch 28/49, Train Loss: 0.9932, Test Loss: 1.1528\n",
      "Epoch 29/49, Train Loss: 0.9924, Test Loss: 1.1446\n",
      "Epoch 30/49, Train Loss: 1.0011, Test Loss: 1.1443\n",
      "Epoch 31/49, Train Loss: 0.9916, Test Loss: 1.1526\n",
      "Epoch 32/49, Train Loss: 0.9889, Test Loss: 1.1678\n",
      "Epoch 33/49, Train Loss: 0.9954, Test Loss: 1.1516\n",
      "Epoch 34/49, Train Loss: 0.9970, Test Loss: 1.1646\n",
      "Epoch 35/49, Train Loss: 0.9913, Test Loss: 1.1777\n",
      "Epoch 36/49, Train Loss: 0.9836, Test Loss: 1.1799\n",
      "Epoch 37/49, Train Loss: 0.9992, Test Loss: 1.1755\n",
      "Epoch 38/49, Train Loss: 0.9806, Test Loss: 1.1796\n",
      "Epoch 39/49, Train Loss: 0.9899, Test Loss: 1.2032\n",
      "Epoch 40/49, Train Loss: 0.9818, Test Loss: 1.1973\n",
      "Epoch 41/49, Train Loss: 0.9788, Test Loss: 1.1707\n",
      "Epoch 42/49, Train Loss: 0.9676, Test Loss: 1.1997\n",
      "Epoch 43/49, Train Loss: 0.9737, Test Loss: 1.2800\n",
      "Epoch 44/49, Train Loss: 0.9692, Test Loss: 1.2458\n",
      "Epoch 45/49, Train Loss: 0.9871, Test Loss: 1.2245\n",
      "Epoch 46/49, Train Loss: 0.9693, Test Loss: 1.2180\n",
      "Epoch 47/49, Train Loss: 0.9649, Test Loss: 1.2229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:22:10,028] Trial 35 finished with value: 1.2137283682823181 and parameters: {'num_hidden_layers': 9, 'layer_0_size': 82, 'layer_1_size': 210, 'layer_2_size': 192, 'layer_3_size': 170, 'layer_4_size': 216, 'layer_5_size': 255, 'layer_6_size': 208, 'layer_7_size': 185, 'layer_8_size': 99, 'dropout_rate': 0.42884300383889523, 'learning_rate': 0.004089169846945232, 'batch_size': 128, 'epochs': 49}. Best is trial 27 with value: 0.8701657056808472.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/49, Train Loss: 0.9665, Test Loss: 1.2443\n",
      "Epoch 49/49, Train Loss: 0.9780, Test Loss: 1.2137\n",
      "Epoch 1/6, Train Loss: 1.2733, Test Loss: 0.8100\n",
      "Epoch 2/6, Train Loss: 1.2194, Test Loss: 0.8259\n",
      "Epoch 3/6, Train Loss: 1.2414, Test Loss: 0.8348\n",
      "Epoch 4/6, Train Loss: 1.1429, Test Loss: 0.8193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:22:10,677] Trial 36 finished with value: 0.8336668908596039 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 158, 'layer_1_size': 176, 'layer_2_size': 209, 'layer_3_size': 80, 'layer_4_size': 90, 'layer_5_size': 165, 'layer_6_size': 149, 'dropout_rate': 0.37467753877241033, 'learning_rate': 0.0001981895409529801, 'batch_size': 64, 'epochs': 6}. Best is trial 36 with value: 0.8336668908596039.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/6, Train Loss: 1.2095, Test Loss: 0.8381\n",
      "Epoch 6/6, Train Loss: 1.1332, Test Loss: 0.8337\n",
      "Epoch 1/94, Train Loss: 1.2666, Test Loss: 0.9217\n",
      "Epoch 2/94, Train Loss: 1.2353, Test Loss: 0.9315\n",
      "Epoch 3/94, Train Loss: 1.1789, Test Loss: 0.9357\n",
      "Epoch 4/94, Train Loss: 1.1927, Test Loss: 0.9277\n",
      "Epoch 5/94, Train Loss: 1.1393, Test Loss: 0.9285\n",
      "Epoch 6/94, Train Loss: 1.1907, Test Loss: 0.9332\n",
      "Epoch 7/94, Train Loss: 1.1641, Test Loss: 0.9319\n",
      "Epoch 8/94, Train Loss: 1.1313, Test Loss: 0.9300\n",
      "Epoch 9/94, Train Loss: 1.1487, Test Loss: 0.9281\n",
      "Epoch 10/94, Train Loss: 1.0765, Test Loss: 0.9332\n",
      "Epoch 11/94, Train Loss: 1.1127, Test Loss: 0.9327\n",
      "Epoch 12/94, Train Loss: 1.1420, Test Loss: 0.9342\n",
      "Epoch 13/94, Train Loss: 1.1364, Test Loss: 0.9297\n",
      "Epoch 14/94, Train Loss: 1.0950, Test Loss: 0.9332\n",
      "Epoch 15/94, Train Loss: 1.0742, Test Loss: 0.9306\n",
      "Epoch 16/94, Train Loss: 1.0868, Test Loss: 0.9314\n",
      "Epoch 17/94, Train Loss: 1.0934, Test Loss: 0.9313\n",
      "Epoch 18/94, Train Loss: 1.0991, Test Loss: 0.9319\n",
      "Epoch 19/94, Train Loss: 1.1386, Test Loss: 0.9365\n",
      "Epoch 20/94, Train Loss: 1.1081, Test Loss: 0.9346\n",
      "Epoch 21/94, Train Loss: 1.0898, Test Loss: 0.9313\n",
      "Epoch 22/94, Train Loss: 1.0835, Test Loss: 0.9298\n",
      "Epoch 23/94, Train Loss: 1.1065, Test Loss: 0.9343\n",
      "Epoch 24/94, Train Loss: 1.0511, Test Loss: 0.9359\n",
      "Epoch 25/94, Train Loss: 1.0955, Test Loss: 0.9310\n",
      "Epoch 26/94, Train Loss: 1.1102, Test Loss: 0.9296\n",
      "Epoch 27/94, Train Loss: 1.0912, Test Loss: 0.9266\n",
      "Epoch 28/94, Train Loss: 1.0814, Test Loss: 0.9243\n",
      "Epoch 29/94, Train Loss: 1.0854, Test Loss: 0.9249\n",
      "Epoch 30/94, Train Loss: 1.0999, Test Loss: 0.9258\n",
      "Epoch 31/94, Train Loss: 1.0526, Test Loss: 0.9222\n",
      "Epoch 32/94, Train Loss: 1.1126, Test Loss: 0.9203\n",
      "Epoch 33/94, Train Loss: 1.0936, Test Loss: 0.9217\n",
      "Epoch 34/94, Train Loss: 1.0665, Test Loss: 0.9242\n",
      "Epoch 35/94, Train Loss: 1.0760, Test Loss: 0.9213\n",
      "Epoch 36/94, Train Loss: 1.0721, Test Loss: 0.9233\n",
      "Epoch 37/94, Train Loss: 1.0897, Test Loss: 0.9223\n",
      "Epoch 38/94, Train Loss: 1.0486, Test Loss: 0.9227\n",
      "Epoch 39/94, Train Loss: 1.0873, Test Loss: 0.9209\n",
      "Epoch 40/94, Train Loss: 1.0655, Test Loss: 0.9202\n",
      "Epoch 41/94, Train Loss: 1.0655, Test Loss: 0.9224\n",
      "Epoch 42/94, Train Loss: 1.0645, Test Loss: 0.9181\n",
      "Epoch 43/94, Train Loss: 1.0491, Test Loss: 0.9190\n",
      "Epoch 44/94, Train Loss: 1.0614, Test Loss: 0.9221\n",
      "Epoch 45/94, Train Loss: 1.0742, Test Loss: 0.9196\n",
      "Epoch 46/94, Train Loss: 1.0563, Test Loss: 0.9220\n",
      "Epoch 47/94, Train Loss: 1.0741, Test Loss: 0.9246\n",
      "Epoch 48/94, Train Loss: 1.0664, Test Loss: 0.9259\n",
      "Epoch 49/94, Train Loss: 1.0691, Test Loss: 0.9269\n",
      "Epoch 50/94, Train Loss: 1.0676, Test Loss: 0.9262\n",
      "Epoch 51/94, Train Loss: 1.0513, Test Loss: 0.9281\n",
      "Epoch 52/94, Train Loss: 1.0518, Test Loss: 0.9329\n",
      "Epoch 53/94, Train Loss: 1.0538, Test Loss: 0.9319\n",
      "Epoch 54/94, Train Loss: 1.0473, Test Loss: 0.9306\n",
      "Epoch 55/94, Train Loss: 1.0503, Test Loss: 0.9263\n",
      "Epoch 56/94, Train Loss: 1.0452, Test Loss: 0.9272\n",
      "Epoch 57/94, Train Loss: 1.0478, Test Loss: 0.9230\n",
      "Epoch 58/94, Train Loss: 1.0702, Test Loss: 0.9295\n",
      "Epoch 59/94, Train Loss: 1.0425, Test Loss: 0.9274\n",
      "Epoch 60/94, Train Loss: 1.0625, Test Loss: 0.9280\n",
      "Epoch 61/94, Train Loss: 1.0700, Test Loss: 0.9301\n",
      "Epoch 62/94, Train Loss: 1.0448, Test Loss: 0.9320\n",
      "Epoch 63/94, Train Loss: 1.0258, Test Loss: 0.9317\n",
      "Epoch 64/94, Train Loss: 1.0458, Test Loss: 0.9285\n",
      "Epoch 65/94, Train Loss: 1.0363, Test Loss: 0.9301\n",
      "Epoch 66/94, Train Loss: 1.0261, Test Loss: 0.9331\n",
      "Epoch 67/94, Train Loss: 1.0445, Test Loss: 0.9333\n",
      "Epoch 68/94, Train Loss: 1.0261, Test Loss: 0.9337\n",
      "Epoch 69/94, Train Loss: 1.0388, Test Loss: 0.9319\n",
      "Epoch 70/94, Train Loss: 1.0377, Test Loss: 0.9318\n",
      "Epoch 71/94, Train Loss: 1.0331, Test Loss: 0.9296\n",
      "Epoch 72/94, Train Loss: 1.0359, Test Loss: 0.9307\n",
      "Epoch 73/94, Train Loss: 1.0363, Test Loss: 0.9281\n",
      "Epoch 74/94, Train Loss: 1.0430, Test Loss: 0.9298\n",
      "Epoch 75/94, Train Loss: 1.0207, Test Loss: 0.9292\n",
      "Epoch 76/94, Train Loss: 1.0239, Test Loss: 0.9299\n",
      "Epoch 77/94, Train Loss: 1.0302, Test Loss: 0.9305\n",
      "Epoch 78/94, Train Loss: 1.0286, Test Loss: 0.9307\n",
      "Epoch 79/94, Train Loss: 1.0332, Test Loss: 0.9294\n",
      "Epoch 80/94, Train Loss: 1.0358, Test Loss: 0.9296\n",
      "Epoch 81/94, Train Loss: 1.0221, Test Loss: 0.9324\n",
      "Epoch 82/94, Train Loss: 1.0117, Test Loss: 0.9299\n",
      "Epoch 83/94, Train Loss: 1.0171, Test Loss: 0.9330\n",
      "Epoch 84/94, Train Loss: 1.0348, Test Loss: 0.9290\n",
      "Epoch 85/94, Train Loss: 1.0310, Test Loss: 0.9311\n",
      "Epoch 86/94, Train Loss: 1.0263, Test Loss: 0.9291\n",
      "Epoch 87/94, Train Loss: 1.0136, Test Loss: 0.9301\n",
      "Epoch 88/94, Train Loss: 1.0221, Test Loss: 0.9303\n",
      "Epoch 89/94, Train Loss: 1.0363, Test Loss: 0.9309\n",
      "Epoch 90/94, Train Loss: 1.0391, Test Loss: 0.9333\n",
      "Epoch 91/94, Train Loss: 1.0306, Test Loss: 0.9345\n",
      "Epoch 92/94, Train Loss: 1.0226, Test Loss: 0.9323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:22:18,965] Trial 37 finished with value: 0.9344448894262314 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 156, 'layer_1_size': 240, 'layer_2_size': 234, 'layer_3_size': 144, 'layer_4_size': 118, 'layer_5_size': 128, 'dropout_rate': 0.36633839051728745, 'learning_rate': 0.00016388366224280883, 'batch_size': 64, 'epochs': 94}. Best is trial 36 with value: 0.8336668908596039.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/94, Train Loss: 1.0197, Test Loss: 0.9352\n",
      "Epoch 94/94, Train Loss: 1.0556, Test Loss: 0.9344\n",
      "Epoch 1/5, Train Loss: 1.3213, Test Loss: 0.9473\n",
      "Epoch 2/5, Train Loss: 1.2521, Test Loss: 0.9642\n",
      "Epoch 3/5, Train Loss: 1.2019, Test Loss: 0.9897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:22:19,522] Trial 38 finished with value: 0.9709673821926117 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 188, 'layer_1_size': 214, 'layer_2_size': 173, 'layer_3_size': 103, 'layer_4_size': 146, 'layer_5_size': 165, 'layer_6_size': 89, 'dropout_rate': 0.4754866450164531, 'learning_rate': 3.352580061071557e-05, 'batch_size': 64, 'epochs': 5}. Best is trial 36 with value: 0.8336668908596039.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Train Loss: 1.2445, Test Loss: 0.9662\n",
      "Epoch 5/5, Train Loss: 1.2592, Test Loss: 0.9710\n",
      "Epoch 1/86, Train Loss: 1.1539, Test Loss: 0.9899\n",
      "Epoch 2/86, Train Loss: 1.1154, Test Loss: 1.0061\n",
      "Epoch 3/86, Train Loss: 1.0470, Test Loss: 0.9938\n",
      "Epoch 4/86, Train Loss: 1.0201, Test Loss: 0.9868\n",
      "Epoch 5/86, Train Loss: 1.0587, Test Loss: 0.9747\n",
      "Epoch 6/86, Train Loss: 1.0188, Test Loss: 0.9720\n",
      "Epoch 7/86, Train Loss: 1.0380, Test Loss: 0.9597\n",
      "Epoch 8/86, Train Loss: 0.9914, Test Loss: 0.9562\n",
      "Epoch 9/86, Train Loss: 0.9916, Test Loss: 0.9587\n",
      "Epoch 10/86, Train Loss: 0.9563, Test Loss: 0.9673\n",
      "Epoch 11/86, Train Loss: 1.0021, Test Loss: 0.9619\n",
      "Epoch 12/86, Train Loss: 0.9746, Test Loss: 0.9620\n",
      "Epoch 13/86, Train Loss: 0.9715, Test Loss: 0.9511\n",
      "Epoch 14/86, Train Loss: 0.9622, Test Loss: 0.9432\n",
      "Epoch 15/86, Train Loss: 0.9514, Test Loss: 0.9405\n",
      "Epoch 16/86, Train Loss: 0.9377, Test Loss: 0.9352\n",
      "Epoch 17/86, Train Loss: 0.9471, Test Loss: 0.9384\n",
      "Epoch 18/86, Train Loss: 0.9650, Test Loss: 0.9350\n",
      "Epoch 19/86, Train Loss: 0.9422, Test Loss: 0.9329\n",
      "Epoch 20/86, Train Loss: 0.9702, Test Loss: 0.9398\n",
      "Epoch 21/86, Train Loss: 0.9407, Test Loss: 0.9419\n",
      "Epoch 22/86, Train Loss: 0.9436, Test Loss: 0.9394\n",
      "Epoch 23/86, Train Loss: 0.9476, Test Loss: 0.9412\n",
      "Epoch 24/86, Train Loss: 0.9355, Test Loss: 0.9425\n",
      "Epoch 25/86, Train Loss: 0.9547, Test Loss: 0.9434\n",
      "Epoch 26/86, Train Loss: 0.9235, Test Loss: 0.9516\n",
      "Epoch 27/86, Train Loss: 0.9441, Test Loss: 0.9441\n",
      "Epoch 28/86, Train Loss: 0.9077, Test Loss: 0.9422\n",
      "Epoch 29/86, Train Loss: 0.9398, Test Loss: 0.9396\n",
      "Epoch 30/86, Train Loss: 0.9427, Test Loss: 0.9377\n",
      "Epoch 31/86, Train Loss: 0.9144, Test Loss: 0.9334\n",
      "Epoch 32/86, Train Loss: 0.9228, Test Loss: 0.9324\n",
      "Epoch 33/86, Train Loss: 0.9020, Test Loss: 0.9342\n",
      "Epoch 34/86, Train Loss: 0.9279, Test Loss: 0.9360\n",
      "Epoch 35/86, Train Loss: 0.8973, Test Loss: 0.9338\n",
      "Epoch 36/86, Train Loss: 0.9177, Test Loss: 0.9388\n",
      "Epoch 37/86, Train Loss: 0.8987, Test Loss: 0.9363\n",
      "Epoch 38/86, Train Loss: 0.9033, Test Loss: 0.9433\n",
      "Epoch 39/86, Train Loss: 0.9003, Test Loss: 0.9390\n",
      "Epoch 40/86, Train Loss: 0.9086, Test Loss: 0.9413\n",
      "Epoch 41/86, Train Loss: 0.8979, Test Loss: 0.9338\n",
      "Epoch 42/86, Train Loss: 0.8653, Test Loss: 0.9324\n",
      "Epoch 43/86, Train Loss: 0.8897, Test Loss: 0.9256\n",
      "Epoch 44/86, Train Loss: 0.8782, Test Loss: 0.9318\n",
      "Epoch 45/86, Train Loss: 0.8824, Test Loss: 0.9285\n",
      "Epoch 46/86, Train Loss: 0.9080, Test Loss: 0.9360\n",
      "Epoch 47/86, Train Loss: 0.8613, Test Loss: 0.9427\n",
      "Epoch 48/86, Train Loss: 0.8769, Test Loss: 0.9434\n",
      "Epoch 49/86, Train Loss: 0.8545, Test Loss: 0.9465\n",
      "Epoch 50/86, Train Loss: 0.8505, Test Loss: 0.9395\n",
      "Epoch 51/86, Train Loss: 0.8564, Test Loss: 0.9343\n",
      "Epoch 52/86, Train Loss: 0.9047, Test Loss: 0.9351\n",
      "Epoch 53/86, Train Loss: 0.8640, Test Loss: 0.9271\n",
      "Epoch 54/86, Train Loss: 0.8757, Test Loss: 0.9372\n",
      "Epoch 55/86, Train Loss: 0.8879, Test Loss: 0.9396\n",
      "Epoch 56/86, Train Loss: 0.8430, Test Loss: 0.9535\n",
      "Epoch 57/86, Train Loss: 0.8501, Test Loss: 0.9569\n",
      "Epoch 58/86, Train Loss: 0.8765, Test Loss: 0.9516\n",
      "Epoch 59/86, Train Loss: 0.8474, Test Loss: 0.9510\n",
      "Epoch 60/86, Train Loss: 0.8369, Test Loss: 0.9524\n",
      "Epoch 61/86, Train Loss: 0.8628, Test Loss: 0.9550\n",
      "Epoch 62/86, Train Loss: 0.8314, Test Loss: 0.9533\n",
      "Epoch 63/86, Train Loss: 0.8199, Test Loss: 0.9529\n",
      "Epoch 64/86, Train Loss: 0.8232, Test Loss: 0.9618\n",
      "Epoch 65/86, Train Loss: 0.8252, Test Loss: 0.9634\n",
      "Epoch 66/86, Train Loss: 0.8350, Test Loss: 0.9678\n",
      "Epoch 67/86, Train Loss: 0.8295, Test Loss: 0.9683\n",
      "Epoch 68/86, Train Loss: 0.8404, Test Loss: 0.9661\n",
      "Epoch 69/86, Train Loss: 0.8213, Test Loss: 0.9553\n",
      "Epoch 70/86, Train Loss: 0.8143, Test Loss: 0.9721\n",
      "Epoch 71/86, Train Loss: 0.8265, Test Loss: 0.9690\n",
      "Epoch 72/86, Train Loss: 0.7954, Test Loss: 0.9717\n",
      "Epoch 73/86, Train Loss: 0.8188, Test Loss: 0.9800\n",
      "Epoch 74/86, Train Loss: 0.8124, Test Loss: 0.9765\n",
      "Epoch 75/86, Train Loss: 0.7721, Test Loss: 0.9764\n",
      "Epoch 76/86, Train Loss: 0.8152, Test Loss: 0.9773\n",
      "Epoch 77/86, Train Loss: 0.8193, Test Loss: 0.9822\n",
      "Epoch 78/86, Train Loss: 0.8266, Test Loss: 0.9950\n",
      "Epoch 79/86, Train Loss: 0.8265, Test Loss: 0.9837\n",
      "Epoch 80/86, Train Loss: 0.8050, Test Loss: 0.9792\n",
      "Epoch 81/86, Train Loss: 0.8096, Test Loss: 0.9808\n",
      "Epoch 82/86, Train Loss: 0.8024, Test Loss: 0.9715\n",
      "Epoch 83/86, Train Loss: 0.8341, Test Loss: 0.9765\n",
      "Epoch 84/86, Train Loss: 0.7919, Test Loss: 0.9844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:22:25,252] Trial 39 finished with value: 1.0217626988887787 and parameters: {'num_hidden_layers': 5, 'layer_0_size': 224, 'layer_1_size': 134, 'layer_2_size': 256, 'layer_3_size': 83, 'layer_4_size': 91, 'dropout_rate': 0.3148395949774366, 'learning_rate': 0.0004502228646154463, 'batch_size': 64, 'epochs': 86}. Best is trial 36 with value: 0.8336668908596039.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/86, Train Loss: 0.7970, Test Loss: 1.0004\n",
      "Epoch 86/86, Train Loss: 0.7743, Test Loss: 1.0218\n",
      "Epoch 1/43, Train Loss: 1.2874, Test Loss: 0.9528\n",
      "Epoch 2/43, Train Loss: 1.2492, Test Loss: 0.9390\n",
      "Epoch 3/43, Train Loss: 1.2868, Test Loss: 0.9324\n",
      "Epoch 4/43, Train Loss: 1.2359, Test Loss: 0.9294\n",
      "Epoch 5/43, Train Loss: 1.2842, Test Loss: 0.9301\n",
      "Epoch 6/43, Train Loss: 1.3109, Test Loss: 0.9299\n",
      "Epoch 7/43, Train Loss: 1.2584, Test Loss: 0.9386\n",
      "Epoch 8/43, Train Loss: 1.2136, Test Loss: 0.9389\n",
      "Epoch 9/43, Train Loss: 1.2540, Test Loss: 0.9381\n",
      "Epoch 10/43, Train Loss: 1.2407, Test Loss: 0.9404\n",
      "Epoch 11/43, Train Loss: 1.1955, Test Loss: 0.9429\n",
      "Epoch 12/43, Train Loss: 1.2596, Test Loss: 0.9474\n",
      "Epoch 13/43, Train Loss: 1.1926, Test Loss: 0.9556\n",
      "Epoch 14/43, Train Loss: 1.1788, Test Loss: 0.9610\n",
      "Epoch 15/43, Train Loss: 1.2230, Test Loss: 0.9668\n",
      "Epoch 16/43, Train Loss: 1.1975, Test Loss: 0.9574\n",
      "Epoch 17/43, Train Loss: 1.2036, Test Loss: 0.9610\n",
      "Epoch 18/43, Train Loss: 1.2276, Test Loss: 0.9594\n",
      "Epoch 19/43, Train Loss: 1.1891, Test Loss: 0.9598\n",
      "Epoch 20/43, Train Loss: 1.1805, Test Loss: 0.9560\n",
      "Epoch 21/43, Train Loss: 1.1561, Test Loss: 0.9568\n",
      "Epoch 22/43, Train Loss: 1.1912, Test Loss: 0.9550\n",
      "Epoch 23/43, Train Loss: 1.1734, Test Loss: 0.9502\n",
      "Epoch 24/43, Train Loss: 1.1579, Test Loss: 0.9574\n",
      "Epoch 25/43, Train Loss: 1.1577, Test Loss: 0.9605\n",
      "Epoch 26/43, Train Loss: 1.1797, Test Loss: 0.9571\n",
      "Epoch 27/43, Train Loss: 1.1553, Test Loss: 0.9530\n",
      "Epoch 28/43, Train Loss: 1.1447, Test Loss: 0.9556\n",
      "Epoch 29/43, Train Loss: 1.1510, Test Loss: 0.9564\n",
      "Epoch 30/43, Train Loss: 1.1383, Test Loss: 0.9581\n",
      "Epoch 31/43, Train Loss: 1.1473, Test Loss: 0.9561\n",
      "Epoch 32/43, Train Loss: 1.1217, Test Loss: 0.9548\n",
      "Epoch 33/43, Train Loss: 1.1856, Test Loss: 0.9523\n",
      "Epoch 34/43, Train Loss: 1.1239, Test Loss: 0.9523\n",
      "Epoch 35/43, Train Loss: 1.1606, Test Loss: 0.9533\n",
      "Epoch 36/43, Train Loss: 1.1314, Test Loss: 0.9537\n",
      "Epoch 37/43, Train Loss: 1.0832, Test Loss: 0.9540\n",
      "Epoch 38/43, Train Loss: 1.1264, Test Loss: 0.9530\n",
      "Epoch 39/43, Train Loss: 1.1281, Test Loss: 0.9551\n",
      "Epoch 40/43, Train Loss: 1.0638, Test Loss: 0.9550\n",
      "Epoch 41/43, Train Loss: 1.1237, Test Loss: 0.9558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:22:27,846] Trial 40 finished with value: 0.9572975486516953 and parameters: {'num_hidden_layers': 3, 'layer_0_size': 166, 'layer_1_size': 106, 'layer_2_size': 222, 'dropout_rate': 0.49813493872310244, 'learning_rate': 0.0001959442279847229, 'batch_size': 64, 'epochs': 43}. Best is trial 36 with value: 0.8336668908596039.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/43, Train Loss: 1.1154, Test Loss: 0.9559\n",
      "Epoch 43/43, Train Loss: 1.1323, Test Loss: 0.9573\n",
      "Epoch 1/12, Train Loss: 1.4105, Test Loss: 1.2177\n",
      "Epoch 2/12, Train Loss: 1.4070, Test Loss: 1.2208\n",
      "Epoch 3/12, Train Loss: 1.3066, Test Loss: 1.2171\n",
      "Epoch 4/12, Train Loss: 1.2692, Test Loss: 1.2114\n",
      "Epoch 5/12, Train Loss: 1.2448, Test Loss: 1.2033\n",
      "Epoch 6/12, Train Loss: 1.2537, Test Loss: 1.1994\n",
      "Epoch 7/12, Train Loss: 1.2641, Test Loss: 1.1981\n",
      "Epoch 8/12, Train Loss: 1.2360, Test Loss: 1.1981\n",
      "Epoch 9/12, Train Loss: 1.2714, Test Loss: 1.1977\n",
      "Epoch 10/12, Train Loss: 1.2486, Test Loss: 1.1962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:22:28,842] Trial 41 finished with value: 1.2003867030143738 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 151, 'layer_1_size': 178, 'layer_2_size': 209, 'layer_3_size': 47, 'layer_4_size': 57, 'layer_5_size': 191, 'layer_6_size': 137, 'layer_7_size': 229, 'dropout_rate': 0.4330068899908116, 'learning_rate': 0.00032104132542879984, 'batch_size': 128, 'epochs': 12}. Best is trial 36 with value: 0.8336668908596039.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/12, Train Loss: 1.2581, Test Loss: 1.1981\n",
      "Epoch 12/12, Train Loss: 1.2436, Test Loss: 1.2004\n",
      "Epoch 1/7, Train Loss: 1.4570, Test Loss: 0.8082\n",
      "Epoch 2/7, Train Loss: 1.3562, Test Loss: 0.8081\n",
      "Epoch 3/7, Train Loss: 1.3280, Test Loss: 0.8065\n",
      "Epoch 4/7, Train Loss: 1.2802, Test Loss: 0.8064\n",
      "Epoch 5/7, Train Loss: 1.3138, Test Loss: 0.8055\n",
      "Epoch 6/7, Train Loss: 1.3362, Test Loss: 0.8058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:22:29,244] Trial 42 finished with value: 0.8046557009220123 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 107, 'layer_1_size': 150, 'layer_2_size': 197, 'layer_3_size': 77, 'layer_4_size': 93, 'layer_5_size': 235, 'layer_6_size': 163, 'dropout_rate': 0.39766151395544086, 'learning_rate': 5.981101634422849e-05, 'batch_size': 128, 'epochs': 7}. Best is trial 42 with value: 0.8046557009220123.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/7, Train Loss: 1.2908, Test Loss: 0.8047\n",
      "Epoch 1/13, Train Loss: 1.2152, Test Loss: 0.9380\n",
      "Epoch 2/13, Train Loss: 1.1684, Test Loss: 0.9438\n",
      "Epoch 3/13, Train Loss: 1.1921, Test Loss: 0.9419\n",
      "Epoch 4/13, Train Loss: 1.2232, Test Loss: 0.9401\n",
      "Epoch 5/13, Train Loss: 1.1866, Test Loss: 0.9349\n",
      "Epoch 6/13, Train Loss: 1.1508, Test Loss: 0.9407\n",
      "Epoch 7/13, Train Loss: 1.1468, Test Loss: 0.9407\n",
      "Epoch 8/13, Train Loss: 1.1154, Test Loss: 0.9408\n",
      "Epoch 9/13, Train Loss: 1.0957, Test Loss: 0.9368\n",
      "Epoch 10/13, Train Loss: 1.1559, Test Loss: 0.9348\n",
      "Epoch 11/13, Train Loss: 1.1342, Test Loss: 0.9393\n",
      "Epoch 12/13, Train Loss: 1.1645, Test Loss: 0.9389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:22:31,570] Trial 43 finished with value: 0.9376562578337533 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 102, 'layer_1_size': 152, 'layer_2_size': 240, 'layer_3_size': 76, 'layer_4_size': 86, 'layer_5_size': 233, 'layer_6_size': 167, 'dropout_rate': 0.40014105657280125, 'learning_rate': 4.613784843033725e-05, 'batch_size': 32, 'epochs': 13}. Best is trial 42 with value: 0.8046557009220123.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/13, Train Loss: 1.1087, Test Loss: 0.9377\n",
      "Epoch 1/99, Train Loss: 1.3246, Test Loss: 0.7599\n",
      "Epoch 2/99, Train Loss: 1.2173, Test Loss: 0.7827\n",
      "Epoch 3/99, Train Loss: 1.2618, Test Loss: 0.7759\n",
      "Epoch 4/99, Train Loss: 1.2060, Test Loss: 0.7739\n",
      "Epoch 5/99, Train Loss: 1.2704, Test Loss: 0.7532\n",
      "Epoch 6/99, Train Loss: 1.3035, Test Loss: 0.7557\n",
      "Epoch 7/99, Train Loss: 1.2007, Test Loss: 0.7467\n",
      "Epoch 8/99, Train Loss: 1.2261, Test Loss: 0.7509\n",
      "Epoch 9/99, Train Loss: 1.2603, Test Loss: 0.7435\n",
      "Epoch 10/99, Train Loss: 1.1865, Test Loss: 0.7442\n",
      "Epoch 11/99, Train Loss: 1.1907, Test Loss: 0.7453\n",
      "Epoch 12/99, Train Loss: 1.2131, Test Loss: 0.7371\n",
      "Epoch 13/99, Train Loss: 1.2058, Test Loss: 0.7370\n",
      "Epoch 14/99, Train Loss: 1.1502, Test Loss: 0.7269\n",
      "Epoch 15/99, Train Loss: 1.2612, Test Loss: 0.7296\n",
      "Epoch 16/99, Train Loss: 1.1906, Test Loss: 0.7297\n",
      "Epoch 17/99, Train Loss: 1.1475, Test Loss: 0.7316\n",
      "Epoch 18/99, Train Loss: 1.1820, Test Loss: 0.7257\n",
      "Epoch 19/99, Train Loss: 1.1773, Test Loss: 0.7252\n",
      "Epoch 20/99, Train Loss: 1.2055, Test Loss: 0.7212\n",
      "Epoch 21/99, Train Loss: 1.1755, Test Loss: 0.7253\n",
      "Epoch 22/99, Train Loss: 1.1611, Test Loss: 0.7218\n",
      "Epoch 23/99, Train Loss: 1.1519, Test Loss: 0.7229\n",
      "Epoch 24/99, Train Loss: 1.1950, Test Loss: 0.7198\n",
      "Epoch 25/99, Train Loss: 1.1675, Test Loss: 0.7257\n",
      "Epoch 26/99, Train Loss: 1.1379, Test Loss: 0.7259\n",
      "Epoch 27/99, Train Loss: 1.1674, Test Loss: 0.7206\n",
      "Epoch 28/99, Train Loss: 1.1638, Test Loss: 0.7225\n",
      "Epoch 29/99, Train Loss: 1.1692, Test Loss: 0.7266\n",
      "Epoch 30/99, Train Loss: 1.1955, Test Loss: 0.7270\n",
      "Epoch 31/99, Train Loss: 1.1341, Test Loss: 0.7238\n",
      "Epoch 32/99, Train Loss: 1.2083, Test Loss: 0.7230\n",
      "Epoch 33/99, Train Loss: 1.1910, Test Loss: 0.7233\n",
      "Epoch 34/99, Train Loss: 1.1543, Test Loss: 0.7193\n",
      "Epoch 35/99, Train Loss: 1.1525, Test Loss: 0.7204\n",
      "Epoch 36/99, Train Loss: 1.1482, Test Loss: 0.7205\n",
      "Epoch 37/99, Train Loss: 1.1122, Test Loss: 0.7200\n",
      "Epoch 38/99, Train Loss: 1.1480, Test Loss: 0.7193\n",
      "Epoch 39/99, Train Loss: 1.1743, Test Loss: 0.7224\n",
      "Epoch 40/99, Train Loss: 1.1450, Test Loss: 0.7254\n",
      "Epoch 41/99, Train Loss: 1.1348, Test Loss: 0.7165\n",
      "Epoch 42/99, Train Loss: 1.1404, Test Loss: 0.7169\n",
      "Epoch 43/99, Train Loss: 1.1793, Test Loss: 0.7196\n",
      "Epoch 44/99, Train Loss: 1.1461, Test Loss: 0.7187\n",
      "Epoch 45/99, Train Loss: 1.0998, Test Loss: 0.7182\n",
      "Epoch 46/99, Train Loss: 1.1380, Test Loss: 0.7182\n",
      "Epoch 47/99, Train Loss: 1.1405, Test Loss: 0.7227\n",
      "Epoch 48/99, Train Loss: 1.1020, Test Loss: 0.7212\n",
      "Epoch 49/99, Train Loss: 1.1241, Test Loss: 0.7255\n",
      "Epoch 50/99, Train Loss: 1.1565, Test Loss: 0.7208\n",
      "Epoch 51/99, Train Loss: 1.1301, Test Loss: 0.7214\n",
      "Epoch 52/99, Train Loss: 1.1772, Test Loss: 0.7220\n",
      "Epoch 53/99, Train Loss: 1.0993, Test Loss: 0.7185\n",
      "Epoch 54/99, Train Loss: 1.1166, Test Loss: 0.7210\n",
      "Epoch 55/99, Train Loss: 1.1294, Test Loss: 0.7225\n",
      "Epoch 56/99, Train Loss: 1.1289, Test Loss: 0.7227\n",
      "Epoch 57/99, Train Loss: 1.0988, Test Loss: 0.7189\n",
      "Epoch 58/99, Train Loss: 1.1782, Test Loss: 0.7193\n",
      "Epoch 59/99, Train Loss: 1.1198, Test Loss: 0.7145\n",
      "Epoch 60/99, Train Loss: 1.1120, Test Loss: 0.7241\n",
      "Epoch 61/99, Train Loss: 1.1432, Test Loss: 0.7187\n",
      "Epoch 62/99, Train Loss: 1.0987, Test Loss: 0.7174\n",
      "Epoch 63/99, Train Loss: 1.1297, Test Loss: 0.7134\n",
      "Epoch 64/99, Train Loss: 1.1104, Test Loss: 0.7139\n",
      "Epoch 65/99, Train Loss: 1.1055, Test Loss: 0.7166\n",
      "Epoch 66/99, Train Loss: 1.0902, Test Loss: 0.7183\n",
      "Epoch 67/99, Train Loss: 1.1196, Test Loss: 0.7165\n",
      "Epoch 68/99, Train Loss: 1.1393, Test Loss: 0.7188\n",
      "Epoch 69/99, Train Loss: 1.1304, Test Loss: 0.7205\n",
      "Epoch 70/99, Train Loss: 1.1354, Test Loss: 0.7176\n",
      "Epoch 71/99, Train Loss: 1.1274, Test Loss: 0.7211\n",
      "Epoch 72/99, Train Loss: 1.1357, Test Loss: 0.7149\n",
      "Epoch 73/99, Train Loss: 1.1369, Test Loss: 0.7190\n",
      "Epoch 74/99, Train Loss: 1.1673, Test Loss: 0.7205\n",
      "Epoch 75/99, Train Loss: 1.1437, Test Loss: 0.7205\n",
      "Epoch 76/99, Train Loss: 1.1377, Test Loss: 0.7176\n",
      "Epoch 77/99, Train Loss: 1.1313, Test Loss: 0.7151\n",
      "Epoch 78/99, Train Loss: 1.1408, Test Loss: 0.7141\n",
      "Epoch 79/99, Train Loss: 1.1045, Test Loss: 0.7162\n",
      "Epoch 80/99, Train Loss: 1.1262, Test Loss: 0.7155\n",
      "Epoch 81/99, Train Loss: 1.0803, Test Loss: 0.7138\n",
      "Epoch 82/99, Train Loss: 1.1225, Test Loss: 0.7158\n",
      "Epoch 83/99, Train Loss: 1.1064, Test Loss: 0.7175\n",
      "Epoch 84/99, Train Loss: 1.1011, Test Loss: 0.7145\n",
      "Epoch 85/99, Train Loss: 1.0851, Test Loss: 0.7153\n",
      "Epoch 86/99, Train Loss: 1.1497, Test Loss: 0.7127\n",
      "Epoch 87/99, Train Loss: 1.1075, Test Loss: 0.7166\n",
      "Epoch 88/99, Train Loss: 1.0893, Test Loss: 0.7202\n",
      "Epoch 89/99, Train Loss: 1.1173, Test Loss: 0.7138\n",
      "Epoch 90/99, Train Loss: 1.1054, Test Loss: 0.7132\n",
      "Epoch 91/99, Train Loss: 1.1078, Test Loss: 0.7120\n",
      "Epoch 92/99, Train Loss: 1.1010, Test Loss: 0.7155\n",
      "Epoch 93/99, Train Loss: 1.1034, Test Loss: 0.7102\n",
      "Epoch 94/99, Train Loss: 1.0967, Test Loss: 0.7163\n",
      "Epoch 95/99, Train Loss: 1.1057, Test Loss: 0.7136\n",
      "Epoch 96/99, Train Loss: 1.1039, Test Loss: 0.7166\n",
      "Epoch 97/99, Train Loss: 1.1139, Test Loss: 0.7150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:22:39,686] Trial 44 finished with value: 0.7144440561532974 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 133, 'layer_1_size': 148, 'layer_2_size': 182, 'layer_3_size': 121, 'layer_4_size': 235, 'layer_5_size': 131, 'dropout_rate': 0.3744673729438832, 'learning_rate': 2.9805716870829137e-05, 'batch_size': 64, 'epochs': 99}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/99, Train Loss: 1.1282, Test Loss: 0.7125\n",
      "Epoch 99/99, Train Loss: 1.1035, Test Loss: 0.7144\n",
      "Epoch 1/100, Train Loss: 1.3119, Test Loss: 0.9901\n",
      "Epoch 2/100, Train Loss: 1.2866, Test Loss: 0.9876\n",
      "Epoch 3/100, Train Loss: 1.3151, Test Loss: 0.9842\n",
      "Epoch 4/100, Train Loss: 1.2871, Test Loss: 0.9845\n",
      "Epoch 5/100, Train Loss: 1.2466, Test Loss: 0.9864\n",
      "Epoch 6/100, Train Loss: 1.2638, Test Loss: 0.9853\n",
      "Epoch 7/100, Train Loss: 1.2786, Test Loss: 0.9843\n",
      "Epoch 8/100, Train Loss: 1.3489, Test Loss: 0.9934\n",
      "Epoch 9/100, Train Loss: 1.2846, Test Loss: 0.9893\n",
      "Epoch 10/100, Train Loss: 1.2843, Test Loss: 0.9842\n",
      "Epoch 11/100, Train Loss: 1.2436, Test Loss: 0.9794\n",
      "Epoch 12/100, Train Loss: 1.2357, Test Loss: 0.9829\n",
      "Epoch 13/100, Train Loss: 1.2491, Test Loss: 0.9836\n",
      "Epoch 14/100, Train Loss: 1.2819, Test Loss: 0.9771\n",
      "Epoch 15/100, Train Loss: 1.2639, Test Loss: 0.9842\n",
      "Epoch 16/100, Train Loss: 1.1933, Test Loss: 0.9846\n",
      "Epoch 17/100, Train Loss: 1.1974, Test Loss: 0.9879\n",
      "Epoch 18/100, Train Loss: 1.2206, Test Loss: 0.9823\n",
      "Epoch 19/100, Train Loss: 1.2297, Test Loss: 0.9866\n",
      "Epoch 20/100, Train Loss: 1.2231, Test Loss: 0.9880\n",
      "Epoch 21/100, Train Loss: 1.2566, Test Loss: 0.9814\n",
      "Epoch 22/100, Train Loss: 1.2621, Test Loss: 0.9791\n",
      "Epoch 23/100, Train Loss: 1.2652, Test Loss: 0.9813\n",
      "Epoch 24/100, Train Loss: 1.2473, Test Loss: 0.9822\n",
      "Epoch 25/100, Train Loss: 1.2354, Test Loss: 0.9814\n",
      "Epoch 26/100, Train Loss: 1.1985, Test Loss: 0.9825\n",
      "Epoch 27/100, Train Loss: 1.2066, Test Loss: 0.9838\n",
      "Epoch 28/100, Train Loss: 1.2352, Test Loss: 0.9851\n",
      "Epoch 29/100, Train Loss: 1.2278, Test Loss: 0.9877\n",
      "Epoch 30/100, Train Loss: 1.1779, Test Loss: 0.9831\n",
      "Epoch 31/100, Train Loss: 1.1795, Test Loss: 0.9816\n",
      "Epoch 32/100, Train Loss: 1.1847, Test Loss: 0.9885\n",
      "Epoch 33/100, Train Loss: 1.1779, Test Loss: 0.9818\n",
      "Epoch 34/100, Train Loss: 1.2547, Test Loss: 0.9871\n",
      "Epoch 35/100, Train Loss: 1.1749, Test Loss: 0.9887\n",
      "Epoch 36/100, Train Loss: 1.1947, Test Loss: 0.9857\n",
      "Epoch 37/100, Train Loss: 1.2031, Test Loss: 0.9825\n",
      "Epoch 38/100, Train Loss: 1.2247, Test Loss: 0.9796\n",
      "Epoch 39/100, Train Loss: 1.2767, Test Loss: 0.9876\n",
      "Epoch 40/100, Train Loss: 1.1791, Test Loss: 0.9800\n",
      "Epoch 41/100, Train Loss: 1.2092, Test Loss: 0.9774\n",
      "Epoch 42/100, Train Loss: 1.2218, Test Loss: 0.9790\n",
      "Epoch 43/100, Train Loss: 1.2062, Test Loss: 0.9800\n",
      "Epoch 44/100, Train Loss: 1.1801, Test Loss: 0.9794\n",
      "Epoch 45/100, Train Loss: 1.2111, Test Loss: 0.9864\n",
      "Epoch 46/100, Train Loss: 1.1807, Test Loss: 0.9866\n",
      "Epoch 47/100, Train Loss: 1.2103, Test Loss: 0.9879\n",
      "Epoch 48/100, Train Loss: 1.1997, Test Loss: 0.9846\n",
      "Epoch 49/100, Train Loss: 1.2022, Test Loss: 0.9818\n",
      "Epoch 50/100, Train Loss: 1.1982, Test Loss: 0.9858\n",
      "Epoch 51/100, Train Loss: 1.2117, Test Loss: 0.9810\n",
      "Epoch 52/100, Train Loss: 1.1684, Test Loss: 0.9820\n",
      "Epoch 53/100, Train Loss: 1.2046, Test Loss: 0.9808\n",
      "Epoch 54/100, Train Loss: 1.1931, Test Loss: 0.9766\n",
      "Epoch 55/100, Train Loss: 1.1806, Test Loss: 0.9844\n",
      "Epoch 56/100, Train Loss: 1.2416, Test Loss: 0.9870\n",
      "Epoch 57/100, Train Loss: 1.2340, Test Loss: 0.9816\n",
      "Epoch 58/100, Train Loss: 1.1783, Test Loss: 0.9847\n",
      "Epoch 59/100, Train Loss: 1.1938, Test Loss: 0.9860\n",
      "Epoch 60/100, Train Loss: 1.2172, Test Loss: 0.9825\n",
      "Epoch 61/100, Train Loss: 1.1862, Test Loss: 0.9790\n",
      "Epoch 62/100, Train Loss: 1.2335, Test Loss: 0.9813\n",
      "Epoch 63/100, Train Loss: 1.1591, Test Loss: 0.9819\n",
      "Epoch 64/100, Train Loss: 1.2087, Test Loss: 0.9852\n",
      "Epoch 65/100, Train Loss: 1.1901, Test Loss: 0.9879\n",
      "Epoch 66/100, Train Loss: 1.1307, Test Loss: 0.9850\n",
      "Epoch 67/100, Train Loss: 1.1720, Test Loss: 0.9826\n",
      "Epoch 68/100, Train Loss: 1.1152, Test Loss: 0.9867\n",
      "Epoch 69/100, Train Loss: 1.1811, Test Loss: 0.9830\n",
      "Epoch 70/100, Train Loss: 1.1862, Test Loss: 0.9852\n",
      "Epoch 71/100, Train Loss: 1.2012, Test Loss: 0.9873\n",
      "Epoch 72/100, Train Loss: 1.1748, Test Loss: 0.9842\n",
      "Epoch 73/100, Train Loss: 1.1572, Test Loss: 0.9855\n",
      "Epoch 74/100, Train Loss: 1.1413, Test Loss: 0.9836\n",
      "Epoch 75/100, Train Loss: 1.1606, Test Loss: 0.9831\n",
      "Epoch 76/100, Train Loss: 1.1856, Test Loss: 0.9817\n",
      "Epoch 77/100, Train Loss: 1.1311, Test Loss: 0.9876\n",
      "Epoch 78/100, Train Loss: 1.1908, Test Loss: 0.9801\n",
      "Epoch 79/100, Train Loss: 1.1815, Test Loss: 0.9803\n",
      "Epoch 80/100, Train Loss: 1.1628, Test Loss: 0.9798\n",
      "Epoch 81/100, Train Loss: 1.1391, Test Loss: 0.9879\n",
      "Epoch 82/100, Train Loss: 1.1158, Test Loss: 0.9855\n",
      "Epoch 83/100, Train Loss: 1.1651, Test Loss: 0.9886\n",
      "Epoch 84/100, Train Loss: 1.1602, Test Loss: 0.9861\n",
      "Epoch 85/100, Train Loss: 1.1945, Test Loss: 0.9880\n",
      "Epoch 86/100, Train Loss: 1.1948, Test Loss: 0.9844\n",
      "Epoch 87/100, Train Loss: 1.1567, Test Loss: 0.9827\n",
      "Epoch 88/100, Train Loss: 1.1795, Test Loss: 0.9801\n",
      "Epoch 89/100, Train Loss: 1.1727, Test Loss: 0.9835\n",
      "Epoch 90/100, Train Loss: 1.1962, Test Loss: 0.9793\n",
      "Epoch 91/100, Train Loss: 1.1767, Test Loss: 0.9864\n",
      "Epoch 92/100, Train Loss: 1.2076, Test Loss: 0.9851\n",
      "Epoch 93/100, Train Loss: 1.1505, Test Loss: 0.9842\n",
      "Epoch 94/100, Train Loss: 1.1223, Test Loss: 0.9838\n",
      "Epoch 95/100, Train Loss: 1.1629, Test Loss: 0.9877\n",
      "Epoch 96/100, Train Loss: 1.1708, Test Loss: 0.9859\n",
      "Epoch 97/100, Train Loss: 1.2062, Test Loss: 0.9861\n",
      "Epoch 98/100, Train Loss: 1.1947, Test Loss: 0.9803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:22:47,394] Trial 45 finished with value: 0.9784428924322128 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 128, 'layer_1_size': 151, 'layer_2_size': 178, 'layer_3_size': 128, 'layer_4_size': 232, 'layer_5_size': 126, 'dropout_rate': 0.38163979121307423, 'learning_rate': 2.9925487896693098e-05, 'batch_size': 64, 'epochs': 100}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100, Train Loss: 1.1753, Test Loss: 0.9848\n",
      "Epoch 100/100, Train Loss: 1.1880, Test Loss: 0.9784\n",
      "Epoch 1/75, Train Loss: 1.2140, Test Loss: 0.9724\n",
      "Epoch 2/75, Train Loss: 1.1779, Test Loss: 0.9727\n",
      "Epoch 3/75, Train Loss: 1.1372, Test Loss: 0.9869\n",
      "Epoch 4/75, Train Loss: 1.1539, Test Loss: 0.9869\n",
      "Epoch 5/75, Train Loss: 1.1765, Test Loss: 0.9850\n",
      "Epoch 6/75, Train Loss: 1.2339, Test Loss: 0.9824\n",
      "Epoch 7/75, Train Loss: 1.1153, Test Loss: 0.9848\n",
      "Epoch 8/75, Train Loss: 1.1337, Test Loss: 0.9794\n",
      "Epoch 9/75, Train Loss: 1.1554, Test Loss: 0.9806\n",
      "Epoch 10/75, Train Loss: 1.1156, Test Loss: 0.9866\n",
      "Epoch 11/75, Train Loss: 1.1382, Test Loss: 0.9876\n",
      "Epoch 12/75, Train Loss: 1.1425, Test Loss: 0.9879\n",
      "Epoch 13/75, Train Loss: 1.1197, Test Loss: 0.9889\n",
      "Epoch 14/75, Train Loss: 1.0954, Test Loss: 0.9820\n",
      "Epoch 15/75, Train Loss: 1.1246, Test Loss: 0.9847\n",
      "Epoch 16/75, Train Loss: 1.0845, Test Loss: 0.9844\n",
      "Epoch 17/75, Train Loss: 1.0879, Test Loss: 0.9950\n",
      "Epoch 18/75, Train Loss: 1.1578, Test Loss: 0.9877\n",
      "Epoch 19/75, Train Loss: 1.1249, Test Loss: 0.9926\n",
      "Epoch 20/75, Train Loss: 1.1032, Test Loss: 0.9962\n",
      "Epoch 21/75, Train Loss: 1.0780, Test Loss: 0.9965\n",
      "Epoch 22/75, Train Loss: 1.0783, Test Loss: 0.9951\n",
      "Epoch 23/75, Train Loss: 1.0326, Test Loss: 0.9945\n",
      "Epoch 24/75, Train Loss: 1.0723, Test Loss: 0.9934\n",
      "Epoch 25/75, Train Loss: 1.0985, Test Loss: 0.9939\n",
      "Epoch 26/75, Train Loss: 1.1269, Test Loss: 0.9962\n",
      "Epoch 27/75, Train Loss: 1.0857, Test Loss: 0.9885\n",
      "Epoch 28/75, Train Loss: 1.0991, Test Loss: 0.9987\n",
      "Epoch 29/75, Train Loss: 1.0974, Test Loss: 0.9927\n",
      "Epoch 30/75, Train Loss: 1.0852, Test Loss: 0.9997\n",
      "Epoch 31/75, Train Loss: 1.1097, Test Loss: 0.9895\n",
      "Epoch 32/75, Train Loss: 1.0999, Test Loss: 0.9944\n",
      "Epoch 33/75, Train Loss: 1.0619, Test Loss: 0.9907\n",
      "Epoch 34/75, Train Loss: 1.0632, Test Loss: 0.9915\n",
      "Epoch 35/75, Train Loss: 1.1163, Test Loss: 0.9901\n",
      "Epoch 36/75, Train Loss: 1.0708, Test Loss: 0.9869\n",
      "Epoch 37/75, Train Loss: 1.0934, Test Loss: 0.9934\n",
      "Epoch 38/75, Train Loss: 1.0614, Test Loss: 0.9936\n",
      "Epoch 39/75, Train Loss: 1.1093, Test Loss: 0.9919\n",
      "Epoch 40/75, Train Loss: 1.1125, Test Loss: 0.9907\n",
      "Epoch 41/75, Train Loss: 1.0750, Test Loss: 0.9932\n",
      "Epoch 42/75, Train Loss: 1.0899, Test Loss: 1.0029\n",
      "Epoch 43/75, Train Loss: 1.0611, Test Loss: 1.0023\n",
      "Epoch 44/75, Train Loss: 1.0886, Test Loss: 1.0013\n",
      "Epoch 45/75, Train Loss: 1.0868, Test Loss: 0.9947\n",
      "Epoch 46/75, Train Loss: 1.0719, Test Loss: 0.9955\n",
      "Epoch 47/75, Train Loss: 1.1131, Test Loss: 0.9976\n",
      "Epoch 48/75, Train Loss: 1.0734, Test Loss: 1.0008\n",
      "Epoch 49/75, Train Loss: 1.0617, Test Loss: 0.9966\n",
      "Epoch 50/75, Train Loss: 1.0730, Test Loss: 1.0032\n",
      "Epoch 51/75, Train Loss: 1.0472, Test Loss: 0.9951\n",
      "Epoch 52/75, Train Loss: 1.0361, Test Loss: 1.0024\n",
      "Epoch 53/75, Train Loss: 1.0947, Test Loss: 0.9973\n",
      "Epoch 54/75, Train Loss: 1.0702, Test Loss: 0.9977\n",
      "Epoch 55/75, Train Loss: 1.0888, Test Loss: 0.9917\n",
      "Epoch 56/75, Train Loss: 1.0808, Test Loss: 0.9888\n",
      "Epoch 57/75, Train Loss: 1.0761, Test Loss: 0.9900\n",
      "Epoch 58/75, Train Loss: 1.0408, Test Loss: 0.9891\n",
      "Epoch 59/75, Train Loss: 1.0567, Test Loss: 0.9920\n",
      "Epoch 60/75, Train Loss: 1.0403, Test Loss: 0.9936\n",
      "Epoch 61/75, Train Loss: 1.0714, Test Loss: 0.9976\n",
      "Epoch 62/75, Train Loss: 1.0509, Test Loss: 0.9987\n",
      "Epoch 63/75, Train Loss: 1.0512, Test Loss: 0.9999\n",
      "Epoch 64/75, Train Loss: 1.0572, Test Loss: 1.0028\n",
      "Epoch 65/75, Train Loss: 1.0706, Test Loss: 1.0008\n",
      "Epoch 66/75, Train Loss: 1.0447, Test Loss: 0.9964\n",
      "Epoch 67/75, Train Loss: 1.0524, Test Loss: 0.9917\n",
      "Epoch 68/75, Train Loss: 1.0576, Test Loss: 0.9889\n",
      "Epoch 69/75, Train Loss: 1.0692, Test Loss: 0.9920\n",
      "Epoch 70/75, Train Loss: 1.0334, Test Loss: 0.9912\n",
      "Epoch 71/75, Train Loss: 1.0682, Test Loss: 0.9988\n",
      "Epoch 72/75, Train Loss: 1.0472, Test Loss: 1.0008\n",
      "Epoch 73/75, Train Loss: 1.0741, Test Loss: 0.9939\n",
      "Epoch 74/75, Train Loss: 1.0559, Test Loss: 0.9961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:22:52,377] Trial 46 finished with value: 0.9939367324113846 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 118, 'layer_1_size': 132, 'layer_2_size': 162, 'layer_3_size': 156, 'layer_4_size': 243, 'layer_5_size': 98, 'dropout_rate': 0.33051022701940724, 'learning_rate': 6.07233301068767e-05, 'batch_size': 64, 'epochs': 75}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/75, Train Loss: 1.0783, Test Loss: 0.9939\n",
      "Epoch 1/87, Train Loss: 1.3217, Test Loss: 1.1011\n",
      "Epoch 2/87, Train Loss: 1.3513, Test Loss: 1.0906\n",
      "Epoch 3/87, Train Loss: 1.3175, Test Loss: 1.0827\n",
      "Epoch 4/87, Train Loss: 1.3168, Test Loss: 1.0782\n",
      "Epoch 5/87, Train Loss: 1.3366, Test Loss: 1.0764\n",
      "Epoch 6/87, Train Loss: 1.3379, Test Loss: 1.0803\n",
      "Epoch 7/87, Train Loss: 1.2838, Test Loss: 1.0702\n",
      "Epoch 8/87, Train Loss: 1.2752, Test Loss: 1.0695\n",
      "Epoch 9/87, Train Loss: 1.2930, Test Loss: 1.0703\n",
      "Epoch 10/87, Train Loss: 1.3113, Test Loss: 1.0684\n",
      "Epoch 11/87, Train Loss: 1.3357, Test Loss: 1.0724\n",
      "Epoch 12/87, Train Loss: 1.2796, Test Loss: 1.0635\n",
      "Epoch 13/87, Train Loss: 1.2449, Test Loss: 1.0631\n",
      "Epoch 14/87, Train Loss: 1.2134, Test Loss: 1.0645\n",
      "Epoch 15/87, Train Loss: 1.3014, Test Loss: 1.0673\n",
      "Epoch 16/87, Train Loss: 1.3093, Test Loss: 1.0628\n",
      "Epoch 17/87, Train Loss: 1.2657, Test Loss: 1.0618\n",
      "Epoch 18/87, Train Loss: 1.2844, Test Loss: 1.0599\n",
      "Epoch 19/87, Train Loss: 1.2309, Test Loss: 1.0584\n",
      "Epoch 20/87, Train Loss: 1.2607, Test Loss: 1.0569\n",
      "Epoch 21/87, Train Loss: 1.2330, Test Loss: 1.0571\n",
      "Epoch 22/87, Train Loss: 1.2223, Test Loss: 1.0566\n",
      "Epoch 23/87, Train Loss: 1.2821, Test Loss: 1.0596\n",
      "Epoch 24/87, Train Loss: 1.2514, Test Loss: 1.0547\n",
      "Epoch 25/87, Train Loss: 1.2540, Test Loss: 1.0522\n",
      "Epoch 26/87, Train Loss: 1.2490, Test Loss: 1.0570\n",
      "Epoch 27/87, Train Loss: 1.2817, Test Loss: 1.0566\n",
      "Epoch 28/87, Train Loss: 1.2489, Test Loss: 1.0503\n",
      "Epoch 29/87, Train Loss: 1.2977, Test Loss: 1.0566\n",
      "Epoch 30/87, Train Loss: 1.2520, Test Loss: 1.0558\n",
      "Epoch 31/87, Train Loss: 1.2245, Test Loss: 1.0540\n",
      "Epoch 32/87, Train Loss: 1.2827, Test Loss: 1.0513\n",
      "Epoch 33/87, Train Loss: 1.2128, Test Loss: 1.0506\n",
      "Epoch 34/87, Train Loss: 1.3062, Test Loss: 1.0486\n",
      "Epoch 35/87, Train Loss: 1.1986, Test Loss: 1.0549\n",
      "Epoch 36/87, Train Loss: 1.2894, Test Loss: 1.0513\n",
      "Epoch 37/87, Train Loss: 1.2195, Test Loss: 1.0484\n",
      "Epoch 38/87, Train Loss: 1.2156, Test Loss: 1.0507\n",
      "Epoch 39/87, Train Loss: 1.2006, Test Loss: 1.0536\n",
      "Epoch 40/87, Train Loss: 1.2619, Test Loss: 1.0496\n",
      "Epoch 41/87, Train Loss: 1.2837, Test Loss: 1.0486\n",
      "Epoch 42/87, Train Loss: 1.2353, Test Loss: 1.0507\n",
      "Epoch 43/87, Train Loss: 1.2381, Test Loss: 1.0481\n",
      "Epoch 44/87, Train Loss: 1.2785, Test Loss: 1.0493\n",
      "Epoch 45/87, Train Loss: 1.2423, Test Loss: 1.0426\n",
      "Epoch 46/87, Train Loss: 1.2404, Test Loss: 1.0424\n",
      "Epoch 47/87, Train Loss: 1.2496, Test Loss: 1.0443\n",
      "Epoch 48/87, Train Loss: 1.2374, Test Loss: 1.0466\n",
      "Epoch 49/87, Train Loss: 1.2200, Test Loss: 1.0412\n",
      "Epoch 50/87, Train Loss: 1.2299, Test Loss: 1.0467\n",
      "Epoch 51/87, Train Loss: 1.2537, Test Loss: 1.0413\n",
      "Epoch 52/87, Train Loss: 1.2373, Test Loss: 1.0431\n",
      "Epoch 53/87, Train Loss: 1.2254, Test Loss: 1.0390\n",
      "Epoch 54/87, Train Loss: 1.2154, Test Loss: 1.0427\n",
      "Epoch 55/87, Train Loss: 1.2513, Test Loss: 1.0417\n",
      "Epoch 56/87, Train Loss: 1.2166, Test Loss: 1.0397\n",
      "Epoch 57/87, Train Loss: 1.2389, Test Loss: 1.0451\n",
      "Epoch 58/87, Train Loss: 1.2533, Test Loss: 1.0450\n",
      "Epoch 59/87, Train Loss: 1.2279, Test Loss: 1.0477\n",
      "Epoch 60/87, Train Loss: 1.1953, Test Loss: 1.0401\n",
      "Epoch 61/87, Train Loss: 1.2263, Test Loss: 1.0444\n",
      "Epoch 62/87, Train Loss: 1.2104, Test Loss: 1.0424\n",
      "Epoch 63/87, Train Loss: 1.2392, Test Loss: 1.0375\n",
      "Epoch 64/87, Train Loss: 1.2270, Test Loss: 1.0376\n",
      "Epoch 65/87, Train Loss: 1.2450, Test Loss: 1.0342\n",
      "Epoch 66/87, Train Loss: 1.2565, Test Loss: 1.0392\n",
      "Epoch 67/87, Train Loss: 1.2163, Test Loss: 1.0381\n",
      "Epoch 68/87, Train Loss: 1.2074, Test Loss: 1.0405\n",
      "Epoch 69/87, Train Loss: 1.2465, Test Loss: 1.0420\n",
      "Epoch 70/87, Train Loss: 1.2141, Test Loss: 1.0397\n",
      "Epoch 71/87, Train Loss: 1.2183, Test Loss: 1.0361\n",
      "Epoch 72/87, Train Loss: 1.2165, Test Loss: 1.0390\n",
      "Epoch 73/87, Train Loss: 1.1889, Test Loss: 1.0367\n",
      "Epoch 74/87, Train Loss: 1.2757, Test Loss: 1.0333\n",
      "Epoch 75/87, Train Loss: 1.2381, Test Loss: 1.0335\n",
      "Epoch 76/87, Train Loss: 1.1835, Test Loss: 1.0329\n",
      "Epoch 77/87, Train Loss: 1.2236, Test Loss: 1.0358\n",
      "Epoch 78/87, Train Loss: 1.2202, Test Loss: 1.0369\n",
      "Epoch 79/87, Train Loss: 1.2325, Test Loss: 1.0401\n",
      "Epoch 80/87, Train Loss: 1.2109, Test Loss: 1.0366\n",
      "Epoch 81/87, Train Loss: 1.2257, Test Loss: 1.0382\n",
      "Epoch 82/87, Train Loss: 1.2163, Test Loss: 1.0372\n",
      "Epoch 83/87, Train Loss: 1.2258, Test Loss: 1.0368\n",
      "Epoch 84/87, Train Loss: 1.2355, Test Loss: 1.0350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:22:56,875] Trial 47 finished with value: 1.0343886017799377 and parameters: {'num_hidden_layers': 4, 'layer_0_size': 93, 'layer_1_size': 199, 'layer_2_size': 190, 'layer_3_size': 112, 'dropout_rate': 0.3669567116723034, 'learning_rate': 1.9866598882389283e-05, 'batch_size': 64, 'epochs': 87}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/87, Train Loss: 1.2511, Test Loss: 1.0326\n",
      "Epoch 86/87, Train Loss: 1.1745, Test Loss: 1.0340\n",
      "Epoch 87/87, Train Loss: 1.2180, Test Loss: 1.0344\n",
      "Epoch 1/96, Train Loss: 1.3073, Test Loss: 0.8478\n",
      "Epoch 2/96, Train Loss: 1.2908, Test Loss: 0.8474\n",
      "Epoch 3/96, Train Loss: 1.1997, Test Loss: 0.8446\n",
      "Epoch 4/96, Train Loss: 1.2141, Test Loss: 0.8324\n",
      "Epoch 5/96, Train Loss: 1.1681, Test Loss: 0.8222\n",
      "Epoch 6/96, Train Loss: 1.1507, Test Loss: 0.8214\n",
      "Epoch 7/96, Train Loss: 1.2208, Test Loss: 0.8198\n",
      "Epoch 8/96, Train Loss: 1.1954, Test Loss: 0.8254\n",
      "Epoch 9/96, Train Loss: 1.1735, Test Loss: 0.8249\n",
      "Epoch 10/96, Train Loss: 1.1797, Test Loss: 0.8285\n",
      "Epoch 11/96, Train Loss: 1.1570, Test Loss: 0.8268\n",
      "Epoch 12/96, Train Loss: 1.1750, Test Loss: 0.8182\n",
      "Epoch 13/96, Train Loss: 1.1843, Test Loss: 0.8248\n",
      "Epoch 14/96, Train Loss: 1.1271, Test Loss: 0.8189\n",
      "Epoch 15/96, Train Loss: 1.1509, Test Loss: 0.8181\n",
      "Epoch 16/96, Train Loss: 1.1576, Test Loss: 0.8254\n",
      "Epoch 17/96, Train Loss: 1.1601, Test Loss: 0.8323\n",
      "Epoch 18/96, Train Loss: 1.1676, Test Loss: 0.8356\n",
      "Epoch 19/96, Train Loss: 1.1099, Test Loss: 0.8277\n",
      "Epoch 20/96, Train Loss: 1.1131, Test Loss: 0.8207\n",
      "Epoch 21/96, Train Loss: 1.1735, Test Loss: 0.8208\n",
      "Epoch 22/96, Train Loss: 1.1283, Test Loss: 0.8179\n",
      "Epoch 23/96, Train Loss: 1.1163, Test Loss: 0.8105\n",
      "Epoch 24/96, Train Loss: 1.1367, Test Loss: 0.8131\n",
      "Epoch 25/96, Train Loss: 1.1403, Test Loss: 0.8082\n",
      "Epoch 26/96, Train Loss: 1.1434, Test Loss: 0.8100\n",
      "Epoch 27/96, Train Loss: 1.1274, Test Loss: 0.8108\n",
      "Epoch 28/96, Train Loss: 1.1360, Test Loss: 0.8110\n",
      "Epoch 29/96, Train Loss: 1.0790, Test Loss: 0.8180\n",
      "Epoch 30/96, Train Loss: 1.1213, Test Loss: 0.8165\n",
      "Epoch 31/96, Train Loss: 1.1279, Test Loss: 0.8233\n",
      "Epoch 32/96, Train Loss: 1.1134, Test Loss: 0.8178\n",
      "Epoch 33/96, Train Loss: 1.1403, Test Loss: 0.8190\n",
      "Epoch 34/96, Train Loss: 1.0947, Test Loss: 0.8136\n",
      "Epoch 35/96, Train Loss: 1.1321, Test Loss: 0.8179\n",
      "Epoch 36/96, Train Loss: 1.0945, Test Loss: 0.8183\n",
      "Epoch 37/96, Train Loss: 1.1048, Test Loss: 0.8231\n",
      "Epoch 38/96, Train Loss: 1.1019, Test Loss: 0.8263\n",
      "Epoch 39/96, Train Loss: 1.1017, Test Loss: 0.8304\n",
      "Epoch 40/96, Train Loss: 1.1029, Test Loss: 0.8337\n",
      "Epoch 41/96, Train Loss: 1.0679, Test Loss: 0.8362\n",
      "Epoch 42/96, Train Loss: 1.1024, Test Loss: 0.8328\n",
      "Epoch 43/96, Train Loss: 1.0938, Test Loss: 0.8326\n",
      "Epoch 44/96, Train Loss: 1.1273, Test Loss: 0.8338\n",
      "Epoch 45/96, Train Loss: 1.0895, Test Loss: 0.8318\n",
      "Epoch 46/96, Train Loss: 1.0554, Test Loss: 0.8247\n",
      "Epoch 47/96, Train Loss: 1.1118, Test Loss: 0.8256\n",
      "Epoch 48/96, Train Loss: 1.1083, Test Loss: 0.8257\n",
      "Epoch 49/96, Train Loss: 1.0771, Test Loss: 0.8171\n",
      "Epoch 50/96, Train Loss: 1.0912, Test Loss: 0.8146\n",
      "Epoch 51/96, Train Loss: 1.0768, Test Loss: 0.8184\n",
      "Epoch 52/96, Train Loss: 1.0751, Test Loss: 0.8153\n",
      "Epoch 53/96, Train Loss: 1.0629, Test Loss: 0.8135\n",
      "Epoch 54/96, Train Loss: 1.0525, Test Loss: 0.8125\n",
      "Epoch 55/96, Train Loss: 1.0847, Test Loss: 0.8148\n",
      "Epoch 56/96, Train Loss: 1.0883, Test Loss: 0.8118\n",
      "Epoch 57/96, Train Loss: 1.0751, Test Loss: 0.8088\n",
      "Epoch 58/96, Train Loss: 1.0540, Test Loss: 0.8126\n",
      "Epoch 59/96, Train Loss: 1.0938, Test Loss: 0.8140\n",
      "Epoch 60/96, Train Loss: 1.0808, Test Loss: 0.8091\n",
      "Epoch 61/96, Train Loss: 1.0827, Test Loss: 0.8070\n",
      "Epoch 62/96, Train Loss: 1.0705, Test Loss: 0.8120\n",
      "Epoch 63/96, Train Loss: 1.0826, Test Loss: 0.8093\n",
      "Epoch 64/96, Train Loss: 1.0976, Test Loss: 0.8122\n",
      "Epoch 65/96, Train Loss: 1.0868, Test Loss: 0.8167\n",
      "Epoch 66/96, Train Loss: 1.0954, Test Loss: 0.8162\n",
      "Epoch 67/96, Train Loss: 1.0834, Test Loss: 0.8153\n",
      "Epoch 68/96, Train Loss: 1.0596, Test Loss: 0.8172\n",
      "Epoch 69/96, Train Loss: 1.0598, Test Loss: 0.8217\n",
      "Epoch 70/96, Train Loss: 1.0722, Test Loss: 0.8235\n",
      "Epoch 71/96, Train Loss: 1.0459, Test Loss: 0.8246\n",
      "Epoch 72/96, Train Loss: 1.0692, Test Loss: 0.8256\n",
      "Epoch 73/96, Train Loss: 1.0722, Test Loss: 0.8214\n",
      "Epoch 74/96, Train Loss: 1.0828, Test Loss: 0.8208\n",
      "Epoch 75/96, Train Loss: 1.0473, Test Loss: 0.8298\n",
      "Epoch 76/96, Train Loss: 1.0553, Test Loss: 0.8310\n",
      "Epoch 77/96, Train Loss: 1.0686, Test Loss: 0.8279\n",
      "Epoch 78/96, Train Loss: 1.0726, Test Loss: 0.8263\n",
      "Epoch 79/96, Train Loss: 1.0648, Test Loss: 0.8273\n",
      "Epoch 80/96, Train Loss: 1.0459, Test Loss: 0.8302\n",
      "Epoch 81/96, Train Loss: 1.0781, Test Loss: 0.8299\n",
      "Epoch 82/96, Train Loss: 1.0774, Test Loss: 0.8263\n",
      "Epoch 83/96, Train Loss: 1.0779, Test Loss: 0.8250\n",
      "Epoch 84/96, Train Loss: 1.0401, Test Loss: 0.8257\n",
      "Epoch 85/96, Train Loss: 1.0502, Test Loss: 0.8236\n",
      "Epoch 86/96, Train Loss: 1.0561, Test Loss: 0.8202\n",
      "Epoch 87/96, Train Loss: 1.0408, Test Loss: 0.8210\n",
      "Epoch 88/96, Train Loss: 1.0559, Test Loss: 0.8216\n",
      "Epoch 89/96, Train Loss: 1.0450, Test Loss: 0.8253\n",
      "Epoch 90/96, Train Loss: 1.0828, Test Loss: 0.8257\n",
      "Epoch 91/96, Train Loss: 1.0603, Test Loss: 0.8253\n",
      "Epoch 92/96, Train Loss: 1.0598, Test Loss: 0.8255\n",
      "Epoch 93/96, Train Loss: 1.0324, Test Loss: 0.8245\n",
      "Epoch 94/96, Train Loss: 1.0654, Test Loss: 0.8268\n",
      "Epoch 95/96, Train Loss: 1.0771, Test Loss: 0.8310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:23:06,821] Trial 48 finished with value: 0.8284917622804642 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 146, 'layer_1_size': 181, 'layer_2_size': 218, 'layer_3_size': 207, 'layer_4_size': 205, 'layer_5_size': 232, 'layer_6_size': 154, 'dropout_rate': 0.3640171882532198, 'learning_rate': 0.00011404765027460344, 'batch_size': 64, 'epochs': 96}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/96, Train Loss: 1.0272, Test Loss: 0.8285\n",
      "Epoch 1/97, Train Loss: 1.2659, Test Loss: 0.9653\n",
      "Epoch 2/97, Train Loss: 1.2195, Test Loss: 0.9836\n",
      "Epoch 3/97, Train Loss: 1.1556, Test Loss: 0.9831\n",
      "Epoch 4/97, Train Loss: 1.2421, Test Loss: 0.9853\n",
      "Epoch 5/97, Train Loss: 1.1414, Test Loss: 0.9803\n",
      "Epoch 6/97, Train Loss: 1.1678, Test Loss: 0.9753\n",
      "Epoch 7/97, Train Loss: 1.1945, Test Loss: 0.9768\n",
      "Epoch 8/97, Train Loss: 1.1599, Test Loss: 0.9741\n",
      "Epoch 9/97, Train Loss: 1.1352, Test Loss: 0.9750\n",
      "Epoch 10/97, Train Loss: 1.1315, Test Loss: 0.9772\n",
      "Epoch 11/97, Train Loss: 1.1530, Test Loss: 0.9799\n",
      "Epoch 12/97, Train Loss: 1.0873, Test Loss: 0.9739\n",
      "Epoch 13/97, Train Loss: 1.0863, Test Loss: 0.9752\n",
      "Epoch 14/97, Train Loss: 1.1259, Test Loss: 0.9728\n",
      "Epoch 15/97, Train Loss: 1.0941, Test Loss: 0.9720\n",
      "Epoch 16/97, Train Loss: 1.1338, Test Loss: 0.9696\n",
      "Epoch 17/97, Train Loss: 1.1403, Test Loss: 0.9693\n",
      "Epoch 18/97, Train Loss: 1.1145, Test Loss: 0.9702\n",
      "Epoch 19/97, Train Loss: 1.1492, Test Loss: 0.9663\n",
      "Epoch 20/97, Train Loss: 1.1480, Test Loss: 0.9677\n",
      "Epoch 21/97, Train Loss: 1.0684, Test Loss: 0.9631\n",
      "Epoch 22/97, Train Loss: 1.0471, Test Loss: 0.9640\n",
      "Epoch 23/97, Train Loss: 1.0964, Test Loss: 0.9655\n",
      "Epoch 24/97, Train Loss: 1.0877, Test Loss: 0.9649\n",
      "Epoch 25/97, Train Loss: 1.0756, Test Loss: 0.9666\n",
      "Epoch 26/97, Train Loss: 1.1013, Test Loss: 0.9706\n",
      "Epoch 27/97, Train Loss: 1.0803, Test Loss: 0.9665\n",
      "Epoch 28/97, Train Loss: 1.0806, Test Loss: 0.9599\n",
      "Epoch 29/97, Train Loss: 1.1186, Test Loss: 0.9604\n",
      "Epoch 30/97, Train Loss: 1.0897, Test Loss: 0.9620\n",
      "Epoch 31/97, Train Loss: 1.0601, Test Loss: 0.9595\n",
      "Epoch 32/97, Train Loss: 1.0977, Test Loss: 0.9626\n",
      "Epoch 33/97, Train Loss: 1.0530, Test Loss: 0.9589\n",
      "Epoch 34/97, Train Loss: 1.0379, Test Loss: 0.9581\n",
      "Epoch 35/97, Train Loss: 1.0686, Test Loss: 0.9615\n",
      "Epoch 36/97, Train Loss: 1.0915, Test Loss: 0.9558\n",
      "Epoch 37/97, Train Loss: 1.1023, Test Loss: 0.9556\n",
      "Epoch 38/97, Train Loss: 1.0878, Test Loss: 0.9564\n",
      "Epoch 39/97, Train Loss: 1.0850, Test Loss: 0.9583\n",
      "Epoch 40/97, Train Loss: 1.0886, Test Loss: 0.9565\n",
      "Epoch 41/97, Train Loss: 1.0694, Test Loss: 0.9552\n",
      "Epoch 42/97, Train Loss: 1.0789, Test Loss: 0.9577\n",
      "Epoch 43/97, Train Loss: 1.0940, Test Loss: 0.9522\n",
      "Epoch 44/97, Train Loss: 1.0791, Test Loss: 0.9538\n",
      "Epoch 45/97, Train Loss: 1.0485, Test Loss: 0.9572\n",
      "Epoch 46/97, Train Loss: 1.0589, Test Loss: 0.9582\n",
      "Epoch 47/97, Train Loss: 1.0603, Test Loss: 0.9617\n",
      "Epoch 48/97, Train Loss: 1.0620, Test Loss: 0.9664\n",
      "Epoch 49/97, Train Loss: 1.0637, Test Loss: 0.9655\n",
      "Epoch 50/97, Train Loss: 1.0743, Test Loss: 0.9641\n",
      "Epoch 51/97, Train Loss: 1.0912, Test Loss: 0.9628\n",
      "Epoch 52/97, Train Loss: 1.0562, Test Loss: 0.9617\n",
      "Epoch 53/97, Train Loss: 1.0759, Test Loss: 0.9634\n",
      "Epoch 54/97, Train Loss: 1.0400, Test Loss: 0.9648\n",
      "Epoch 55/97, Train Loss: 1.0484, Test Loss: 0.9648\n",
      "Epoch 56/97, Train Loss: 1.0569, Test Loss: 0.9612\n",
      "Epoch 57/97, Train Loss: 1.0871, Test Loss: 0.9644\n",
      "Epoch 58/97, Train Loss: 1.0433, Test Loss: 0.9599\n",
      "Epoch 59/97, Train Loss: 1.0608, Test Loss: 0.9627\n",
      "Epoch 60/97, Train Loss: 1.0493, Test Loss: 0.9626\n",
      "Epoch 61/97, Train Loss: 1.0618, Test Loss: 0.9639\n",
      "Epoch 62/97, Train Loss: 1.0337, Test Loss: 0.9643\n",
      "Epoch 63/97, Train Loss: 1.0253, Test Loss: 0.9658\n",
      "Epoch 64/97, Train Loss: 1.0575, Test Loss: 0.9665\n",
      "Epoch 65/97, Train Loss: 1.0550, Test Loss: 0.9676\n",
      "Epoch 66/97, Train Loss: 1.0478, Test Loss: 0.9683\n",
      "Epoch 67/97, Train Loss: 1.0558, Test Loss: 0.9676\n",
      "Epoch 68/97, Train Loss: 1.0448, Test Loss: 0.9626\n",
      "Epoch 69/97, Train Loss: 1.0319, Test Loss: 0.9642\n",
      "Epoch 70/97, Train Loss: 1.0438, Test Loss: 0.9636\n",
      "Epoch 71/97, Train Loss: 1.0286, Test Loss: 0.9638\n",
      "Epoch 72/97, Train Loss: 1.0578, Test Loss: 0.9674\n",
      "Epoch 73/97, Train Loss: 1.0482, Test Loss: 0.9663\n",
      "Epoch 74/97, Train Loss: 1.0333, Test Loss: 0.9653\n",
      "Epoch 75/97, Train Loss: 1.0559, Test Loss: 0.9640\n",
      "Epoch 76/97, Train Loss: 1.0260, Test Loss: 0.9636\n",
      "Epoch 77/97, Train Loss: 1.0491, Test Loss: 0.9620\n",
      "Epoch 78/97, Train Loss: 1.0237, Test Loss: 0.9637\n",
      "Epoch 79/97, Train Loss: 1.0450, Test Loss: 0.9628\n",
      "Epoch 80/97, Train Loss: 1.0260, Test Loss: 0.9613\n",
      "Epoch 81/97, Train Loss: 1.0328, Test Loss: 0.9616\n",
      "Epoch 82/97, Train Loss: 1.0176, Test Loss: 0.9607\n",
      "Epoch 83/97, Train Loss: 1.0262, Test Loss: 0.9601\n",
      "Epoch 84/97, Train Loss: 1.0763, Test Loss: 0.9646\n",
      "Epoch 85/97, Train Loss: 1.0214, Test Loss: 0.9638\n",
      "Epoch 86/97, Train Loss: 1.0460, Test Loss: 0.9636\n",
      "Epoch 87/97, Train Loss: 1.0176, Test Loss: 0.9678\n",
      "Epoch 88/97, Train Loss: 1.0221, Test Loss: 0.9670\n",
      "Epoch 89/97, Train Loss: 1.0351, Test Loss: 0.9685\n",
      "Epoch 90/97, Train Loss: 1.0276, Test Loss: 0.9695\n",
      "Epoch 91/97, Train Loss: 1.0398, Test Loss: 0.9708\n",
      "Epoch 92/97, Train Loss: 1.0516, Test Loss: 0.9696\n",
      "Epoch 93/97, Train Loss: 0.9921, Test Loss: 0.9727\n",
      "Epoch 94/97, Train Loss: 1.0230, Test Loss: 0.9749\n",
      "Epoch 95/97, Train Loss: 1.0107, Test Loss: 0.9756\n",
      "Epoch 96/97, Train Loss: 1.0036, Test Loss: 0.9725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:23:13,625] Trial 49 finished with value: 0.9747064411640167 and parameters: {'num_hidden_layers': 5, 'layer_0_size': 146, 'layer_1_size': 177, 'layer_2_size': 217, 'layer_3_size': 213, 'layer_4_size': 213, 'dropout_rate': 0.3371209612310532, 'learning_rate': 0.00012436584492616208, 'batch_size': 64, 'epochs': 97}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/97, Train Loss: 1.0312, Test Loss: 0.9747\n",
      "Epoch 1/83, Train Loss: 1.3027, Test Loss: 0.9479\n",
      "Epoch 2/83, Train Loss: 1.2262, Test Loss: 0.9578\n",
      "Epoch 3/83, Train Loss: 1.2121, Test Loss: 0.9604\n",
      "Epoch 4/83, Train Loss: 1.2568, Test Loss: 0.9591\n",
      "Epoch 5/83, Train Loss: 1.1943, Test Loss: 0.9566\n",
      "Epoch 6/83, Train Loss: 1.1843, Test Loss: 0.9576\n",
      "Epoch 7/83, Train Loss: 1.1790, Test Loss: 0.9552\n",
      "Epoch 8/83, Train Loss: 1.1672, Test Loss: 0.9549\n",
      "Epoch 9/83, Train Loss: 1.1867, Test Loss: 0.9535\n",
      "Epoch 10/83, Train Loss: 1.1356, Test Loss: 0.9538\n",
      "Epoch 11/83, Train Loss: 1.1643, Test Loss: 0.9550\n",
      "Epoch 12/83, Train Loss: 1.1486, Test Loss: 0.9562\n",
      "Epoch 13/83, Train Loss: 1.1564, Test Loss: 0.9576\n",
      "Epoch 14/83, Train Loss: 1.1323, Test Loss: 0.9568\n",
      "Epoch 15/83, Train Loss: 1.1158, Test Loss: 0.9561\n",
      "Epoch 16/83, Train Loss: 1.1428, Test Loss: 0.9560\n",
      "Epoch 17/83, Train Loss: 1.1146, Test Loss: 0.9545\n",
      "Epoch 18/83, Train Loss: 1.1056, Test Loss: 0.9559\n",
      "Epoch 19/83, Train Loss: 1.1419, Test Loss: 0.9567\n",
      "Epoch 20/83, Train Loss: 1.0869, Test Loss: 0.9548\n",
      "Epoch 21/83, Train Loss: 1.1325, Test Loss: 0.9565\n",
      "Epoch 22/83, Train Loss: 1.1168, Test Loss: 0.9535\n",
      "Epoch 23/83, Train Loss: 1.1442, Test Loss: 0.9553\n",
      "Epoch 24/83, Train Loss: 1.1304, Test Loss: 0.9546\n",
      "Epoch 25/83, Train Loss: 1.1171, Test Loss: 0.9551\n",
      "Epoch 26/83, Train Loss: 1.1147, Test Loss: 0.9537\n",
      "Epoch 27/83, Train Loss: 1.0847, Test Loss: 0.9566\n",
      "Epoch 28/83, Train Loss: 1.0916, Test Loss: 0.9547\n",
      "Epoch 29/83, Train Loss: 1.1104, Test Loss: 0.9602\n",
      "Epoch 30/83, Train Loss: 1.0871, Test Loss: 0.9584\n",
      "Epoch 31/83, Train Loss: 1.0771, Test Loss: 0.9575\n",
      "Epoch 32/83, Train Loss: 1.1139, Test Loss: 0.9588\n",
      "Epoch 33/83, Train Loss: 1.0837, Test Loss: 0.9598\n",
      "Epoch 34/83, Train Loss: 1.0923, Test Loss: 0.9579\n",
      "Epoch 35/83, Train Loss: 1.0721, Test Loss: 0.9583\n",
      "Epoch 36/83, Train Loss: 1.0598, Test Loss: 0.9586\n",
      "Epoch 37/83, Train Loss: 1.0981, Test Loss: 0.9598\n",
      "Epoch 38/83, Train Loss: 1.0833, Test Loss: 0.9579\n",
      "Epoch 39/83, Train Loss: 1.0601, Test Loss: 0.9593\n",
      "Epoch 40/83, Train Loss: 1.0667, Test Loss: 0.9613\n",
      "Epoch 41/83, Train Loss: 1.0858, Test Loss: 0.9593\n",
      "Epoch 42/83, Train Loss: 1.0781, Test Loss: 0.9597\n",
      "Epoch 43/83, Train Loss: 1.0612, Test Loss: 0.9604\n",
      "Epoch 44/83, Train Loss: 1.0588, Test Loss: 0.9605\n",
      "Epoch 45/83, Train Loss: 1.0715, Test Loss: 0.9610\n",
      "Epoch 46/83, Train Loss: 1.0659, Test Loss: 0.9611\n",
      "Epoch 47/83, Train Loss: 1.0794, Test Loss: 0.9604\n",
      "Epoch 48/83, Train Loss: 1.0903, Test Loss: 0.9601\n",
      "Epoch 49/83, Train Loss: 1.0668, Test Loss: 0.9592\n",
      "Epoch 50/83, Train Loss: 1.0700, Test Loss: 0.9604\n",
      "Epoch 51/83, Train Loss: 1.0764, Test Loss: 0.9611\n",
      "Epoch 52/83, Train Loss: 1.0759, Test Loss: 0.9610\n",
      "Epoch 53/83, Train Loss: 1.0646, Test Loss: 0.9614\n",
      "Epoch 54/83, Train Loss: 1.0414, Test Loss: 0.9613\n",
      "Epoch 55/83, Train Loss: 1.0783, Test Loss: 0.9613\n",
      "Epoch 56/83, Train Loss: 1.0850, Test Loss: 0.9606\n",
      "Epoch 57/83, Train Loss: 1.0727, Test Loss: 0.9612\n",
      "Epoch 58/83, Train Loss: 1.0695, Test Loss: 0.9640\n",
      "Epoch 59/83, Train Loss: 1.0609, Test Loss: 0.9624\n",
      "Epoch 60/83, Train Loss: 1.0698, Test Loss: 0.9621\n",
      "Epoch 61/83, Train Loss: 1.0627, Test Loss: 0.9620\n",
      "Epoch 62/83, Train Loss: 1.0614, Test Loss: 0.9616\n",
      "Epoch 63/83, Train Loss: 1.0651, Test Loss: 0.9612\n",
      "Epoch 64/83, Train Loss: 1.0485, Test Loss: 0.9646\n",
      "Epoch 65/83, Train Loss: 1.0576, Test Loss: 0.9636\n",
      "Epoch 66/83, Train Loss: 1.0400, Test Loss: 0.9623\n",
      "Epoch 67/83, Train Loss: 1.0549, Test Loss: 0.9627\n",
      "Epoch 68/83, Train Loss: 1.0637, Test Loss: 0.9635\n",
      "Epoch 69/83, Train Loss: 1.0292, Test Loss: 0.9631\n",
      "Epoch 70/83, Train Loss: 1.0447, Test Loss: 0.9625\n",
      "Epoch 71/83, Train Loss: 1.0430, Test Loss: 0.9631\n",
      "Epoch 72/83, Train Loss: 1.0630, Test Loss: 0.9621\n",
      "Epoch 73/83, Train Loss: 1.0621, Test Loss: 0.9635\n",
      "Epoch 74/83, Train Loss: 1.0618, Test Loss: 0.9631\n",
      "Epoch 75/83, Train Loss: 1.0536, Test Loss: 0.9639\n",
      "Epoch 76/83, Train Loss: 1.0108, Test Loss: 0.9627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:23:14,535] Trial 50 finished with value: 0.9658781737089157 and parameters: {'num_hidden_layers': 1, 'layer_0_size': 105, 'dropout_rate': 0.36275753900216656, 'learning_rate': 7.239761340579139e-05, 'batch_size': 64, 'epochs': 83}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/83, Train Loss: 1.0394, Test Loss: 0.9645\n",
      "Epoch 78/83, Train Loss: 1.0285, Test Loss: 0.9653\n",
      "Epoch 79/83, Train Loss: 1.0678, Test Loss: 0.9653\n",
      "Epoch 80/83, Train Loss: 1.0306, Test Loss: 0.9643\n",
      "Epoch 81/83, Train Loss: 1.0574, Test Loss: 0.9621\n",
      "Epoch 82/83, Train Loss: 1.0251, Test Loss: 0.9631\n",
      "Epoch 83/83, Train Loss: 1.0340, Test Loss: 0.9659\n",
      "Epoch 1/95, Train Loss: 1.2835, Test Loss: 0.9443\n",
      "Epoch 2/95, Train Loss: 1.1354, Test Loss: 0.9619\n",
      "Epoch 3/95, Train Loss: 1.0989, Test Loss: 0.9617\n",
      "Epoch 4/95, Train Loss: 1.1231, Test Loss: 0.9575\n",
      "Epoch 5/95, Train Loss: 1.1392, Test Loss: 0.9531\n",
      "Epoch 6/95, Train Loss: 1.1258, Test Loss: 0.9543\n",
      "Epoch 7/95, Train Loss: 1.1344, Test Loss: 0.9558\n",
      "Epoch 8/95, Train Loss: 1.1185, Test Loss: 0.9564\n",
      "Epoch 9/95, Train Loss: 1.1382, Test Loss: 0.9588\n",
      "Epoch 10/95, Train Loss: 1.1338, Test Loss: 0.9587\n",
      "Epoch 11/95, Train Loss: 1.1147, Test Loss: 0.9675\n",
      "Epoch 12/95, Train Loss: 1.1283, Test Loss: 0.9617\n",
      "Epoch 13/95, Train Loss: 1.0867, Test Loss: 0.9713\n",
      "Epoch 14/95, Train Loss: 1.0631, Test Loss: 0.9722\n",
      "Epoch 15/95, Train Loss: 1.0402, Test Loss: 0.9672\n",
      "Epoch 16/95, Train Loss: 1.0571, Test Loss: 0.9661\n",
      "Epoch 17/95, Train Loss: 1.0965, Test Loss: 0.9685\n",
      "Epoch 18/95, Train Loss: 1.0821, Test Loss: 0.9663\n",
      "Epoch 19/95, Train Loss: 1.1154, Test Loss: 0.9590\n",
      "Epoch 20/95, Train Loss: 1.0617, Test Loss: 0.9593\n",
      "Epoch 21/95, Train Loss: 1.0480, Test Loss: 0.9609\n",
      "Epoch 22/95, Train Loss: 1.0461, Test Loss: 0.9597\n",
      "Epoch 23/95, Train Loss: 1.1109, Test Loss: 0.9556\n",
      "Epoch 24/95, Train Loss: 1.0493, Test Loss: 0.9549\n",
      "Epoch 25/95, Train Loss: 1.0640, Test Loss: 0.9519\n",
      "Epoch 26/95, Train Loss: 1.0850, Test Loss: 0.9514\n",
      "Epoch 27/95, Train Loss: 1.0848, Test Loss: 0.9515\n",
      "Epoch 28/95, Train Loss: 1.0814, Test Loss: 0.9512\n",
      "Epoch 29/95, Train Loss: 1.0535, Test Loss: 0.9475\n",
      "Epoch 30/95, Train Loss: 1.0463, Test Loss: 0.9450\n",
      "Epoch 31/95, Train Loss: 1.0606, Test Loss: 0.9519\n",
      "Epoch 32/95, Train Loss: 1.0481, Test Loss: 0.9545\n",
      "Epoch 33/95, Train Loss: 1.0442, Test Loss: 0.9526\n",
      "Epoch 34/95, Train Loss: 1.0457, Test Loss: 0.9592\n",
      "Epoch 35/95, Train Loss: 1.0462, Test Loss: 0.9594\n",
      "Epoch 36/95, Train Loss: 1.0445, Test Loss: 0.9605\n",
      "Epoch 37/95, Train Loss: 1.0257, Test Loss: 0.9550\n",
      "Epoch 38/95, Train Loss: 1.0245, Test Loss: 0.9540\n",
      "Epoch 39/95, Train Loss: 1.0322, Test Loss: 0.9565\n",
      "Epoch 40/95, Train Loss: 1.0264, Test Loss: 0.9603\n",
      "Epoch 41/95, Train Loss: 1.0296, Test Loss: 0.9622\n",
      "Epoch 42/95, Train Loss: 1.0067, Test Loss: 0.9729\n",
      "Epoch 43/95, Train Loss: 1.0241, Test Loss: 0.9708\n",
      "Epoch 44/95, Train Loss: 1.0395, Test Loss: 0.9806\n",
      "Epoch 45/95, Train Loss: 1.0071, Test Loss: 0.9789\n",
      "Epoch 46/95, Train Loss: 1.0218, Test Loss: 0.9705\n",
      "Epoch 47/95, Train Loss: 1.0262, Test Loss: 0.9695\n",
      "Epoch 48/95, Train Loss: 1.0396, Test Loss: 0.9604\n",
      "Epoch 49/95, Train Loss: 1.0176, Test Loss: 0.9561\n",
      "Epoch 50/95, Train Loss: 1.0175, Test Loss: 0.9475\n",
      "Epoch 51/95, Train Loss: 1.0398, Test Loss: 0.9430\n",
      "Epoch 52/95, Train Loss: 1.0189, Test Loss: 0.9440\n",
      "Epoch 53/95, Train Loss: 1.0248, Test Loss: 0.9469\n",
      "Epoch 54/95, Train Loss: 1.0258, Test Loss: 0.9449\n",
      "Epoch 55/95, Train Loss: 1.0040, Test Loss: 0.9466\n",
      "Epoch 56/95, Train Loss: 1.0074, Test Loss: 0.9512\n",
      "Epoch 57/95, Train Loss: 1.0193, Test Loss: 0.9531\n",
      "Epoch 58/95, Train Loss: 1.0145, Test Loss: 0.9561\n",
      "Epoch 59/95, Train Loss: 1.0053, Test Loss: 0.9563\n",
      "Epoch 60/95, Train Loss: 1.0102, Test Loss: 0.9519\n",
      "Epoch 61/95, Train Loss: 1.0087, Test Loss: 0.9495\n",
      "Epoch 62/95, Train Loss: 1.0067, Test Loss: 0.9512\n",
      "Epoch 63/95, Train Loss: 1.0176, Test Loss: 0.9554\n",
      "Epoch 64/95, Train Loss: 1.0324, Test Loss: 0.9568\n",
      "Epoch 65/95, Train Loss: 1.0175, Test Loss: 0.9566\n",
      "Epoch 66/95, Train Loss: 1.0086, Test Loss: 0.9543\n",
      "Epoch 67/95, Train Loss: 0.9913, Test Loss: 0.9554\n",
      "Epoch 68/95, Train Loss: 0.9901, Test Loss: 0.9578\n",
      "Epoch 69/95, Train Loss: 1.0144, Test Loss: 0.9601\n",
      "Epoch 70/95, Train Loss: 1.0025, Test Loss: 0.9603\n",
      "Epoch 71/95, Train Loss: 0.9963, Test Loss: 0.9613\n",
      "Epoch 72/95, Train Loss: 1.0078, Test Loss: 0.9595\n",
      "Epoch 73/95, Train Loss: 0.9939, Test Loss: 0.9621\n",
      "Epoch 74/95, Train Loss: 1.0154, Test Loss: 0.9592\n",
      "Epoch 75/95, Train Loss: 1.0079, Test Loss: 0.9593\n",
      "Epoch 76/95, Train Loss: 1.0168, Test Loss: 0.9590\n",
      "Epoch 77/95, Train Loss: 1.0087, Test Loss: 0.9558\n",
      "Epoch 78/95, Train Loss: 1.0159, Test Loss: 0.9546\n",
      "Epoch 79/95, Train Loss: 1.0382, Test Loss: 0.9493\n",
      "Epoch 80/95, Train Loss: 0.9887, Test Loss: 0.9507\n",
      "Epoch 81/95, Train Loss: 0.9964, Test Loss: 0.9547\n",
      "Epoch 82/95, Train Loss: 1.0106, Test Loss: 0.9538\n",
      "Epoch 83/95, Train Loss: 1.0086, Test Loss: 0.9538\n",
      "Epoch 84/95, Train Loss: 0.9892, Test Loss: 0.9521\n",
      "Epoch 85/95, Train Loss: 0.9880, Test Loss: 0.9547\n",
      "Epoch 86/95, Train Loss: 0.9919, Test Loss: 0.9607\n",
      "Epoch 87/95, Train Loss: 0.9970, Test Loss: 0.9645\n",
      "Epoch 88/95, Train Loss: 0.9983, Test Loss: 0.9616\n",
      "Epoch 89/95, Train Loss: 0.9866, Test Loss: 0.9585\n",
      "Epoch 90/95, Train Loss: 0.9801, Test Loss: 0.9585\n",
      "Epoch 91/95, Train Loss: 1.0098, Test Loss: 0.9645\n",
      "Epoch 92/95, Train Loss: 0.9936, Test Loss: 0.9683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:23:23,567] Trial 51 finished with value: 0.9653671830892563 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 131, 'layer_1_size': 160, 'layer_2_size': 199, 'layer_3_size': 254, 'layer_4_size': 202, 'layer_5_size': 234, 'layer_6_size': 151, 'dropout_rate': 0.3981848310113827, 'learning_rate': 0.00019171579515024314, 'batch_size': 64, 'epochs': 95}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/95, Train Loss: 0.9944, Test Loss: 0.9682\n",
      "Epoch 94/95, Train Loss: 0.9929, Test Loss: 0.9661\n",
      "Epoch 95/95, Train Loss: 0.9950, Test Loss: 0.9654\n",
      "Epoch 1/90, Train Loss: 1.2733, Test Loss: 0.9241\n",
      "Epoch 2/90, Train Loss: 1.2486, Test Loss: 0.9237\n",
      "Epoch 3/90, Train Loss: 1.2581, Test Loss: 0.9213\n",
      "Epoch 4/90, Train Loss: 1.2702, Test Loss: 0.9203\n",
      "Epoch 5/90, Train Loss: 1.2618, Test Loss: 0.9204\n",
      "Epoch 6/90, Train Loss: 1.2790, Test Loss: 0.9191\n",
      "Epoch 7/90, Train Loss: 1.2367, Test Loss: 0.9177\n",
      "Epoch 8/90, Train Loss: 1.3035, Test Loss: 0.9187\n",
      "Epoch 9/90, Train Loss: 1.2236, Test Loss: 0.9219\n",
      "Epoch 10/90, Train Loss: 1.2801, Test Loss: 0.9176\n",
      "Epoch 11/90, Train Loss: 1.2362, Test Loss: 0.9182\n",
      "Epoch 12/90, Train Loss: 1.2384, Test Loss: 0.9194\n",
      "Epoch 13/90, Train Loss: 1.2768, Test Loss: 0.9208\n",
      "Epoch 14/90, Train Loss: 1.2333, Test Loss: 0.9190\n",
      "Epoch 15/90, Train Loss: 1.2486, Test Loss: 0.9161\n",
      "Epoch 16/90, Train Loss: 1.2867, Test Loss: 0.9167\n",
      "Epoch 17/90, Train Loss: 1.2515, Test Loss: 0.9186\n",
      "Epoch 18/90, Train Loss: 1.2227, Test Loss: 0.9192\n",
      "Epoch 19/90, Train Loss: 1.2363, Test Loss: 0.9191\n",
      "Epoch 20/90, Train Loss: 1.2630, Test Loss: 0.9196\n",
      "Epoch 21/90, Train Loss: 1.1802, Test Loss: 0.9178\n",
      "Epoch 22/90, Train Loss: 1.1596, Test Loss: 0.9167\n",
      "Epoch 23/90, Train Loss: 1.2351, Test Loss: 0.9182\n",
      "Epoch 24/90, Train Loss: 1.2661, Test Loss: 0.9160\n",
      "Epoch 25/90, Train Loss: 1.2276, Test Loss: 0.9167\n",
      "Epoch 26/90, Train Loss: 1.2669, Test Loss: 0.9167\n",
      "Epoch 27/90, Train Loss: 1.1927, Test Loss: 0.9197\n",
      "Epoch 28/90, Train Loss: 1.2226, Test Loss: 0.9218\n",
      "Epoch 29/90, Train Loss: 1.2023, Test Loss: 0.9206\n",
      "Epoch 30/90, Train Loss: 1.2170, Test Loss: 0.9234\n",
      "Epoch 31/90, Train Loss: 1.1539, Test Loss: 0.9246\n",
      "Epoch 32/90, Train Loss: 1.2270, Test Loss: 0.9237\n",
      "Epoch 33/90, Train Loss: 1.2634, Test Loss: 0.9216\n",
      "Epoch 34/90, Train Loss: 1.2173, Test Loss: 0.9227\n",
      "Epoch 35/90, Train Loss: 1.2383, Test Loss: 0.9239\n",
      "Epoch 36/90, Train Loss: 1.2320, Test Loss: 0.9243\n",
      "Epoch 37/90, Train Loss: 1.2095, Test Loss: 0.9217\n",
      "Epoch 38/90, Train Loss: 1.2193, Test Loss: 0.9206\n",
      "Epoch 39/90, Train Loss: 1.1883, Test Loss: 0.9179\n",
      "Epoch 40/90, Train Loss: 1.1573, Test Loss: 0.9176\n",
      "Epoch 41/90, Train Loss: 1.2418, Test Loss: 0.9164\n",
      "Epoch 42/90, Train Loss: 1.1789, Test Loss: 0.9142\n",
      "Epoch 43/90, Train Loss: 1.1739, Test Loss: 0.9137\n",
      "Epoch 44/90, Train Loss: 1.2513, Test Loss: 0.9150\n",
      "Epoch 45/90, Train Loss: 1.2112, Test Loss: 0.9169\n",
      "Epoch 46/90, Train Loss: 1.1892, Test Loss: 0.9178\n",
      "Epoch 47/90, Train Loss: 1.1833, Test Loss: 0.9163\n",
      "Epoch 48/90, Train Loss: 1.2402, Test Loss: 0.9164\n",
      "Epoch 49/90, Train Loss: 1.1760, Test Loss: 0.9148\n",
      "Epoch 50/90, Train Loss: 1.1815, Test Loss: 0.9164\n",
      "Epoch 51/90, Train Loss: 1.2407, Test Loss: 0.9173\n",
      "Epoch 52/90, Train Loss: 1.1831, Test Loss: 0.9172\n",
      "Epoch 53/90, Train Loss: 1.1743, Test Loss: 0.9192\n",
      "Epoch 54/90, Train Loss: 1.2024, Test Loss: 0.9170\n",
      "Epoch 55/90, Train Loss: 1.1925, Test Loss: 0.9196\n",
      "Epoch 56/90, Train Loss: 1.1763, Test Loss: 0.9189\n",
      "Epoch 57/90, Train Loss: 1.2052, Test Loss: 0.9201\n",
      "Epoch 58/90, Train Loss: 1.1995, Test Loss: 0.9176\n",
      "Epoch 59/90, Train Loss: 1.2083, Test Loss: 0.9195\n",
      "Epoch 60/90, Train Loss: 1.1938, Test Loss: 0.9186\n",
      "Epoch 61/90, Train Loss: 1.2182, Test Loss: 0.9183\n",
      "Epoch 62/90, Train Loss: 1.1931, Test Loss: 0.9183\n",
      "Epoch 63/90, Train Loss: 1.1734, Test Loss: 0.9174\n",
      "Epoch 64/90, Train Loss: 1.2072, Test Loss: 0.9169\n",
      "Epoch 65/90, Train Loss: 1.1933, Test Loss: 0.9149\n",
      "Epoch 66/90, Train Loss: 1.1773, Test Loss: 0.9152\n",
      "Epoch 67/90, Train Loss: 1.1912, Test Loss: 0.9150\n",
      "Epoch 68/90, Train Loss: 1.1668, Test Loss: 0.9161\n",
      "Epoch 69/90, Train Loss: 1.1549, Test Loss: 0.9161\n",
      "Epoch 70/90, Train Loss: 1.1925, Test Loss: 0.9169\n",
      "Epoch 71/90, Train Loss: 1.1762, Test Loss: 0.9155\n",
      "Epoch 72/90, Train Loss: 1.2142, Test Loss: 0.9157\n",
      "Epoch 73/90, Train Loss: 1.1682, Test Loss: 0.9140\n",
      "Epoch 74/90, Train Loss: 1.2033, Test Loss: 0.9154\n",
      "Epoch 75/90, Train Loss: 1.1679, Test Loss: 0.9160\n",
      "Epoch 76/90, Train Loss: 1.1793, Test Loss: 0.9196\n",
      "Epoch 77/90, Train Loss: 1.1923, Test Loss: 0.9195\n",
      "Epoch 78/90, Train Loss: 1.1803, Test Loss: 0.9195\n",
      "Epoch 79/90, Train Loss: 1.1584, Test Loss: 0.9198\n",
      "Epoch 80/90, Train Loss: 1.1833, Test Loss: 0.9194\n",
      "Epoch 81/90, Train Loss: 1.1908, Test Loss: 0.9176\n",
      "Epoch 82/90, Train Loss: 1.1422, Test Loss: 0.9156\n",
      "Epoch 83/90, Train Loss: 1.1213, Test Loss: 0.9176\n",
      "Epoch 84/90, Train Loss: 1.1223, Test Loss: 0.9176\n",
      "Epoch 85/90, Train Loss: 1.1733, Test Loss: 0.9193\n",
      "Epoch 86/90, Train Loss: 1.1830, Test Loss: 0.9208\n",
      "Epoch 87/90, Train Loss: 1.1544, Test Loss: 0.9201\n",
      "Epoch 88/90, Train Loss: 1.1464, Test Loss: 0.9198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:23:28,790] Trial 52 finished with value: 0.9185730218887329 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 170, 'layer_1_size': 146, 'layer_2_size': 218, 'layer_3_size': 201, 'layer_4_size': 244, 'layer_5_size': 219, 'layer_6_size': 145, 'dropout_rate': 0.4295769816064008, 'learning_rate': 4.464642399797831e-05, 'batch_size': 256, 'epochs': 90}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/90, Train Loss: 1.1805, Test Loss: 0.9207\n",
      "Epoch 90/90, Train Loss: 1.1743, Test Loss: 0.9186\n",
      "Epoch 1/100, Train Loss: 1.1863, Test Loss: 0.9968\n",
      "Epoch 2/100, Train Loss: 1.1497, Test Loss: 0.9813\n",
      "Epoch 3/100, Train Loss: 1.1526, Test Loss: 0.9912\n",
      "Epoch 4/100, Train Loss: 1.1283, Test Loss: 0.9827\n",
      "Epoch 5/100, Train Loss: 1.1280, Test Loss: 0.9884\n",
      "Epoch 6/100, Train Loss: 1.1122, Test Loss: 0.9857\n",
      "Epoch 7/100, Train Loss: 1.1157, Test Loss: 0.9954\n",
      "Epoch 8/100, Train Loss: 1.1060, Test Loss: 0.9809\n",
      "Epoch 9/100, Train Loss: 1.0914, Test Loss: 0.9832\n",
      "Epoch 10/100, Train Loss: 1.0865, Test Loss: 0.9757\n",
      "Epoch 11/100, Train Loss: 1.1095, Test Loss: 0.9768\n",
      "Epoch 12/100, Train Loss: 1.1021, Test Loss: 0.9820\n",
      "Epoch 13/100, Train Loss: 1.1156, Test Loss: 0.9743\n",
      "Epoch 14/100, Train Loss: 1.0937, Test Loss: 0.9804\n",
      "Epoch 15/100, Train Loss: 1.0819, Test Loss: 0.9816\n",
      "Epoch 16/100, Train Loss: 1.0918, Test Loss: 0.9774\n",
      "Epoch 17/100, Train Loss: 1.1100, Test Loss: 0.9771\n",
      "Epoch 18/100, Train Loss: 1.0717, Test Loss: 0.9747\n",
      "Epoch 19/100, Train Loss: 1.1228, Test Loss: 0.9722\n",
      "Epoch 20/100, Train Loss: 1.1255, Test Loss: 0.9707\n",
      "Epoch 21/100, Train Loss: 1.1005, Test Loss: 0.9700\n",
      "Epoch 22/100, Train Loss: 1.0571, Test Loss: 0.9724\n",
      "Epoch 23/100, Train Loss: 1.1179, Test Loss: 0.9743\n",
      "Epoch 24/100, Train Loss: 1.1440, Test Loss: 0.9724\n",
      "Epoch 25/100, Train Loss: 1.0584, Test Loss: 0.9679\n",
      "Epoch 26/100, Train Loss: 1.1201, Test Loss: 0.9690\n",
      "Epoch 27/100, Train Loss: 1.1119, Test Loss: 0.9688\n",
      "Epoch 28/100, Train Loss: 1.0612, Test Loss: 0.9709\n",
      "Epoch 29/100, Train Loss: 1.0733, Test Loss: 0.9702\n",
      "Epoch 30/100, Train Loss: 1.0823, Test Loss: 0.9709\n",
      "Epoch 31/100, Train Loss: 1.1017, Test Loss: 0.9690\n",
      "Epoch 32/100, Train Loss: 1.0668, Test Loss: 0.9696\n",
      "Epoch 33/100, Train Loss: 1.0691, Test Loss: 0.9748\n",
      "Epoch 34/100, Train Loss: 1.1061, Test Loss: 0.9730\n",
      "Epoch 35/100, Train Loss: 1.0725, Test Loss: 0.9709\n",
      "Epoch 36/100, Train Loss: 1.0741, Test Loss: 0.9672\n",
      "Epoch 37/100, Train Loss: 1.0648, Test Loss: 0.9717\n",
      "Epoch 38/100, Train Loss: 1.1009, Test Loss: 0.9710\n",
      "Epoch 39/100, Train Loss: 1.0879, Test Loss: 0.9698\n",
      "Epoch 40/100, Train Loss: 1.0352, Test Loss: 0.9642\n",
      "Epoch 41/100, Train Loss: 1.0505, Test Loss: 0.9677\n",
      "Epoch 42/100, Train Loss: 1.1019, Test Loss: 0.9644\n",
      "Epoch 43/100, Train Loss: 1.0677, Test Loss: 0.9692\n",
      "Epoch 44/100, Train Loss: 1.0777, Test Loss: 0.9647\n",
      "Epoch 45/100, Train Loss: 1.0675, Test Loss: 0.9670\n",
      "Epoch 46/100, Train Loss: 1.0947, Test Loss: 0.9707\n",
      "Epoch 47/100, Train Loss: 1.0588, Test Loss: 0.9724\n",
      "Epoch 48/100, Train Loss: 1.0689, Test Loss: 0.9707\n",
      "Epoch 49/100, Train Loss: 1.0597, Test Loss: 0.9709\n",
      "Epoch 50/100, Train Loss: 1.0624, Test Loss: 0.9701\n",
      "Epoch 51/100, Train Loss: 1.0297, Test Loss: 0.9726\n",
      "Epoch 52/100, Train Loss: 1.1071, Test Loss: 0.9696\n",
      "Epoch 53/100, Train Loss: 1.1104, Test Loss: 0.9721\n",
      "Epoch 54/100, Train Loss: 1.0880, Test Loss: 0.9694\n",
      "Epoch 55/100, Train Loss: 1.0655, Test Loss: 0.9708\n",
      "Epoch 56/100, Train Loss: 1.0769, Test Loss: 0.9687\n",
      "Epoch 57/100, Train Loss: 1.0499, Test Loss: 0.9694\n",
      "Epoch 58/100, Train Loss: 1.0681, Test Loss: 0.9660\n",
      "Epoch 59/100, Train Loss: 1.0332, Test Loss: 0.9714\n",
      "Epoch 60/100, Train Loss: 1.0823, Test Loss: 0.9709\n",
      "Epoch 61/100, Train Loss: 1.0667, Test Loss: 0.9744\n",
      "Epoch 62/100, Train Loss: 1.1118, Test Loss: 0.9756\n",
      "Epoch 63/100, Train Loss: 1.0544, Test Loss: 0.9712\n",
      "Epoch 64/100, Train Loss: 1.0407, Test Loss: 0.9709\n",
      "Epoch 65/100, Train Loss: 1.0596, Test Loss: 0.9706\n",
      "Epoch 66/100, Train Loss: 1.0391, Test Loss: 0.9714\n",
      "Epoch 67/100, Train Loss: 1.0377, Test Loss: 0.9737\n",
      "Epoch 68/100, Train Loss: 1.0497, Test Loss: 0.9722\n",
      "Epoch 69/100, Train Loss: 1.0570, Test Loss: 0.9760\n",
      "Epoch 70/100, Train Loss: 1.0669, Test Loss: 0.9731\n",
      "Epoch 71/100, Train Loss: 1.0766, Test Loss: 0.9703\n",
      "Epoch 72/100, Train Loss: 1.0452, Test Loss: 0.9706\n",
      "Epoch 73/100, Train Loss: 1.0546, Test Loss: 0.9680\n",
      "Epoch 74/100, Train Loss: 1.0521, Test Loss: 0.9714\n",
      "Epoch 75/100, Train Loss: 1.0313, Test Loss: 0.9706\n",
      "Epoch 76/100, Train Loss: 1.0489, Test Loss: 0.9703\n",
      "Epoch 77/100, Train Loss: 1.0418, Test Loss: 0.9714\n",
      "Epoch 78/100, Train Loss: 1.0794, Test Loss: 0.9714\n",
      "Epoch 79/100, Train Loss: 1.0430, Test Loss: 0.9757\n",
      "Epoch 80/100, Train Loss: 1.0332, Test Loss: 0.9734\n",
      "Epoch 81/100, Train Loss: 1.0493, Test Loss: 0.9731\n",
      "Epoch 82/100, Train Loss: 1.0610, Test Loss: 0.9721\n",
      "Epoch 83/100, Train Loss: 1.0748, Test Loss: 0.9726\n",
      "Epoch 84/100, Train Loss: 1.0549, Test Loss: 0.9743\n",
      "Epoch 85/100, Train Loss: 1.0488, Test Loss: 0.9735\n",
      "Epoch 86/100, Train Loss: 1.0496, Test Loss: 0.9774\n",
      "Epoch 87/100, Train Loss: 1.0333, Test Loss: 0.9759\n",
      "Epoch 88/100, Train Loss: 1.0215, Test Loss: 0.9744\n",
      "Epoch 89/100, Train Loss: 1.0385, Test Loss: 0.9743\n",
      "Epoch 90/100, Train Loss: 1.0655, Test Loss: 0.9742\n",
      "Epoch 91/100, Train Loss: 1.0410, Test Loss: 0.9745\n",
      "Epoch 92/100, Train Loss: 1.0579, Test Loss: 0.9750\n",
      "Epoch 93/100, Train Loss: 1.0558, Test Loss: 0.9720\n",
      "Epoch 94/100, Train Loss: 1.0566, Test Loss: 0.9729\n",
      "Epoch 95/100, Train Loss: 1.0391, Test Loss: 0.9747\n",
      "Epoch 96/100, Train Loss: 1.0337, Test Loss: 0.9771\n",
      "Epoch 97/100, Train Loss: 1.0567, Test Loss: 0.9760\n",
      "Epoch 98/100, Train Loss: 1.0602, Test Loss: 0.9724\n",
      "Epoch 99/100, Train Loss: 1.0546, Test Loss: 0.9713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:23:49,534] Trial 53 finished with value: 0.969934344291687 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 187, 'layer_1_size': 183, 'layer_2_size': 185, 'layer_3_size': 90, 'layer_4_size': 256, 'layer_5_size': 134, 'layer_6_size': 126, 'layer_7_size': 144, 'dropout_rate': 0.2980777393748507, 'learning_rate': 0.000140433223928556, 'batch_size': 32, 'epochs': 100}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100, Train Loss: 1.0305, Test Loss: 0.9699\n",
      "Epoch 1/96, Train Loss: 1.2390, Test Loss: 0.7976\n",
      "Epoch 2/96, Train Loss: 1.2438, Test Loss: 0.7924\n",
      "Epoch 3/96, Train Loss: 1.1788, Test Loss: 0.7899\n",
      "Epoch 4/96, Train Loss: 1.2602, Test Loss: 0.7900\n",
      "Epoch 5/96, Train Loss: 1.2371, Test Loss: 0.7931\n",
      "Epoch 6/96, Train Loss: 1.2071, Test Loss: 0.7960\n",
      "Epoch 7/96, Train Loss: 1.2259, Test Loss: 0.7949\n",
      "Epoch 8/96, Train Loss: 1.1947, Test Loss: 0.7983\n",
      "Epoch 9/96, Train Loss: 1.2142, Test Loss: 0.7956\n",
      "Epoch 10/96, Train Loss: 1.1557, Test Loss: 0.7895\n",
      "Epoch 11/96, Train Loss: 1.1431, Test Loss: 0.7902\n",
      "Epoch 12/96, Train Loss: 1.1838, Test Loss: 0.7856\n",
      "Epoch 13/96, Train Loss: 1.1686, Test Loss: 0.7865\n",
      "Epoch 14/96, Train Loss: 1.1929, Test Loss: 0.7874\n",
      "Epoch 15/96, Train Loss: 1.1539, Test Loss: 0.7923\n",
      "Epoch 16/96, Train Loss: 1.1725, Test Loss: 0.7910\n",
      "Epoch 17/96, Train Loss: 1.1097, Test Loss: 0.7966\n",
      "Epoch 18/96, Train Loss: 1.1899, Test Loss: 0.7935\n",
      "Epoch 19/96, Train Loss: 1.1581, Test Loss: 0.7974\n",
      "Epoch 20/96, Train Loss: 1.1060, Test Loss: 0.8039\n",
      "Epoch 21/96, Train Loss: 1.1786, Test Loss: 0.8029\n",
      "Epoch 22/96, Train Loss: 1.1320, Test Loss: 0.7994\n",
      "Epoch 23/96, Train Loss: 1.0982, Test Loss: 0.8009\n",
      "Epoch 24/96, Train Loss: 1.1190, Test Loss: 0.8026\n",
      "Epoch 25/96, Train Loss: 1.1061, Test Loss: 0.8017\n",
      "Epoch 26/96, Train Loss: 1.1037, Test Loss: 0.7989\n",
      "Epoch 27/96, Train Loss: 1.1367, Test Loss: 0.8000\n",
      "Epoch 28/96, Train Loss: 1.1314, Test Loss: 0.8015\n",
      "Epoch 29/96, Train Loss: 1.1389, Test Loss: 0.8022\n",
      "Epoch 30/96, Train Loss: 1.1042, Test Loss: 0.8021\n",
      "Epoch 31/96, Train Loss: 1.0744, Test Loss: 0.7985\n",
      "Epoch 32/96, Train Loss: 1.1202, Test Loss: 0.7962\n",
      "Epoch 33/96, Train Loss: 1.0802, Test Loss: 0.7960\n",
      "Epoch 34/96, Train Loss: 1.1104, Test Loss: 0.8016\n",
      "Epoch 35/96, Train Loss: 1.0732, Test Loss: 0.7979\n",
      "Epoch 36/96, Train Loss: 1.1213, Test Loss: 0.8006\n",
      "Epoch 37/96, Train Loss: 1.0884, Test Loss: 0.7977\n",
      "Epoch 38/96, Train Loss: 1.0965, Test Loss: 0.7975\n",
      "Epoch 39/96, Train Loss: 1.0429, Test Loss: 0.7975\n",
      "Epoch 40/96, Train Loss: 1.0598, Test Loss: 0.7957\n",
      "Epoch 41/96, Train Loss: 1.0942, Test Loss: 0.7977\n",
      "Epoch 42/96, Train Loss: 1.0838, Test Loss: 0.7976\n",
      "Epoch 43/96, Train Loss: 1.0869, Test Loss: 0.8002\n",
      "Epoch 44/96, Train Loss: 1.0857, Test Loss: 0.8006\n",
      "Epoch 45/96, Train Loss: 1.0917, Test Loss: 0.7985\n",
      "Epoch 46/96, Train Loss: 1.1080, Test Loss: 0.7989\n",
      "Epoch 47/96, Train Loss: 1.0485, Test Loss: 0.7995\n",
      "Epoch 48/96, Train Loss: 1.0793, Test Loss: 0.8025\n",
      "Epoch 49/96, Train Loss: 1.0433, Test Loss: 0.7987\n",
      "Epoch 50/96, Train Loss: 1.0570, Test Loss: 0.7992\n",
      "Epoch 51/96, Train Loss: 1.0916, Test Loss: 0.8048\n",
      "Epoch 52/96, Train Loss: 1.0688, Test Loss: 0.8011\n",
      "Epoch 53/96, Train Loss: 1.0536, Test Loss: 0.8058\n",
      "Epoch 54/96, Train Loss: 1.0917, Test Loss: 0.8023\n",
      "Epoch 55/96, Train Loss: 1.0701, Test Loss: 0.8011\n",
      "Epoch 56/96, Train Loss: 1.0617, Test Loss: 0.8025\n",
      "Epoch 57/96, Train Loss: 1.0424, Test Loss: 0.8022\n",
      "Epoch 58/96, Train Loss: 1.0331, Test Loss: 0.8045\n",
      "Epoch 59/96, Train Loss: 1.0918, Test Loss: 0.7998\n",
      "Epoch 60/96, Train Loss: 1.0707, Test Loss: 0.8022\n",
      "Epoch 61/96, Train Loss: 1.0851, Test Loss: 0.8023\n",
      "Epoch 62/96, Train Loss: 1.0846, Test Loss: 0.8036\n",
      "Epoch 63/96, Train Loss: 1.0713, Test Loss: 0.8022\n",
      "Epoch 64/96, Train Loss: 1.0723, Test Loss: 0.8004\n",
      "Epoch 65/96, Train Loss: 1.0666, Test Loss: 0.8034\n",
      "Epoch 66/96, Train Loss: 1.0803, Test Loss: 0.8038\n",
      "Epoch 67/96, Train Loss: 1.0220, Test Loss: 0.8024\n",
      "Epoch 68/96, Train Loss: 1.0850, Test Loss: 0.8050\n",
      "Epoch 69/96, Train Loss: 1.0666, Test Loss: 0.8077\n",
      "Epoch 70/96, Train Loss: 1.0152, Test Loss: 0.8062\n",
      "Epoch 71/96, Train Loss: 1.0849, Test Loss: 0.8043\n",
      "Epoch 72/96, Train Loss: 1.0515, Test Loss: 0.8011\n",
      "Epoch 73/96, Train Loss: 1.0348, Test Loss: 0.8031\n",
      "Epoch 74/96, Train Loss: 1.0598, Test Loss: 0.8041\n",
      "Epoch 75/96, Train Loss: 1.0534, Test Loss: 0.8017\n",
      "Epoch 76/96, Train Loss: 1.0443, Test Loss: 0.8042\n",
      "Epoch 77/96, Train Loss: 1.0323, Test Loss: 0.8009\n",
      "Epoch 78/96, Train Loss: 1.0400, Test Loss: 0.8065\n",
      "Epoch 79/96, Train Loss: 1.0402, Test Loss: 0.8036\n",
      "Epoch 80/96, Train Loss: 1.0424, Test Loss: 0.8062\n",
      "Epoch 81/96, Train Loss: 1.0276, Test Loss: 0.8063\n",
      "Epoch 82/96, Train Loss: 1.0383, Test Loss: 0.8034\n",
      "Epoch 83/96, Train Loss: 1.0443, Test Loss: 0.8033\n",
      "Epoch 84/96, Train Loss: 1.0703, Test Loss: 0.8017\n",
      "Epoch 85/96, Train Loss: 1.0363, Test Loss: 0.8002\n",
      "Epoch 86/96, Train Loss: 1.0591, Test Loss: 0.8026\n",
      "Epoch 87/96, Train Loss: 1.0508, Test Loss: 0.8032\n",
      "Epoch 88/96, Train Loss: 1.0172, Test Loss: 0.8015\n",
      "Epoch 89/96, Train Loss: 1.0646, Test Loss: 0.8061\n",
      "Epoch 90/96, Train Loss: 1.0349, Test Loss: 0.8045\n",
      "Epoch 91/96, Train Loss: 1.0247, Test Loss: 0.8053\n",
      "Epoch 92/96, Train Loss: 1.0414, Test Loss: 0.8046\n",
      "Epoch 93/96, Train Loss: 1.0244, Test Loss: 0.8042\n",
      "Epoch 94/96, Train Loss: 1.0256, Test Loss: 0.8038\n",
      "Epoch 95/96, Train Loss: 1.0297, Test Loss: 0.8043\n",
      "Epoch 96/96, Train Loss: 1.0340, Test Loss: 0.8056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:24:00,463] Trial 54 finished with value: 0.8056043535470963 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 212, 'layer_1_size': 168, 'layer_2_size': 202, 'layer_3_size': 191, 'layer_4_size': 222, 'layer_5_size': 254, 'dropout_rate': 0.4758571026284323, 'learning_rate': 9.162903894913176e-05, 'batch_size': 64, 'epochs': 96}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/96, Train Loss: 1.4489, Test Loss: 1.1391\n",
      "Epoch 2/96, Train Loss: 1.2934, Test Loss: 1.1943\n",
      "Epoch 3/96, Train Loss: 1.2161, Test Loss: 1.1613\n",
      "Epoch 4/96, Train Loss: 1.1302, Test Loss: 1.1515\n",
      "Epoch 5/96, Train Loss: 1.1859, Test Loss: 1.1270\n",
      "Epoch 6/96, Train Loss: 1.0675, Test Loss: 1.1021\n",
      "Epoch 7/96, Train Loss: 1.0866, Test Loss: 1.0921\n",
      "Epoch 8/96, Train Loss: 1.0811, Test Loss: 1.0921\n",
      "Epoch 9/96, Train Loss: 1.0665, Test Loss: 1.0840\n",
      "Epoch 10/96, Train Loss: 1.0764, Test Loss: 1.0849\n",
      "Epoch 11/96, Train Loss: 1.1142, Test Loss: 1.0846\n",
      "Epoch 12/96, Train Loss: 1.0861, Test Loss: 1.0833\n",
      "Epoch 13/96, Train Loss: 1.0258, Test Loss: 1.0839\n",
      "Epoch 14/96, Train Loss: 1.0727, Test Loss: 1.0894\n",
      "Epoch 15/96, Train Loss: 1.0779, Test Loss: 1.0917\n",
      "Epoch 16/96, Train Loss: 1.0702, Test Loss: 1.0947\n",
      "Epoch 17/96, Train Loss: 1.0778, Test Loss: 1.0876\n",
      "Epoch 18/96, Train Loss: 1.0335, Test Loss: 1.0910\n",
      "Epoch 19/96, Train Loss: 1.0151, Test Loss: 1.0938\n",
      "Epoch 20/96, Train Loss: 1.0867, Test Loss: 1.0911\n",
      "Epoch 21/96, Train Loss: 1.1286, Test Loss: 1.0862\n",
      "Epoch 22/96, Train Loss: 1.0645, Test Loss: 1.0838\n",
      "Epoch 23/96, Train Loss: 1.0427, Test Loss: 1.0871\n",
      "Epoch 24/96, Train Loss: 1.0648, Test Loss: 1.0853\n",
      "Epoch 25/96, Train Loss: 1.0842, Test Loss: 1.0889\n",
      "Epoch 26/96, Train Loss: 1.0324, Test Loss: 1.0866\n",
      "Epoch 27/96, Train Loss: 1.0799, Test Loss: 1.0911\n",
      "Epoch 28/96, Train Loss: 1.0069, Test Loss: 1.0910\n",
      "Epoch 29/96, Train Loss: 0.9845, Test Loss: 1.0927\n",
      "Epoch 30/96, Train Loss: 1.0379, Test Loss: 1.0929\n",
      "Epoch 31/96, Train Loss: 1.0281, Test Loss: 1.0903\n",
      "Epoch 32/96, Train Loss: 1.0020, Test Loss: 1.0869\n",
      "Epoch 33/96, Train Loss: 1.0261, Test Loss: 1.0888\n",
      "Epoch 34/96, Train Loss: 1.0016, Test Loss: 1.0900\n",
      "Epoch 35/96, Train Loss: 1.0550, Test Loss: 1.0940\n",
      "Epoch 36/96, Train Loss: 1.0504, Test Loss: 1.0945\n",
      "Epoch 37/96, Train Loss: 0.9688, Test Loss: 1.0941\n",
      "Epoch 38/96, Train Loss: 1.0329, Test Loss: 1.0923\n",
      "Epoch 39/96, Train Loss: 1.0426, Test Loss: 1.0922\n",
      "Epoch 40/96, Train Loss: 1.0438, Test Loss: 1.0914\n",
      "Epoch 41/96, Train Loss: 1.0059, Test Loss: 1.0960\n",
      "Epoch 42/96, Train Loss: 0.9911, Test Loss: 1.0923\n",
      "Epoch 43/96, Train Loss: 1.0073, Test Loss: 1.0873\n",
      "Epoch 44/96, Train Loss: 1.0037, Test Loss: 1.0881\n",
      "Epoch 45/96, Train Loss: 0.9932, Test Loss: 1.0900\n",
      "Epoch 46/96, Train Loss: 0.9951, Test Loss: 1.0894\n",
      "Epoch 47/96, Train Loss: 1.0068, Test Loss: 1.0912\n",
      "Epoch 48/96, Train Loss: 1.0342, Test Loss: 1.0900\n",
      "Epoch 49/96, Train Loss: 0.9891, Test Loss: 1.0907\n",
      "Epoch 50/96, Train Loss: 1.0153, Test Loss: 1.0931\n",
      "Epoch 51/96, Train Loss: 1.0173, Test Loss: 1.0907\n",
      "Epoch 52/96, Train Loss: 1.0254, Test Loss: 1.0886\n",
      "Epoch 53/96, Train Loss: 0.9673, Test Loss: 1.0889\n",
      "Epoch 54/96, Train Loss: 0.9967, Test Loss: 1.0977\n",
      "Epoch 55/96, Train Loss: 0.9950, Test Loss: 1.0919\n",
      "Epoch 56/96, Train Loss: 0.9762, Test Loss: 1.0935\n",
      "Epoch 57/96, Train Loss: 0.9915, Test Loss: 1.0936\n",
      "Epoch 58/96, Train Loss: 1.0192, Test Loss: 1.0912\n",
      "Epoch 59/96, Train Loss: 0.9961, Test Loss: 1.0861\n",
      "Epoch 60/96, Train Loss: 1.0041, Test Loss: 1.0907\n",
      "Epoch 61/96, Train Loss: 1.0101, Test Loss: 1.0885\n",
      "Epoch 62/96, Train Loss: 0.9932, Test Loss: 1.0926\n",
      "Epoch 63/96, Train Loss: 1.0361, Test Loss: 1.0900\n",
      "Epoch 64/96, Train Loss: 0.9825, Test Loss: 1.0911\n",
      "Epoch 65/96, Train Loss: 0.9661, Test Loss: 1.0862\n",
      "Epoch 66/96, Train Loss: 0.9769, Test Loss: 1.0874\n",
      "Epoch 67/96, Train Loss: 0.9821, Test Loss: 1.0893\n",
      "Epoch 68/96, Train Loss: 1.0189, Test Loss: 1.0912\n",
      "Epoch 69/96, Train Loss: 0.9804, Test Loss: 1.0853\n",
      "Epoch 70/96, Train Loss: 1.0064, Test Loss: 1.0887\n",
      "Epoch 71/96, Train Loss: 0.9641, Test Loss: 1.0860\n",
      "Epoch 72/96, Train Loss: 1.0131, Test Loss: 1.0893\n",
      "Epoch 73/96, Train Loss: 1.0178, Test Loss: 1.0852\n",
      "Epoch 74/96, Train Loss: 0.9836, Test Loss: 1.0883\n",
      "Epoch 75/96, Train Loss: 1.0098, Test Loss: 1.0923\n",
      "Epoch 76/96, Train Loss: 0.9856, Test Loss: 1.0934\n",
      "Epoch 77/96, Train Loss: 0.9807, Test Loss: 1.0921\n",
      "Epoch 78/96, Train Loss: 0.9755, Test Loss: 1.0902\n",
      "Epoch 79/96, Train Loss: 0.9896, Test Loss: 1.0943\n",
      "Epoch 80/96, Train Loss: 0.9992, Test Loss: 1.0955\n",
      "Epoch 81/96, Train Loss: 0.9641, Test Loss: 1.0920\n",
      "Epoch 82/96, Train Loss: 0.9965, Test Loss: 1.0949\n",
      "Epoch 83/96, Train Loss: 0.9690, Test Loss: 1.0926\n",
      "Epoch 84/96, Train Loss: 0.9859, Test Loss: 1.0934\n",
      "Epoch 85/96, Train Loss: 0.9810, Test Loss: 1.0967\n",
      "Epoch 86/96, Train Loss: 0.9981, Test Loss: 1.1020\n",
      "Epoch 87/96, Train Loss: 0.9807, Test Loss: 1.1051\n",
      "Epoch 88/96, Train Loss: 1.0121, Test Loss: 1.1129\n",
      "Epoch 89/96, Train Loss: 0.9687, Test Loss: 1.1117\n",
      "Epoch 90/96, Train Loss: 0.9738, Test Loss: 1.1123\n",
      "Epoch 91/96, Train Loss: 0.9820, Test Loss: 1.1094\n",
      "Epoch 92/96, Train Loss: 0.9688, Test Loss: 1.1138\n",
      "Epoch 93/96, Train Loss: 0.9473, Test Loss: 1.1124\n",
      "Epoch 94/96, Train Loss: 0.9957, Test Loss: 1.1108\n",
      "Epoch 95/96, Train Loss: 0.9644, Test Loss: 1.1151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:24:11,823] Trial 55 finished with value: 1.1139369308948517 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 237, 'layer_1_size': 194, 'layer_2_size': 199, 'layer_3_size': 182, 'layer_4_size': 222, 'layer_5_size': 250, 'dropout_rate': 0.4108233365208618, 'learning_rate': 8.418107884772017e-05, 'batch_size': 64, 'epochs': 96}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/96, Train Loss: 0.9651, Test Loss: 1.1139\n",
      "Epoch 1/82, Train Loss: 1.2034, Test Loss: 1.0126\n",
      "Epoch 2/82, Train Loss: 1.1181, Test Loss: 1.0141\n",
      "Epoch 3/82, Train Loss: 1.1225, Test Loss: 1.0039\n",
      "Epoch 4/82, Train Loss: 1.1057, Test Loss: 1.0032\n",
      "Epoch 5/82, Train Loss: 1.1102, Test Loss: 0.9916\n",
      "Epoch 6/82, Train Loss: 1.0902, Test Loss: 0.9942\n",
      "Epoch 7/82, Train Loss: 1.1139, Test Loss: 0.9993\n",
      "Epoch 8/82, Train Loss: 1.0787, Test Loss: 0.9995\n",
      "Epoch 9/82, Train Loss: 1.0762, Test Loss: 1.0009\n",
      "Epoch 10/82, Train Loss: 1.1025, Test Loss: 0.9984\n",
      "Epoch 11/82, Train Loss: 1.0852, Test Loss: 0.9882\n",
      "Epoch 12/82, Train Loss: 1.0932, Test Loss: 0.9907\n",
      "Epoch 13/82, Train Loss: 1.0703, Test Loss: 0.9922\n",
      "Epoch 14/82, Train Loss: 1.0710, Test Loss: 0.9858\n",
      "Epoch 15/82, Train Loss: 1.0544, Test Loss: 0.9927\n",
      "Epoch 16/82, Train Loss: 1.1013, Test Loss: 1.0001\n",
      "Epoch 17/82, Train Loss: 1.0561, Test Loss: 0.9999\n",
      "Epoch 18/82, Train Loss: 1.0543, Test Loss: 1.0076\n",
      "Epoch 19/82, Train Loss: 1.0563, Test Loss: 1.0089\n",
      "Epoch 20/82, Train Loss: 1.0749, Test Loss: 1.0067\n",
      "Epoch 21/82, Train Loss: 1.0861, Test Loss: 1.0054\n",
      "Epoch 22/82, Train Loss: 1.0520, Test Loss: 1.0048\n",
      "Epoch 23/82, Train Loss: 1.0134, Test Loss: 1.0079\n",
      "Epoch 24/82, Train Loss: 1.0586, Test Loss: 1.0021\n",
      "Epoch 25/82, Train Loss: 1.0520, Test Loss: 0.9993\n",
      "Epoch 26/82, Train Loss: 1.0316, Test Loss: 1.0028\n",
      "Epoch 27/82, Train Loss: 1.0133, Test Loss: 1.0032\n",
      "Epoch 28/82, Train Loss: 1.0296, Test Loss: 1.0063\n",
      "Epoch 29/82, Train Loss: 1.0270, Test Loss: 1.0077\n",
      "Epoch 30/82, Train Loss: 1.0220, Test Loss: 1.0118\n",
      "Epoch 31/82, Train Loss: 1.0405, Test Loss: 1.0086\n",
      "Epoch 32/82, Train Loss: 1.0083, Test Loss: 1.0066\n",
      "Epoch 33/82, Train Loss: 1.0482, Test Loss: 1.0061\n",
      "Epoch 34/82, Train Loss: 1.0219, Test Loss: 1.0027\n",
      "Epoch 35/82, Train Loss: 1.0155, Test Loss: 1.0048\n",
      "Epoch 36/82, Train Loss: 1.0177, Test Loss: 0.9985\n",
      "Epoch 37/82, Train Loss: 1.0265, Test Loss: 0.9975\n",
      "Epoch 38/82, Train Loss: 1.0264, Test Loss: 0.9938\n",
      "Epoch 39/82, Train Loss: 1.0057, Test Loss: 0.9930\n",
      "Epoch 40/82, Train Loss: 1.0254, Test Loss: 0.9901\n",
      "Epoch 41/82, Train Loss: 1.0260, Test Loss: 0.9903\n",
      "Epoch 42/82, Train Loss: 0.9704, Test Loss: 0.9930\n",
      "Epoch 43/82, Train Loss: 0.9973, Test Loss: 0.9948\n",
      "Epoch 44/82, Train Loss: 0.9896, Test Loss: 0.9991\n",
      "Epoch 45/82, Train Loss: 0.9880, Test Loss: 0.9962\n",
      "Epoch 46/82, Train Loss: 1.0006, Test Loss: 0.9938\n",
      "Epoch 47/82, Train Loss: 1.0034, Test Loss: 0.9957\n",
      "Epoch 48/82, Train Loss: 1.0218, Test Loss: 0.9894\n",
      "Epoch 49/82, Train Loss: 1.0071, Test Loss: 0.9931\n",
      "Epoch 50/82, Train Loss: 0.9854, Test Loss: 0.9905\n",
      "Epoch 51/82, Train Loss: 0.9802, Test Loss: 0.9776\n",
      "Epoch 52/82, Train Loss: 1.0084, Test Loss: 0.9764\n",
      "Epoch 53/82, Train Loss: 0.9770, Test Loss: 0.9741\n",
      "Epoch 54/82, Train Loss: 0.9745, Test Loss: 0.9742\n",
      "Epoch 55/82, Train Loss: 0.9646, Test Loss: 0.9857\n",
      "Epoch 56/82, Train Loss: 0.9691, Test Loss: 0.9864\n",
      "Epoch 57/82, Train Loss: 0.9757, Test Loss: 0.9879\n",
      "Epoch 58/82, Train Loss: 0.9800, Test Loss: 0.9906\n",
      "Epoch 59/82, Train Loss: 0.9752, Test Loss: 0.9952\n",
      "Epoch 60/82, Train Loss: 0.9721, Test Loss: 0.9943\n",
      "Epoch 61/82, Train Loss: 0.9687, Test Loss: 0.9940\n",
      "Epoch 62/82, Train Loss: 0.9690, Test Loss: 0.9933\n",
      "Epoch 63/82, Train Loss: 0.9573, Test Loss: 0.9958\n",
      "Epoch 64/82, Train Loss: 0.9596, Test Loss: 0.9962\n",
      "Epoch 65/82, Train Loss: 0.9538, Test Loss: 0.9944\n",
      "Epoch 66/82, Train Loss: 0.9454, Test Loss: 0.9987\n",
      "Epoch 67/82, Train Loss: 0.9655, Test Loss: 1.0059\n",
      "Epoch 68/82, Train Loss: 0.9476, Test Loss: 1.0052\n",
      "Epoch 69/82, Train Loss: 0.9468, Test Loss: 1.0133\n",
      "Epoch 70/82, Train Loss: 0.9333, Test Loss: 1.0189\n",
      "Epoch 71/82, Train Loss: 0.9383, Test Loss: 1.0234\n",
      "Epoch 72/82, Train Loss: 0.9227, Test Loss: 1.0276\n",
      "Epoch 73/82, Train Loss: 0.9422, Test Loss: 1.0215\n",
      "Epoch 74/82, Train Loss: 0.9137, Test Loss: 1.0154\n",
      "Epoch 75/82, Train Loss: 0.9328, Test Loss: 1.0159\n",
      "Epoch 76/82, Train Loss: 0.9171, Test Loss: 1.0310\n",
      "Epoch 77/82, Train Loss: 0.9433, Test Loss: 1.0322\n",
      "Epoch 78/82, Train Loss: 0.9160, Test Loss: 1.0288\n",
      "Epoch 79/82, Train Loss: 0.9330, Test Loss: 1.0244\n",
      "Epoch 80/82, Train Loss: 0.9384, Test Loss: 1.0222\n",
      "Epoch 81/82, Train Loss: 0.9389, Test Loss: 1.0352\n",
      "Epoch 82/82, Train Loss: 0.9170, Test Loss: 1.0325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:24:20,949] Trial 56 finished with value: 1.0324957966804504 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 211, 'layer_1_size': 166, 'layer_2_size': 242, 'layer_3_size': 217, 'layer_4_size': 176, 'layer_5_size': 80, 'dropout_rate': 0.27840839363853853, 'learning_rate': 0.0002524137513057969, 'batch_size': 64, 'epochs': 82}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/89, Train Loss: 1.2187, Test Loss: 0.9760\n",
      "Epoch 2/89, Train Loss: 1.2374, Test Loss: 0.9831\n",
      "Epoch 3/89, Train Loss: 1.1704, Test Loss: 0.9827\n",
      "Epoch 4/89, Train Loss: 1.2304, Test Loss: 0.9847\n",
      "Epoch 5/89, Train Loss: 1.2224, Test Loss: 0.9815\n",
      "Epoch 6/89, Train Loss: 1.2286, Test Loss: 0.9788\n",
      "Epoch 7/89, Train Loss: 1.1570, Test Loss: 0.9814\n",
      "Epoch 8/89, Train Loss: 1.1782, Test Loss: 0.9786\n",
      "Epoch 9/89, Train Loss: 1.1858, Test Loss: 0.9792\n",
      "Epoch 10/89, Train Loss: 1.2111, Test Loss: 0.9753\n",
      "Epoch 11/89, Train Loss: 1.1656, Test Loss: 0.9811\n",
      "Epoch 12/89, Train Loss: 1.1682, Test Loss: 0.9767\n",
      "Epoch 13/89, Train Loss: 1.1979, Test Loss: 0.9775\n",
      "Epoch 14/89, Train Loss: 1.1337, Test Loss: 0.9858\n",
      "Epoch 15/89, Train Loss: 1.1386, Test Loss: 0.9764\n",
      "Epoch 16/89, Train Loss: 1.2157, Test Loss: 0.9779\n",
      "Epoch 17/89, Train Loss: 1.1505, Test Loss: 0.9721\n",
      "Epoch 18/89, Train Loss: 1.1227, Test Loss: 0.9765\n",
      "Epoch 19/89, Train Loss: 1.1787, Test Loss: 0.9797\n",
      "Epoch 20/89, Train Loss: 1.1618, Test Loss: 0.9748\n",
      "Epoch 21/89, Train Loss: 1.1199, Test Loss: 0.9773\n",
      "Epoch 22/89, Train Loss: 1.1250, Test Loss: 0.9768\n",
      "Epoch 23/89, Train Loss: 1.1253, Test Loss: 0.9734\n",
      "Epoch 24/89, Train Loss: 1.1280, Test Loss: 0.9661\n",
      "Epoch 25/89, Train Loss: 1.1317, Test Loss: 0.9727\n",
      "Epoch 26/89, Train Loss: 1.1325, Test Loss: 0.9704\n",
      "Epoch 27/89, Train Loss: 1.1393, Test Loss: 0.9714\n",
      "Epoch 28/89, Train Loss: 1.1439, Test Loss: 0.9735\n",
      "Epoch 29/89, Train Loss: 1.0887, Test Loss: 0.9757\n",
      "Epoch 30/89, Train Loss: 1.1900, Test Loss: 0.9725\n",
      "Epoch 31/89, Train Loss: 1.0720, Test Loss: 0.9746\n",
      "Epoch 32/89, Train Loss: 1.0803, Test Loss: 0.9720\n",
      "Epoch 33/89, Train Loss: 1.1137, Test Loss: 0.9690\n",
      "Epoch 34/89, Train Loss: 1.0725, Test Loss: 0.9819\n",
      "Epoch 35/89, Train Loss: 1.0641, Test Loss: 0.9784\n",
      "Epoch 36/89, Train Loss: 1.1012, Test Loss: 0.9744\n",
      "Epoch 37/89, Train Loss: 1.1034, Test Loss: 0.9759\n",
      "Epoch 38/89, Train Loss: 1.1628, Test Loss: 0.9733\n",
      "Epoch 39/89, Train Loss: 1.1679, Test Loss: 0.9767\n",
      "Epoch 40/89, Train Loss: 1.1281, Test Loss: 0.9681\n",
      "Epoch 41/89, Train Loss: 1.0614, Test Loss: 0.9776\n",
      "Epoch 42/89, Train Loss: 1.1358, Test Loss: 0.9757\n",
      "Epoch 43/89, Train Loss: 1.1217, Test Loss: 0.9766\n",
      "Epoch 44/89, Train Loss: 1.0853, Test Loss: 0.9743\n",
      "Epoch 45/89, Train Loss: 1.0967, Test Loss: 0.9769\n",
      "Epoch 46/89, Train Loss: 1.0916, Test Loss: 0.9786\n",
      "Epoch 47/89, Train Loss: 1.0625, Test Loss: 0.9816\n",
      "Epoch 48/89, Train Loss: 1.1078, Test Loss: 0.9752\n",
      "Epoch 49/89, Train Loss: 1.1311, Test Loss: 0.9809\n",
      "Epoch 50/89, Train Loss: 1.1380, Test Loss: 0.9755\n",
      "Epoch 51/89, Train Loss: 1.0559, Test Loss: 0.9772\n",
      "Epoch 52/89, Train Loss: 1.1324, Test Loss: 0.9820\n",
      "Epoch 53/89, Train Loss: 1.1004, Test Loss: 0.9800\n",
      "Epoch 54/89, Train Loss: 1.0934, Test Loss: 0.9827\n",
      "Epoch 55/89, Train Loss: 1.0974, Test Loss: 0.9758\n",
      "Epoch 56/89, Train Loss: 1.1407, Test Loss: 0.9721\n",
      "Epoch 57/89, Train Loss: 1.1311, Test Loss: 0.9803\n",
      "Epoch 58/89, Train Loss: 1.0535, Test Loss: 0.9814\n",
      "Epoch 59/89, Train Loss: 1.0372, Test Loss: 0.9813\n",
      "Epoch 60/89, Train Loss: 1.1126, Test Loss: 0.9889\n",
      "Epoch 61/89, Train Loss: 1.0592, Test Loss: 0.9797\n",
      "Epoch 62/89, Train Loss: 1.0753, Test Loss: 0.9782\n",
      "Epoch 63/89, Train Loss: 1.1248, Test Loss: 0.9784\n",
      "Epoch 64/89, Train Loss: 1.0535, Test Loss: 0.9782\n",
      "Epoch 65/89, Train Loss: 1.1518, Test Loss: 0.9787\n",
      "Epoch 66/89, Train Loss: 1.0608, Test Loss: 0.9771\n",
      "Epoch 67/89, Train Loss: 1.1256, Test Loss: 0.9729\n",
      "Epoch 68/89, Train Loss: 1.0589, Test Loss: 0.9703\n",
      "Epoch 69/89, Train Loss: 1.0058, Test Loss: 0.9751\n",
      "Epoch 70/89, Train Loss: 1.0600, Test Loss: 0.9782\n",
      "Epoch 71/89, Train Loss: 1.0827, Test Loss: 0.9840\n",
      "Epoch 72/89, Train Loss: 1.0839, Test Loss: 0.9759\n",
      "Epoch 73/89, Train Loss: 1.0674, Test Loss: 0.9796\n",
      "Epoch 74/89, Train Loss: 1.0913, Test Loss: 0.9789\n",
      "Epoch 75/89, Train Loss: 1.0591, Test Loss: 0.9752\n",
      "Epoch 76/89, Train Loss: 1.0676, Test Loss: 0.9722\n",
      "Epoch 77/89, Train Loss: 1.0998, Test Loss: 0.9775\n",
      "Epoch 78/89, Train Loss: 1.0664, Test Loss: 0.9694\n",
      "Epoch 79/89, Train Loss: 1.0586, Test Loss: 0.9710\n",
      "Epoch 80/89, Train Loss: 1.1146, Test Loss: 0.9722\n",
      "Epoch 81/89, Train Loss: 1.0903, Test Loss: 0.9699\n",
      "Epoch 82/89, Train Loss: 1.0623, Test Loss: 0.9733\n",
      "Epoch 83/89, Train Loss: 1.0653, Test Loss: 0.9782\n",
      "Epoch 84/89, Train Loss: 1.0763, Test Loss: 0.9758\n",
      "Epoch 85/89, Train Loss: 1.0428, Test Loss: 0.9729\n",
      "Epoch 86/89, Train Loss: 1.0752, Test Loss: 0.9723\n",
      "Epoch 87/89, Train Loss: 1.0477, Test Loss: 0.9718\n",
      "Epoch 88/89, Train Loss: 1.0697, Test Loss: 0.9743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:24:25,353] Trial 57 finished with value: 0.9715378135442734 and parameters: {'num_hidden_layers': 3, 'layer_0_size': 119, 'layer_1_size': 174, 'layer_2_size': 230, 'dropout_rate': 0.3730003722011575, 'learning_rate': 1.8538798793730225e-05, 'batch_size': 64, 'epochs': 89}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/89, Train Loss: 1.1225, Test Loss: 0.9715\n",
      "Epoch 1/94, Train Loss: 1.2154, Test Loss: 0.9215\n",
      "Epoch 2/94, Train Loss: 1.2369, Test Loss: 0.9308\n",
      "Epoch 3/94, Train Loss: 1.1908, Test Loss: 0.9454\n",
      "Epoch 4/94, Train Loss: 1.2035, Test Loss: 0.9305\n",
      "Epoch 5/94, Train Loss: 1.1705, Test Loss: 0.9302\n",
      "Epoch 6/94, Train Loss: 1.2189, Test Loss: 0.9255\n",
      "Epoch 7/94, Train Loss: 1.1848, Test Loss: 0.9200\n",
      "Epoch 8/94, Train Loss: 1.1701, Test Loss: 0.9289\n",
      "Epoch 9/94, Train Loss: 1.2096, Test Loss: 0.9228\n",
      "Epoch 10/94, Train Loss: 1.1571, Test Loss: 0.9179\n",
      "Epoch 11/94, Train Loss: 1.1662, Test Loss: 0.9188\n",
      "Epoch 12/94, Train Loss: 1.1802, Test Loss: 0.9148\n",
      "Epoch 13/94, Train Loss: 1.1558, Test Loss: 0.9156\n",
      "Epoch 14/94, Train Loss: 1.1578, Test Loss: 0.9127\n",
      "Epoch 15/94, Train Loss: 1.1237, Test Loss: 0.9078\n",
      "Epoch 16/94, Train Loss: 1.1255, Test Loss: 0.9041\n",
      "Epoch 17/94, Train Loss: 1.1057, Test Loss: 0.9066\n",
      "Epoch 18/94, Train Loss: 1.1635, Test Loss: 0.9019\n",
      "Epoch 19/94, Train Loss: 1.1259, Test Loss: 0.8969\n",
      "Epoch 20/94, Train Loss: 1.1530, Test Loss: 0.8970\n",
      "Epoch 21/94, Train Loss: 1.1362, Test Loss: 0.9005\n",
      "Epoch 22/94, Train Loss: 1.1299, Test Loss: 0.9005\n",
      "Epoch 23/94, Train Loss: 1.1467, Test Loss: 0.8987\n",
      "Epoch 24/94, Train Loss: 1.1216, Test Loss: 0.8996\n",
      "Epoch 25/94, Train Loss: 1.1115, Test Loss: 0.8902\n",
      "Epoch 26/94, Train Loss: 1.1218, Test Loss: 0.8842\n",
      "Epoch 27/94, Train Loss: 1.1544, Test Loss: 0.8930\n",
      "Epoch 28/94, Train Loss: 1.1081, Test Loss: 0.8946\n",
      "Epoch 29/94, Train Loss: 1.0963, Test Loss: 0.8973\n",
      "Epoch 30/94, Train Loss: 1.0897, Test Loss: 0.8978\n",
      "Epoch 31/94, Train Loss: 1.1395, Test Loss: 0.8983\n",
      "Epoch 32/94, Train Loss: 1.0972, Test Loss: 0.8953\n",
      "Epoch 33/94, Train Loss: 1.0826, Test Loss: 0.8998\n",
      "Epoch 34/94, Train Loss: 1.1135, Test Loss: 0.8976\n",
      "Epoch 35/94, Train Loss: 1.1344, Test Loss: 0.8962\n",
      "Epoch 36/94, Train Loss: 1.1445, Test Loss: 0.9018\n",
      "Epoch 37/94, Train Loss: 1.1096, Test Loss: 0.9019\n",
      "Epoch 38/94, Train Loss: 1.1152, Test Loss: 0.9026\n",
      "Epoch 39/94, Train Loss: 1.1014, Test Loss: 0.8976\n",
      "Epoch 40/94, Train Loss: 1.1156, Test Loss: 0.8996\n",
      "Epoch 41/94, Train Loss: 1.1162, Test Loss: 0.8960\n",
      "Epoch 42/94, Train Loss: 1.1162, Test Loss: 0.8999\n",
      "Epoch 43/94, Train Loss: 1.0763, Test Loss: 0.8931\n",
      "Epoch 44/94, Train Loss: 1.0786, Test Loss: 0.8941\n",
      "Epoch 45/94, Train Loss: 1.0955, Test Loss: 0.8952\n",
      "Epoch 46/94, Train Loss: 1.0959, Test Loss: 0.8978\n",
      "Epoch 47/94, Train Loss: 1.0863, Test Loss: 0.9018\n",
      "Epoch 48/94, Train Loss: 1.0919, Test Loss: 0.8986\n",
      "Epoch 49/94, Train Loss: 1.0978, Test Loss: 0.8934\n",
      "Epoch 50/94, Train Loss: 1.1095, Test Loss: 0.8937\n",
      "Epoch 51/94, Train Loss: 1.0947, Test Loss: 0.8949\n",
      "Epoch 52/94, Train Loss: 1.0871, Test Loss: 0.8868\n",
      "Epoch 53/94, Train Loss: 1.1105, Test Loss: 0.8913\n",
      "Epoch 54/94, Train Loss: 1.0700, Test Loss: 0.8878\n",
      "Epoch 55/94, Train Loss: 1.0857, Test Loss: 0.8906\n",
      "Epoch 56/94, Train Loss: 1.1188, Test Loss: 0.8925\n",
      "Epoch 57/94, Train Loss: 1.1080, Test Loss: 0.8899\n",
      "Epoch 58/94, Train Loss: 1.1148, Test Loss: 0.8892\n",
      "Epoch 59/94, Train Loss: 1.1105, Test Loss: 0.8905\n",
      "Epoch 60/94, Train Loss: 1.0622, Test Loss: 0.8893\n",
      "Epoch 61/94, Train Loss: 1.0892, Test Loss: 0.8888\n",
      "Epoch 62/94, Train Loss: 1.1035, Test Loss: 0.8834\n",
      "Epoch 63/94, Train Loss: 1.0884, Test Loss: 0.8818\n",
      "Epoch 64/94, Train Loss: 1.0838, Test Loss: 0.8861\n",
      "Epoch 65/94, Train Loss: 1.0953, Test Loss: 0.8823\n",
      "Epoch 66/94, Train Loss: 1.0488, Test Loss: 0.8833\n",
      "Epoch 67/94, Train Loss: 1.1013, Test Loss: 0.8887\n",
      "Epoch 68/94, Train Loss: 1.0679, Test Loss: 0.8881\n",
      "Epoch 69/94, Train Loss: 1.0588, Test Loss: 0.8857\n",
      "Epoch 70/94, Train Loss: 1.0665, Test Loss: 0.8883\n",
      "Epoch 71/94, Train Loss: 1.0527, Test Loss: 0.8863\n",
      "Epoch 72/94, Train Loss: 1.0399, Test Loss: 0.8846\n",
      "Epoch 73/94, Train Loss: 1.0639, Test Loss: 0.8837\n",
      "Epoch 74/94, Train Loss: 1.0601, Test Loss: 0.8869\n",
      "Epoch 75/94, Train Loss: 1.0817, Test Loss: 0.8855\n",
      "Epoch 76/94, Train Loss: 1.1055, Test Loss: 0.8836\n",
      "Epoch 77/94, Train Loss: 1.0592, Test Loss: 0.8866\n",
      "Epoch 78/94, Train Loss: 1.0697, Test Loss: 0.8825\n",
      "Epoch 79/94, Train Loss: 1.0317, Test Loss: 0.8810\n",
      "Epoch 80/94, Train Loss: 1.0524, Test Loss: 0.8802\n",
      "Epoch 81/94, Train Loss: 1.0936, Test Loss: 0.8819\n",
      "Epoch 82/94, Train Loss: 1.0791, Test Loss: 0.8827\n",
      "Epoch 83/94, Train Loss: 1.0421, Test Loss: 0.8840\n",
      "Epoch 84/94, Train Loss: 1.0590, Test Loss: 0.8813\n",
      "Epoch 85/94, Train Loss: 1.0643, Test Loss: 0.8803\n",
      "Epoch 86/94, Train Loss: 1.0598, Test Loss: 0.8880\n",
      "Epoch 87/94, Train Loss: 1.0851, Test Loss: 0.8874\n",
      "Epoch 88/94, Train Loss: 1.0913, Test Loss: 0.8903\n",
      "Epoch 89/94, Train Loss: 1.0395, Test Loss: 0.8918\n",
      "Epoch 90/94, Train Loss: 1.0492, Test Loss: 0.8935\n",
      "Epoch 91/94, Train Loss: 1.0441, Test Loss: 0.8942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:24:31,547] Trial 58 finished with value: 0.8959902673959732 and parameters: {'num_hidden_layers': 5, 'layer_0_size': 134, 'layer_1_size': 139, 'layer_2_size': 207, 'layer_3_size': 192, 'layer_4_size': 199, 'dropout_rate': 0.33566516903680266, 'learning_rate': 9.87431892241588e-05, 'batch_size': 64, 'epochs': 94}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/94, Train Loss: 1.0685, Test Loss: 0.8965\n",
      "Epoch 93/94, Train Loss: 1.0672, Test Loss: 0.8979\n",
      "Epoch 94/94, Train Loss: 1.0579, Test Loss: 0.8960\n",
      "Epoch 1/77, Train Loss: 1.2568, Test Loss: 0.8582\n",
      "Epoch 2/77, Train Loss: 1.3087, Test Loss: 0.8857\n",
      "Epoch 3/77, Train Loss: 1.3291, Test Loss: 0.8786\n",
      "Epoch 4/77, Train Loss: 1.2631, Test Loss: 0.8765\n",
      "Epoch 5/77, Train Loss: 1.2329, Test Loss: 0.8734\n",
      "Epoch 6/77, Train Loss: 1.2335, Test Loss: 0.8704\n",
      "Epoch 7/77, Train Loss: 1.2237, Test Loss: 0.8714\n",
      "Epoch 8/77, Train Loss: 1.2053, Test Loss: 0.8700\n",
      "Epoch 9/77, Train Loss: 1.1974, Test Loss: 0.8703\n",
      "Epoch 10/77, Train Loss: 1.1639, Test Loss: 0.8624\n",
      "Epoch 11/77, Train Loss: 1.1973, Test Loss: 0.8653\n",
      "Epoch 12/77, Train Loss: 1.1801, Test Loss: 0.8638\n",
      "Epoch 13/77, Train Loss: 1.2176, Test Loss: 0.8608\n",
      "Epoch 14/77, Train Loss: 1.2345, Test Loss: 0.8651\n",
      "Epoch 15/77, Train Loss: 1.2090, Test Loss: 0.8598\n",
      "Epoch 16/77, Train Loss: 1.1650, Test Loss: 0.8580\n",
      "Epoch 17/77, Train Loss: 1.1706, Test Loss: 0.8525\n",
      "Epoch 18/77, Train Loss: 1.1565, Test Loss: 0.8552\n",
      "Epoch 19/77, Train Loss: 1.1096, Test Loss: 0.8558\n",
      "Epoch 20/77, Train Loss: 1.2007, Test Loss: 0.8577\n",
      "Epoch 21/77, Train Loss: 1.2451, Test Loss: 0.8541\n",
      "Epoch 22/77, Train Loss: 1.1654, Test Loss: 0.8543\n",
      "Epoch 23/77, Train Loss: 1.1654, Test Loss: 0.8525\n",
      "Epoch 24/77, Train Loss: 1.1521, Test Loss: 0.8519\n",
      "Epoch 25/77, Train Loss: 1.1646, Test Loss: 0.8517\n",
      "Epoch 26/77, Train Loss: 1.1445, Test Loss: 0.8523\n",
      "Epoch 27/77, Train Loss: 1.1232, Test Loss: 0.8518\n",
      "Epoch 28/77, Train Loss: 1.0903, Test Loss: 0.8528\n",
      "Epoch 29/77, Train Loss: 1.1675, Test Loss: 0.8517\n",
      "Epoch 30/77, Train Loss: 1.1819, Test Loss: 0.8501\n",
      "Epoch 31/77, Train Loss: 1.0925, Test Loss: 0.8529\n",
      "Epoch 32/77, Train Loss: 1.1209, Test Loss: 0.8507\n",
      "Epoch 33/77, Train Loss: 1.1145, Test Loss: 0.8510\n",
      "Epoch 34/77, Train Loss: 1.1324, Test Loss: 0.8523\n",
      "Epoch 35/77, Train Loss: 1.1325, Test Loss: 0.8537\n",
      "Epoch 36/77, Train Loss: 1.1281, Test Loss: 0.8531\n",
      "Epoch 37/77, Train Loss: 1.1491, Test Loss: 0.8534\n",
      "Epoch 38/77, Train Loss: 1.1422, Test Loss: 0.8530\n",
      "Epoch 39/77, Train Loss: 1.1214, Test Loss: 0.8550\n",
      "Epoch 40/77, Train Loss: 1.1607, Test Loss: 0.8552\n",
      "Epoch 41/77, Train Loss: 1.1101, Test Loss: 0.8541\n",
      "Epoch 42/77, Train Loss: 1.1342, Test Loss: 0.8536\n",
      "Epoch 43/77, Train Loss: 1.1064, Test Loss: 0.8550\n",
      "Epoch 44/77, Train Loss: 1.0970, Test Loss: 0.8562\n",
      "Epoch 45/77, Train Loss: 1.0937, Test Loss: 0.8567\n",
      "Epoch 46/77, Train Loss: 1.1172, Test Loss: 0.8556\n",
      "Epoch 47/77, Train Loss: 1.1095, Test Loss: 0.8591\n",
      "Epoch 48/77, Train Loss: 1.0856, Test Loss: 0.8604\n",
      "Epoch 49/77, Train Loss: 1.1006, Test Loss: 0.8542\n",
      "Epoch 50/77, Train Loss: 1.0812, Test Loss: 0.8537\n",
      "Epoch 51/77, Train Loss: 1.0595, Test Loss: 0.8554\n",
      "Epoch 52/77, Train Loss: 1.1195, Test Loss: 0.8563\n",
      "Epoch 53/77, Train Loss: 1.0824, Test Loss: 0.8578\n",
      "Epoch 54/77, Train Loss: 1.0858, Test Loss: 0.8589\n",
      "Epoch 55/77, Train Loss: 1.0836, Test Loss: 0.8575\n",
      "Epoch 56/77, Train Loss: 1.1112, Test Loss: 0.8635\n",
      "Epoch 57/77, Train Loss: 1.0985, Test Loss: 0.8603\n",
      "Epoch 58/77, Train Loss: 1.0730, Test Loss: 0.8581\n",
      "Epoch 59/77, Train Loss: 1.0687, Test Loss: 0.8588\n",
      "Epoch 60/77, Train Loss: 1.0981, Test Loss: 0.8615\n",
      "Epoch 61/77, Train Loss: 1.0469, Test Loss: 0.8635\n",
      "Epoch 62/77, Train Loss: 1.1067, Test Loss: 0.8643\n",
      "Epoch 63/77, Train Loss: 1.1024, Test Loss: 0.8628\n",
      "Epoch 64/77, Train Loss: 1.0976, Test Loss: 0.8616\n",
      "Epoch 65/77, Train Loss: 1.1327, Test Loss: 0.8634\n",
      "Epoch 66/77, Train Loss: 1.0725, Test Loss: 0.8670\n",
      "Epoch 67/77, Train Loss: 1.0446, Test Loss: 0.8688\n",
      "Epoch 68/77, Train Loss: 1.0589, Test Loss: 0.8663\n",
      "Epoch 69/77, Train Loss: 1.0685, Test Loss: 0.8673\n",
      "Epoch 70/77, Train Loss: 1.0524, Test Loss: 0.8753\n",
      "Epoch 71/77, Train Loss: 1.0715, Test Loss: 0.8704\n",
      "Epoch 72/77, Train Loss: 1.0845, Test Loss: 0.8710\n",
      "Epoch 73/77, Train Loss: 1.0612, Test Loss: 0.8732\n",
      "Epoch 74/77, Train Loss: 1.0611, Test Loss: 0.8684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:24:39,697] Trial 59 finished with value: 0.8717296570539474 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 148, 'layer_1_size': 125, 'layer_2_size': 248, 'layer_3_size': 231, 'layer_4_size': 157, 'layer_5_size': 236, 'layer_6_size': 155, 'layer_7_size': 70, 'dropout_rate': 0.4731224701050273, 'learning_rate': 5.30441606877704e-05, 'batch_size': 64, 'epochs': 77}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/77, Train Loss: 1.0793, Test Loss: 0.8702\n",
      "Epoch 76/77, Train Loss: 1.0771, Test Loss: 0.8699\n",
      "Epoch 77/77, Train Loss: 1.1003, Test Loss: 0.8717\n",
      "Epoch 1/23, Train Loss: 1.2054, Test Loss: 0.9974\n",
      "Epoch 2/23, Train Loss: 1.1910, Test Loss: 1.0260\n",
      "Epoch 3/23, Train Loss: 1.2423, Test Loss: 1.0267\n",
      "Epoch 4/23, Train Loss: 1.1615, Test Loss: 1.0274\n",
      "Epoch 5/23, Train Loss: 1.1945, Test Loss: 1.0237\n",
      "Epoch 6/23, Train Loss: 1.2280, Test Loss: 1.0282\n",
      "Epoch 7/23, Train Loss: 1.1969, Test Loss: 1.0124\n",
      "Epoch 8/23, Train Loss: 1.1172, Test Loss: 1.0221\n",
      "Epoch 9/23, Train Loss: 1.1717, Test Loss: 1.0156\n",
      "Epoch 10/23, Train Loss: 1.1571, Test Loss: 1.0128\n",
      "Epoch 11/23, Train Loss: 1.1246, Test Loss: 1.0079\n",
      "Epoch 12/23, Train Loss: 1.1542, Test Loss: 1.0031\n",
      "Epoch 13/23, Train Loss: 1.2027, Test Loss: 1.0024\n",
      "Epoch 14/23, Train Loss: 1.1833, Test Loss: 0.9962\n",
      "Epoch 15/23, Train Loss: 1.1717, Test Loss: 0.9919\n",
      "Epoch 16/23, Train Loss: 1.1988, Test Loss: 0.9923\n",
      "Epoch 17/23, Train Loss: 1.1606, Test Loss: 1.0009\n",
      "Epoch 18/23, Train Loss: 1.2167, Test Loss: 0.9977\n",
      "Epoch 19/23, Train Loss: 1.1580, Test Loss: 0.9977\n",
      "Epoch 20/23, Train Loss: 1.1772, Test Loss: 0.9964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:24:41,961] Trial 60 finished with value: 0.9959601312875748 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 109, 'layer_1_size': 155, 'layer_2_size': 215, 'layer_3_size': 174, 'layer_4_size': 204, 'layer_5_size': 241, 'layer_6_size': 187, 'dropout_rate': 0.35747016482685623, 'learning_rate': 3.613778189211156e-05, 'batch_size': 64, 'epochs': 23}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/23, Train Loss: 1.1030, Test Loss: 0.9993\n",
      "Epoch 22/23, Train Loss: 1.1322, Test Loss: 1.0017\n",
      "Epoch 23/23, Train Loss: 1.1808, Test Loss: 0.9960\n",
      "Epoch 1/85, Train Loss: 1.2280, Test Loss: 1.0902\n",
      "Epoch 2/85, Train Loss: 1.2388, Test Loss: 1.0890\n",
      "Epoch 3/85, Train Loss: 1.2027, Test Loss: 1.0865\n",
      "Epoch 4/85, Train Loss: 1.2230, Test Loss: 1.0933\n",
      "Epoch 5/85, Train Loss: 1.1914, Test Loss: 1.0960\n",
      "Epoch 6/85, Train Loss: 1.1488, Test Loss: 1.0930\n",
      "Epoch 7/85, Train Loss: 1.2407, Test Loss: 1.0930\n",
      "Epoch 8/85, Train Loss: 1.2207, Test Loss: 1.0958\n",
      "Epoch 9/85, Train Loss: 1.2148, Test Loss: 1.0941\n",
      "Epoch 10/85, Train Loss: 1.1754, Test Loss: 1.0911\n",
      "Epoch 11/85, Train Loss: 1.1701, Test Loss: 1.0918\n",
      "Epoch 12/85, Train Loss: 1.1941, Test Loss: 1.0918\n",
      "Epoch 13/85, Train Loss: 1.1799, Test Loss: 1.0939\n",
      "Epoch 14/85, Train Loss: 1.1651, Test Loss: 1.0953\n",
      "Epoch 15/85, Train Loss: 1.1846, Test Loss: 1.0970\n",
      "Epoch 16/85, Train Loss: 1.1842, Test Loss: 1.0962\n",
      "Epoch 17/85, Train Loss: 1.1895, Test Loss: 1.0982\n",
      "Epoch 18/85, Train Loss: 1.1585, Test Loss: 1.0999\n",
      "Epoch 19/85, Train Loss: 1.1497, Test Loss: 1.0967\n",
      "Epoch 20/85, Train Loss: 1.1018, Test Loss: 1.0983\n",
      "Epoch 21/85, Train Loss: 1.1488, Test Loss: 1.0967\n",
      "Epoch 22/85, Train Loss: 1.1253, Test Loss: 1.0960\n",
      "Epoch 23/85, Train Loss: 1.1236, Test Loss: 1.0972\n",
      "Epoch 24/85, Train Loss: 1.1243, Test Loss: 1.0995\n",
      "Epoch 25/85, Train Loss: 1.1122, Test Loss: 1.1021\n",
      "Epoch 26/85, Train Loss: 1.0579, Test Loss: 1.1039\n",
      "Epoch 27/85, Train Loss: 1.1226, Test Loss: 1.0951\n",
      "Epoch 28/85, Train Loss: 1.0681, Test Loss: 1.0998\n",
      "Epoch 29/85, Train Loss: 1.1021, Test Loss: 1.1006\n",
      "Epoch 30/85, Train Loss: 1.1079, Test Loss: 1.1000\n",
      "Epoch 31/85, Train Loss: 1.1652, Test Loss: 1.0997\n",
      "Epoch 32/85, Train Loss: 1.1105, Test Loss: 1.0989\n",
      "Epoch 33/85, Train Loss: 1.0716, Test Loss: 1.0983\n",
      "Epoch 34/85, Train Loss: 1.0841, Test Loss: 1.1019\n",
      "Epoch 35/85, Train Loss: 1.1127, Test Loss: 1.1024\n",
      "Epoch 36/85, Train Loss: 1.0945, Test Loss: 1.1005\n",
      "Epoch 37/85, Train Loss: 1.0757, Test Loss: 1.0998\n",
      "Epoch 38/85, Train Loss: 1.0983, Test Loss: 1.0975\n",
      "Epoch 39/85, Train Loss: 1.1496, Test Loss: 1.1018\n",
      "Epoch 40/85, Train Loss: 1.0442, Test Loss: 1.1014\n",
      "Epoch 41/85, Train Loss: 1.0918, Test Loss: 1.0973\n",
      "Epoch 42/85, Train Loss: 1.0864, Test Loss: 1.0996\n",
      "Epoch 43/85, Train Loss: 1.0454, Test Loss: 1.1031\n",
      "Epoch 44/85, Train Loss: 1.0836, Test Loss: 1.1052\n",
      "Epoch 45/85, Train Loss: 1.1094, Test Loss: 1.1001\n",
      "Epoch 46/85, Train Loss: 1.1222, Test Loss: 1.0977\n",
      "Epoch 47/85, Train Loss: 1.1265, Test Loss: 1.1017\n",
      "Epoch 48/85, Train Loss: 1.0762, Test Loss: 1.1041\n",
      "Epoch 49/85, Train Loss: 1.1041, Test Loss: 1.1006\n",
      "Epoch 50/85, Train Loss: 1.0729, Test Loss: 1.0989\n",
      "Epoch 51/85, Train Loss: 1.0473, Test Loss: 1.0987\n",
      "Epoch 52/85, Train Loss: 1.1277, Test Loss: 1.0974\n",
      "Epoch 53/85, Train Loss: 1.0898, Test Loss: 1.0970\n",
      "Epoch 54/85, Train Loss: 1.0822, Test Loss: 1.0990\n",
      "Epoch 55/85, Train Loss: 1.0961, Test Loss: 1.0991\n",
      "Epoch 56/85, Train Loss: 1.0887, Test Loss: 1.1007\n",
      "Epoch 57/85, Train Loss: 1.0899, Test Loss: 1.0981\n",
      "Epoch 58/85, Train Loss: 1.0807, Test Loss: 1.0985\n",
      "Epoch 59/85, Train Loss: 1.0800, Test Loss: 1.1005\n",
      "Epoch 60/85, Train Loss: 1.0294, Test Loss: 1.0982\n",
      "Epoch 61/85, Train Loss: 1.0488, Test Loss: 1.1002\n",
      "Epoch 62/85, Train Loss: 1.0742, Test Loss: 1.1025\n",
      "Epoch 63/85, Train Loss: 1.0815, Test Loss: 1.1019\n",
      "Epoch 64/85, Train Loss: 1.0666, Test Loss: 1.1057\n",
      "Epoch 65/85, Train Loss: 1.0434, Test Loss: 1.1049\n",
      "Epoch 66/85, Train Loss: 1.0309, Test Loss: 1.1056\n",
      "Epoch 67/85, Train Loss: 1.0371, Test Loss: 1.1040\n",
      "Epoch 68/85, Train Loss: 1.0747, Test Loss: 1.1028\n",
      "Epoch 69/85, Train Loss: 1.0574, Test Loss: 1.1069\n",
      "Epoch 70/85, Train Loss: 1.0815, Test Loss: 1.1056\n",
      "Epoch 71/85, Train Loss: 1.0175, Test Loss: 1.1069\n",
      "Epoch 72/85, Train Loss: 1.0528, Test Loss: 1.1027\n",
      "Epoch 73/85, Train Loss: 1.0748, Test Loss: 1.1045\n",
      "Epoch 74/85, Train Loss: 1.0421, Test Loss: 1.1010\n",
      "Epoch 75/85, Train Loss: 1.0735, Test Loss: 1.1041\n",
      "Epoch 76/85, Train Loss: 1.0144, Test Loss: 1.1007\n",
      "Epoch 77/85, Train Loss: 1.0428, Test Loss: 1.1013\n",
      "Epoch 78/85, Train Loss: 1.0332, Test Loss: 1.0981\n",
      "Epoch 79/85, Train Loss: 1.0557, Test Loss: 1.1021\n",
      "Epoch 80/85, Train Loss: 1.0136, Test Loss: 1.1012\n",
      "Epoch 81/85, Train Loss: 1.0661, Test Loss: 1.0995\n",
      "Epoch 82/85, Train Loss: 1.0120, Test Loss: 1.1036\n",
      "Epoch 83/85, Train Loss: 1.0065, Test Loss: 1.1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:24:50,081] Trial 61 finished with value: 1.0971640348434448 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 140, 'layer_1_size': 122, 'layer_2_size': 224, 'layer_3_size': 236, 'layer_4_size': 162, 'layer_5_size': 231, 'layer_6_size': 155, 'dropout_rate': 0.47593454339303987, 'learning_rate': 5.7457091091588796e-05, 'batch_size': 64, 'epochs': 85}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/85, Train Loss: 1.0772, Test Loss: 1.0999\n",
      "Epoch 85/85, Train Loss: 1.0118, Test Loss: 1.0972\n",
      "Epoch 1/97, Train Loss: 1.2727, Test Loss: 0.9663\n",
      "Epoch 2/97, Train Loss: 1.2989, Test Loss: 0.9952\n",
      "Epoch 3/97, Train Loss: 1.2878, Test Loss: 1.0057\n",
      "Epoch 4/97, Train Loss: 1.3000, Test Loss: 0.9790\n",
      "Epoch 5/97, Train Loss: 1.2926, Test Loss: 0.9871\n",
      "Epoch 6/97, Train Loss: 1.2391, Test Loss: 0.9855\n",
      "Epoch 7/97, Train Loss: 1.1774, Test Loss: 0.9838\n",
      "Epoch 8/97, Train Loss: 1.2942, Test Loss: 0.9783\n",
      "Epoch 9/97, Train Loss: 1.2677, Test Loss: 0.9539\n",
      "Epoch 10/97, Train Loss: 1.3050, Test Loss: 0.9677\n",
      "Epoch 11/97, Train Loss: 1.2810, Test Loss: 0.9967\n",
      "Epoch 12/97, Train Loss: 1.2915, Test Loss: 1.0001\n",
      "Epoch 13/97, Train Loss: 1.2702, Test Loss: 0.9987\n",
      "Epoch 14/97, Train Loss: 1.2491, Test Loss: 0.9898\n",
      "Epoch 15/97, Train Loss: 1.2449, Test Loss: 0.9950\n",
      "Epoch 16/97, Train Loss: 1.2700, Test Loss: 0.9917\n",
      "Epoch 17/97, Train Loss: 1.2297, Test Loss: 0.9777\n",
      "Epoch 18/97, Train Loss: 1.1941, Test Loss: 0.9871\n",
      "Epoch 19/97, Train Loss: 1.2671, Test Loss: 0.9973\n",
      "Epoch 20/97, Train Loss: 1.2286, Test Loss: 0.9719\n",
      "Epoch 21/97, Train Loss: 1.1114, Test Loss: 0.9674\n",
      "Epoch 22/97, Train Loss: 1.2101, Test Loss: 0.9743\n",
      "Epoch 23/97, Train Loss: 1.1701, Test Loss: 0.9689\n",
      "Epoch 24/97, Train Loss: 1.2379, Test Loss: 0.9782\n",
      "Epoch 25/97, Train Loss: 1.2459, Test Loss: 0.9698\n",
      "Epoch 26/97, Train Loss: 1.2051, Test Loss: 0.9856\n",
      "Epoch 27/97, Train Loss: 1.2650, Test Loss: 0.9799\n",
      "Epoch 28/97, Train Loss: 1.2168, Test Loss: 0.9666\n",
      "Epoch 29/97, Train Loss: 1.2176, Test Loss: 0.9670\n",
      "Epoch 30/97, Train Loss: 1.1903, Test Loss: 0.9670\n",
      "Epoch 31/97, Train Loss: 1.1904, Test Loss: 0.9804\n",
      "Epoch 32/97, Train Loss: 1.1666, Test Loss: 0.9872\n",
      "Epoch 33/97, Train Loss: 1.1792, Test Loss: 0.9921\n",
      "Epoch 34/97, Train Loss: 1.2087, Test Loss: 0.9706\n",
      "Epoch 35/97, Train Loss: 1.1766, Test Loss: 0.9728\n",
      "Epoch 36/97, Train Loss: 1.1842, Test Loss: 0.9749\n",
      "Epoch 37/97, Train Loss: 1.1914, Test Loss: 0.9691\n",
      "Epoch 38/97, Train Loss: 1.1541, Test Loss: 0.9732\n",
      "Epoch 39/97, Train Loss: 1.1709, Test Loss: 0.9704\n",
      "Epoch 40/97, Train Loss: 1.1651, Test Loss: 0.9723\n",
      "Epoch 41/97, Train Loss: 1.1682, Test Loss: 0.9726\n",
      "Epoch 42/97, Train Loss: 1.2224, Test Loss: 0.9631\n",
      "Epoch 43/97, Train Loss: 1.1436, Test Loss: 0.9578\n",
      "Epoch 44/97, Train Loss: 1.1870, Test Loss: 0.9504\n",
      "Epoch 45/97, Train Loss: 1.2565, Test Loss: 0.9504\n",
      "Epoch 46/97, Train Loss: 1.1933, Test Loss: 0.9583\n",
      "Epoch 47/97, Train Loss: 1.1821, Test Loss: 0.9524\n",
      "Epoch 48/97, Train Loss: 1.1840, Test Loss: 0.9588\n",
      "Epoch 49/97, Train Loss: 1.1732, Test Loss: 0.9593\n",
      "Epoch 50/97, Train Loss: 1.1659, Test Loss: 0.9821\n",
      "Epoch 51/97, Train Loss: 1.1827, Test Loss: 0.9828\n",
      "Epoch 52/97, Train Loss: 1.1663, Test Loss: 0.9629\n",
      "Epoch 53/97, Train Loss: 1.1799, Test Loss: 0.9685\n",
      "Epoch 54/97, Train Loss: 1.1692, Test Loss: 0.9613\n",
      "Epoch 55/97, Train Loss: 1.1642, Test Loss: 0.9662\n",
      "Epoch 56/97, Train Loss: 1.1760, Test Loss: 0.9591\n",
      "Epoch 57/97, Train Loss: 1.1344, Test Loss: 0.9712\n",
      "Epoch 58/97, Train Loss: 1.1981, Test Loss: 0.9777\n",
      "Epoch 59/97, Train Loss: 1.1221, Test Loss: 0.9633\n",
      "Epoch 60/97, Train Loss: 1.1536, Test Loss: 0.9766\n",
      "Epoch 61/97, Train Loss: 1.1737, Test Loss: 0.9708\n",
      "Epoch 62/97, Train Loss: 1.1382, Test Loss: 0.9549\n",
      "Epoch 63/97, Train Loss: 1.1775, Test Loss: 0.9547\n",
      "Epoch 64/97, Train Loss: 1.1119, Test Loss: 0.9636\n",
      "Epoch 65/97, Train Loss: 1.1052, Test Loss: 0.9680\n",
      "Epoch 66/97, Train Loss: 1.1394, Test Loss: 0.9684\n",
      "Epoch 67/97, Train Loss: 1.2112, Test Loss: 0.9723\n",
      "Epoch 68/97, Train Loss: 1.0986, Test Loss: 0.9588\n",
      "Epoch 69/97, Train Loss: 1.1435, Test Loss: 0.9707\n",
      "Epoch 70/97, Train Loss: 1.1488, Test Loss: 0.9749\n",
      "Epoch 71/97, Train Loss: 1.1186, Test Loss: 0.9714\n",
      "Epoch 72/97, Train Loss: 1.2042, Test Loss: 0.9707\n",
      "Epoch 73/97, Train Loss: 1.1362, Test Loss: 0.9576\n",
      "Epoch 74/97, Train Loss: 1.1584, Test Loss: 0.9801\n",
      "Epoch 75/97, Train Loss: 1.1198, Test Loss: 0.9799\n",
      "Epoch 76/97, Train Loss: 1.1656, Test Loss: 0.9626\n",
      "Epoch 77/97, Train Loss: 1.1272, Test Loss: 0.9574\n",
      "Epoch 78/97, Train Loss: 1.1062, Test Loss: 0.9829\n",
      "Epoch 79/97, Train Loss: 1.1253, Test Loss: 0.9747\n",
      "Epoch 80/97, Train Loss: 1.0705, Test Loss: 0.9739\n",
      "Epoch 81/97, Train Loss: 1.1488, Test Loss: 0.9665\n",
      "Epoch 82/97, Train Loss: 1.1595, Test Loss: 0.9618\n",
      "Epoch 83/97, Train Loss: 1.1385, Test Loss: 0.9637\n",
      "Epoch 84/97, Train Loss: 1.1426, Test Loss: 0.9643\n",
      "Epoch 85/97, Train Loss: 1.1633, Test Loss: 0.9573\n",
      "Epoch 86/97, Train Loss: 1.1214, Test Loss: 0.9607\n",
      "Epoch 87/97, Train Loss: 1.1114, Test Loss: 0.9666\n",
      "Epoch 88/97, Train Loss: 1.1629, Test Loss: 0.9629\n",
      "Epoch 89/97, Train Loss: 1.1040, Test Loss: 0.9658\n",
      "Epoch 90/97, Train Loss: 1.1340, Test Loss: 0.9614\n",
      "Epoch 91/97, Train Loss: 1.1188, Test Loss: 0.9867\n",
      "Epoch 92/97, Train Loss: 1.0780, Test Loss: 0.9674\n",
      "Epoch 93/97, Train Loss: 1.1428, Test Loss: 0.9780\n",
      "Epoch 94/97, Train Loss: 1.1170, Test Loss: 0.9619\n",
      "Epoch 95/97, Train Loss: 1.1857, Test Loss: 0.9704\n",
      "Epoch 96/97, Train Loss: 1.1401, Test Loss: 0.9647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:24:58,585] Trial 62 finished with value: 0.9617799073457718 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 124, 'layer_1_size': 146, 'layer_2_size': 250, 'layer_3_size': 217, 'layer_4_size': 133, 'layer_5_size': 40, 'layer_6_size': 132, 'layer_7_size': 71, 'dropout_rate': 0.457651598704225, 'learning_rate': 2.5511438292204634e-05, 'batch_size': 64, 'epochs': 97}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/97, Train Loss: 1.0900, Test Loss: 0.9618\n",
      "Epoch 1/78, Train Loss: 1.2211, Test Loss: 0.8392\n",
      "Epoch 2/78, Train Loss: 1.2177, Test Loss: 0.8522\n",
      "Epoch 3/78, Train Loss: 1.1962, Test Loss: 0.8593\n",
      "Epoch 4/78, Train Loss: 1.2374, Test Loss: 0.8607\n",
      "Epoch 5/78, Train Loss: 1.1680, Test Loss: 0.8638\n",
      "Epoch 6/78, Train Loss: 1.2086, Test Loss: 0.8565\n",
      "Epoch 7/78, Train Loss: 1.1878, Test Loss: 0.8539\n",
      "Epoch 8/78, Train Loss: 1.1465, Test Loss: 0.8641\n",
      "Epoch 9/78, Train Loss: 1.1610, Test Loss: 0.8616\n",
      "Epoch 10/78, Train Loss: 1.1586, Test Loss: 0.8600\n",
      "Epoch 11/78, Train Loss: 1.1197, Test Loss: 0.8535\n",
      "Epoch 12/78, Train Loss: 1.1038, Test Loss: 0.8542\n",
      "Epoch 13/78, Train Loss: 1.1495, Test Loss: 0.8576\n",
      "Epoch 14/78, Train Loss: 1.1481, Test Loss: 0.8546\n",
      "Epoch 15/78, Train Loss: 1.1748, Test Loss: 0.8501\n",
      "Epoch 16/78, Train Loss: 1.1924, Test Loss: 0.8453\n",
      "Epoch 17/78, Train Loss: 1.0840, Test Loss: 0.8448\n",
      "Epoch 18/78, Train Loss: 1.1272, Test Loss: 0.8472\n",
      "Epoch 19/78, Train Loss: 1.1323, Test Loss: 0.8472\n",
      "Epoch 20/78, Train Loss: 1.1194, Test Loss: 0.8453\n",
      "Epoch 21/78, Train Loss: 1.1787, Test Loss: 0.8435\n",
      "Epoch 22/78, Train Loss: 1.1248, Test Loss: 0.8454\n",
      "Epoch 23/78, Train Loss: 1.0909, Test Loss: 0.8438\n",
      "Epoch 24/78, Train Loss: 1.0957, Test Loss: 0.8428\n",
      "Epoch 25/78, Train Loss: 1.1040, Test Loss: 0.8478\n",
      "Epoch 26/78, Train Loss: 1.0948, Test Loss: 0.8481\n",
      "Epoch 27/78, Train Loss: 1.1388, Test Loss: 0.8492\n",
      "Epoch 28/78, Train Loss: 1.1258, Test Loss: 0.8481\n",
      "Epoch 29/78, Train Loss: 1.0933, Test Loss: 0.8472\n",
      "Epoch 30/78, Train Loss: 1.0778, Test Loss: 0.8494\n",
      "Epoch 31/78, Train Loss: 1.0777, Test Loss: 0.8470\n",
      "Epoch 32/78, Train Loss: 1.0754, Test Loss: 0.8458\n",
      "Epoch 33/78, Train Loss: 1.0927, Test Loss: 0.8459\n",
      "Epoch 34/78, Train Loss: 1.1294, Test Loss: 0.8448\n",
      "Epoch 35/78, Train Loss: 1.0917, Test Loss: 0.8465\n",
      "Epoch 36/78, Train Loss: 1.1098, Test Loss: 0.8450\n",
      "Epoch 37/78, Train Loss: 1.0942, Test Loss: 0.8465\n",
      "Epoch 38/78, Train Loss: 1.0820, Test Loss: 0.8478\n",
      "Epoch 39/78, Train Loss: 1.1144, Test Loss: 0.8443\n",
      "Epoch 40/78, Train Loss: 1.0317, Test Loss: 0.8495\n",
      "Epoch 41/78, Train Loss: 1.0493, Test Loss: 0.8468\n",
      "Epoch 42/78, Train Loss: 1.0881, Test Loss: 0.8498\n",
      "Epoch 43/78, Train Loss: 1.1193, Test Loss: 0.8519\n",
      "Epoch 44/78, Train Loss: 1.1094, Test Loss: 0.8487\n",
      "Epoch 45/78, Train Loss: 1.0928, Test Loss: 0.8505\n",
      "Epoch 46/78, Train Loss: 1.0766, Test Loss: 0.8476\n",
      "Epoch 47/78, Train Loss: 1.0471, Test Loss: 0.8477\n",
      "Epoch 48/78, Train Loss: 1.0569, Test Loss: 0.8440\n",
      "Epoch 49/78, Train Loss: 1.0775, Test Loss: 0.8444\n",
      "Epoch 50/78, Train Loss: 1.1240, Test Loss: 0.8436\n",
      "Epoch 51/78, Train Loss: 1.0776, Test Loss: 0.8456\n",
      "Epoch 52/78, Train Loss: 1.0796, Test Loss: 0.8455\n",
      "Epoch 53/78, Train Loss: 1.1003, Test Loss: 0.8441\n",
      "Epoch 54/78, Train Loss: 1.0629, Test Loss: 0.8433\n",
      "Epoch 55/78, Train Loss: 1.1039, Test Loss: 0.8445\n",
      "Epoch 56/78, Train Loss: 1.0878, Test Loss: 0.8457\n",
      "Epoch 57/78, Train Loss: 1.0570, Test Loss: 0.8415\n",
      "Epoch 58/78, Train Loss: 1.1101, Test Loss: 0.8410\n",
      "Epoch 59/78, Train Loss: 1.0645, Test Loss: 0.8418\n",
      "Epoch 60/78, Train Loss: 1.0878, Test Loss: 0.8429\n",
      "Epoch 61/78, Train Loss: 1.0426, Test Loss: 0.8436\n",
      "Epoch 62/78, Train Loss: 1.0661, Test Loss: 0.8419\n",
      "Epoch 63/78, Train Loss: 1.0122, Test Loss: 0.8444\n",
      "Epoch 64/78, Train Loss: 1.0407, Test Loss: 0.8430\n",
      "Epoch 65/78, Train Loss: 1.0518, Test Loss: 0.8392\n",
      "Epoch 66/78, Train Loss: 1.0616, Test Loss: 0.8444\n",
      "Epoch 67/78, Train Loss: 1.1033, Test Loss: 0.8447\n",
      "Epoch 68/78, Train Loss: 1.0472, Test Loss: 0.8410\n",
      "Epoch 69/78, Train Loss: 1.0666, Test Loss: 0.8437\n",
      "Epoch 70/78, Train Loss: 1.0774, Test Loss: 0.8487\n",
      "Epoch 71/78, Train Loss: 1.0453, Test Loss: 0.8490\n",
      "Epoch 72/78, Train Loss: 1.0485, Test Loss: 0.8427\n",
      "Epoch 73/78, Train Loss: 1.1189, Test Loss: 0.8453\n",
      "Epoch 74/78, Train Loss: 1.0231, Test Loss: 0.8457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:25:05,137] Trial 63 finished with value: 0.8482570424675941 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 163, 'layer_1_size': 123, 'layer_2_size': 234, 'layer_3_size': 207, 'layer_4_size': 223, 'layer_5_size': 256, 'dropout_rate': 0.4365018141150898, 'learning_rate': 5.135892605561561e-05, 'batch_size': 64, 'epochs': 78}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/78, Train Loss: 1.0354, Test Loss: 0.8462\n",
      "Epoch 76/78, Train Loss: 1.0691, Test Loss: 0.8441\n",
      "Epoch 77/78, Train Loss: 1.0529, Test Loss: 0.8487\n",
      "Epoch 78/78, Train Loss: 1.0551, Test Loss: 0.8483\n",
      "Epoch 1/66, Train Loss: 1.2987, Test Loss: 1.0055\n",
      "Epoch 2/66, Train Loss: 1.1952, Test Loss: 1.0163\n",
      "Epoch 3/66, Train Loss: 1.2566, Test Loss: 1.0286\n",
      "Epoch 4/66, Train Loss: 1.2301, Test Loss: 1.0308\n",
      "Epoch 5/66, Train Loss: 1.1608, Test Loss: 1.0448\n",
      "Epoch 6/66, Train Loss: 1.1563, Test Loss: 1.0404\n",
      "Epoch 7/66, Train Loss: 1.2347, Test Loss: 1.0515\n",
      "Epoch 8/66, Train Loss: 1.2158, Test Loss: 1.0499\n",
      "Epoch 9/66, Train Loss: 1.1892, Test Loss: 1.0564\n",
      "Epoch 10/66, Train Loss: 1.1569, Test Loss: 1.0532\n",
      "Epoch 11/66, Train Loss: 1.1395, Test Loss: 1.0403\n",
      "Epoch 12/66, Train Loss: 1.1406, Test Loss: 1.0443\n",
      "Epoch 13/66, Train Loss: 1.1971, Test Loss: 1.0470\n",
      "Epoch 14/66, Train Loss: 1.1467, Test Loss: 1.0557\n",
      "Epoch 15/66, Train Loss: 1.1564, Test Loss: 1.0600\n",
      "Epoch 16/66, Train Loss: 1.1767, Test Loss: 1.0619\n",
      "Epoch 17/66, Train Loss: 1.1761, Test Loss: 1.0789\n",
      "Epoch 18/66, Train Loss: 1.1245, Test Loss: 1.0872\n",
      "Epoch 19/66, Train Loss: 1.1117, Test Loss: 1.0754\n",
      "Epoch 20/66, Train Loss: 1.1343, Test Loss: 1.0763\n",
      "Epoch 21/66, Train Loss: 1.1114, Test Loss: 1.0811\n",
      "Epoch 22/66, Train Loss: 1.0922, Test Loss: 1.0767\n",
      "Epoch 23/66, Train Loss: 1.0950, Test Loss: 1.0688\n",
      "Epoch 24/66, Train Loss: 1.1145, Test Loss: 1.0581\n",
      "Epoch 25/66, Train Loss: 1.1178, Test Loss: 1.0663\n",
      "Epoch 26/66, Train Loss: 1.1040, Test Loss: 1.0621\n",
      "Epoch 27/66, Train Loss: 1.1317, Test Loss: 1.0566\n",
      "Epoch 28/66, Train Loss: 1.1194, Test Loss: 1.0634\n",
      "Epoch 29/66, Train Loss: 1.1017, Test Loss: 1.0626\n",
      "Epoch 30/66, Train Loss: 1.0889, Test Loss: 1.0492\n",
      "Epoch 31/66, Train Loss: 1.1150, Test Loss: 1.0593\n",
      "Epoch 32/66, Train Loss: 1.1224, Test Loss: 1.0730\n",
      "Epoch 33/66, Train Loss: 1.0788, Test Loss: 1.0643\n",
      "Epoch 34/66, Train Loss: 1.1098, Test Loss: 1.0726\n",
      "Epoch 35/66, Train Loss: 1.0554, Test Loss: 1.0774\n",
      "Epoch 36/66, Train Loss: 1.1263, Test Loss: 1.0886\n",
      "Epoch 37/66, Train Loss: 1.0735, Test Loss: 1.0846\n",
      "Epoch 38/66, Train Loss: 1.1016, Test Loss: 1.0851\n",
      "Epoch 39/66, Train Loss: 1.1184, Test Loss: 1.0938\n",
      "Epoch 40/66, Train Loss: 1.0896, Test Loss: 1.0943\n",
      "Epoch 41/66, Train Loss: 1.0532, Test Loss: 1.0876\n",
      "Epoch 42/66, Train Loss: 1.0916, Test Loss: 1.0841\n",
      "Epoch 43/66, Train Loss: 1.0991, Test Loss: 1.0839\n",
      "Epoch 44/66, Train Loss: 1.0767, Test Loss: 1.0757\n",
      "Epoch 45/66, Train Loss: 1.0986, Test Loss: 1.0676\n",
      "Epoch 46/66, Train Loss: 1.0827, Test Loss: 1.0689\n",
      "Epoch 47/66, Train Loss: 1.0940, Test Loss: 1.0699\n",
      "Epoch 48/66, Train Loss: 1.1046, Test Loss: 1.0657\n",
      "Epoch 49/66, Train Loss: 1.0840, Test Loss: 1.0659\n",
      "Epoch 50/66, Train Loss: 1.0787, Test Loss: 1.0717\n",
      "Epoch 51/66, Train Loss: 1.0723, Test Loss: 1.0814\n",
      "Epoch 52/66, Train Loss: 1.0910, Test Loss: 1.0760\n",
      "Epoch 53/66, Train Loss: 1.0666, Test Loss: 1.0961\n",
      "Epoch 54/66, Train Loss: 1.1088, Test Loss: 1.0989\n",
      "Epoch 55/66, Train Loss: 1.0978, Test Loss: 1.0795\n",
      "Epoch 56/66, Train Loss: 1.0796, Test Loss: 1.0773\n",
      "Epoch 57/66, Train Loss: 1.1101, Test Loss: 1.0846\n",
      "Epoch 58/66, Train Loss: 1.0522, Test Loss: 1.0869\n",
      "Epoch 59/66, Train Loss: 1.0821, Test Loss: 1.0993\n",
      "Epoch 60/66, Train Loss: 1.0401, Test Loss: 1.1015\n",
      "Epoch 61/66, Train Loss: 1.0789, Test Loss: 1.0983\n",
      "Epoch 62/66, Train Loss: 1.1091, Test Loss: 1.0994\n",
      "Epoch 63/66, Train Loss: 1.0839, Test Loss: 1.0927\n",
      "Epoch 64/66, Train Loss: 1.0870, Test Loss: 1.0924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:25:09,921] Trial 64 finished with value: 1.092351347208023 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 164, 'layer_1_size': 111, 'layer_2_size': 237, 'layer_3_size': 208, 'layer_4_size': 222, 'layer_5_size': 256, 'dropout_rate': 0.4370737703555692, 'learning_rate': 0.00011679919705779412, 'batch_size': 64, 'epochs': 66}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/66, Train Loss: 1.0706, Test Loss: 1.0931\n",
      "Epoch 66/66, Train Loss: 1.0417, Test Loss: 1.0924\n",
      "Epoch 1/30, Train Loss: 1.2760, Test Loss: 1.1731\n",
      "Epoch 2/30, Train Loss: 1.1836, Test Loss: 1.1733\n",
      "Epoch 3/30, Train Loss: 1.2353, Test Loss: 1.1735\n",
      "Epoch 4/30, Train Loss: 1.2492, Test Loss: 1.1746\n",
      "Epoch 5/30, Train Loss: 1.1816, Test Loss: 1.1747\n",
      "Epoch 6/30, Train Loss: 1.2295, Test Loss: 1.1756\n",
      "Epoch 7/30, Train Loss: 1.2003, Test Loss: 1.1747\n",
      "Epoch 8/30, Train Loss: 1.1662, Test Loss: 1.1748\n",
      "Epoch 9/30, Train Loss: 1.2117, Test Loss: 1.1742\n",
      "Epoch 10/30, Train Loss: 1.1850, Test Loss: 1.1747\n",
      "Epoch 11/30, Train Loss: 1.1758, Test Loss: 1.1744\n",
      "Epoch 12/30, Train Loss: 1.1723, Test Loss: 1.1727\n",
      "Epoch 13/30, Train Loss: 1.2000, Test Loss: 1.1732\n",
      "Epoch 14/30, Train Loss: 1.2013, Test Loss: 1.1736\n",
      "Epoch 15/30, Train Loss: 1.2060, Test Loss: 1.1732\n",
      "Epoch 16/30, Train Loss: 1.1621, Test Loss: 1.1720\n",
      "Epoch 17/30, Train Loss: 1.1523, Test Loss: 1.1726\n",
      "Epoch 18/30, Train Loss: 1.1557, Test Loss: 1.1719\n",
      "Epoch 19/30, Train Loss: 1.1611, Test Loss: 1.1715\n",
      "Epoch 20/30, Train Loss: 1.1978, Test Loss: 1.1718\n",
      "Epoch 21/30, Train Loss: 1.1841, Test Loss: 1.1723\n",
      "Epoch 22/30, Train Loss: 1.1555, Test Loss: 1.1728\n",
      "Epoch 23/30, Train Loss: 1.1326, Test Loss: 1.1733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:25:11,182] Trial 65 finished with value: 1.1721397638320923 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 177, 'layer_1_size': 163, 'layer_2_size': 200, 'layer_3_size': 188, 'layer_4_size': 237, 'layer_5_size': 211, 'dropout_rate': 0.39193408587562734, 'learning_rate': 7.229670780560231e-05, 'batch_size': 256, 'epochs': 30}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30, Train Loss: 1.1852, Test Loss: 1.1733\n",
      "Epoch 25/30, Train Loss: 1.1488, Test Loss: 1.1737\n",
      "Epoch 26/30, Train Loss: 1.1548, Test Loss: 1.1738\n",
      "Epoch 27/30, Train Loss: 1.1485, Test Loss: 1.1738\n",
      "Epoch 28/30, Train Loss: 1.1959, Test Loss: 1.1734\n",
      "Epoch 29/30, Train Loss: 1.1531, Test Loss: 1.1728\n",
      "Epoch 30/30, Train Loss: 1.1654, Test Loss: 1.1721\n",
      "Epoch 1/90, Train Loss: 1.2133, Test Loss: 1.0265\n",
      "Epoch 2/90, Train Loss: 1.2428, Test Loss: 1.0054\n",
      "Epoch 3/90, Train Loss: 1.2910, Test Loss: 1.0050\n",
      "Epoch 4/90, Train Loss: 1.1983, Test Loss: 1.0075\n",
      "Epoch 5/90, Train Loss: 1.2108, Test Loss: 1.0020\n",
      "Epoch 6/90, Train Loss: 1.1979, Test Loss: 1.0034\n",
      "Epoch 7/90, Train Loss: 1.2225, Test Loss: 1.0037\n",
      "Epoch 8/90, Train Loss: 1.2042, Test Loss: 0.9953\n",
      "Epoch 9/90, Train Loss: 1.1849, Test Loss: 0.9939\n",
      "Epoch 10/90, Train Loss: 1.1465, Test Loss: 0.9981\n",
      "Epoch 11/90, Train Loss: 1.1287, Test Loss: 0.9956\n",
      "Epoch 12/90, Train Loss: 1.1616, Test Loss: 0.9983\n",
      "Epoch 13/90, Train Loss: 1.1559, Test Loss: 0.9984\n",
      "Epoch 14/90, Train Loss: 1.1520, Test Loss: 0.9969\n",
      "Epoch 15/90, Train Loss: 1.1415, Test Loss: 0.9916\n",
      "Epoch 16/90, Train Loss: 1.1693, Test Loss: 0.9898\n",
      "Epoch 17/90, Train Loss: 1.1944, Test Loss: 0.9913\n",
      "Epoch 18/90, Train Loss: 1.1248, Test Loss: 0.9996\n",
      "Epoch 19/90, Train Loss: 1.2036, Test Loss: 0.9910\n",
      "Epoch 20/90, Train Loss: 1.0848, Test Loss: 0.9961\n",
      "Epoch 21/90, Train Loss: 1.1186, Test Loss: 0.9939\n",
      "Epoch 22/90, Train Loss: 1.1914, Test Loss: 0.9927\n",
      "Epoch 23/90, Train Loss: 1.1103, Test Loss: 0.9917\n",
      "Epoch 24/90, Train Loss: 1.1336, Test Loss: 0.9921\n",
      "Epoch 25/90, Train Loss: 1.0943, Test Loss: 0.9958\n",
      "Epoch 26/90, Train Loss: 1.1030, Test Loss: 0.9908\n",
      "Epoch 27/90, Train Loss: 1.1138, Test Loss: 0.9942\n",
      "Epoch 28/90, Train Loss: 1.1813, Test Loss: 0.9895\n",
      "Epoch 29/90, Train Loss: 1.1049, Test Loss: 0.9910\n",
      "Epoch 30/90, Train Loss: 1.1133, Test Loss: 0.9876\n",
      "Epoch 31/90, Train Loss: 1.0891, Test Loss: 0.9902\n",
      "Epoch 32/90, Train Loss: 1.1200, Test Loss: 0.9903\n",
      "Epoch 33/90, Train Loss: 1.1459, Test Loss: 0.9910\n",
      "Epoch 34/90, Train Loss: 1.1353, Test Loss: 0.9872\n",
      "Epoch 35/90, Train Loss: 1.1056, Test Loss: 0.9923\n",
      "Epoch 36/90, Train Loss: 1.1314, Test Loss: 0.9918\n",
      "Epoch 37/90, Train Loss: 1.1192, Test Loss: 0.9911\n",
      "Epoch 38/90, Train Loss: 1.1016, Test Loss: 0.9936\n",
      "Epoch 39/90, Train Loss: 1.1211, Test Loss: 0.9910\n",
      "Epoch 40/90, Train Loss: 1.1410, Test Loss: 0.9934\n",
      "Epoch 41/90, Train Loss: 1.1381, Test Loss: 0.9966\n",
      "Epoch 42/90, Train Loss: 1.1177, Test Loss: 0.9957\n",
      "Epoch 43/90, Train Loss: 1.0932, Test Loss: 0.9918\n",
      "Epoch 44/90, Train Loss: 1.0882, Test Loss: 0.9988\n",
      "Epoch 45/90, Train Loss: 1.1095, Test Loss: 0.9978\n",
      "Epoch 46/90, Train Loss: 1.1181, Test Loss: 0.9901\n",
      "Epoch 47/90, Train Loss: 1.1072, Test Loss: 0.9951\n",
      "Epoch 48/90, Train Loss: 1.0495, Test Loss: 0.9909\n",
      "Epoch 49/90, Train Loss: 1.0886, Test Loss: 0.9871\n",
      "Epoch 50/90, Train Loss: 1.1190, Test Loss: 0.9926\n",
      "Epoch 51/90, Train Loss: 1.1366, Test Loss: 0.9922\n",
      "Epoch 52/90, Train Loss: 1.1135, Test Loss: 0.9949\n",
      "Epoch 53/90, Train Loss: 1.0945, Test Loss: 1.0004\n",
      "Epoch 54/90, Train Loss: 1.0989, Test Loss: 0.9988\n",
      "Epoch 55/90, Train Loss: 1.0866, Test Loss: 0.9940\n",
      "Epoch 56/90, Train Loss: 1.1023, Test Loss: 0.9982\n",
      "Epoch 57/90, Train Loss: 1.0930, Test Loss: 0.9930\n",
      "Epoch 58/90, Train Loss: 1.1030, Test Loss: 0.9929\n",
      "Epoch 59/90, Train Loss: 1.0791, Test Loss: 0.9971\n",
      "Epoch 60/90, Train Loss: 1.1034, Test Loss: 0.9918\n",
      "Epoch 61/90, Train Loss: 1.0830, Test Loss: 0.9940\n",
      "Epoch 62/90, Train Loss: 1.1174, Test Loss: 0.9921\n",
      "Epoch 63/90, Train Loss: 1.0946, Test Loss: 0.9910\n",
      "Epoch 64/90, Train Loss: 1.0595, Test Loss: 0.9928\n",
      "Epoch 65/90, Train Loss: 1.1012, Test Loss: 0.9937\n",
      "Epoch 66/90, Train Loss: 1.1099, Test Loss: 0.9960\n",
      "Epoch 67/90, Train Loss: 1.0867, Test Loss: 0.9950\n",
      "Epoch 68/90, Train Loss: 1.0727, Test Loss: 0.9954\n",
      "Epoch 69/90, Train Loss: 1.1030, Test Loss: 0.9956\n",
      "Epoch 70/90, Train Loss: 1.1205, Test Loss: 0.9949\n",
      "Epoch 71/90, Train Loss: 1.0886, Test Loss: 0.9982\n",
      "Epoch 72/90, Train Loss: 1.0982, Test Loss: 0.9964\n",
      "Epoch 73/90, Train Loss: 1.0860, Test Loss: 0.9976\n",
      "Epoch 74/90, Train Loss: 1.0728, Test Loss: 0.9969\n",
      "Epoch 75/90, Train Loss: 1.0592, Test Loss: 0.9981\n",
      "Epoch 76/90, Train Loss: 1.0747, Test Loss: 0.9962\n",
      "Epoch 77/90, Train Loss: 1.1067, Test Loss: 0.9954\n",
      "Epoch 78/90, Train Loss: 1.0824, Test Loss: 0.9962\n",
      "Epoch 79/90, Train Loss: 1.0854, Test Loss: 0.9952\n",
      "Epoch 80/90, Train Loss: 1.1096, Test Loss: 0.9942\n",
      "Epoch 81/90, Train Loss: 1.0879, Test Loss: 0.9962\n",
      "Epoch 82/90, Train Loss: 1.0867, Test Loss: 0.9942\n",
      "Epoch 83/90, Train Loss: 1.1023, Test Loss: 0.9962\n",
      "Epoch 84/90, Train Loss: 1.0870, Test Loss: 0.9988\n",
      "Epoch 85/90, Train Loss: 1.0919, Test Loss: 1.0009\n",
      "Epoch 86/90, Train Loss: 1.1028, Test Loss: 0.9985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:25:16,224] Trial 66 finished with value: 1.0036491751670837 and parameters: {'num_hidden_layers': 5, 'layer_0_size': 212, 'layer_1_size': 138, 'layer_2_size': 204, 'layer_3_size': 202, 'layer_4_size': 248, 'dropout_rate': 0.41653969989585005, 'learning_rate': 4.121287744617779e-05, 'batch_size': 64, 'epochs': 90}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/90, Train Loss: 1.1028, Test Loss: 0.9953\n",
      "Epoch 88/90, Train Loss: 1.0807, Test Loss: 0.9997\n",
      "Epoch 89/90, Train Loss: 1.0851, Test Loss: 0.9980\n",
      "Epoch 90/90, Train Loss: 1.0659, Test Loss: 1.0036\n",
      "Epoch 1/93, Train Loss: 1.1686, Test Loss: 0.9611\n",
      "Epoch 2/93, Train Loss: 1.1273, Test Loss: 0.9665\n",
      "Epoch 3/93, Train Loss: 1.1461, Test Loss: 0.9683\n",
      "Epoch 4/93, Train Loss: 1.2048, Test Loss: 0.9711\n",
      "Epoch 5/93, Train Loss: 1.2171, Test Loss: 0.9699\n",
      "Epoch 6/93, Train Loss: 1.1360, Test Loss: 0.9692\n",
      "Epoch 7/93, Train Loss: 1.1908, Test Loss: 0.9704\n",
      "Epoch 8/93, Train Loss: 1.2012, Test Loss: 0.9691\n",
      "Epoch 9/93, Train Loss: 1.1002, Test Loss: 0.9708\n",
      "Epoch 10/93, Train Loss: 1.1727, Test Loss: 0.9694\n",
      "Epoch 11/93, Train Loss: 1.1423, Test Loss: 0.9685\n",
      "Epoch 12/93, Train Loss: 1.1616, Test Loss: 0.9688\n",
      "Epoch 13/93, Train Loss: 1.1471, Test Loss: 0.9684\n",
      "Epoch 14/93, Train Loss: 1.1659, Test Loss: 0.9674\n",
      "Epoch 15/93, Train Loss: 1.1451, Test Loss: 0.9668\n",
      "Epoch 16/93, Train Loss: 1.1685, Test Loss: 0.9670\n",
      "Epoch 17/93, Train Loss: 1.1671, Test Loss: 0.9693\n",
      "Epoch 18/93, Train Loss: 1.1640, Test Loss: 0.9673\n",
      "Epoch 19/93, Train Loss: 1.1460, Test Loss: 0.9649\n",
      "Epoch 20/93, Train Loss: 1.1298, Test Loss: 0.9686\n",
      "Epoch 21/93, Train Loss: 1.1562, Test Loss: 0.9680\n",
      "Epoch 22/93, Train Loss: 1.1505, Test Loss: 0.9695\n",
      "Epoch 23/93, Train Loss: 1.0856, Test Loss: 0.9711\n",
      "Epoch 24/93, Train Loss: 1.1464, Test Loss: 0.9669\n",
      "Epoch 25/93, Train Loss: 1.1883, Test Loss: 0.9644\n",
      "Epoch 26/93, Train Loss: 1.1342, Test Loss: 0.9674\n",
      "Epoch 27/93, Train Loss: 1.1584, Test Loss: 0.9701\n",
      "Epoch 28/93, Train Loss: 1.1729, Test Loss: 0.9705\n",
      "Epoch 29/93, Train Loss: 1.0792, Test Loss: 0.9670\n",
      "Epoch 30/93, Train Loss: 1.1811, Test Loss: 0.9685\n",
      "Epoch 31/93, Train Loss: 1.1331, Test Loss: 0.9658\n",
      "Epoch 32/93, Train Loss: 1.1397, Test Loss: 0.9696\n",
      "Epoch 33/93, Train Loss: 1.1180, Test Loss: 0.9676\n",
      "Epoch 34/93, Train Loss: 1.1078, Test Loss: 0.9668\n",
      "Epoch 35/93, Train Loss: 1.1427, Test Loss: 0.9701\n",
      "Epoch 36/93, Train Loss: 1.0749, Test Loss: 0.9686\n",
      "Epoch 37/93, Train Loss: 1.1719, Test Loss: 0.9697\n",
      "Epoch 38/93, Train Loss: 1.1231, Test Loss: 0.9655\n",
      "Epoch 39/93, Train Loss: 1.1192, Test Loss: 0.9663\n",
      "Epoch 40/93, Train Loss: 1.1195, Test Loss: 0.9686\n",
      "Epoch 41/93, Train Loss: 1.1594, Test Loss: 0.9713\n",
      "Epoch 42/93, Train Loss: 1.1224, Test Loss: 0.9721\n",
      "Epoch 43/93, Train Loss: 1.1122, Test Loss: 0.9703\n",
      "Epoch 44/93, Train Loss: 1.0980, Test Loss: 0.9674\n",
      "Epoch 45/93, Train Loss: 1.1013, Test Loss: 0.9669\n",
      "Epoch 46/93, Train Loss: 1.1408, Test Loss: 0.9698\n",
      "Epoch 47/93, Train Loss: 1.1032, Test Loss: 0.9656\n",
      "Epoch 48/93, Train Loss: 1.1508, Test Loss: 0.9699\n",
      "Epoch 49/93, Train Loss: 1.1139, Test Loss: 0.9709\n",
      "Epoch 50/93, Train Loss: 1.1236, Test Loss: 0.9675\n",
      "Epoch 51/93, Train Loss: 1.1034, Test Loss: 0.9674\n",
      "Epoch 52/93, Train Loss: 1.1323, Test Loss: 0.9681\n",
      "Epoch 53/93, Train Loss: 1.1413, Test Loss: 0.9686\n",
      "Epoch 54/93, Train Loss: 1.1044, Test Loss: 0.9706\n",
      "Epoch 55/93, Train Loss: 1.1331, Test Loss: 0.9665\n",
      "Epoch 56/93, Train Loss: 1.0759, Test Loss: 0.9675\n",
      "Epoch 57/93, Train Loss: 1.1207, Test Loss: 0.9659\n",
      "Epoch 58/93, Train Loss: 1.0613, Test Loss: 0.9656\n",
      "Epoch 59/93, Train Loss: 1.0975, Test Loss: 0.9685\n",
      "Epoch 60/93, Train Loss: 1.0864, Test Loss: 0.9708\n",
      "Epoch 61/93, Train Loss: 1.0867, Test Loss: 0.9694\n",
      "Epoch 62/93, Train Loss: 1.0953, Test Loss: 0.9678\n",
      "Epoch 63/93, Train Loss: 1.1026, Test Loss: 0.9708\n",
      "Epoch 64/93, Train Loss: 1.1345, Test Loss: 0.9653\n",
      "Epoch 65/93, Train Loss: 1.1040, Test Loss: 0.9667\n",
      "Epoch 66/93, Train Loss: 1.1184, Test Loss: 0.9661\n",
      "Epoch 67/93, Train Loss: 1.1038, Test Loss: 0.9656\n",
      "Epoch 68/93, Train Loss: 1.1014, Test Loss: 0.9698\n",
      "Epoch 69/93, Train Loss: 1.0986, Test Loss: 0.9679\n",
      "Epoch 70/93, Train Loss: 1.1110, Test Loss: 0.9684\n",
      "Epoch 71/93, Train Loss: 1.0957, Test Loss: 0.9677\n",
      "Epoch 72/93, Train Loss: 1.1003, Test Loss: 0.9680\n",
      "Epoch 73/93, Train Loss: 1.1216, Test Loss: 0.9656\n",
      "Epoch 74/93, Train Loss: 1.0775, Test Loss: 0.9669\n",
      "Epoch 75/93, Train Loss: 1.1220, Test Loss: 0.9669\n",
      "Epoch 76/93, Train Loss: 1.1042, Test Loss: 0.9656\n",
      "Epoch 77/93, Train Loss: 1.0962, Test Loss: 0.9655\n",
      "Epoch 78/93, Train Loss: 1.1039, Test Loss: 0.9686\n",
      "Epoch 79/93, Train Loss: 1.0951, Test Loss: 0.9680\n",
      "Epoch 80/93, Train Loss: 1.0962, Test Loss: 0.9682\n",
      "Epoch 81/93, Train Loss: 1.1210, Test Loss: 0.9649\n",
      "Epoch 82/93, Train Loss: 1.0794, Test Loss: 0.9662\n",
      "Epoch 83/93, Train Loss: 1.1379, Test Loss: 0.9657\n",
      "Epoch 84/93, Train Loss: 1.0987, Test Loss: 0.9666\n",
      "Epoch 85/93, Train Loss: 1.0447, Test Loss: 0.9656\n",
      "Epoch 86/93, Train Loss: 1.0661, Test Loss: 0.9662\n",
      "Epoch 87/93, Train Loss: 1.1062, Test Loss: 0.9650\n",
      "Epoch 88/93, Train Loss: 1.1029, Test Loss: 0.9657\n",
      "Epoch 89/93, Train Loss: 1.0595, Test Loss: 0.9635\n",
      "Epoch 90/93, Train Loss: 1.0983, Test Loss: 0.9680\n",
      "Epoch 91/93, Train Loss: 1.0891, Test Loss: 0.9662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:25:22,451] Trial 67 finished with value: 0.964599221944809 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 97, 'layer_1_size': 169, 'layer_2_size': 189, 'layer_3_size': 98, 'layer_4_size': 105, 'layer_5_size': 245, 'layer_6_size': 110, 'dropout_rate': 0.3786144634738603, 'learning_rate': 2.4212467434247987e-05, 'batch_size': 64, 'epochs': 93}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/93, Train Loss: 1.1147, Test Loss: 0.9651\n",
      "Epoch 93/93, Train Loss: 1.0922, Test Loss: 0.9646\n",
      "Epoch 1/98, Train Loss: 1.2923, Test Loss: 1.0090\n",
      "Epoch 2/98, Train Loss: 1.2406, Test Loss: 1.0067\n",
      "Epoch 3/98, Train Loss: 1.1883, Test Loss: 0.9951\n",
      "Epoch 4/98, Train Loss: 1.2213, Test Loss: 0.9897\n",
      "Epoch 5/98, Train Loss: 1.1980, Test Loss: 0.9818\n",
      "Epoch 6/98, Train Loss: 1.2135, Test Loss: 0.9842\n",
      "Epoch 7/98, Train Loss: 1.2054, Test Loss: 0.9798\n",
      "Epoch 8/98, Train Loss: 1.1807, Test Loss: 0.9712\n",
      "Epoch 9/98, Train Loss: 1.2089, Test Loss: 0.9737\n",
      "Epoch 10/98, Train Loss: 1.1378, Test Loss: 0.9712\n",
      "Epoch 11/98, Train Loss: 1.2234, Test Loss: 0.9715\n",
      "Epoch 12/98, Train Loss: 1.1681, Test Loss: 0.9779\n",
      "Epoch 13/98, Train Loss: 1.1638, Test Loss: 0.9756\n",
      "Epoch 14/98, Train Loss: 1.1707, Test Loss: 0.9756\n",
      "Epoch 15/98, Train Loss: 1.1606, Test Loss: 0.9725\n",
      "Epoch 16/98, Train Loss: 1.2474, Test Loss: 0.9712\n",
      "Epoch 17/98, Train Loss: 1.1942, Test Loss: 0.9691\n",
      "Epoch 18/98, Train Loss: 1.1547, Test Loss: 0.9699\n",
      "Epoch 19/98, Train Loss: 1.1863, Test Loss: 0.9695\n",
      "Epoch 20/98, Train Loss: 1.2222, Test Loss: 0.9693\n",
      "Epoch 21/98, Train Loss: 1.1338, Test Loss: 0.9688\n",
      "Epoch 22/98, Train Loss: 1.1651, Test Loss: 0.9694\n",
      "Epoch 23/98, Train Loss: 1.1689, Test Loss: 0.9719\n",
      "Epoch 24/98, Train Loss: 1.1769, Test Loss: 0.9727\n",
      "Epoch 25/98, Train Loss: 1.1351, Test Loss: 0.9719\n",
      "Epoch 26/98, Train Loss: 1.1476, Test Loss: 0.9695\n",
      "Epoch 27/98, Train Loss: 1.1228, Test Loss: 0.9705\n",
      "Epoch 28/98, Train Loss: 1.1651, Test Loss: 0.9729\n",
      "Epoch 29/98, Train Loss: 1.1907, Test Loss: 0.9739\n",
      "Epoch 30/98, Train Loss: 1.1242, Test Loss: 0.9692\n",
      "Epoch 31/98, Train Loss: 1.1263, Test Loss: 0.9714\n",
      "Epoch 32/98, Train Loss: 1.1573, Test Loss: 0.9755\n",
      "Epoch 33/98, Train Loss: 1.1794, Test Loss: 0.9725\n",
      "Epoch 34/98, Train Loss: 1.1579, Test Loss: 0.9720\n",
      "Epoch 35/98, Train Loss: 1.1126, Test Loss: 0.9711\n",
      "Epoch 36/98, Train Loss: 1.1315, Test Loss: 0.9708\n",
      "Epoch 37/98, Train Loss: 1.1573, Test Loss: 0.9732\n",
      "Epoch 38/98, Train Loss: 1.1432, Test Loss: 0.9702\n",
      "Epoch 39/98, Train Loss: 1.1163, Test Loss: 0.9695\n",
      "Epoch 40/98, Train Loss: 1.1360, Test Loss: 0.9713\n",
      "Epoch 41/98, Train Loss: 1.1317, Test Loss: 0.9764\n",
      "Epoch 42/98, Train Loss: 1.1153, Test Loss: 0.9741\n",
      "Epoch 43/98, Train Loss: 1.1434, Test Loss: 0.9746\n",
      "Epoch 44/98, Train Loss: 1.1325, Test Loss: 0.9768\n",
      "Epoch 45/98, Train Loss: 1.1094, Test Loss: 0.9758\n",
      "Epoch 46/98, Train Loss: 1.0952, Test Loss: 0.9763\n",
      "Epoch 47/98, Train Loss: 1.1140, Test Loss: 0.9782\n",
      "Epoch 48/98, Train Loss: 1.1002, Test Loss: 0.9786\n",
      "Epoch 49/98, Train Loss: 1.0937, Test Loss: 0.9808\n",
      "Epoch 50/98, Train Loss: 1.1347, Test Loss: 0.9789\n",
      "Epoch 51/98, Train Loss: 1.1164, Test Loss: 0.9760\n",
      "Epoch 52/98, Train Loss: 1.1208, Test Loss: 0.9739\n",
      "Epoch 53/98, Train Loss: 1.1357, Test Loss: 0.9756\n",
      "Epoch 54/98, Train Loss: 1.0496, Test Loss: 0.9749\n",
      "Epoch 55/98, Train Loss: 1.0916, Test Loss: 0.9751\n",
      "Epoch 56/98, Train Loss: 1.1223, Test Loss: 0.9728\n",
      "Epoch 57/98, Train Loss: 1.1108, Test Loss: 0.9747\n",
      "Epoch 58/98, Train Loss: 1.0914, Test Loss: 0.9752\n",
      "Epoch 59/98, Train Loss: 1.0819, Test Loss: 0.9741\n",
      "Epoch 60/98, Train Loss: 1.1222, Test Loss: 0.9759\n",
      "Epoch 61/98, Train Loss: 1.1122, Test Loss: 0.9766\n",
      "Epoch 62/98, Train Loss: 1.1301, Test Loss: 0.9749\n",
      "Epoch 63/98, Train Loss: 1.0850, Test Loss: 0.9765\n",
      "Epoch 64/98, Train Loss: 1.0929, Test Loss: 0.9776\n",
      "Epoch 65/98, Train Loss: 1.1063, Test Loss: 0.9780\n",
      "Epoch 66/98, Train Loss: 1.1137, Test Loss: 0.9794\n",
      "Epoch 67/98, Train Loss: 1.1064, Test Loss: 0.9764\n",
      "Epoch 68/98, Train Loss: 1.0852, Test Loss: 0.9769\n",
      "Epoch 69/98, Train Loss: 1.0987, Test Loss: 0.9775\n",
      "Epoch 70/98, Train Loss: 1.1270, Test Loss: 0.9800\n",
      "Epoch 71/98, Train Loss: 1.0809, Test Loss: 0.9803\n",
      "Epoch 72/98, Train Loss: 1.0911, Test Loss: 0.9793\n",
      "Epoch 73/98, Train Loss: 1.1193, Test Loss: 0.9759\n",
      "Epoch 74/98, Train Loss: 1.1063, Test Loss: 0.9791\n",
      "Epoch 75/98, Train Loss: 1.0678, Test Loss: 0.9789\n",
      "Epoch 76/98, Train Loss: 1.1066, Test Loss: 0.9794\n",
      "Epoch 77/98, Train Loss: 1.1114, Test Loss: 0.9780\n",
      "Epoch 78/98, Train Loss: 1.0947, Test Loss: 0.9798\n",
      "Epoch 79/98, Train Loss: 1.1013, Test Loss: 0.9780\n",
      "Epoch 80/98, Train Loss: 1.1155, Test Loss: 0.9796\n",
      "Epoch 81/98, Train Loss: 1.0980, Test Loss: 0.9787\n",
      "Epoch 82/98, Train Loss: 1.0980, Test Loss: 0.9813\n",
      "Epoch 83/98, Train Loss: 1.0973, Test Loss: 0.9794\n",
      "Epoch 84/98, Train Loss: 1.0939, Test Loss: 0.9814\n",
      "Epoch 85/98, Train Loss: 1.0756, Test Loss: 0.9811\n",
      "Epoch 86/98, Train Loss: 1.0945, Test Loss: 0.9818\n",
      "Epoch 87/98, Train Loss: 1.1140, Test Loss: 0.9802\n",
      "Epoch 88/98, Train Loss: 1.1263, Test Loss: 0.9810\n",
      "Epoch 89/98, Train Loss: 1.1103, Test Loss: 0.9815\n",
      "Epoch 90/98, Train Loss: 1.1025, Test Loss: 0.9814\n",
      "Epoch 91/98, Train Loss: 1.1017, Test Loss: 0.9810\n",
      "Epoch 92/98, Train Loss: 1.1035, Test Loss: 0.9790\n",
      "Epoch 93/98, Train Loss: 1.0494, Test Loss: 0.9776\n",
      "Epoch 94/98, Train Loss: 1.1033, Test Loss: 0.9812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:25:30,634] Trial 68 finished with value: 0.97620689868927 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 237, 'layer_1_size': 188, 'layer_2_size': 227, 'layer_3_size': 226, 'layer_4_size': 215, 'layer_5_size': 224, 'dropout_rate': 0.4221639413032384, 'learning_rate': 9.232294800687363e-05, 'batch_size': 64, 'epochs': 98}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/98, Train Loss: 1.0820, Test Loss: 0.9803\n",
      "Epoch 96/98, Train Loss: 1.0886, Test Loss: 0.9782\n",
      "Epoch 97/98, Train Loss: 1.0642, Test Loss: 0.9781\n",
      "Epoch 98/98, Train Loss: 1.0998, Test Loss: 0.9762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:25:30,900] Trial 69 finished with value: 1.0890569686889648 and parameters: {'num_hidden_layers': 5, 'layer_0_size': 137, 'layer_1_size': 149, 'layer_2_size': 123, 'layer_3_size': 63, 'layer_4_size': 227, 'dropout_rate': 0.44975543372090193, 'learning_rate': 0.0005597629467515863, 'batch_size': 128, 'epochs': 8}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8, Train Loss: 1.2864, Test Loss: 1.0923\n",
      "Epoch 2/8, Train Loss: 1.2260, Test Loss: 1.0925\n",
      "Epoch 3/8, Train Loss: 1.2651, Test Loss: 1.0915\n",
      "Epoch 4/8, Train Loss: 1.2194, Test Loss: 1.0916\n",
      "Epoch 5/8, Train Loss: 1.2097, Test Loss: 1.0885\n",
      "Epoch 6/8, Train Loss: 1.1487, Test Loss: 1.0859\n",
      "Epoch 7/8, Train Loss: 1.1324, Test Loss: 1.0868\n",
      "Epoch 8/8, Train Loss: 1.1190, Test Loss: 1.0891\n",
      "Epoch 1/79, Train Loss: 1.3293, Test Loss: 0.9001\n",
      "Epoch 2/79, Train Loss: 1.3271, Test Loss: 0.9133\n",
      "Epoch 3/79, Train Loss: 1.2626, Test Loss: 0.9139\n",
      "Epoch 4/79, Train Loss: 1.3654, Test Loss: 0.9164\n",
      "Epoch 5/79, Train Loss: 1.3192, Test Loss: 0.9230\n",
      "Epoch 6/79, Train Loss: 1.2533, Test Loss: 0.9190\n",
      "Epoch 7/79, Train Loss: 1.2791, Test Loss: 0.9114\n",
      "Epoch 8/79, Train Loss: 1.2431, Test Loss: 0.9147\n",
      "Epoch 9/79, Train Loss: 1.2928, Test Loss: 0.9160\n",
      "Epoch 10/79, Train Loss: 1.2704, Test Loss: 0.9149\n",
      "Epoch 11/79, Train Loss: 1.2597, Test Loss: 0.9074\n",
      "Epoch 12/79, Train Loss: 1.1987, Test Loss: 0.9067\n",
      "Epoch 13/79, Train Loss: 1.2264, Test Loss: 0.9040\n",
      "Epoch 14/79, Train Loss: 1.2143, Test Loss: 0.9077\n",
      "Epoch 15/79, Train Loss: 1.2257, Test Loss: 0.9030\n",
      "Epoch 16/79, Train Loss: 1.2340, Test Loss: 0.9115\n",
      "Epoch 17/79, Train Loss: 1.2843, Test Loss: 0.9075\n",
      "Epoch 18/79, Train Loss: 1.1555, Test Loss: 0.9053\n",
      "Epoch 19/79, Train Loss: 1.1972, Test Loss: 0.9019\n",
      "Epoch 20/79, Train Loss: 1.1967, Test Loss: 0.9064\n",
      "Epoch 21/79, Train Loss: 1.1771, Test Loss: 0.9088\n",
      "Epoch 22/79, Train Loss: 1.2120, Test Loss: 0.9036\n",
      "Epoch 23/79, Train Loss: 1.2344, Test Loss: 0.9030\n",
      "Epoch 24/79, Train Loss: 1.1748, Test Loss: 0.9049\n",
      "Epoch 25/79, Train Loss: 1.1766, Test Loss: 0.9024\n",
      "Epoch 26/79, Train Loss: 1.1959, Test Loss: 0.9034\n",
      "Epoch 27/79, Train Loss: 1.1953, Test Loss: 0.9010\n",
      "Epoch 28/79, Train Loss: 1.1777, Test Loss: 0.9035\n",
      "Epoch 29/79, Train Loss: 1.1726, Test Loss: 0.9064\n",
      "Epoch 30/79, Train Loss: 1.1811, Test Loss: 0.9028\n",
      "Epoch 31/79, Train Loss: 1.1774, Test Loss: 0.9003\n",
      "Epoch 32/79, Train Loss: 1.2021, Test Loss: 0.8975\n",
      "Epoch 33/79, Train Loss: 1.1698, Test Loss: 0.9007\n",
      "Epoch 34/79, Train Loss: 1.2126, Test Loss: 0.8983\n",
      "Epoch 35/79, Train Loss: 1.2134, Test Loss: 0.8960\n",
      "Epoch 36/79, Train Loss: 1.1753, Test Loss: 0.8969\n",
      "Epoch 37/79, Train Loss: 1.1733, Test Loss: 0.9005\n",
      "Epoch 38/79, Train Loss: 1.1154, Test Loss: 0.9003\n",
      "Epoch 39/79, Train Loss: 1.1339, Test Loss: 0.8938\n",
      "Epoch 40/79, Train Loss: 1.1451, Test Loss: 0.8979\n",
      "Epoch 41/79, Train Loss: 1.2166, Test Loss: 0.8998\n",
      "Epoch 42/79, Train Loss: 1.1758, Test Loss: 0.8997\n",
      "Epoch 43/79, Train Loss: 1.1325, Test Loss: 0.9028\n",
      "Epoch 44/79, Train Loss: 1.1613, Test Loss: 0.8999\n",
      "Epoch 45/79, Train Loss: 1.2183, Test Loss: 0.8953\n",
      "Epoch 46/79, Train Loss: 1.2117, Test Loss: 0.8999\n",
      "Epoch 47/79, Train Loss: 1.1209, Test Loss: 0.8975\n",
      "Epoch 48/79, Train Loss: 1.2039, Test Loss: 0.8998\n",
      "Epoch 49/79, Train Loss: 1.1496, Test Loss: 0.9007\n",
      "Epoch 50/79, Train Loss: 1.1903, Test Loss: 0.8958\n",
      "Epoch 51/79, Train Loss: 1.1393, Test Loss: 0.8984\n",
      "Epoch 52/79, Train Loss: 1.1738, Test Loss: 0.8933\n",
      "Epoch 53/79, Train Loss: 1.1781, Test Loss: 0.8992\n",
      "Epoch 54/79, Train Loss: 1.1790, Test Loss: 0.8955\n",
      "Epoch 55/79, Train Loss: 1.1492, Test Loss: 0.8946\n",
      "Epoch 56/79, Train Loss: 1.1458, Test Loss: 0.8947\n",
      "Epoch 57/79, Train Loss: 1.1636, Test Loss: 0.8963\n",
      "Epoch 58/79, Train Loss: 1.1091, Test Loss: 0.8978\n",
      "Epoch 59/79, Train Loss: 1.1160, Test Loss: 0.8989\n",
      "Epoch 60/79, Train Loss: 1.1484, Test Loss: 0.8992\n",
      "Epoch 61/79, Train Loss: 1.1565, Test Loss: 0.8935\n",
      "Epoch 62/79, Train Loss: 1.1118, Test Loss: 0.8928\n",
      "Epoch 63/79, Train Loss: 1.2057, Test Loss: 0.8934\n",
      "Epoch 64/79, Train Loss: 1.1284, Test Loss: 0.8953\n",
      "Epoch 65/79, Train Loss: 1.1401, Test Loss: 0.8904\n",
      "Epoch 66/79, Train Loss: 1.1544, Test Loss: 0.8908\n",
      "Epoch 67/79, Train Loss: 1.1221, Test Loss: 0.8950\n",
      "Epoch 68/79, Train Loss: 1.1993, Test Loss: 0.8898\n",
      "Epoch 69/79, Train Loss: 1.1393, Test Loss: 0.8963\n",
      "Epoch 70/79, Train Loss: 1.1716, Test Loss: 0.8947\n",
      "Epoch 71/79, Train Loss: 1.1368, Test Loss: 0.8961\n",
      "Epoch 72/79, Train Loss: 1.1720, Test Loss: 0.8897\n",
      "Epoch 73/79, Train Loss: 1.1584, Test Loss: 0.8949\n",
      "Epoch 74/79, Train Loss: 1.1115, Test Loss: 0.8933\n",
      "Epoch 75/79, Train Loss: 1.1144, Test Loss: 0.8933\n",
      "Epoch 76/79, Train Loss: 1.1289, Test Loss: 0.8933\n",
      "Epoch 77/79, Train Loss: 1.1896, Test Loss: 0.8897\n",
      "Epoch 78/79, Train Loss: 1.1482, Test Loss: 0.8908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:25:34,487] Trial 70 finished with value: 0.8916318714618683 and parameters: {'num_hidden_layers': 4, 'layer_0_size': 162, 'layer_1_size': 174, 'layer_2_size': 213, 'layer_3_size': 248, 'dropout_rate': 0.4071331545786698, 'learning_rate': 1.328321332881541e-05, 'batch_size': 64, 'epochs': 79}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/79, Train Loss: 1.1275, Test Loss: 0.8916\n",
      "Epoch 1/74, Train Loss: 1.2508, Test Loss: 1.0071\n",
      "Epoch 2/74, Train Loss: 1.2335, Test Loss: 1.0123\n",
      "Epoch 3/74, Train Loss: 1.2036, Test Loss: 1.0122\n",
      "Epoch 4/74, Train Loss: 1.2416, Test Loss: 1.0111\n",
      "Epoch 5/74, Train Loss: 1.2715, Test Loss: 1.0085\n",
      "Epoch 6/74, Train Loss: 1.2524, Test Loss: 1.0060\n",
      "Epoch 7/74, Train Loss: 1.2598, Test Loss: 1.0036\n",
      "Epoch 8/74, Train Loss: 1.1400, Test Loss: 1.0030\n",
      "Epoch 9/74, Train Loss: 1.2207, Test Loss: 1.0031\n",
      "Epoch 10/74, Train Loss: 1.2009, Test Loss: 1.0005\n",
      "Epoch 11/74, Train Loss: 1.1665, Test Loss: 1.0005\n",
      "Epoch 12/74, Train Loss: 1.1909, Test Loss: 0.9997\n",
      "Epoch 13/74, Train Loss: 1.1673, Test Loss: 1.0017\n",
      "Epoch 14/74, Train Loss: 1.1992, Test Loss: 0.9990\n",
      "Epoch 15/74, Train Loss: 1.2001, Test Loss: 0.9981\n",
      "Epoch 16/74, Train Loss: 1.2014, Test Loss: 0.9986\n",
      "Epoch 17/74, Train Loss: 1.2342, Test Loss: 0.9993\n",
      "Epoch 18/74, Train Loss: 1.1764, Test Loss: 0.9973\n",
      "Epoch 19/74, Train Loss: 1.1638, Test Loss: 0.9948\n",
      "Epoch 20/74, Train Loss: 1.2288, Test Loss: 0.9965\n",
      "Epoch 21/74, Train Loss: 1.1196, Test Loss: 0.9983\n",
      "Epoch 22/74, Train Loss: 1.1676, Test Loss: 0.9988\n",
      "Epoch 23/74, Train Loss: 1.1732, Test Loss: 0.9990\n",
      "Epoch 24/74, Train Loss: 1.1815, Test Loss: 0.9981\n",
      "Epoch 25/74, Train Loss: 1.1199, Test Loss: 0.9955\n",
      "Epoch 26/74, Train Loss: 1.2111, Test Loss: 0.9959\n",
      "Epoch 27/74, Train Loss: 1.1752, Test Loss: 0.9952\n",
      "Epoch 28/74, Train Loss: 1.1748, Test Loss: 0.9943\n",
      "Epoch 29/74, Train Loss: 1.1147, Test Loss: 0.9965\n",
      "Epoch 30/74, Train Loss: 1.1507, Test Loss: 0.9970\n",
      "Epoch 31/74, Train Loss: 1.1593, Test Loss: 0.9951\n",
      "Epoch 32/74, Train Loss: 1.1436, Test Loss: 0.9956\n",
      "Epoch 33/74, Train Loss: 1.1626, Test Loss: 0.9963\n",
      "Epoch 34/74, Train Loss: 1.1054, Test Loss: 0.9960\n",
      "Epoch 35/74, Train Loss: 1.0885, Test Loss: 0.9975\n",
      "Epoch 36/74, Train Loss: 1.1381, Test Loss: 0.9961\n",
      "Epoch 37/74, Train Loss: 1.1480, Test Loss: 0.9968\n",
      "Epoch 38/74, Train Loss: 1.1247, Test Loss: 0.9987\n",
      "Epoch 39/74, Train Loss: 1.1432, Test Loss: 0.9958\n",
      "Epoch 40/74, Train Loss: 1.1522, Test Loss: 0.9931\n",
      "Epoch 41/74, Train Loss: 1.1433, Test Loss: 0.9940\n",
      "Epoch 42/74, Train Loss: 1.1383, Test Loss: 0.9955\n",
      "Epoch 43/74, Train Loss: 1.1192, Test Loss: 0.9956\n",
      "Epoch 44/74, Train Loss: 1.0718, Test Loss: 0.9960\n",
      "Epoch 45/74, Train Loss: 1.1210, Test Loss: 0.9943\n",
      "Epoch 46/74, Train Loss: 1.1097, Test Loss: 0.9942\n",
      "Epoch 47/74, Train Loss: 1.0974, Test Loss: 0.9951\n",
      "Epoch 48/74, Train Loss: 1.1076, Test Loss: 0.9944\n",
      "Epoch 49/74, Train Loss: 1.1314, Test Loss: 0.9944\n",
      "Epoch 50/74, Train Loss: 1.1535, Test Loss: 0.9947\n",
      "Epoch 51/74, Train Loss: 1.1097, Test Loss: 0.9947\n",
      "Epoch 52/74, Train Loss: 1.1078, Test Loss: 0.9943\n",
      "Epoch 53/74, Train Loss: 1.0928, Test Loss: 0.9961\n",
      "Epoch 54/74, Train Loss: 1.1497, Test Loss: 0.9947\n",
      "Epoch 55/74, Train Loss: 1.1179, Test Loss: 0.9932\n",
      "Epoch 56/74, Train Loss: 1.1191, Test Loss: 0.9948\n",
      "Epoch 57/74, Train Loss: 1.1061, Test Loss: 0.9944\n",
      "Epoch 58/74, Train Loss: 1.1160, Test Loss: 0.9917\n",
      "Epoch 59/74, Train Loss: 1.1262, Test Loss: 0.9912\n",
      "Epoch 60/74, Train Loss: 1.1242, Test Loss: 0.9923\n",
      "Epoch 61/74, Train Loss: 1.1358, Test Loss: 0.9939\n",
      "Epoch 62/74, Train Loss: 1.0625, Test Loss: 0.9932\n",
      "Epoch 63/74, Train Loss: 1.0982, Test Loss: 0.9932\n",
      "Epoch 64/74, Train Loss: 1.0828, Test Loss: 0.9931\n",
      "Epoch 65/74, Train Loss: 1.1311, Test Loss: 0.9938\n",
      "Epoch 66/74, Train Loss: 1.1069, Test Loss: 0.9928\n",
      "Epoch 67/74, Train Loss: 1.0986, Test Loss: 0.9934\n",
      "Epoch 68/74, Train Loss: 1.0925, Test Loss: 0.9948\n",
      "Epoch 69/74, Train Loss: 1.0864, Test Loss: 0.9947\n",
      "Epoch 70/74, Train Loss: 1.0693, Test Loss: 0.9939\n",
      "Epoch 71/74, Train Loss: 1.0999, Test Loss: 0.9920\n",
      "Epoch 72/74, Train Loss: 1.1147, Test Loss: 0.9936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:25:42,455] Trial 71 finished with value: 0.9923808574676514 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 149, 'layer_1_size': 125, 'layer_2_size': 247, 'layer_3_size': 227, 'layer_4_size': 206, 'layer_5_size': 236, 'layer_6_size': 161, 'layer_7_size': 84, 'dropout_rate': 0.4658698586526603, 'learning_rate': 5.616329268214912e-05, 'batch_size': 64, 'epochs': 74}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/74, Train Loss: 1.1252, Test Loss: 0.9935\n",
      "Epoch 74/74, Train Loss: 1.0644, Test Loss: 0.9924\n",
      "Epoch 1/76, Train Loss: 1.2870, Test Loss: 0.8372\n",
      "Epoch 2/76, Train Loss: 1.2823, Test Loss: 0.8362\n",
      "Epoch 3/76, Train Loss: 1.2340, Test Loss: 0.8332\n",
      "Epoch 4/76, Train Loss: 1.2829, Test Loss: 0.8344\n",
      "Epoch 5/76, Train Loss: 1.2537, Test Loss: 0.8321\n",
      "Epoch 6/76, Train Loss: 1.2583, Test Loss: 0.8311\n",
      "Epoch 7/76, Train Loss: 1.2379, Test Loss: 0.8347\n",
      "Epoch 8/76, Train Loss: 1.2821, Test Loss: 0.8335\n",
      "Epoch 9/76, Train Loss: 1.2137, Test Loss: 0.8369\n",
      "Epoch 10/76, Train Loss: 1.2190, Test Loss: 0.8393\n",
      "Epoch 11/76, Train Loss: 1.2364, Test Loss: 0.8399\n",
      "Epoch 12/76, Train Loss: 1.1716, Test Loss: 0.8417\n",
      "Epoch 13/76, Train Loss: 1.1844, Test Loss: 0.8393\n",
      "Epoch 14/76, Train Loss: 1.1879, Test Loss: 0.8397\n",
      "Epoch 15/76, Train Loss: 1.1358, Test Loss: 0.8386\n",
      "Epoch 16/76, Train Loss: 1.1637, Test Loss: 0.8382\n",
      "Epoch 17/76, Train Loss: 1.1513, Test Loss: 0.8405\n",
      "Epoch 18/76, Train Loss: 1.1487, Test Loss: 0.8412\n",
      "Epoch 19/76, Train Loss: 1.1918, Test Loss: 0.8421\n",
      "Epoch 20/76, Train Loss: 1.1443, Test Loss: 0.8430\n",
      "Epoch 21/76, Train Loss: 1.1589, Test Loss: 0.8444\n",
      "Epoch 22/76, Train Loss: 1.1480, Test Loss: 0.8428\n",
      "Epoch 23/76, Train Loss: 1.1201, Test Loss: 0.8405\n",
      "Epoch 24/76, Train Loss: 1.1136, Test Loss: 0.8440\n",
      "Epoch 25/76, Train Loss: 1.1308, Test Loss: 0.8423\n",
      "Epoch 26/76, Train Loss: 1.1313, Test Loss: 0.8428\n",
      "Epoch 27/76, Train Loss: 1.1308, Test Loss: 0.8425\n",
      "Epoch 28/76, Train Loss: 1.1487, Test Loss: 0.8459\n",
      "Epoch 29/76, Train Loss: 1.1218, Test Loss: 0.8458\n",
      "Epoch 30/76, Train Loss: 1.1088, Test Loss: 0.8422\n",
      "Epoch 31/76, Train Loss: 1.1410, Test Loss: 0.8453\n",
      "Epoch 32/76, Train Loss: 1.1156, Test Loss: 0.8463\n",
      "Epoch 33/76, Train Loss: 1.1156, Test Loss: 0.8487\n",
      "Epoch 34/76, Train Loss: 1.1311, Test Loss: 0.8432\n",
      "Epoch 35/76, Train Loss: 1.1242, Test Loss: 0.8443\n",
      "Epoch 36/76, Train Loss: 1.0978, Test Loss: 0.8459\n",
      "Epoch 37/76, Train Loss: 1.1095, Test Loss: 0.8446\n",
      "Epoch 38/76, Train Loss: 1.0726, Test Loss: 0.8423\n",
      "Epoch 39/76, Train Loss: 1.0993, Test Loss: 0.8475\n",
      "Epoch 40/76, Train Loss: 1.0886, Test Loss: 0.8482\n",
      "Epoch 41/76, Train Loss: 1.0913, Test Loss: 0.8476\n",
      "Epoch 42/76, Train Loss: 1.0882, Test Loss: 0.8520\n",
      "Epoch 43/76, Train Loss: 1.0639, Test Loss: 0.8494\n",
      "Epoch 44/76, Train Loss: 1.0890, Test Loss: 0.8481\n",
      "Epoch 45/76, Train Loss: 1.0916, Test Loss: 0.8462\n",
      "Epoch 46/76, Train Loss: 1.0601, Test Loss: 0.8435\n",
      "Epoch 47/76, Train Loss: 1.0966, Test Loss: 0.8428\n",
      "Epoch 48/76, Train Loss: 1.0755, Test Loss: 0.8437\n",
      "Epoch 49/76, Train Loss: 1.1092, Test Loss: 0.8445\n",
      "Epoch 50/76, Train Loss: 1.0772, Test Loss: 0.8455\n",
      "Epoch 51/76, Train Loss: 1.0955, Test Loss: 0.8423\n",
      "Epoch 52/76, Train Loss: 1.0722, Test Loss: 0.8443\n",
      "Epoch 53/76, Train Loss: 1.0665, Test Loss: 0.8423\n",
      "Epoch 54/76, Train Loss: 1.0277, Test Loss: 0.8453\n",
      "Epoch 55/76, Train Loss: 1.0703, Test Loss: 0.8433\n",
      "Epoch 56/76, Train Loss: 1.0325, Test Loss: 0.8432\n",
      "Epoch 57/76, Train Loss: 1.0400, Test Loss: 0.8427\n",
      "Epoch 58/76, Train Loss: 1.0823, Test Loss: 0.8422\n",
      "Epoch 59/76, Train Loss: 1.0387, Test Loss: 0.8419\n",
      "Epoch 60/76, Train Loss: 1.0751, Test Loss: 0.8410\n",
      "Epoch 61/76, Train Loss: 1.0849, Test Loss: 0.8417\n",
      "Epoch 62/76, Train Loss: 1.0478, Test Loss: 0.8399\n",
      "Epoch 63/76, Train Loss: 1.0746, Test Loss: 0.8401\n",
      "Epoch 64/76, Train Loss: 1.0945, Test Loss: 0.8414\n",
      "Epoch 65/76, Train Loss: 1.0461, Test Loss: 0.8394\n",
      "Epoch 66/76, Train Loss: 1.0609, Test Loss: 0.8396\n",
      "Epoch 67/76, Train Loss: 1.0868, Test Loss: 0.8379\n",
      "Epoch 68/76, Train Loss: 1.0723, Test Loss: 0.8394\n",
      "Epoch 69/76, Train Loss: 1.0296, Test Loss: 0.8401\n",
      "Epoch 70/76, Train Loss: 1.0622, Test Loss: 0.8395\n",
      "Epoch 71/76, Train Loss: 1.0575, Test Loss: 0.8398\n",
      "Epoch 72/76, Train Loss: 1.0370, Test Loss: 0.8407\n",
      "Epoch 73/76, Train Loss: 1.0695, Test Loss: 0.8394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:25:49,718] Trial 72 finished with value: 0.8385646343231201 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 115, 'layer_1_size': 96, 'layer_2_size': 235, 'layer_3_size': 241, 'layer_4_size': 191, 'layer_5_size': 248, 'layer_6_size': 144, 'layer_7_size': 37, 'dropout_rate': 0.4911638487625426, 'learning_rate': 0.0001459958486463779, 'batch_size': 64, 'epochs': 76}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/76, Train Loss: 1.0420, Test Loss: 0.8396\n",
      "Epoch 75/76, Train Loss: 1.0377, Test Loss: 0.8388\n",
      "Epoch 76/76, Train Loss: 1.0543, Test Loss: 0.8386\n",
      "Epoch 1/83, Train Loss: 1.2485, Test Loss: 1.0356\n",
      "Epoch 2/83, Train Loss: 1.2677, Test Loss: 1.0357\n",
      "Epoch 3/83, Train Loss: 1.2627, Test Loss: 1.0436\n",
      "Epoch 4/83, Train Loss: 1.2411, Test Loss: 1.0409\n",
      "Epoch 5/83, Train Loss: 1.2190, Test Loss: 1.0413\n",
      "Epoch 6/83, Train Loss: 1.1892, Test Loss: 1.0382\n",
      "Epoch 7/83, Train Loss: 1.1713, Test Loss: 1.0370\n",
      "Epoch 8/83, Train Loss: 1.1387, Test Loss: 1.0351\n",
      "Epoch 9/83, Train Loss: 1.1668, Test Loss: 1.0335\n",
      "Epoch 10/83, Train Loss: 1.1686, Test Loss: 1.0317\n",
      "Epoch 11/83, Train Loss: 1.1536, Test Loss: 1.0321\n",
      "Epoch 12/83, Train Loss: 1.1636, Test Loss: 1.0346\n",
      "Epoch 13/83, Train Loss: 1.1270, Test Loss: 1.0357\n",
      "Epoch 14/83, Train Loss: 1.1243, Test Loss: 1.0334\n",
      "Epoch 15/83, Train Loss: 1.1365, Test Loss: 1.0344\n",
      "Epoch 16/83, Train Loss: 1.1213, Test Loss: 1.0327\n",
      "Epoch 17/83, Train Loss: 1.1190, Test Loss: 1.0346\n",
      "Epoch 18/83, Train Loss: 1.0832, Test Loss: 1.0336\n",
      "Epoch 19/83, Train Loss: 1.1623, Test Loss: 1.0327\n",
      "Epoch 20/83, Train Loss: 1.1041, Test Loss: 1.0315\n",
      "Epoch 21/83, Train Loss: 1.0779, Test Loss: 1.0311\n",
      "Epoch 22/83, Train Loss: 1.0958, Test Loss: 1.0325\n",
      "Epoch 23/83, Train Loss: 1.0800, Test Loss: 1.0328\n",
      "Epoch 24/83, Train Loss: 1.1052, Test Loss: 1.0328\n",
      "Epoch 25/83, Train Loss: 1.0789, Test Loss: 1.0325\n",
      "Epoch 26/83, Train Loss: 1.0888, Test Loss: 1.0344\n",
      "Epoch 27/83, Train Loss: 1.0854, Test Loss: 1.0336\n",
      "Epoch 28/83, Train Loss: 1.0797, Test Loss: 1.0334\n",
      "Epoch 29/83, Train Loss: 1.0710, Test Loss: 1.0322\n",
      "Epoch 30/83, Train Loss: 1.1055, Test Loss: 1.0329\n",
      "Epoch 31/83, Train Loss: 1.0666, Test Loss: 1.0332\n",
      "Epoch 32/83, Train Loss: 1.0539, Test Loss: 1.0343\n",
      "Epoch 33/83, Train Loss: 1.0449, Test Loss: 1.0347\n",
      "Epoch 34/83, Train Loss: 1.0360, Test Loss: 1.0341\n",
      "Epoch 35/83, Train Loss: 1.0426, Test Loss: 1.0341\n",
      "Epoch 36/83, Train Loss: 1.0576, Test Loss: 1.0338\n",
      "Epoch 37/83, Train Loss: 1.0585, Test Loss: 1.0329\n",
      "Epoch 38/83, Train Loss: 1.0417, Test Loss: 1.0339\n",
      "Epoch 39/83, Train Loss: 1.0553, Test Loss: 1.0346\n",
      "Epoch 40/83, Train Loss: 1.0585, Test Loss: 1.0357\n",
      "Epoch 41/83, Train Loss: 1.0281, Test Loss: 1.0360\n",
      "Epoch 42/83, Train Loss: 1.0695, Test Loss: 1.0363\n",
      "Epoch 43/83, Train Loss: 1.0474, Test Loss: 1.0364\n",
      "Epoch 44/83, Train Loss: 1.0770, Test Loss: 1.0370\n",
      "Epoch 45/83, Train Loss: 1.0557, Test Loss: 1.0364\n",
      "Epoch 46/83, Train Loss: 1.0196, Test Loss: 1.0354\n",
      "Epoch 47/83, Train Loss: 1.0414, Test Loss: 1.0352\n",
      "Epoch 48/83, Train Loss: 1.0464, Test Loss: 1.0368\n",
      "Epoch 49/83, Train Loss: 1.0558, Test Loss: 1.0369\n",
      "Epoch 50/83, Train Loss: 1.0482, Test Loss: 1.0380\n",
      "Epoch 51/83, Train Loss: 1.0433, Test Loss: 1.0370\n",
      "Epoch 52/83, Train Loss: 1.0348, Test Loss: 1.0371\n",
      "Epoch 53/83, Train Loss: 1.0269, Test Loss: 1.0374\n",
      "Epoch 54/83, Train Loss: 1.0348, Test Loss: 1.0374\n",
      "Epoch 55/83, Train Loss: 1.0316, Test Loss: 1.0375\n",
      "Epoch 56/83, Train Loss: 1.0341, Test Loss: 1.0394\n",
      "Epoch 57/83, Train Loss: 1.0369, Test Loss: 1.0366\n",
      "Epoch 58/83, Train Loss: 1.0343, Test Loss: 1.0370\n",
      "Epoch 59/83, Train Loss: 1.0083, Test Loss: 1.0388\n",
      "Epoch 60/83, Train Loss: 1.0332, Test Loss: 1.0384\n",
      "Epoch 61/83, Train Loss: 1.0228, Test Loss: 1.0375\n",
      "Epoch 62/83, Train Loss: 1.0287, Test Loss: 1.0374\n",
      "Epoch 63/83, Train Loss: 1.0324, Test Loss: 1.0370\n",
      "Epoch 64/83, Train Loss: 1.0231, Test Loss: 1.0362\n",
      "Epoch 65/83, Train Loss: 1.0159, Test Loss: 1.0367\n",
      "Epoch 66/83, Train Loss: 1.0316, Test Loss: 1.0382\n",
      "Epoch 67/83, Train Loss: 1.0220, Test Loss: 1.0386\n",
      "Epoch 68/83, Train Loss: 1.0153, Test Loss: 1.0384\n",
      "Epoch 69/83, Train Loss: 1.0021, Test Loss: 1.0377\n",
      "Epoch 70/83, Train Loss: 1.0408, Test Loss: 1.0380\n",
      "Epoch 71/83, Train Loss: 1.0084, Test Loss: 1.0374\n",
      "Epoch 72/83, Train Loss: 1.0280, Test Loss: 1.0378\n",
      "Epoch 73/83, Train Loss: 1.0132, Test Loss: 1.0378\n",
      "Epoch 74/83, Train Loss: 1.0063, Test Loss: 1.0379\n",
      "Epoch 75/83, Train Loss: 1.0132, Test Loss: 1.0374\n",
      "Epoch 76/83, Train Loss: 1.0259, Test Loss: 1.0379\n",
      "Epoch 77/83, Train Loss: 1.0150, Test Loss: 1.0373\n",
      "Epoch 78/83, Train Loss: 1.0174, Test Loss: 1.0364\n",
      "Epoch 79/83, Train Loss: 1.0013, Test Loss: 1.0363\n",
      "Epoch 80/83, Train Loss: 1.0099, Test Loss: 1.0368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:25:56,301] Trial 73 finished with value: 1.0359668582677841 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 115, 'layer_1_size': 82, 'layer_2_size': 232, 'layer_3_size': 77, 'layer_4_size': 194, 'layer_5_size': 256, 'layer_6_size': 138, 'dropout_rate': 0.4842856264646408, 'learning_rate': 0.000264882678326318, 'batch_size': 64, 'epochs': 83}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/83, Train Loss: 1.0039, Test Loss: 1.0360\n",
      "Epoch 82/83, Train Loss: 1.0047, Test Loss: 1.0357\n",
      "Epoch 83/83, Train Loss: 1.0076, Test Loss: 1.0360\n",
      "Epoch 1/55, Train Loss: 1.3148, Test Loss: 0.7385\n",
      "Epoch 2/55, Train Loss: 1.2725, Test Loss: 0.7385\n",
      "Epoch 3/55, Train Loss: 1.2811, Test Loss: 0.7384\n",
      "Epoch 4/55, Train Loss: 1.1948, Test Loss: 0.7413\n",
      "Epoch 5/55, Train Loss: 1.1842, Test Loss: 0.7410\n",
      "Epoch 6/55, Train Loss: 1.2340, Test Loss: 0.7412\n",
      "Epoch 7/55, Train Loss: 1.2919, Test Loss: 0.7437\n",
      "Epoch 8/55, Train Loss: 1.2362, Test Loss: 0.7425\n",
      "Epoch 9/55, Train Loss: 1.1808, Test Loss: 0.7421\n",
      "Epoch 10/55, Train Loss: 1.2201, Test Loss: 0.7468\n",
      "Epoch 11/55, Train Loss: 1.2226, Test Loss: 0.7473\n",
      "Epoch 12/55, Train Loss: 1.1929, Test Loss: 0.7446\n",
      "Epoch 13/55, Train Loss: 1.2172, Test Loss: 0.7486\n",
      "Epoch 14/55, Train Loss: 1.1780, Test Loss: 0.7466\n",
      "Epoch 15/55, Train Loss: 1.1720, Test Loss: 0.7452\n",
      "Epoch 16/55, Train Loss: 1.1685, Test Loss: 0.7488\n",
      "Epoch 17/55, Train Loss: 1.1688, Test Loss: 0.7458\n",
      "Epoch 18/55, Train Loss: 1.1571, Test Loss: 0.7444\n",
      "Epoch 19/55, Train Loss: 1.1434, Test Loss: 0.7464\n",
      "Epoch 20/55, Train Loss: 1.2186, Test Loss: 0.7431\n",
      "Epoch 21/55, Train Loss: 1.1897, Test Loss: 0.7465\n",
      "Epoch 22/55, Train Loss: 1.1660, Test Loss: 0.7516\n",
      "Epoch 23/55, Train Loss: 1.1778, Test Loss: 0.7514\n",
      "Epoch 24/55, Train Loss: 1.1560, Test Loss: 0.7499\n",
      "Epoch 25/55, Train Loss: 1.1645, Test Loss: 0.7487\n",
      "Epoch 26/55, Train Loss: 1.1597, Test Loss: 0.7484\n",
      "Epoch 27/55, Train Loss: 1.1485, Test Loss: 0.7475\n",
      "Epoch 28/55, Train Loss: 1.1623, Test Loss: 0.7463\n",
      "Epoch 29/55, Train Loss: 1.1683, Test Loss: 0.7487\n",
      "Epoch 30/55, Train Loss: 1.1721, Test Loss: 0.7550\n",
      "Epoch 31/55, Train Loss: 1.1625, Test Loss: 0.7547\n",
      "Epoch 32/55, Train Loss: 1.1153, Test Loss: 0.7546\n",
      "Epoch 33/55, Train Loss: 1.1237, Test Loss: 0.7566\n",
      "Epoch 34/55, Train Loss: 1.1200, Test Loss: 0.7597\n",
      "Epoch 35/55, Train Loss: 1.1293, Test Loss: 0.7603\n",
      "Epoch 36/55, Train Loss: 1.1645, Test Loss: 0.7640\n",
      "Epoch 37/55, Train Loss: 1.1344, Test Loss: 0.7610\n",
      "Epoch 38/55, Train Loss: 1.1430, Test Loss: 0.7560\n",
      "Epoch 39/55, Train Loss: 1.1334, Test Loss: 0.7517\n",
      "Epoch 40/55, Train Loss: 1.1528, Test Loss: 0.7545\n",
      "Epoch 41/55, Train Loss: 1.1355, Test Loss: 0.7562\n",
      "Epoch 42/55, Train Loss: 1.1243, Test Loss: 0.7558\n",
      "Epoch 43/55, Train Loss: 1.1350, Test Loss: 0.7579\n",
      "Epoch 44/55, Train Loss: 1.1356, Test Loss: 0.7534\n",
      "Epoch 45/55, Train Loss: 1.1418, Test Loss: 0.7517\n",
      "Epoch 46/55, Train Loss: 1.1218, Test Loss: 0.7538\n",
      "Epoch 47/55, Train Loss: 1.1117, Test Loss: 0.7543\n",
      "Epoch 48/55, Train Loss: 1.1124, Test Loss: 0.7604\n",
      "Epoch 49/55, Train Loss: 1.1315, Test Loss: 0.7651\n",
      "Epoch 50/55, Train Loss: 1.1113, Test Loss: 0.7624\n",
      "Epoch 51/55, Train Loss: 1.1206, Test Loss: 0.7565\n",
      "Epoch 52/55, Train Loss: 1.0930, Test Loss: 0.7559\n",
      "Epoch 53/55, Train Loss: 1.1106, Test Loss: 0.7574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:26:01,504] Trial 74 finished with value: 0.748815082013607 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 126, 'layer_1_size': 91, 'layer_2_size': 222, 'layer_3_size': 243, 'layer_4_size': 184, 'layer_5_size': 226, 'layer_6_size': 184, 'dropout_rate': 0.44037315892969925, 'learning_rate': 0.0001673583611932109, 'batch_size': 64, 'epochs': 55}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/55, Train Loss: 1.1051, Test Loss: 0.7503\n",
      "Epoch 55/55, Train Loss: 1.1055, Test Loss: 0.7488\n",
      "Epoch 1/61, Train Loss: 1.2457, Test Loss: 0.9605\n",
      "Epoch 2/61, Train Loss: 1.2119, Test Loss: 0.9607\n",
      "Epoch 3/61, Train Loss: 1.1713, Test Loss: 0.9614\n",
      "Epoch 4/61, Train Loss: 1.2048, Test Loss: 0.9624\n",
      "Epoch 5/61, Train Loss: 1.2160, Test Loss: 0.9634\n",
      "Epoch 6/61, Train Loss: 1.2018, Test Loss: 0.9646\n",
      "Epoch 7/61, Train Loss: 1.2410, Test Loss: 0.9656\n",
      "Epoch 8/61, Train Loss: 1.1510, Test Loss: 0.9672\n",
      "Epoch 9/61, Train Loss: 1.1729, Test Loss: 0.9694\n",
      "Epoch 10/61, Train Loss: 1.1712, Test Loss: 0.9704\n",
      "Epoch 11/61, Train Loss: 1.1256, Test Loss: 0.9704\n",
      "Epoch 12/61, Train Loss: 1.1363, Test Loss: 0.9718\n",
      "Epoch 13/61, Train Loss: 1.1217, Test Loss: 0.9723\n",
      "Epoch 14/61, Train Loss: 1.1685, Test Loss: 0.9708\n",
      "Epoch 15/61, Train Loss: 1.1735, Test Loss: 0.9703\n",
      "Epoch 16/61, Train Loss: 1.1595, Test Loss: 0.9712\n",
      "Epoch 17/61, Train Loss: 1.1785, Test Loss: 0.9716\n",
      "Epoch 18/61, Train Loss: 1.1506, Test Loss: 0.9718\n",
      "Epoch 19/61, Train Loss: 1.1335, Test Loss: 0.9718\n",
      "Epoch 20/61, Train Loss: 1.1811, Test Loss: 0.9719\n",
      "Epoch 21/61, Train Loss: 1.1104, Test Loss: 0.9722\n",
      "Epoch 22/61, Train Loss: 1.1199, Test Loss: 0.9726\n",
      "Epoch 23/61, Train Loss: 1.1290, Test Loss: 0.9720\n",
      "Epoch 24/61, Train Loss: 1.1221, Test Loss: 0.9719\n",
      "Epoch 25/61, Train Loss: 1.1173, Test Loss: 0.9709\n",
      "Epoch 26/61, Train Loss: 1.1236, Test Loss: 0.9703\n",
      "Epoch 27/61, Train Loss: 1.1061, Test Loss: 0.9708\n",
      "Epoch 28/61, Train Loss: 1.1079, Test Loss: 0.9704\n",
      "Epoch 29/61, Train Loss: 1.1198, Test Loss: 0.9704\n",
      "Epoch 30/61, Train Loss: 1.1114, Test Loss: 0.9709\n",
      "Epoch 31/61, Train Loss: 1.0567, Test Loss: 0.9715\n",
      "Epoch 32/61, Train Loss: 1.1058, Test Loss: 0.9715\n",
      "Epoch 33/61, Train Loss: 1.0989, Test Loss: 0.9721\n",
      "Epoch 34/61, Train Loss: 1.0855, Test Loss: 0.9715\n",
      "Epoch 35/61, Train Loss: 1.0742, Test Loss: 0.9712\n",
      "Epoch 36/61, Train Loss: 1.0905, Test Loss: 0.9697\n",
      "Epoch 37/61, Train Loss: 1.0870, Test Loss: 0.9701\n",
      "Epoch 38/61, Train Loss: 1.0765, Test Loss: 0.9695\n",
      "Epoch 39/61, Train Loss: 1.1014, Test Loss: 0.9699\n",
      "Epoch 40/61, Train Loss: 1.0958, Test Loss: 0.9697\n",
      "Epoch 41/61, Train Loss: 1.0330, Test Loss: 0.9693\n",
      "Epoch 42/61, Train Loss: 1.0789, Test Loss: 0.9693\n",
      "Epoch 43/61, Train Loss: 1.0683, Test Loss: 0.9691\n",
      "Epoch 44/61, Train Loss: 1.1179, Test Loss: 0.9685\n",
      "Epoch 45/61, Train Loss: 1.1070, Test Loss: 0.9688\n",
      "Epoch 46/61, Train Loss: 1.0468, Test Loss: 0.9690\n",
      "Epoch 47/61, Train Loss: 1.0618, Test Loss: 0.9687\n",
      "Epoch 48/61, Train Loss: 1.0767, Test Loss: 0.9680\n",
      "Epoch 49/61, Train Loss: 1.0909, Test Loss: 0.9674\n",
      "Epoch 50/61, Train Loss: 1.0941, Test Loss: 0.9669\n",
      "Epoch 51/61, Train Loss: 1.0887, Test Loss: 0.9666\n",
      "Epoch 52/61, Train Loss: 1.1045, Test Loss: 0.9668\n",
      "Epoch 53/61, Train Loss: 1.0529, Test Loss: 0.9670\n",
      "Epoch 54/61, Train Loss: 1.0471, Test Loss: 0.9670\n",
      "Epoch 55/61, Train Loss: 1.0783, Test Loss: 0.9675\n",
      "Epoch 56/61, Train Loss: 1.0567, Test Loss: 0.9682\n",
      "Epoch 57/61, Train Loss: 1.0578, Test Loss: 0.9688\n",
      "Epoch 58/61, Train Loss: 1.0764, Test Loss: 0.9691\n",
      "Epoch 59/61, Train Loss: 1.0577, Test Loss: 0.9693\n",
      "Epoch 60/61, Train Loss: 1.0613, Test Loss: 0.9696\n",
      "Epoch 61/61, Train Loss: 1.0487, Test Loss: 0.9690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:26:04,642] Trial 75 finished with value: 0.9689753651618958 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 124, 'layer_1_size': 96, 'layer_2_size': 240, 'layer_3_size': 241, 'layer_4_size': 190, 'layer_5_size': 247, 'layer_6_size': 190, 'dropout_rate': 0.45019609028590224, 'learning_rate': 0.0001851945237985418, 'batch_size': 256, 'epochs': 61}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/69, Train Loss: 1.4281, Test Loss: 0.8071\n",
      "Epoch 2/69, Train Loss: 1.3123, Test Loss: 0.8063\n",
      "Epoch 3/69, Train Loss: 1.2722, Test Loss: 0.8083\n",
      "Epoch 4/69, Train Loss: 1.2619, Test Loss: 0.8185\n",
      "Epoch 5/69, Train Loss: 1.2449, Test Loss: 0.8147\n",
      "Epoch 6/69, Train Loss: 1.2548, Test Loss: 0.8084\n",
      "Epoch 7/69, Train Loss: 1.3439, Test Loss: 0.8204\n",
      "Epoch 8/69, Train Loss: 1.2114, Test Loss: 0.8041\n",
      "Epoch 9/69, Train Loss: 1.2149, Test Loss: 0.8006\n",
      "Epoch 10/69, Train Loss: 1.2019, Test Loss: 0.7991\n",
      "Epoch 11/69, Train Loss: 1.2917, Test Loss: 0.8096\n",
      "Epoch 12/69, Train Loss: 1.2054, Test Loss: 0.8169\n",
      "Epoch 13/69, Train Loss: 1.1913, Test Loss: 0.8158\n",
      "Epoch 14/69, Train Loss: 1.1939, Test Loss: 0.8295\n",
      "Epoch 15/69, Train Loss: 1.1477, Test Loss: 0.8245\n",
      "Epoch 16/69, Train Loss: 1.1802, Test Loss: 0.8271\n",
      "Epoch 17/69, Train Loss: 1.1936, Test Loss: 0.8172\n",
      "Epoch 18/69, Train Loss: 1.2197, Test Loss: 0.8222\n",
      "Epoch 19/69, Train Loss: 1.2070, Test Loss: 0.8269\n",
      "Epoch 20/69, Train Loss: 1.1729, Test Loss: 0.8224\n",
      "Epoch 21/69, Train Loss: 1.1740, Test Loss: 0.8203\n",
      "Epoch 22/69, Train Loss: 1.1218, Test Loss: 0.8312\n",
      "Epoch 23/69, Train Loss: 1.1624, Test Loss: 0.8378\n",
      "Epoch 24/69, Train Loss: 1.1862, Test Loss: 0.8324\n",
      "Epoch 25/69, Train Loss: 1.1455, Test Loss: 0.8289\n",
      "Epoch 26/69, Train Loss: 1.1833, Test Loss: 0.8167\n",
      "Epoch 27/69, Train Loss: 1.1603, Test Loss: 0.8114\n",
      "Epoch 28/69, Train Loss: 1.1195, Test Loss: 0.8207\n",
      "Epoch 29/69, Train Loss: 1.1345, Test Loss: 0.8303\n",
      "Epoch 30/69, Train Loss: 1.1268, Test Loss: 0.8340\n",
      "Epoch 31/69, Train Loss: 1.1593, Test Loss: 0.8392\n",
      "Epoch 32/69, Train Loss: 1.1290, Test Loss: 0.8410\n",
      "Epoch 33/69, Train Loss: 1.1506, Test Loss: 0.8455\n",
      "Epoch 34/69, Train Loss: 1.1524, Test Loss: 0.8368\n",
      "Epoch 35/69, Train Loss: 1.1341, Test Loss: 0.8354\n",
      "Epoch 36/69, Train Loss: 1.1274, Test Loss: 0.8225\n",
      "Epoch 37/69, Train Loss: 1.1314, Test Loss: 0.8372\n",
      "Epoch 38/69, Train Loss: 1.1498, Test Loss: 0.8454\n",
      "Epoch 39/69, Train Loss: 1.1318, Test Loss: 0.8422\n",
      "Epoch 40/69, Train Loss: 1.1131, Test Loss: 0.8306\n",
      "Epoch 41/69, Train Loss: 1.1248, Test Loss: 0.8278\n",
      "Epoch 42/69, Train Loss: 1.1315, Test Loss: 0.8248\n",
      "Epoch 43/69, Train Loss: 1.1279, Test Loss: 0.8205\n",
      "Epoch 44/69, Train Loss: 1.1173, Test Loss: 0.8263\n",
      "Epoch 45/69, Train Loss: 1.1361, Test Loss: 0.8268\n",
      "Epoch 46/69, Train Loss: 1.1192, Test Loss: 0.8283\n",
      "Epoch 47/69, Train Loss: 1.1112, Test Loss: 0.8299\n",
      "Epoch 48/69, Train Loss: 1.1013, Test Loss: 0.8297\n",
      "Epoch 49/69, Train Loss: 1.1067, Test Loss: 0.8245\n",
      "Epoch 50/69, Train Loss: 1.1126, Test Loss: 0.8221\n",
      "Epoch 51/69, Train Loss: 1.1222, Test Loss: 0.8215\n",
      "Epoch 52/69, Train Loss: 1.0961, Test Loss: 0.8193\n",
      "Epoch 53/69, Train Loss: 1.0680, Test Loss: 0.8203\n",
      "Epoch 54/69, Train Loss: 1.1114, Test Loss: 0.8203\n",
      "Epoch 55/69, Train Loss: 1.1158, Test Loss: 0.8305\n",
      "Epoch 56/69, Train Loss: 1.1148, Test Loss: 0.8246\n",
      "Epoch 57/69, Train Loss: 1.0908, Test Loss: 0.8221\n",
      "Epoch 58/69, Train Loss: 1.1073, Test Loss: 0.8195\n",
      "Epoch 59/69, Train Loss: 1.1068, Test Loss: 0.8230\n",
      "Epoch 60/69, Train Loss: 1.0828, Test Loss: 0.8278\n",
      "Epoch 61/69, Train Loss: 1.0799, Test Loss: 0.8301\n",
      "Epoch 62/69, Train Loss: 1.0780, Test Loss: 0.8346\n",
      "Epoch 63/69, Train Loss: 1.1428, Test Loss: 0.8339\n",
      "Epoch 64/69, Train Loss: 1.0761, Test Loss: 0.8297\n",
      "Epoch 65/69, Train Loss: 1.0958, Test Loss: 0.8261\n",
      "Epoch 66/69, Train Loss: 1.0875, Test Loss: 0.8290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:26:11,390] Trial 76 finished with value: 0.8283257186412811 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 154, 'layer_1_size': 67, 'layer_2_size': 204, 'layer_3_size': 245, 'layer_4_size': 187, 'layer_5_size': 211, 'layer_6_size': 181, 'dropout_rate': 0.44076340728783336, 'learning_rate': 0.00014595547345595046, 'batch_size': 64, 'epochs': 69}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/69, Train Loss: 1.1020, Test Loss: 0.8268\n",
      "Epoch 68/69, Train Loss: 1.0871, Test Loss: 0.8303\n",
      "Epoch 69/69, Train Loss: 1.1289, Test Loss: 0.8283\n",
      "Epoch 1/68, Train Loss: 1.1774, Test Loss: 0.9114\n",
      "Epoch 2/68, Train Loss: 1.2338, Test Loss: 0.9146\n",
      "Epoch 3/68, Train Loss: 1.1659, Test Loss: 0.9193\n",
      "Epoch 4/68, Train Loss: 1.1221, Test Loss: 0.9219\n",
      "Epoch 5/68, Train Loss: 1.1358, Test Loss: 0.9231\n",
      "Epoch 6/68, Train Loss: 1.1091, Test Loss: 0.9221\n",
      "Epoch 7/68, Train Loss: 1.1315, Test Loss: 0.9241\n",
      "Epoch 8/68, Train Loss: 1.0524, Test Loss: 0.9217\n",
      "Epoch 9/68, Train Loss: 1.0992, Test Loss: 0.9183\n",
      "Epoch 10/68, Train Loss: 1.0535, Test Loss: 0.9159\n",
      "Epoch 11/68, Train Loss: 1.0894, Test Loss: 0.9152\n",
      "Epoch 12/68, Train Loss: 1.0437, Test Loss: 0.9150\n",
      "Epoch 13/68, Train Loss: 1.0506, Test Loss: 0.9139\n",
      "Epoch 14/68, Train Loss: 1.0552, Test Loss: 0.9140\n",
      "Epoch 15/68, Train Loss: 1.0319, Test Loss: 0.9132\n",
      "Epoch 16/68, Train Loss: 1.0073, Test Loss: 0.9133\n",
      "Epoch 17/68, Train Loss: 1.0588, Test Loss: 0.9131\n",
      "Epoch 18/68, Train Loss: 1.0606, Test Loss: 0.9139\n",
      "Epoch 19/68, Train Loss: 1.0252, Test Loss: 0.9133\n",
      "Epoch 20/68, Train Loss: 1.0274, Test Loss: 0.9127\n",
      "Epoch 21/68, Train Loss: 1.0378, Test Loss: 0.9134\n",
      "Epoch 22/68, Train Loss: 0.9985, Test Loss: 0.9136\n",
      "Epoch 23/68, Train Loss: 1.0077, Test Loss: 0.9138\n",
      "Epoch 24/68, Train Loss: 1.0252, Test Loss: 0.9143\n",
      "Epoch 25/68, Train Loss: 1.0027, Test Loss: 0.9147\n",
      "Epoch 26/68, Train Loss: 1.0128, Test Loss: 0.9155\n",
      "Epoch 27/68, Train Loss: 0.9894, Test Loss: 0.9148\n",
      "Epoch 28/68, Train Loss: 1.0113, Test Loss: 0.9145\n",
      "Epoch 29/68, Train Loss: 0.9734, Test Loss: 0.9147\n",
      "Epoch 30/68, Train Loss: 0.9993, Test Loss: 0.9140\n",
      "Epoch 31/68, Train Loss: 0.9943, Test Loss: 0.9137\n",
      "Epoch 32/68, Train Loss: 1.0029, Test Loss: 0.9139\n",
      "Epoch 33/68, Train Loss: 0.9833, Test Loss: 0.9135\n",
      "Epoch 34/68, Train Loss: 0.9856, Test Loss: 0.9134\n",
      "Epoch 35/68, Train Loss: 0.9795, Test Loss: 0.9135\n",
      "Epoch 36/68, Train Loss: 1.0016, Test Loss: 0.9134\n",
      "Epoch 37/68, Train Loss: 0.9798, Test Loss: 0.9136\n",
      "Epoch 38/68, Train Loss: 0.9653, Test Loss: 0.9132\n",
      "Epoch 39/68, Train Loss: 0.9899, Test Loss: 0.9129\n",
      "Epoch 40/68, Train Loss: 0.9963, Test Loss: 0.9130\n",
      "Epoch 41/68, Train Loss: 0.9742, Test Loss: 0.9137\n",
      "Epoch 42/68, Train Loss: 0.9800, Test Loss: 0.9143\n",
      "Epoch 43/68, Train Loss: 0.9765, Test Loss: 0.9143\n",
      "Epoch 44/68, Train Loss: 0.9560, Test Loss: 0.9143\n",
      "Epoch 45/68, Train Loss: 0.9602, Test Loss: 0.9138\n",
      "Epoch 46/68, Train Loss: 0.9768, Test Loss: 0.9138\n",
      "Epoch 47/68, Train Loss: 0.9772, Test Loss: 0.9129\n",
      "Epoch 48/68, Train Loss: 0.9660, Test Loss: 0.9129\n",
      "Epoch 49/68, Train Loss: 0.9602, Test Loss: 0.9127\n",
      "Epoch 50/68, Train Loss: 0.9683, Test Loss: 0.9126\n",
      "Epoch 51/68, Train Loss: 0.9534, Test Loss: 0.9120\n",
      "Epoch 52/68, Train Loss: 0.9669, Test Loss: 0.9118\n",
      "Epoch 53/68, Train Loss: 0.9445, Test Loss: 0.9118\n",
      "Epoch 54/68, Train Loss: 0.9717, Test Loss: 0.9118\n",
      "Epoch 55/68, Train Loss: 0.9551, Test Loss: 0.9120\n",
      "Epoch 56/68, Train Loss: 0.9682, Test Loss: 0.9120\n",
      "Epoch 57/68, Train Loss: 0.9475, Test Loss: 0.9119\n",
      "Epoch 58/68, Train Loss: 0.9577, Test Loss: 0.9121\n",
      "Epoch 59/68, Train Loss: 0.9408, Test Loss: 0.9124\n",
      "Epoch 60/68, Train Loss: 0.9543, Test Loss: 0.9121\n",
      "Epoch 61/68, Train Loss: 0.9707, Test Loss: 0.9123\n",
      "Epoch 62/68, Train Loss: 0.9402, Test Loss: 0.9121\n",
      "Epoch 63/68, Train Loss: 0.9494, Test Loss: 0.9121\n",
      "Epoch 64/68, Train Loss: 0.9617, Test Loss: 0.9119\n",
      "Epoch 65/68, Train Loss: 0.9730, Test Loss: 0.9120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:26:15,692] Trial 77 finished with value: 0.9117878675460815 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 133, 'layer_1_size': 56, 'layer_2_size': 167, 'layer_3_size': 244, 'layer_4_size': 171, 'layer_5_size': 211, 'layer_6_size': 181, 'layer_7_size': 37, 'dropout_rate': 0.49888650000974566, 'learning_rate': 0.0003436480252583541, 'batch_size': 128, 'epochs': 68}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/68, Train Loss: 0.9696, Test Loss: 0.9118\n",
      "Epoch 67/68, Train Loss: 0.9660, Test Loss: 0.9118\n",
      "Epoch 68/68, Train Loss: 0.9538, Test Loss: 0.9118\n",
      "Epoch 1/48, Train Loss: 1.1703, Test Loss: 1.0734\n",
      "Epoch 2/48, Train Loss: 1.1085, Test Loss: 1.0693\n",
      "Epoch 3/48, Train Loss: 1.1187, Test Loss: 1.0636\n",
      "Epoch 4/48, Train Loss: 1.0728, Test Loss: 1.0558\n",
      "Epoch 5/48, Train Loss: 1.1039, Test Loss: 1.0586\n",
      "Epoch 6/48, Train Loss: 1.0892, Test Loss: 1.0549\n",
      "Epoch 7/48, Train Loss: 1.0514, Test Loss: 1.0498\n",
      "Epoch 8/48, Train Loss: 1.0436, Test Loss: 1.0463\n",
      "Epoch 9/48, Train Loss: 1.0850, Test Loss: 1.0461\n",
      "Epoch 10/48, Train Loss: 1.0952, Test Loss: 1.0482\n",
      "Epoch 11/48, Train Loss: 1.0635, Test Loss: 1.0490\n",
      "Epoch 12/48, Train Loss: 1.0670, Test Loss: 1.0514\n",
      "Epoch 13/48, Train Loss: 1.1114, Test Loss: 1.0489\n",
      "Epoch 14/48, Train Loss: 1.0487, Test Loss: 1.0506\n",
      "Epoch 15/48, Train Loss: 1.0445, Test Loss: 1.0507\n",
      "Epoch 16/48, Train Loss: 1.0458, Test Loss: 1.0500\n",
      "Epoch 17/48, Train Loss: 0.9971, Test Loss: 1.0498\n",
      "Epoch 18/48, Train Loss: 1.0447, Test Loss: 1.0459\n",
      "Epoch 19/48, Train Loss: 1.0426, Test Loss: 1.0468\n",
      "Epoch 20/48, Train Loss: 1.0407, Test Loss: 1.0470\n",
      "Epoch 21/48, Train Loss: 1.0412, Test Loss: 1.0473\n",
      "Epoch 22/48, Train Loss: 1.0541, Test Loss: 1.0504\n",
      "Epoch 23/48, Train Loss: 1.0175, Test Loss: 1.0591\n",
      "Epoch 24/48, Train Loss: 1.0261, Test Loss: 1.0567\n",
      "Epoch 25/48, Train Loss: 1.0145, Test Loss: 1.0554\n",
      "Epoch 26/48, Train Loss: 1.0152, Test Loss: 1.0502\n",
      "Epoch 27/48, Train Loss: 1.0046, Test Loss: 1.0487\n",
      "Epoch 28/48, Train Loss: 1.0167, Test Loss: 1.0459\n",
      "Epoch 29/48, Train Loss: 1.0074, Test Loss: 1.0443\n",
      "Epoch 30/48, Train Loss: 1.0087, Test Loss: 1.0472\n",
      "Epoch 31/48, Train Loss: 1.0146, Test Loss: 1.0501\n",
      "Epoch 32/48, Train Loss: 1.0051, Test Loss: 1.0516\n",
      "Epoch 33/48, Train Loss: 0.9878, Test Loss: 1.0506\n",
      "Epoch 34/48, Train Loss: 1.0035, Test Loss: 1.0542\n",
      "Epoch 35/48, Train Loss: 1.0051, Test Loss: 1.0514\n",
      "Epoch 36/48, Train Loss: 1.0420, Test Loss: 1.0537\n",
      "Epoch 37/48, Train Loss: 1.0132, Test Loss: 1.0544\n",
      "Epoch 38/48, Train Loss: 1.0183, Test Loss: 1.0546\n",
      "Epoch 39/48, Train Loss: 1.0163, Test Loss: 1.0584\n",
      "Epoch 40/48, Train Loss: 0.9899, Test Loss: 1.0555\n",
      "Epoch 41/48, Train Loss: 1.0224, Test Loss: 1.0567\n",
      "Epoch 42/48, Train Loss: 1.0134, Test Loss: 1.0583\n",
      "Epoch 43/48, Train Loss: 1.0148, Test Loss: 1.0604\n",
      "Epoch 44/48, Train Loss: 0.9859, Test Loss: 1.0620\n",
      "Epoch 45/48, Train Loss: 1.0085, Test Loss: 1.0676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:26:19,392] Trial 78 finished with value: 1.0648044347763062 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 141, 'layer_1_size': 66, 'layer_2_size': 204, 'layer_3_size': 255, 'layer_4_size': 185, 'layer_5_size': 138, 'layer_6_size': 162, 'dropout_rate': 0.35119895951152436, 'learning_rate': 0.00015884382101023697, 'batch_size': 64, 'epochs': 48}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/48, Train Loss: 0.9974, Test Loss: 1.0646\n",
      "Epoch 47/48, Train Loss: 0.9902, Test Loss: 1.0652\n",
      "Epoch 48/48, Train Loss: 1.0008, Test Loss: 1.0648\n",
      "Epoch 1/51, Train Loss: 1.3942, Test Loss: 0.9689\n",
      "Epoch 2/51, Train Loss: 1.2996, Test Loss: 0.9844\n",
      "Epoch 3/51, Train Loss: 1.2559, Test Loss: 0.9890\n",
      "Epoch 4/51, Train Loss: 1.2496, Test Loss: 0.9806\n",
      "Epoch 5/51, Train Loss: 1.2027, Test Loss: 0.9682\n",
      "Epoch 6/51, Train Loss: 1.1488, Test Loss: 0.9618\n",
      "Epoch 7/51, Train Loss: 1.1674, Test Loss: 0.9533\n",
      "Epoch 8/51, Train Loss: 1.2104, Test Loss: 0.9466\n",
      "Epoch 9/51, Train Loss: 1.1647, Test Loss: 0.9385\n",
      "Epoch 10/51, Train Loss: 1.1824, Test Loss: 0.9350\n",
      "Epoch 11/51, Train Loss: 1.1557, Test Loss: 0.9436\n",
      "Epoch 12/51, Train Loss: 1.1774, Test Loss: 0.9502\n",
      "Epoch 13/51, Train Loss: 1.1020, Test Loss: 0.9462\n",
      "Epoch 14/51, Train Loss: 1.0702, Test Loss: 0.9382\n",
      "Epoch 15/51, Train Loss: 1.0810, Test Loss: 0.9358\n",
      "Epoch 16/51, Train Loss: 1.1329, Test Loss: 0.9391\n",
      "Epoch 17/51, Train Loss: 1.1131, Test Loss: 0.9337\n",
      "Epoch 18/51, Train Loss: 1.0819, Test Loss: 0.9329\n",
      "Epoch 19/51, Train Loss: 1.0856, Test Loss: 0.9319\n",
      "Epoch 20/51, Train Loss: 1.1019, Test Loss: 0.9319\n",
      "Epoch 21/51, Train Loss: 1.0515, Test Loss: 0.9336\n",
      "Epoch 22/51, Train Loss: 1.0618, Test Loss: 0.9382\n",
      "Epoch 23/51, Train Loss: 1.1044, Test Loss: 0.9336\n",
      "Epoch 24/51, Train Loss: 1.0670, Test Loss: 0.9366\n",
      "Epoch 25/51, Train Loss: 1.1043, Test Loss: 0.9401\n",
      "Epoch 26/51, Train Loss: 1.0759, Test Loss: 0.9405\n",
      "Epoch 27/51, Train Loss: 1.0822, Test Loss: 0.9392\n",
      "Epoch 28/51, Train Loss: 1.0697, Test Loss: 0.9382\n",
      "Epoch 29/51, Train Loss: 1.0645, Test Loss: 0.9351\n",
      "Epoch 30/51, Train Loss: 1.0470, Test Loss: 0.9329\n",
      "Epoch 31/51, Train Loss: 1.0635, Test Loss: 0.9343\n",
      "Epoch 32/51, Train Loss: 1.0487, Test Loss: 0.9290\n",
      "Epoch 33/51, Train Loss: 1.0294, Test Loss: 0.9334\n",
      "Epoch 34/51, Train Loss: 1.0357, Test Loss: 0.9374\n",
      "Epoch 35/51, Train Loss: 1.0369, Test Loss: 0.9418\n",
      "Epoch 36/51, Train Loss: 1.0267, Test Loss: 0.9457\n",
      "Epoch 37/51, Train Loss: 1.0688, Test Loss: 0.9497\n",
      "Epoch 38/51, Train Loss: 1.0706, Test Loss: 0.9462\n",
      "Epoch 39/51, Train Loss: 1.0395, Test Loss: 0.9435\n",
      "Epoch 40/51, Train Loss: 1.0334, Test Loss: 0.9390\n",
      "Epoch 41/51, Train Loss: 1.0449, Test Loss: 0.9402\n",
      "Epoch 42/51, Train Loss: 1.0469, Test Loss: 0.9329\n",
      "Epoch 43/51, Train Loss: 1.0540, Test Loss: 0.9328\n",
      "Epoch 44/51, Train Loss: 1.0401, Test Loss: 0.9322\n",
      "Epoch 45/51, Train Loss: 1.0787, Test Loss: 0.9326\n",
      "Epoch 46/51, Train Loss: 1.0520, Test Loss: 0.9327\n",
      "Epoch 47/51, Train Loss: 1.0501, Test Loss: 0.9377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:26:23,177] Trial 79 finished with value: 0.9402918964624405 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 152, 'layer_1_size': 44, 'layer_2_size': 179, 'layer_3_size': 222, 'layer_4_size': 179, 'layer_5_size': 181, 'layer_6_size': 148, 'dropout_rate': 0.48687479346330825, 'learning_rate': 0.00021787717351731708, 'batch_size': 64, 'epochs': 51}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/51, Train Loss: 1.0361, Test Loss: 0.9404\n",
      "Epoch 49/51, Train Loss: 1.0268, Test Loss: 0.9393\n",
      "Epoch 50/51, Train Loss: 1.0206, Test Loss: 0.9393\n",
      "Epoch 51/51, Train Loss: 1.0116, Test Loss: 0.9403\n",
      "Epoch 1/56, Train Loss: 1.3097, Test Loss: 0.8912\n",
      "Epoch 2/56, Train Loss: 1.2542, Test Loss: 0.8832\n",
      "Epoch 3/56, Train Loss: 1.2432, Test Loss: 0.8852\n",
      "Epoch 4/56, Train Loss: 1.2297, Test Loss: 0.8832\n",
      "Epoch 5/56, Train Loss: 1.2798, Test Loss: 0.8898\n",
      "Epoch 6/56, Train Loss: 1.2155, Test Loss: 0.8859\n",
      "Epoch 7/56, Train Loss: 1.2091, Test Loss: 0.8842\n",
      "Epoch 8/56, Train Loss: 1.2430, Test Loss: 0.8902\n",
      "Epoch 9/56, Train Loss: 1.1866, Test Loss: 0.8895\n",
      "Epoch 10/56, Train Loss: 1.1665, Test Loss: 0.8880\n",
      "Epoch 11/56, Train Loss: 1.2051, Test Loss: 0.8937\n",
      "Epoch 12/56, Train Loss: 1.1793, Test Loss: 0.8912\n",
      "Epoch 13/56, Train Loss: 1.1524, Test Loss: 0.8918\n",
      "Epoch 14/56, Train Loss: 1.1671, Test Loss: 0.8930\n",
      "Epoch 15/56, Train Loss: 1.1581, Test Loss: 0.8905\n",
      "Epoch 16/56, Train Loss: 1.1517, Test Loss: 0.8931\n",
      "Epoch 17/56, Train Loss: 1.1848, Test Loss: 0.8952\n",
      "Epoch 18/56, Train Loss: 1.1301, Test Loss: 0.8983\n",
      "Epoch 19/56, Train Loss: 1.2042, Test Loss: 0.8963\n",
      "Epoch 20/56, Train Loss: 1.1635, Test Loss: 0.8894\n",
      "Epoch 21/56, Train Loss: 1.1585, Test Loss: 0.8931\n",
      "Epoch 22/56, Train Loss: 1.1312, Test Loss: 0.8904\n",
      "Epoch 23/56, Train Loss: 1.1683, Test Loss: 0.8906\n",
      "Epoch 24/56, Train Loss: 1.1243, Test Loss: 0.8899\n",
      "Epoch 25/56, Train Loss: 1.1489, Test Loss: 0.8946\n",
      "Epoch 26/56, Train Loss: 1.1457, Test Loss: 0.8973\n",
      "Epoch 27/56, Train Loss: 1.1612, Test Loss: 0.8950\n",
      "Epoch 28/56, Train Loss: 1.1597, Test Loss: 0.8956\n",
      "Epoch 29/56, Train Loss: 1.1565, Test Loss: 0.8982\n",
      "Epoch 30/56, Train Loss: 1.1500, Test Loss: 0.8939\n",
      "Epoch 31/56, Train Loss: 1.1046, Test Loss: 0.8910\n",
      "Epoch 32/56, Train Loss: 1.1569, Test Loss: 0.8925\n",
      "Epoch 33/56, Train Loss: 1.1304, Test Loss: 0.8935\n",
      "Epoch 34/56, Train Loss: 1.1495, Test Loss: 0.8953\n",
      "Epoch 35/56, Train Loss: 1.1424, Test Loss: 0.8948\n",
      "Epoch 36/56, Train Loss: 1.1224, Test Loss: 0.8986\n",
      "Epoch 37/56, Train Loss: 1.1043, Test Loss: 0.8945\n",
      "Epoch 38/56, Train Loss: 1.1212, Test Loss: 0.8881\n",
      "Epoch 39/56, Train Loss: 1.1254, Test Loss: 0.8871\n",
      "Epoch 40/56, Train Loss: 1.1329, Test Loss: 0.8975\n",
      "Epoch 41/56, Train Loss: 1.1081, Test Loss: 0.8941\n",
      "Epoch 42/56, Train Loss: 1.1312, Test Loss: 0.8942\n",
      "Epoch 43/56, Train Loss: 1.1141, Test Loss: 0.8910\n",
      "Epoch 44/56, Train Loss: 1.1250, Test Loss: 0.8896\n",
      "Epoch 45/56, Train Loss: 1.1373, Test Loss: 0.8843\n",
      "Epoch 46/56, Train Loss: 1.0997, Test Loss: 0.8887\n",
      "Epoch 47/56, Train Loss: 1.1121, Test Loss: 0.8872\n",
      "Epoch 48/56, Train Loss: 1.1381, Test Loss: 0.8868\n",
      "Epoch 49/56, Train Loss: 1.1159, Test Loss: 0.8884\n",
      "Epoch 50/56, Train Loss: 1.1292, Test Loss: 0.8914\n",
      "Epoch 51/56, Train Loss: 1.1341, Test Loss: 0.8943\n",
      "Epoch 52/56, Train Loss: 1.0979, Test Loss: 0.8893\n",
      "Epoch 53/56, Train Loss: 1.1268, Test Loss: 0.8903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:26:27,520] Trial 80 finished with value: 0.8901263028383255 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 87, 'layer_1_size': 89, 'layer_2_size': 146, 'layer_3_size': 234, 'layer_4_size': 124, 'layer_5_size': 149, 'layer_6_size': 177, 'dropout_rate': 0.39448357701869236, 'learning_rate': 0.00012710259301123528, 'batch_size': 64, 'epochs': 56}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/56, Train Loss: 1.1088, Test Loss: 0.8892\n",
      "Epoch 55/56, Train Loss: 1.0959, Test Loss: 0.8899\n",
      "Epoch 56/56, Train Loss: 1.1042, Test Loss: 0.8901\n",
      "Epoch 1/44, Train Loss: 1.2765, Test Loss: 1.0484\n",
      "Epoch 2/44, Train Loss: 1.3169, Test Loss: 1.0458\n",
      "Epoch 3/44, Train Loss: 1.2882, Test Loss: 1.0345\n",
      "Epoch 4/44, Train Loss: 1.2284, Test Loss: 1.0310\n",
      "Epoch 5/44, Train Loss: 1.2186, Test Loss: 1.0368\n",
      "Epoch 6/44, Train Loss: 1.2365, Test Loss: 1.0387\n",
      "Epoch 7/44, Train Loss: 1.2055, Test Loss: 1.0405\n",
      "Epoch 8/44, Train Loss: 1.2060, Test Loss: 1.0355\n",
      "Epoch 9/44, Train Loss: 1.2633, Test Loss: 1.0351\n",
      "Epoch 10/44, Train Loss: 1.2617, Test Loss: 1.0340\n",
      "Epoch 11/44, Train Loss: 1.1670, Test Loss: 1.0358\n",
      "Epoch 12/44, Train Loss: 1.1936, Test Loss: 1.0360\n",
      "Epoch 13/44, Train Loss: 1.2221, Test Loss: 1.0369\n",
      "Epoch 14/44, Train Loss: 1.2081, Test Loss: 1.0338\n",
      "Epoch 15/44, Train Loss: 1.2163, Test Loss: 1.0354\n",
      "Epoch 16/44, Train Loss: 1.1780, Test Loss: 1.0350\n",
      "Epoch 17/44, Train Loss: 1.1927, Test Loss: 1.0377\n",
      "Epoch 18/44, Train Loss: 1.2074, Test Loss: 1.0381\n",
      "Epoch 19/44, Train Loss: 1.1567, Test Loss: 1.0321\n",
      "Epoch 20/44, Train Loss: 1.1443, Test Loss: 1.0332\n",
      "Epoch 21/44, Train Loss: 1.1826, Test Loss: 1.0339\n",
      "Epoch 22/44, Train Loss: 1.1992, Test Loss: 1.0333\n",
      "Epoch 23/44, Train Loss: 1.1685, Test Loss: 1.0365\n",
      "Epoch 24/44, Train Loss: 1.1744, Test Loss: 1.0333\n",
      "Epoch 25/44, Train Loss: 1.2059, Test Loss: 1.0327\n",
      "Epoch 26/44, Train Loss: 1.1942, Test Loss: 1.0357\n",
      "Epoch 27/44, Train Loss: 1.1659, Test Loss: 1.0385\n",
      "Epoch 28/44, Train Loss: 1.1862, Test Loss: 1.0422\n",
      "Epoch 29/44, Train Loss: 1.1569, Test Loss: 1.0364\n",
      "Epoch 30/44, Train Loss: 1.1635, Test Loss: 1.0381\n",
      "Epoch 31/44, Train Loss: 1.1598, Test Loss: 1.0378\n",
      "Epoch 32/44, Train Loss: 1.1300, Test Loss: 1.0382\n",
      "Epoch 33/44, Train Loss: 1.1493, Test Loss: 1.0397\n",
      "Epoch 34/44, Train Loss: 1.1520, Test Loss: 1.0376\n",
      "Epoch 35/44, Train Loss: 1.1463, Test Loss: 1.0371\n",
      "Epoch 36/44, Train Loss: 1.1703, Test Loss: 1.0368\n",
      "Epoch 37/44, Train Loss: 1.1275, Test Loss: 1.0364\n",
      "Epoch 38/44, Train Loss: 1.1383, Test Loss: 1.0377\n",
      "Epoch 39/44, Train Loss: 1.1852, Test Loss: 1.0386\n",
      "Epoch 40/44, Train Loss: 1.1673, Test Loss: 1.0362\n",
      "Epoch 41/44, Train Loss: 1.1506, Test Loss: 1.0384\n",
      "Epoch 42/44, Train Loss: 1.1202, Test Loss: 1.0396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:26:30,879] Trial 81 finished with value: 1.0383488088846207 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 175, 'layer_1_size': 72, 'layer_2_size': 221, 'layer_3_size': 249, 'layer_4_size': 209, 'layer_5_size': 238, 'dropout_rate': 0.43783833709569625, 'learning_rate': 0.00015391785422123584, 'batch_size': 64, 'epochs': 44}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/44, Train Loss: 1.1616, Test Loss: 1.0386\n",
      "Epoch 44/44, Train Loss: 1.1165, Test Loss: 1.0383\n",
      "Epoch 1/70, Train Loss: 1.1459, Test Loss: 0.9670\n",
      "Epoch 2/70, Train Loss: 1.1247, Test Loss: 0.9702\n",
      "Epoch 3/70, Train Loss: 1.1401, Test Loss: 0.9739\n",
      "Epoch 4/70, Train Loss: 1.1701, Test Loss: 0.9710\n",
      "Epoch 5/70, Train Loss: 1.1206, Test Loss: 0.9682\n",
      "Epoch 6/70, Train Loss: 1.1510, Test Loss: 0.9659\n",
      "Epoch 7/70, Train Loss: 1.0829, Test Loss: 0.9661\n",
      "Epoch 8/70, Train Loss: 1.0761, Test Loss: 0.9648\n",
      "Epoch 9/70, Train Loss: 1.0719, Test Loss: 0.9637\n",
      "Epoch 10/70, Train Loss: 1.0752, Test Loss: 0.9612\n",
      "Epoch 11/70, Train Loss: 1.1023, Test Loss: 0.9635\n",
      "Epoch 12/70, Train Loss: 1.0117, Test Loss: 0.9659\n",
      "Epoch 13/70, Train Loss: 1.0689, Test Loss: 0.9638\n",
      "Epoch 14/70, Train Loss: 1.0332, Test Loss: 0.9673\n",
      "Epoch 15/70, Train Loss: 1.0668, Test Loss: 0.9642\n",
      "Epoch 16/70, Train Loss: 1.0970, Test Loss: 0.9644\n",
      "Epoch 17/70, Train Loss: 1.1107, Test Loss: 0.9625\n",
      "Epoch 18/70, Train Loss: 1.1179, Test Loss: 0.9665\n",
      "Epoch 19/70, Train Loss: 1.0410, Test Loss: 0.9662\n",
      "Epoch 20/70, Train Loss: 1.0208, Test Loss: 0.9625\n",
      "Epoch 21/70, Train Loss: 1.0596, Test Loss: 0.9646\n",
      "Epoch 22/70, Train Loss: 1.0509, Test Loss: 0.9630\n",
      "Epoch 23/70, Train Loss: 1.0831, Test Loss: 0.9652\n",
      "Epoch 24/70, Train Loss: 1.1046, Test Loss: 0.9693\n",
      "Epoch 25/70, Train Loss: 1.0064, Test Loss: 0.9660\n",
      "Epoch 26/70, Train Loss: 1.0405, Test Loss: 0.9639\n",
      "Epoch 27/70, Train Loss: 1.0051, Test Loss: 0.9660\n",
      "Epoch 28/70, Train Loss: 1.0222, Test Loss: 0.9652\n",
      "Epoch 29/70, Train Loss: 1.0680, Test Loss: 0.9650\n",
      "Epoch 30/70, Train Loss: 1.0234, Test Loss: 0.9667\n",
      "Epoch 31/70, Train Loss: 1.0300, Test Loss: 0.9663\n",
      "Epoch 32/70, Train Loss: 1.0590, Test Loss: 0.9628\n",
      "Epoch 33/70, Train Loss: 1.0809, Test Loss: 0.9612\n",
      "Epoch 34/70, Train Loss: 1.0154, Test Loss: 0.9636\n",
      "Epoch 35/70, Train Loss: 1.0074, Test Loss: 0.9659\n",
      "Epoch 36/70, Train Loss: 0.9881, Test Loss: 0.9675\n",
      "Epoch 37/70, Train Loss: 0.9340, Test Loss: 0.9671\n",
      "Epoch 38/70, Train Loss: 1.0063, Test Loss: 0.9684\n",
      "Epoch 39/70, Train Loss: 0.9609, Test Loss: 0.9666\n",
      "Epoch 40/70, Train Loss: 1.0380, Test Loss: 0.9660\n",
      "Epoch 41/70, Train Loss: 0.9954, Test Loss: 0.9676\n",
      "Epoch 42/70, Train Loss: 1.0229, Test Loss: 0.9702\n",
      "Epoch 43/70, Train Loss: 0.9946, Test Loss: 0.9670\n",
      "Epoch 44/70, Train Loss: 1.0397, Test Loss: 0.9698\n",
      "Epoch 45/70, Train Loss: 1.0216, Test Loss: 0.9676\n",
      "Epoch 46/70, Train Loss: 1.0111, Test Loss: 0.9671\n",
      "Epoch 47/70, Train Loss: 1.0145, Test Loss: 0.9638\n",
      "Epoch 48/70, Train Loss: 1.0250, Test Loss: 0.9644\n",
      "Epoch 49/70, Train Loss: 1.0019, Test Loss: 0.9642\n",
      "Epoch 50/70, Train Loss: 0.9997, Test Loss: 0.9659\n",
      "Epoch 51/70, Train Loss: 1.0118, Test Loss: 0.9650\n",
      "Epoch 52/70, Train Loss: 0.9635, Test Loss: 0.9635\n",
      "Epoch 53/70, Train Loss: 0.9820, Test Loss: 0.9648\n",
      "Epoch 54/70, Train Loss: 1.0170, Test Loss: 0.9660\n",
      "Epoch 55/70, Train Loss: 1.0281, Test Loss: 0.9627\n",
      "Epoch 56/70, Train Loss: 0.9800, Test Loss: 0.9611\n",
      "Epoch 57/70, Train Loss: 0.9649, Test Loss: 0.9620\n",
      "Epoch 58/70, Train Loss: 1.0062, Test Loss: 0.9636\n",
      "Epoch 59/70, Train Loss: 1.0004, Test Loss: 0.9618\n",
      "Epoch 60/70, Train Loss: 0.9834, Test Loss: 0.9660\n",
      "Epoch 61/70, Train Loss: 0.9718, Test Loss: 0.9652\n",
      "Epoch 62/70, Train Loss: 0.9948, Test Loss: 0.9632\n",
      "Epoch 63/70, Train Loss: 1.0013, Test Loss: 0.9656\n",
      "Epoch 64/70, Train Loss: 0.9353, Test Loss: 0.9637\n",
      "Epoch 65/70, Train Loss: 0.9612, Test Loss: 0.9623\n",
      "Epoch 66/70, Train Loss: 0.9462, Test Loss: 0.9633\n",
      "Epoch 67/70, Train Loss: 0.9582, Test Loss: 0.9668\n",
      "Epoch 68/70, Train Loss: 0.9925, Test Loss: 0.9643\n",
      "Epoch 69/70, Train Loss: 0.9882, Test Loss: 0.9668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:26:36,717] Trial 82 finished with value: 0.9646165817975998 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 155, 'layer_1_size': 100, 'layer_2_size': 193, 'layer_3_size': 137, 'layer_4_size': 193, 'layer_5_size': 155, 'dropout_rate': 0.4417613339433967, 'learning_rate': 7.683722728200785e-05, 'batch_size': 64, 'epochs': 70}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/70, Train Loss: 0.9852, Test Loss: 0.9646\n",
      "Epoch 1/5, Train Loss: 1.2676, Test Loss: 1.0465\n",
      "Epoch 2/5, Train Loss: 1.2579, Test Loss: 1.0645\n",
      "Epoch 3/5, Train Loss: 1.2512, Test Loss: 1.0677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:26:37,336] Trial 83 finished with value: 1.0705087184906006 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 126, 'layer_1_size': 111, 'layer_2_size': 227, 'layer_3_size': 237, 'layer_4_size': 166, 'layer_5_size': 249, 'layer_6_size': 201, 'dropout_rate': 0.42154531444973176, 'learning_rate': 9.811439402591764e-05, 'batch_size': 64, 'epochs': 5}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Train Loss: 1.2516, Test Loss: 1.0655\n",
      "Epoch 5/5, Train Loss: 1.2704, Test Loss: 1.0705\n",
      "Epoch 1/64, Train Loss: 1.2030, Test Loss: 1.0221\n",
      "Epoch 2/64, Train Loss: 1.2041, Test Loss: 1.0320\n",
      "Epoch 3/64, Train Loss: 1.1649, Test Loss: 1.0371\n",
      "Epoch 4/64, Train Loss: 1.1340, Test Loss: 1.0367\n",
      "Epoch 5/64, Train Loss: 1.0907, Test Loss: 1.0379\n",
      "Epoch 6/64, Train Loss: 1.1552, Test Loss: 1.0465\n",
      "Epoch 7/64, Train Loss: 1.1786, Test Loss: 1.0390\n",
      "Epoch 8/64, Train Loss: 1.1673, Test Loss: 1.0325\n",
      "Epoch 9/64, Train Loss: 1.1377, Test Loss: 1.0287\n",
      "Epoch 10/64, Train Loss: 1.0939, Test Loss: 1.0345\n",
      "Epoch 11/64, Train Loss: 1.1227, Test Loss: 1.0348\n",
      "Epoch 12/64, Train Loss: 1.0986, Test Loss: 1.0332\n",
      "Epoch 13/64, Train Loss: 1.1231, Test Loss: 1.0345\n",
      "Epoch 14/64, Train Loss: 1.0611, Test Loss: 1.0377\n",
      "Epoch 15/64, Train Loss: 1.0981, Test Loss: 1.0391\n",
      "Epoch 16/64, Train Loss: 1.1025, Test Loss: 1.0417\n",
      "Epoch 17/64, Train Loss: 1.1013, Test Loss: 1.0457\n",
      "Epoch 18/64, Train Loss: 1.0877, Test Loss: 1.0436\n",
      "Epoch 19/64, Train Loss: 1.0795, Test Loss: 1.0486\n",
      "Epoch 20/64, Train Loss: 1.0890, Test Loss: 1.0374\n",
      "Epoch 21/64, Train Loss: 1.0971, Test Loss: 1.0389\n",
      "Epoch 22/64, Train Loss: 1.0822, Test Loss: 1.0363\n",
      "Epoch 23/64, Train Loss: 1.0743, Test Loss: 1.0414\n",
      "Epoch 24/64, Train Loss: 1.0988, Test Loss: 1.0371\n",
      "Epoch 25/64, Train Loss: 1.0440, Test Loss: 1.0334\n",
      "Epoch 26/64, Train Loss: 1.0741, Test Loss: 1.0355\n",
      "Epoch 27/64, Train Loss: 1.1009, Test Loss: 1.0351\n",
      "Epoch 28/64, Train Loss: 1.0871, Test Loss: 1.0335\n",
      "Epoch 29/64, Train Loss: 1.0715, Test Loss: 1.0325\n",
      "Epoch 30/64, Train Loss: 1.0506, Test Loss: 1.0353\n",
      "Epoch 31/64, Train Loss: 1.0642, Test Loss: 1.0348\n",
      "Epoch 32/64, Train Loss: 1.0577, Test Loss: 1.0338\n",
      "Epoch 33/64, Train Loss: 1.0823, Test Loss: 1.0291\n",
      "Epoch 34/64, Train Loss: 1.0412, Test Loss: 1.0286\n",
      "Epoch 35/64, Train Loss: 1.0592, Test Loss: 1.0316\n",
      "Epoch 36/64, Train Loss: 1.0451, Test Loss: 1.0327\n",
      "Epoch 37/64, Train Loss: 1.0504, Test Loss: 1.0336\n",
      "Epoch 38/64, Train Loss: 1.0692, Test Loss: 1.0311\n",
      "Epoch 39/64, Train Loss: 1.0590, Test Loss: 1.0328\n",
      "Epoch 40/64, Train Loss: 1.0469, Test Loss: 1.0304\n",
      "Epoch 41/64, Train Loss: 1.0554, Test Loss: 1.0321\n",
      "Epoch 42/64, Train Loss: 1.0622, Test Loss: 1.0306\n",
      "Epoch 43/64, Train Loss: 1.0244, Test Loss: 1.0251\n",
      "Epoch 44/64, Train Loss: 1.0348, Test Loss: 1.0266\n",
      "Epoch 45/64, Train Loss: 1.0663, Test Loss: 1.0254\n",
      "Epoch 46/64, Train Loss: 1.0466, Test Loss: 1.0289\n",
      "Epoch 47/64, Train Loss: 1.0692, Test Loss: 1.0283\n",
      "Epoch 48/64, Train Loss: 1.0228, Test Loss: 1.0282\n",
      "Epoch 49/64, Train Loss: 1.0610, Test Loss: 1.0284\n",
      "Epoch 50/64, Train Loss: 1.0494, Test Loss: 1.0265\n",
      "Epoch 51/64, Train Loss: 1.0371, Test Loss: 1.0273\n",
      "Epoch 52/64, Train Loss: 1.0507, Test Loss: 1.0278\n",
      "Epoch 53/64, Train Loss: 1.0735, Test Loss: 1.0290\n",
      "Epoch 54/64, Train Loss: 1.0479, Test Loss: 1.0280\n",
      "Epoch 55/64, Train Loss: 1.0140, Test Loss: 1.0288\n",
      "Epoch 56/64, Train Loss: 1.0462, Test Loss: 1.0268\n",
      "Epoch 57/64, Train Loss: 1.0673, Test Loss: 1.0271\n",
      "Epoch 58/64, Train Loss: 1.0330, Test Loss: 1.0271\n",
      "Epoch 59/64, Train Loss: 1.0465, Test Loss: 1.0267\n",
      "Epoch 60/64, Train Loss: 1.0144, Test Loss: 1.0234\n",
      "Epoch 61/64, Train Loss: 1.0493, Test Loss: 1.0229\n",
      "Epoch 62/64, Train Loss: 1.0450, Test Loss: 1.0236\n",
      "Epoch 63/64, Train Loss: 1.0339, Test Loss: 1.0264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:26:43,115] Trial 84 finished with value: 1.0277902483940125 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 184, 'layer_1_size': 77, 'layer_2_size': 213, 'layer_3_size': 207, 'layer_4_size': 216, 'layer_5_size': 227, 'dropout_rate': 0.38714977334214645, 'learning_rate': 0.00010858406498200172, 'batch_size': 64, 'epochs': 64}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/64, Train Loss: 1.0456, Test Loss: 1.0278\n",
      "Epoch 1/72, Train Loss: 1.3480, Test Loss: 0.9911\n",
      "Epoch 2/72, Train Loss: 1.3667, Test Loss: 0.9942\n",
      "Epoch 3/72, Train Loss: 1.3114, Test Loss: 0.9989\n",
      "Epoch 4/72, Train Loss: 1.3409, Test Loss: 1.0002\n",
      "Epoch 5/72, Train Loss: 1.2770, Test Loss: 1.0008\n",
      "Epoch 6/72, Train Loss: 1.3000, Test Loss: 1.0041\n",
      "Epoch 7/72, Train Loss: 1.2412, Test Loss: 1.0120\n",
      "Epoch 8/72, Train Loss: 1.2490, Test Loss: 1.0103\n",
      "Epoch 9/72, Train Loss: 1.1548, Test Loss: 1.0084\n",
      "Epoch 10/72, Train Loss: 1.2825, Test Loss: 1.0092\n",
      "Epoch 11/72, Train Loss: 1.1940, Test Loss: 1.0058\n",
      "Epoch 12/72, Train Loss: 1.2540, Test Loss: 1.0017\n",
      "Epoch 13/72, Train Loss: 1.1818, Test Loss: 1.0027\n",
      "Epoch 14/72, Train Loss: 1.2024, Test Loss: 1.0004\n",
      "Epoch 15/72, Train Loss: 1.1494, Test Loss: 0.9987\n",
      "Epoch 16/72, Train Loss: 1.2036, Test Loss: 0.9969\n",
      "Epoch 17/72, Train Loss: 1.2340, Test Loss: 0.9984\n",
      "Epoch 18/72, Train Loss: 1.1564, Test Loss: 0.9974\n",
      "Epoch 19/72, Train Loss: 1.1706, Test Loss: 0.9957\n",
      "Epoch 20/72, Train Loss: 1.1395, Test Loss: 0.9953\n",
      "Epoch 21/72, Train Loss: 1.1879, Test Loss: 0.9947\n",
      "Epoch 22/72, Train Loss: 1.1819, Test Loss: 0.9943\n",
      "Epoch 23/72, Train Loss: 1.1727, Test Loss: 0.9955\n",
      "Epoch 24/72, Train Loss: 1.1888, Test Loss: 0.9953\n",
      "Epoch 25/72, Train Loss: 1.2149, Test Loss: 0.9966\n",
      "Epoch 26/72, Train Loss: 1.1960, Test Loss: 0.9947\n",
      "Epoch 27/72, Train Loss: 1.1581, Test Loss: 0.9950\n",
      "Epoch 28/72, Train Loss: 1.2279, Test Loss: 0.9947\n",
      "Epoch 29/72, Train Loss: 1.1689, Test Loss: 0.9947\n",
      "Epoch 30/72, Train Loss: 1.1834, Test Loss: 0.9939\n",
      "Epoch 31/72, Train Loss: 1.1254, Test Loss: 0.9940\n",
      "Epoch 32/72, Train Loss: 1.1524, Test Loss: 0.9928\n",
      "Epoch 33/72, Train Loss: 1.1551, Test Loss: 0.9934\n",
      "Epoch 34/72, Train Loss: 1.1984, Test Loss: 0.9922\n",
      "Epoch 35/72, Train Loss: 1.1017, Test Loss: 0.9913\n",
      "Epoch 36/72, Train Loss: 1.1139, Test Loss: 0.9915\n",
      "Epoch 37/72, Train Loss: 1.1495, Test Loss: 0.9902\n",
      "Epoch 38/72, Train Loss: 1.2007, Test Loss: 0.9906\n",
      "Epoch 39/72, Train Loss: 1.1679, Test Loss: 0.9910\n",
      "Epoch 40/72, Train Loss: 1.1718, Test Loss: 0.9904\n",
      "Epoch 41/72, Train Loss: 1.1079, Test Loss: 0.9907\n",
      "Epoch 42/72, Train Loss: 1.1112, Test Loss: 0.9905\n",
      "Epoch 43/72, Train Loss: 1.1352, Test Loss: 0.9907\n",
      "Epoch 44/72, Train Loss: 1.1597, Test Loss: 0.9909\n",
      "Epoch 45/72, Train Loss: 1.1510, Test Loss: 0.9899\n",
      "Epoch 46/72, Train Loss: 1.1044, Test Loss: 0.9900\n",
      "Epoch 47/72, Train Loss: 1.1624, Test Loss: 0.9896\n",
      "Epoch 48/72, Train Loss: 1.1126, Test Loss: 0.9902\n",
      "Epoch 49/72, Train Loss: 1.0807, Test Loss: 0.9901\n",
      "Epoch 50/72, Train Loss: 1.1446, Test Loss: 0.9897\n",
      "Epoch 51/72, Train Loss: 1.1192, Test Loss: 0.9900\n",
      "Epoch 52/72, Train Loss: 1.1583, Test Loss: 0.9912\n",
      "Epoch 53/72, Train Loss: 1.1210, Test Loss: 0.9915\n",
      "Epoch 54/72, Train Loss: 1.1662, Test Loss: 0.9913\n",
      "Epoch 55/72, Train Loss: 1.0958, Test Loss: 0.9912\n",
      "Epoch 56/72, Train Loss: 1.1524, Test Loss: 0.9919\n",
      "Epoch 57/72, Train Loss: 1.1584, Test Loss: 0.9922\n",
      "Epoch 58/72, Train Loss: 1.1501, Test Loss: 0.9911\n",
      "Epoch 59/72, Train Loss: 1.1392, Test Loss: 0.9915\n",
      "Epoch 60/72, Train Loss: 1.1169, Test Loss: 0.9908\n",
      "Epoch 61/72, Train Loss: 1.1008, Test Loss: 0.9905\n",
      "Epoch 62/72, Train Loss: 1.1877, Test Loss: 0.9912\n",
      "Epoch 63/72, Train Loss: 1.1191, Test Loss: 0.9909\n",
      "Epoch 64/72, Train Loss: 1.1320, Test Loss: 0.9905\n",
      "Epoch 65/72, Train Loss: 1.0892, Test Loss: 0.9906\n",
      "Epoch 66/72, Train Loss: 1.0720, Test Loss: 0.9914\n",
      "Epoch 67/72, Train Loss: 1.0956, Test Loss: 0.9922\n",
      "Epoch 68/72, Train Loss: 1.1463, Test Loss: 0.9921\n",
      "Epoch 69/72, Train Loss: 1.1063, Test Loss: 0.9916\n",
      "Epoch 70/72, Train Loss: 1.0941, Test Loss: 0.9911\n",
      "Epoch 71/72, Train Loss: 1.1252, Test Loss: 0.9913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:26:47,699] Trial 85 finished with value: 0.9913994371891022 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 198, 'layer_1_size': 103, 'layer_2_size': 235, 'layer_3_size': 195, 'layer_4_size': 81, 'layer_5_size': 217, 'layer_6_size': 165, 'dropout_rate': 0.4558224243899219, 'learning_rate': 6.638213658734811e-05, 'batch_size': 128, 'epochs': 72}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/72, Train Loss: 1.1366, Test Loss: 0.9914\n",
      "Epoch 1/56, Train Loss: 1.2715, Test Loss: 0.9423\n",
      "Epoch 2/56, Train Loss: 1.2378, Test Loss: 0.9426\n",
      "Epoch 3/56, Train Loss: 1.1990, Test Loss: 0.9436\n",
      "Epoch 4/56, Train Loss: 1.1796, Test Loss: 0.9393\n",
      "Epoch 5/56, Train Loss: 1.1206, Test Loss: 0.9390\n",
      "Epoch 6/56, Train Loss: 1.1167, Test Loss: 0.9383\n",
      "Epoch 7/56, Train Loss: 1.1514, Test Loss: 0.9335\n",
      "Epoch 8/56, Train Loss: 1.1493, Test Loss: 0.9356\n",
      "Epoch 9/56, Train Loss: 1.1298, Test Loss: 0.9367\n",
      "Epoch 10/56, Train Loss: 1.1629, Test Loss: 0.9351\n",
      "Epoch 11/56, Train Loss: 1.1154, Test Loss: 0.9378\n",
      "Epoch 12/56, Train Loss: 1.0993, Test Loss: 0.9354\n",
      "Epoch 13/56, Train Loss: 1.0642, Test Loss: 0.9382\n",
      "Epoch 14/56, Train Loss: 1.0550, Test Loss: 0.9354\n",
      "Epoch 15/56, Train Loss: 1.0776, Test Loss: 0.9356\n",
      "Epoch 16/56, Train Loss: 1.0889, Test Loss: 0.9421\n",
      "Epoch 17/56, Train Loss: 1.0764, Test Loss: 0.9441\n",
      "Epoch 18/56, Train Loss: 1.0842, Test Loss: 0.9435\n",
      "Epoch 19/56, Train Loss: 1.0881, Test Loss: 0.9447\n",
      "Epoch 20/56, Train Loss: 1.0599, Test Loss: 0.9466\n",
      "Epoch 21/56, Train Loss: 1.0218, Test Loss: 0.9542\n",
      "Epoch 22/56, Train Loss: 1.0991, Test Loss: 0.9553\n",
      "Epoch 23/56, Train Loss: 1.0576, Test Loss: 0.9564\n",
      "Epoch 24/56, Train Loss: 1.0608, Test Loss: 0.9484\n",
      "Epoch 25/56, Train Loss: 1.0570, Test Loss: 0.9463\n",
      "Epoch 26/56, Train Loss: 1.0303, Test Loss: 0.9480\n",
      "Epoch 27/56, Train Loss: 1.0613, Test Loss: 0.9419\n",
      "Epoch 28/56, Train Loss: 1.0538, Test Loss: 0.9449\n",
      "Epoch 29/56, Train Loss: 1.0314, Test Loss: 0.9399\n",
      "Epoch 30/56, Train Loss: 1.0548, Test Loss: 0.9390\n",
      "Epoch 31/56, Train Loss: 1.0469, Test Loss: 0.9395\n",
      "Epoch 32/56, Train Loss: 0.9915, Test Loss: 0.9444\n",
      "Epoch 33/56, Train Loss: 1.0334, Test Loss: 0.9465\n",
      "Epoch 34/56, Train Loss: 1.0248, Test Loss: 0.9506\n",
      "Epoch 35/56, Train Loss: 1.0291, Test Loss: 0.9469\n",
      "Epoch 36/56, Train Loss: 1.0274, Test Loss: 0.9435\n",
      "Epoch 37/56, Train Loss: 1.0163, Test Loss: 0.9432\n",
      "Epoch 38/56, Train Loss: 0.9965, Test Loss: 0.9417\n",
      "Epoch 39/56, Train Loss: 1.0223, Test Loss: 0.9403\n",
      "Epoch 40/56, Train Loss: 0.9980, Test Loss: 0.9406\n",
      "Epoch 41/56, Train Loss: 1.0172, Test Loss: 0.9435\n",
      "Epoch 42/56, Train Loss: 0.9888, Test Loss: 0.9453\n",
      "Epoch 43/56, Train Loss: 1.0241, Test Loss: 0.9480\n",
      "Epoch 44/56, Train Loss: 1.0359, Test Loss: 0.9504\n",
      "Epoch 45/56, Train Loss: 1.0194, Test Loss: 0.9474\n",
      "Epoch 46/56, Train Loss: 1.0039, Test Loss: 0.9460\n",
      "Epoch 47/56, Train Loss: 1.0026, Test Loss: 0.9481\n",
      "Epoch 48/56, Train Loss: 0.9938, Test Loss: 0.9490\n",
      "Epoch 49/56, Train Loss: 1.0124, Test Loss: 0.9543\n",
      "Epoch 50/56, Train Loss: 0.9959, Test Loss: 0.9559\n",
      "Epoch 51/56, Train Loss: 0.9883, Test Loss: 0.9574\n",
      "Epoch 52/56, Train Loss: 0.9894, Test Loss: 0.9614\n",
      "Epoch 53/56, Train Loss: 1.0010, Test Loss: 0.9587\n",
      "Epoch 54/56, Train Loss: 0.9799, Test Loss: 0.9581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:26:53,676] Trial 86 finished with value: 0.9471541941165924 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 103, 'layer_1_size': 52, 'layer_2_size': 187, 'layer_3_size': 151, 'layer_4_size': 221, 'layer_5_size': 198, 'layer_6_size': 183, 'layer_7_size': 94, 'dropout_rate': 0.4051456466013592, 'learning_rate': 0.00022728437481161058, 'batch_size': 64, 'epochs': 56}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/56, Train Loss: 0.9785, Test Loss: 0.9579\n",
      "Epoch 56/56, Train Loss: 1.0183, Test Loss: 0.9472\n",
      "Epoch 1/16, Train Loss: 1.2679, Test Loss: 1.0613\n",
      "Epoch 2/16, Train Loss: 1.2783, Test Loss: 1.0792\n",
      "Epoch 3/16, Train Loss: 1.2310, Test Loss: 1.0612\n",
      "Epoch 4/16, Train Loss: 1.2204, Test Loss: 1.0518\n",
      "Epoch 5/16, Train Loss: 1.1827, Test Loss: 1.0606\n",
      "Epoch 6/16, Train Loss: 1.1845, Test Loss: 1.0545\n",
      "Epoch 7/16, Train Loss: 1.1846, Test Loss: 1.0524\n",
      "Epoch 8/16, Train Loss: 1.2131, Test Loss: 1.0455\n",
      "Epoch 9/16, Train Loss: 1.2286, Test Loss: 1.0518\n",
      "Epoch 10/16, Train Loss: 1.2259, Test Loss: 1.0531\n",
      "Epoch 11/16, Train Loss: 1.1589, Test Loss: 1.0555\n",
      "Epoch 12/16, Train Loss: 1.1741, Test Loss: 1.0520\n",
      "Epoch 13/16, Train Loss: 1.1785, Test Loss: 1.0541\n",
      "Epoch 14/16, Train Loss: 1.1915, Test Loss: 1.0550\n",
      "Epoch 15/16, Train Loss: 1.1476, Test Loss: 1.0542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:26:55,076] Trial 87 finished with value: 1.0599231719970703 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 171, 'layer_1_size': 86, 'layer_2_size': 196, 'layer_3_size': 222, 'layer_4_size': 236, 'layer_5_size': 175, 'dropout_rate': 0.4270465655692522, 'learning_rate': 0.00013608302158554, 'batch_size': 64, 'epochs': 16}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/16, Train Loss: 1.1469, Test Loss: 1.0599\n",
      "Epoch 1/79, Train Loss: 1.1735, Test Loss: 0.8513\n",
      "Epoch 2/79, Train Loss: 1.1321, Test Loss: 0.8621\n",
      "Epoch 3/79, Train Loss: 1.1203, Test Loss: 0.8635\n",
      "Epoch 4/79, Train Loss: 1.0724, Test Loss: 0.8602\n",
      "Epoch 5/79, Train Loss: 1.1220, Test Loss: 0.8602\n",
      "Epoch 6/79, Train Loss: 1.0979, Test Loss: 0.8604\n",
      "Epoch 7/79, Train Loss: 1.0531, Test Loss: 0.8627\n",
      "Epoch 8/79, Train Loss: 1.0846, Test Loss: 0.8594\n",
      "Epoch 9/79, Train Loss: 1.0773, Test Loss: 0.8589\n",
      "Epoch 10/79, Train Loss: 1.0516, Test Loss: 0.8571\n",
      "Epoch 11/79, Train Loss: 1.0681, Test Loss: 0.8592\n",
      "Epoch 12/79, Train Loss: 1.0839, Test Loss: 0.8581\n",
      "Epoch 13/79, Train Loss: 1.0481, Test Loss: 0.8605\n",
      "Epoch 14/79, Train Loss: 1.0488, Test Loss: 0.8642\n",
      "Epoch 15/79, Train Loss: 1.0535, Test Loss: 0.8637\n",
      "Epoch 16/79, Train Loss: 1.0326, Test Loss: 0.8634\n",
      "Epoch 17/79, Train Loss: 1.0240, Test Loss: 0.8646\n",
      "Epoch 18/79, Train Loss: 1.0279, Test Loss: 0.8678\n",
      "Epoch 19/79, Train Loss: 1.0511, Test Loss: 0.8654\n",
      "Epoch 20/79, Train Loss: 0.9941, Test Loss: 0.8667\n",
      "Epoch 21/79, Train Loss: 1.0263, Test Loss: 0.8590\n",
      "Epoch 22/79, Train Loss: 1.0241, Test Loss: 0.8616\n",
      "Epoch 23/79, Train Loss: 1.0456, Test Loss: 0.8635\n",
      "Epoch 24/79, Train Loss: 1.0313, Test Loss: 0.8648\n",
      "Epoch 25/79, Train Loss: 1.0548, Test Loss: 0.8655\n",
      "Epoch 26/79, Train Loss: 1.0089, Test Loss: 0.8629\n",
      "Epoch 27/79, Train Loss: 0.9909, Test Loss: 0.8616\n",
      "Epoch 28/79, Train Loss: 0.9973, Test Loss: 0.8619\n",
      "Epoch 29/79, Train Loss: 1.0286, Test Loss: 0.8673\n",
      "Epoch 30/79, Train Loss: 1.0137, Test Loss: 0.8624\n",
      "Epoch 31/79, Train Loss: 1.0361, Test Loss: 0.8649\n",
      "Epoch 32/79, Train Loss: 0.9921, Test Loss: 0.8625\n",
      "Epoch 33/79, Train Loss: 1.0054, Test Loss: 0.8625\n",
      "Epoch 34/79, Train Loss: 1.0343, Test Loss: 0.8622\n",
      "Epoch 35/79, Train Loss: 1.0116, Test Loss: 0.8644\n",
      "Epoch 36/79, Train Loss: 1.0027, Test Loss: 0.8618\n",
      "Epoch 37/79, Train Loss: 1.0249, Test Loss: 0.8611\n",
      "Epoch 38/79, Train Loss: 1.0197, Test Loss: 0.8646\n",
      "Epoch 39/79, Train Loss: 1.0228, Test Loss: 0.8616\n",
      "Epoch 40/79, Train Loss: 1.0070, Test Loss: 0.8593\n",
      "Epoch 41/79, Train Loss: 1.0369, Test Loss: 0.8601\n",
      "Epoch 42/79, Train Loss: 1.0136, Test Loss: 0.8609\n",
      "Epoch 43/79, Train Loss: 1.0373, Test Loss: 0.8608\n",
      "Epoch 44/79, Train Loss: 1.0237, Test Loss: 0.8633\n",
      "Epoch 45/79, Train Loss: 1.0408, Test Loss: 0.8604\n",
      "Epoch 46/79, Train Loss: 1.0058, Test Loss: 0.8587\n",
      "Epoch 47/79, Train Loss: 1.0221, Test Loss: 0.8619\n",
      "Epoch 48/79, Train Loss: 1.0233, Test Loss: 0.8630\n",
      "Epoch 49/79, Train Loss: 1.0135, Test Loss: 0.8617\n",
      "Epoch 50/79, Train Loss: 1.0300, Test Loss: 0.8616\n",
      "Epoch 51/79, Train Loss: 1.0160, Test Loss: 0.8647\n",
      "Epoch 52/79, Train Loss: 1.0085, Test Loss: 0.8613\n",
      "Epoch 53/79, Train Loss: 0.9817, Test Loss: 0.8627\n",
      "Epoch 54/79, Train Loss: 1.0200, Test Loss: 0.8604\n",
      "Epoch 55/79, Train Loss: 0.9953, Test Loss: 0.8622\n",
      "Epoch 56/79, Train Loss: 1.0129, Test Loss: 0.8614\n",
      "Epoch 57/79, Train Loss: 0.9991, Test Loss: 0.8588\n",
      "Epoch 58/79, Train Loss: 1.0143, Test Loss: 0.8579\n",
      "Epoch 59/79, Train Loss: 1.0106, Test Loss: 0.8592\n",
      "Epoch 60/79, Train Loss: 0.9809, Test Loss: 0.8595\n",
      "Epoch 61/79, Train Loss: 1.0206, Test Loss: 0.8587\n",
      "Epoch 62/79, Train Loss: 1.0129, Test Loss: 0.8608\n",
      "Epoch 63/79, Train Loss: 1.0163, Test Loss: 0.8577\n",
      "Epoch 64/79, Train Loss: 0.9896, Test Loss: 0.8578\n",
      "Epoch 65/79, Train Loss: 0.9693, Test Loss: 0.8579\n",
      "Epoch 66/79, Train Loss: 0.9863, Test Loss: 0.8599\n",
      "Epoch 67/79, Train Loss: 1.0009, Test Loss: 0.8609\n",
      "Epoch 68/79, Train Loss: 1.0079, Test Loss: 0.8557\n",
      "Epoch 69/79, Train Loss: 0.9965, Test Loss: 0.8576\n",
      "Epoch 70/79, Train Loss: 0.9983, Test Loss: 0.8563\n",
      "Epoch 71/79, Train Loss: 1.0285, Test Loss: 0.8567\n",
      "Epoch 72/79, Train Loss: 1.0269, Test Loss: 0.8581\n",
      "Epoch 73/79, Train Loss: 0.9835, Test Loss: 0.8572\n",
      "Epoch 74/79, Train Loss: 0.9608, Test Loss: 0.8559\n",
      "Epoch 75/79, Train Loss: 1.0064, Test Loss: 0.8594\n",
      "Epoch 76/79, Train Loss: 1.0015, Test Loss: 0.8582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:27:00,302] Trial 88 finished with value: 0.8568789809942245 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 160, 'layer_1_size': 40, 'layer_2_size': 209, 'layer_3_size': 164, 'layer_4_size': 43, 'layer_5_size': 242, 'dropout_rate': 0.32352043381895296, 'learning_rate': 8.336332209140762e-05, 'batch_size': 64, 'epochs': 79}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/79, Train Loss: 0.9868, Test Loss: 0.8559\n",
      "Epoch 78/79, Train Loss: 0.9918, Test Loss: 0.8566\n",
      "Epoch 79/79, Train Loss: 1.0065, Test Loss: 0.8569\n",
      "Epoch 1/32, Train Loss: 1.2299, Test Loss: 0.8547\n",
      "Epoch 2/32, Train Loss: 1.2295, Test Loss: 0.8537\n",
      "Epoch 3/32, Train Loss: 1.2219, Test Loss: 0.8524\n",
      "Epoch 4/32, Train Loss: 1.1600, Test Loss: 0.8611\n",
      "Epoch 5/32, Train Loss: 1.2347, Test Loss: 0.8562\n",
      "Epoch 6/32, Train Loss: 1.1836, Test Loss: 0.8528\n",
      "Epoch 7/32, Train Loss: 1.1507, Test Loss: 0.8503\n",
      "Epoch 8/32, Train Loss: 1.2218, Test Loss: 0.8483\n",
      "Epoch 9/32, Train Loss: 1.2026, Test Loss: 0.8490\n",
      "Epoch 10/32, Train Loss: 1.1602, Test Loss: 0.8518\n",
      "Epoch 11/32, Train Loss: 1.1253, Test Loss: 0.8518\n",
      "Epoch 12/32, Train Loss: 1.1314, Test Loss: 0.8521\n",
      "Epoch 13/32, Train Loss: 1.1678, Test Loss: 0.8496\n",
      "Epoch 14/32, Train Loss: 1.1558, Test Loss: 0.8481\n",
      "Epoch 15/32, Train Loss: 1.1618, Test Loss: 0.8483\n",
      "Epoch 16/32, Train Loss: 1.1345, Test Loss: 0.8465\n",
      "Epoch 17/32, Train Loss: 1.1360, Test Loss: 0.8448\n",
      "Epoch 18/32, Train Loss: 1.1394, Test Loss: 0.8500\n",
      "Epoch 19/32, Train Loss: 1.1653, Test Loss: 0.8439\n",
      "Epoch 20/32, Train Loss: 1.1021, Test Loss: 0.8452\n",
      "Epoch 21/32, Train Loss: 1.1546, Test Loss: 0.8484\n",
      "Epoch 22/32, Train Loss: 1.1479, Test Loss: 0.8469\n",
      "Epoch 23/32, Train Loss: 1.1584, Test Loss: 0.8474\n",
      "Epoch 24/32, Train Loss: 1.2081, Test Loss: 0.8495\n",
      "Epoch 25/32, Train Loss: 1.1164, Test Loss: 0.8446\n",
      "Epoch 26/32, Train Loss: 1.1263, Test Loss: 0.8430\n",
      "Epoch 27/32, Train Loss: 1.1280, Test Loss: 0.8435\n",
      "Epoch 28/32, Train Loss: 1.1112, Test Loss: 0.8444\n",
      "Epoch 29/32, Train Loss: 1.1765, Test Loss: 0.8429\n",
      "Epoch 30/32, Train Loss: 1.1331, Test Loss: 0.8418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:27:03,372] Trial 89 finished with value: 0.8446057438850403 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 146, 'layer_1_size': 192, 'layer_2_size': 221, 'layer_3_size': 37, 'layer_4_size': 148, 'layer_5_size': 249, 'layer_6_size': 142, 'layer_7_size': 147, 'dropout_rate': 0.3768728149938971, 'learning_rate': 4.751996573581699e-05, 'batch_size': 64, 'epochs': 32}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/32, Train Loss: 1.1371, Test Loss: 0.8453\n",
      "Epoch 32/32, Train Loss: 1.1556, Test Loss: 0.8446\n",
      "Epoch 1/34, Train Loss: 1.1665, Test Loss: 0.9715\n",
      "Epoch 2/34, Train Loss: 1.1744, Test Loss: 0.9726\n",
      "Epoch 3/34, Train Loss: 1.1650, Test Loss: 0.9736\n",
      "Epoch 4/34, Train Loss: 1.0959, Test Loss: 0.9760\n",
      "Epoch 5/34, Train Loss: 1.1276, Test Loss: 0.9773\n",
      "Epoch 6/34, Train Loss: 1.0796, Test Loss: 0.9774\n",
      "Epoch 7/34, Train Loss: 1.0550, Test Loss: 0.9780\n",
      "Epoch 8/34, Train Loss: 1.0417, Test Loss: 0.9813\n",
      "Epoch 9/34, Train Loss: 1.0556, Test Loss: 0.9810\n",
      "Epoch 10/34, Train Loss: 1.0515, Test Loss: 0.9810\n",
      "Epoch 11/34, Train Loss: 1.0788, Test Loss: 0.9784\n",
      "Epoch 12/34, Train Loss: 1.0662, Test Loss: 0.9800\n",
      "Epoch 13/34, Train Loss: 1.0512, Test Loss: 0.9793\n",
      "Epoch 14/34, Train Loss: 1.0681, Test Loss: 0.9826\n",
      "Epoch 15/34, Train Loss: 1.0539, Test Loss: 0.9849\n",
      "Epoch 16/34, Train Loss: 1.0554, Test Loss: 0.9840\n",
      "Epoch 17/34, Train Loss: 1.0622, Test Loss: 0.9831\n",
      "Epoch 18/34, Train Loss: 1.0605, Test Loss: 0.9832\n",
      "Epoch 19/34, Train Loss: 1.0506, Test Loss: 0.9800\n",
      "Epoch 20/34, Train Loss: 1.0085, Test Loss: 0.9788\n",
      "Epoch 21/34, Train Loss: 1.0096, Test Loss: 0.9776\n",
      "Epoch 22/34, Train Loss: 1.0525, Test Loss: 0.9778\n",
      "Epoch 23/34, Train Loss: 1.0200, Test Loss: 0.9756\n",
      "Epoch 24/34, Train Loss: 1.0458, Test Loss: 0.9741\n",
      "Epoch 25/34, Train Loss: 1.0264, Test Loss: 0.9752\n",
      "Epoch 26/34, Train Loss: 1.0524, Test Loss: 0.9750\n",
      "Epoch 27/34, Train Loss: 1.0658, Test Loss: 0.9751\n",
      "Epoch 28/34, Train Loss: 1.0292, Test Loss: 0.9767\n",
      "Epoch 29/34, Train Loss: 1.0060, Test Loss: 0.9765\n",
      "Epoch 30/34, Train Loss: 1.0451, Test Loss: 0.9771\n",
      "Epoch 31/34, Train Loss: 1.0573, Test Loss: 0.9766\n",
      "Epoch 32/34, Train Loss: 1.0245, Test Loss: 0.9753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:27:05,691] Trial 90 finished with value: 0.9734693467617035 and parameters: {'num_hidden_layers': 9, 'layer_0_size': 75, 'layer_1_size': 207, 'layer_2_size': 218, 'layer_3_size': 42, 'layer_4_size': 186, 'layer_5_size': 230, 'layer_6_size': 110, 'layer_7_size': 143, 'layer_8_size': 243, 'dropout_rate': 0.3473618520703249, 'learning_rate': 0.0003502801435571535, 'batch_size': 128, 'epochs': 34}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/34, Train Loss: 1.0334, Test Loss: 0.9754\n",
      "Epoch 34/34, Train Loss: 1.0221, Test Loss: 0.9735\n",
      "Epoch 1/10, Train Loss: 1.1942, Test Loss: 0.9730\n",
      "Epoch 2/10, Train Loss: 1.1183, Test Loss: 0.9792\n",
      "Epoch 3/10, Train Loss: 1.1879, Test Loss: 0.9822\n",
      "Epoch 4/10, Train Loss: 1.2204, Test Loss: 0.9804\n",
      "Epoch 5/10, Train Loss: 1.1410, Test Loss: 0.9823\n",
      "Epoch 6/10, Train Loss: 1.1459, Test Loss: 0.9860\n",
      "Epoch 7/10, Train Loss: 1.1514, Test Loss: 0.9853\n",
      "Epoch 8/10, Train Loss: 1.1221, Test Loss: 0.9879\n",
      "Epoch 9/10, Train Loss: 1.1502, Test Loss: 0.9874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:27:06,760] Trial 91 finished with value: 0.9868189245462418 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 145, 'layer_1_size': 193, 'layer_2_size': 203, 'layer_3_size': 36, 'layer_4_size': 147, 'layer_5_size': 252, 'layer_6_size': 144, 'layer_7_size': 147, 'dropout_rate': 0.37825723121374233, 'learning_rate': 3.7924212204052525e-05, 'batch_size': 64, 'epochs': 10}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Loss: 1.1586, Test Loss: 0.9868\n",
      "Epoch 1/88, Train Loss: 1.2469, Test Loss: 0.9216\n",
      "Epoch 2/88, Train Loss: 1.2443, Test Loss: 0.9327\n",
      "Epoch 3/88, Train Loss: 1.2261, Test Loss: 0.9378\n",
      "Epoch 4/88, Train Loss: 1.2388, Test Loss: 0.9368\n",
      "Epoch 5/88, Train Loss: 1.2231, Test Loss: 0.9426\n",
      "Epoch 6/88, Train Loss: 1.2363, Test Loss: 0.9448\n",
      "Epoch 7/88, Train Loss: 1.2365, Test Loss: 0.9456\n",
      "Epoch 8/88, Train Loss: 1.2552, Test Loss: 0.9411\n",
      "Epoch 9/88, Train Loss: 1.2258, Test Loss: 0.9377\n",
      "Epoch 10/88, Train Loss: 1.2366, Test Loss: 0.9359\n",
      "Epoch 11/88, Train Loss: 1.2611, Test Loss: 0.9417\n",
      "Epoch 12/88, Train Loss: 1.2155, Test Loss: 0.9432\n",
      "Epoch 13/88, Train Loss: 1.2401, Test Loss: 0.9390\n",
      "Epoch 14/88, Train Loss: 1.2070, Test Loss: 0.9368\n",
      "Epoch 15/88, Train Loss: 1.2127, Test Loss: 0.9397\n",
      "Epoch 16/88, Train Loss: 1.1597, Test Loss: 0.9394\n",
      "Epoch 17/88, Train Loss: 1.2207, Test Loss: 0.9355\n",
      "Epoch 18/88, Train Loss: 1.2029, Test Loss: 0.9388\n",
      "Epoch 19/88, Train Loss: 1.1960, Test Loss: 0.9430\n",
      "Epoch 20/88, Train Loss: 1.1489, Test Loss: 0.9352\n",
      "Epoch 21/88, Train Loss: 1.1935, Test Loss: 0.9360\n",
      "Epoch 22/88, Train Loss: 1.1647, Test Loss: 0.9339\n",
      "Epoch 23/88, Train Loss: 1.2133, Test Loss: 0.9369\n",
      "Epoch 24/88, Train Loss: 1.1638, Test Loss: 0.9398\n",
      "Epoch 25/88, Train Loss: 1.1613, Test Loss: 0.9397\n",
      "Epoch 26/88, Train Loss: 1.1891, Test Loss: 0.9362\n",
      "Epoch 27/88, Train Loss: 1.1673, Test Loss: 0.9398\n",
      "Epoch 28/88, Train Loss: 1.1873, Test Loss: 0.9341\n",
      "Epoch 29/88, Train Loss: 1.1773, Test Loss: 0.9277\n",
      "Epoch 30/88, Train Loss: 1.1837, Test Loss: 0.9329\n",
      "Epoch 31/88, Train Loss: 1.1315, Test Loss: 0.9309\n",
      "Epoch 32/88, Train Loss: 1.1745, Test Loss: 0.9337\n",
      "Epoch 33/88, Train Loss: 1.1506, Test Loss: 0.9362\n",
      "Epoch 34/88, Train Loss: 1.1785, Test Loss: 0.9395\n",
      "Epoch 35/88, Train Loss: 1.1754, Test Loss: 0.9357\n",
      "Epoch 36/88, Train Loss: 1.1212, Test Loss: 0.9303\n",
      "Epoch 37/88, Train Loss: 1.1060, Test Loss: 0.9391\n",
      "Epoch 38/88, Train Loss: 1.1787, Test Loss: 0.9389\n",
      "Epoch 39/88, Train Loss: 1.1653, Test Loss: 0.9390\n",
      "Epoch 40/88, Train Loss: 1.1721, Test Loss: 0.9357\n",
      "Epoch 41/88, Train Loss: 1.1503, Test Loss: 0.9334\n",
      "Epoch 42/88, Train Loss: 1.1957, Test Loss: 0.9363\n",
      "Epoch 43/88, Train Loss: 1.1798, Test Loss: 0.9325\n",
      "Epoch 44/88, Train Loss: 1.1192, Test Loss: 0.9362\n",
      "Epoch 45/88, Train Loss: 1.1076, Test Loss: 0.9396\n",
      "Epoch 46/88, Train Loss: 1.1488, Test Loss: 0.9357\n",
      "Epoch 47/88, Train Loss: 1.1772, Test Loss: 0.9334\n",
      "Epoch 48/88, Train Loss: 1.1362, Test Loss: 0.9362\n",
      "Epoch 49/88, Train Loss: 1.1240, Test Loss: 0.9379\n",
      "Epoch 50/88, Train Loss: 1.1667, Test Loss: 0.9356\n",
      "Epoch 51/88, Train Loss: 1.1619, Test Loss: 0.9322\n",
      "Epoch 52/88, Train Loss: 1.1510, Test Loss: 0.9360\n",
      "Epoch 53/88, Train Loss: 1.1514, Test Loss: 0.9304\n",
      "Epoch 54/88, Train Loss: 1.1657, Test Loss: 0.9382\n",
      "Epoch 55/88, Train Loss: 1.1500, Test Loss: 0.9369\n",
      "Epoch 56/88, Train Loss: 1.1045, Test Loss: 0.9376\n",
      "Epoch 57/88, Train Loss: 1.1882, Test Loss: 0.9418\n",
      "Epoch 58/88, Train Loss: 1.1436, Test Loss: 0.9332\n",
      "Epoch 59/88, Train Loss: 1.1482, Test Loss: 0.9342\n",
      "Epoch 60/88, Train Loss: 1.1167, Test Loss: 0.9310\n",
      "Epoch 61/88, Train Loss: 1.1373, Test Loss: 0.9321\n",
      "Epoch 62/88, Train Loss: 1.1035, Test Loss: 0.9373\n",
      "Epoch 63/88, Train Loss: 1.1337, Test Loss: 0.9313\n",
      "Epoch 64/88, Train Loss: 1.0990, Test Loss: 0.9368\n",
      "Epoch 65/88, Train Loss: 1.1514, Test Loss: 0.9315\n",
      "Epoch 66/88, Train Loss: 1.1210, Test Loss: 0.9358\n",
      "Epoch 67/88, Train Loss: 1.1337, Test Loss: 0.9361\n",
      "Epoch 68/88, Train Loss: 1.1727, Test Loss: 0.9308\n",
      "Epoch 69/88, Train Loss: 1.1618, Test Loss: 0.9322\n",
      "Epoch 70/88, Train Loss: 1.1306, Test Loss: 0.9360\n",
      "Epoch 71/88, Train Loss: 1.1091, Test Loss: 0.9387\n",
      "Epoch 72/88, Train Loss: 1.1043, Test Loss: 0.9331\n",
      "Epoch 73/88, Train Loss: 1.1323, Test Loss: 0.9357\n",
      "Epoch 74/88, Train Loss: 1.1397, Test Loss: 0.9351\n",
      "Epoch 75/88, Train Loss: 1.1111, Test Loss: 0.9351\n",
      "Epoch 76/88, Train Loss: 1.1181, Test Loss: 0.9353\n",
      "Epoch 77/88, Train Loss: 1.1144, Test Loss: 0.9407\n",
      "Epoch 78/88, Train Loss: 1.1219, Test Loss: 0.9378\n",
      "Epoch 79/88, Train Loss: 1.1391, Test Loss: 0.9364\n",
      "Epoch 80/88, Train Loss: 1.1308, Test Loss: 0.9301\n",
      "Epoch 81/88, Train Loss: 1.0997, Test Loss: 0.9314\n",
      "Epoch 82/88, Train Loss: 1.0945, Test Loss: 0.9320\n",
      "Epoch 83/88, Train Loss: 1.1353, Test Loss: 0.9345\n",
      "Epoch 84/88, Train Loss: 1.1556, Test Loss: 0.9332\n",
      "Epoch 85/88, Train Loss: 1.1026, Test Loss: 0.9323\n",
      "Epoch 86/88, Train Loss: 1.1357, Test Loss: 0.9310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:27:14,609] Trial 92 finished with value: 0.9332976639270782 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 136, 'layer_1_size': 65, 'layer_2_size': 223, 'layer_3_size': 123, 'layer_4_size': 176, 'layer_5_size': 242, 'layer_6_size': 131, 'layer_7_size': 123, 'dropout_rate': 0.37212168242454313, 'learning_rate': 2.8854221903788325e-05, 'batch_size': 64, 'epochs': 88}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/88, Train Loss: 1.1050, Test Loss: 0.9353\n",
      "Epoch 88/88, Train Loss: 1.1120, Test Loss: 0.9333\n",
      "Epoch 1/76, Train Loss: 1.1886, Test Loss: 0.8078\n",
      "Epoch 2/76, Train Loss: 1.2246, Test Loss: 0.8153\n",
      "Epoch 3/76, Train Loss: 1.1999, Test Loss: 0.8125\n",
      "Epoch 4/76, Train Loss: 1.2220, Test Loss: 0.8112\n",
      "Epoch 5/76, Train Loss: 1.2347, Test Loss: 0.8125\n",
      "Epoch 6/76, Train Loss: 1.2186, Test Loss: 0.8137\n",
      "Epoch 7/76, Train Loss: 1.1907, Test Loss: 0.8187\n",
      "Epoch 8/76, Train Loss: 1.1564, Test Loss: 0.8143\n",
      "Epoch 9/76, Train Loss: 1.2506, Test Loss: 0.8137\n",
      "Epoch 10/76, Train Loss: 1.1957, Test Loss: 0.8130\n",
      "Epoch 11/76, Train Loss: 1.1781, Test Loss: 0.8198\n",
      "Epoch 12/76, Train Loss: 1.1655, Test Loss: 0.8252\n",
      "Epoch 13/76, Train Loss: 1.1643, Test Loss: 0.8180\n",
      "Epoch 14/76, Train Loss: 1.1735, Test Loss: 0.8246\n",
      "Epoch 15/76, Train Loss: 1.1740, Test Loss: 0.8354\n",
      "Epoch 16/76, Train Loss: 1.1781, Test Loss: 0.8307\n",
      "Epoch 17/76, Train Loss: 1.1264, Test Loss: 0.8285\n",
      "Epoch 18/76, Train Loss: 1.1053, Test Loss: 0.8312\n",
      "Epoch 19/76, Train Loss: 1.1727, Test Loss: 0.8372\n",
      "Epoch 20/76, Train Loss: 1.1339, Test Loss: 0.8412\n",
      "Epoch 21/76, Train Loss: 1.1801, Test Loss: 0.8384\n",
      "Epoch 22/76, Train Loss: 1.2262, Test Loss: 0.8297\n",
      "Epoch 23/76, Train Loss: 1.1322, Test Loss: 0.8316\n",
      "Epoch 24/76, Train Loss: 1.1511, Test Loss: 0.8232\n",
      "Epoch 25/76, Train Loss: 1.1606, Test Loss: 0.8264\n",
      "Epoch 26/76, Train Loss: 1.1611, Test Loss: 0.8238\n",
      "Epoch 27/76, Train Loss: 1.1600, Test Loss: 0.8186\n",
      "Epoch 28/76, Train Loss: 1.1044, Test Loss: 0.8176\n",
      "Epoch 29/76, Train Loss: 1.1545, Test Loss: 0.8207\n",
      "Epoch 30/76, Train Loss: 1.1216, Test Loss: 0.8194\n",
      "Epoch 31/76, Train Loss: 1.1115, Test Loss: 0.8159\n",
      "Epoch 32/76, Train Loss: 1.1002, Test Loss: 0.8171\n",
      "Epoch 33/76, Train Loss: 1.1265, Test Loss: 0.8139\n",
      "Epoch 34/76, Train Loss: 1.0977, Test Loss: 0.8187\n",
      "Epoch 35/76, Train Loss: 1.0915, Test Loss: 0.8152\n",
      "Epoch 36/76, Train Loss: 1.0804, Test Loss: 0.8116\n",
      "Epoch 37/76, Train Loss: 1.1400, Test Loss: 0.8146\n",
      "Epoch 38/76, Train Loss: 1.1055, Test Loss: 0.8112\n",
      "Epoch 39/76, Train Loss: 1.1054, Test Loss: 0.8146\n",
      "Epoch 40/76, Train Loss: 1.1544, Test Loss: 0.8149\n",
      "Epoch 41/76, Train Loss: 1.0967, Test Loss: 0.8126\n",
      "Epoch 42/76, Train Loss: 1.1392, Test Loss: 0.8147\n",
      "Epoch 43/76, Train Loss: 1.0701, Test Loss: 0.8131\n",
      "Epoch 44/76, Train Loss: 1.1270, Test Loss: 0.8107\n",
      "Epoch 45/76, Train Loss: 1.1402, Test Loss: 0.8131\n",
      "Epoch 46/76, Train Loss: 1.1004, Test Loss: 0.8122\n",
      "Epoch 47/76, Train Loss: 1.1029, Test Loss: 0.8126\n",
      "Epoch 48/76, Train Loss: 1.1020, Test Loss: 0.8095\n",
      "Epoch 49/76, Train Loss: 1.1211, Test Loss: 0.8125\n",
      "Epoch 50/76, Train Loss: 1.0539, Test Loss: 0.8113\n",
      "Epoch 51/76, Train Loss: 1.1266, Test Loss: 0.8101\n",
      "Epoch 52/76, Train Loss: 1.1358, Test Loss: 0.8112\n",
      "Epoch 53/76, Train Loss: 1.0972, Test Loss: 0.8117\n",
      "Epoch 54/76, Train Loss: 1.1126, Test Loss: 0.8156\n",
      "Epoch 55/76, Train Loss: 1.0943, Test Loss: 0.8134\n",
      "Epoch 56/76, Train Loss: 1.0955, Test Loss: 0.8157\n",
      "Epoch 57/76, Train Loss: 1.0921, Test Loss: 0.8145\n",
      "Epoch 58/76, Train Loss: 1.1264, Test Loss: 0.8126\n",
      "Epoch 59/76, Train Loss: 1.0443, Test Loss: 0.8144\n",
      "Epoch 60/76, Train Loss: 1.0776, Test Loss: 0.8140\n",
      "Epoch 61/76, Train Loss: 1.1052, Test Loss: 0.8193\n",
      "Epoch 62/76, Train Loss: 1.1238, Test Loss: 0.8177\n",
      "Epoch 63/76, Train Loss: 1.0719, Test Loss: 0.8188\n",
      "Epoch 64/76, Train Loss: 1.0788, Test Loss: 0.8170\n",
      "Epoch 65/76, Train Loss: 1.1039, Test Loss: 0.8187\n",
      "Epoch 66/76, Train Loss: 1.0739, Test Loss: 0.8150\n",
      "Epoch 67/76, Train Loss: 1.0687, Test Loss: 0.8113\n",
      "Epoch 68/76, Train Loss: 1.1077, Test Loss: 0.8146\n",
      "Epoch 69/76, Train Loss: 1.0629, Test Loss: 0.8193\n",
      "Epoch 70/76, Train Loss: 1.0757, Test Loss: 0.8206\n",
      "Epoch 71/76, Train Loss: 1.0793, Test Loss: 0.8178\n",
      "Epoch 72/76, Train Loss: 1.0690, Test Loss: 0.8147\n",
      "Epoch 73/76, Train Loss: 1.0798, Test Loss: 0.8170\n",
      "Epoch 74/76, Train Loss: 1.0347, Test Loss: 0.8199\n",
      "Epoch 75/76, Train Loss: 1.0818, Test Loss: 0.8226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:27:21,193] Trial 93 finished with value: 0.8210475891828537 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 108, 'layer_1_size': 180, 'layer_2_size': 212, 'layer_3_size': 59, 'layer_4_size': 208, 'layer_5_size': 225, 'layer_6_size': 157, 'dropout_rate': 0.4026161034390681, 'learning_rate': 5.116744254713448e-05, 'batch_size': 64, 'epochs': 76}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/76, Train Loss: 1.0633, Test Loss: 0.8210\n",
      "Epoch 1/40, Train Loss: 1.3562, Test Loss: 0.9937\n",
      "Epoch 2/40, Train Loss: 1.3221, Test Loss: 0.9933\n",
      "Epoch 3/40, Train Loss: 1.3184, Test Loss: 1.0012\n",
      "Epoch 4/40, Train Loss: 1.2317, Test Loss: 1.0015\n",
      "Epoch 5/40, Train Loss: 1.2271, Test Loss: 1.0007\n",
      "Epoch 6/40, Train Loss: 1.1869, Test Loss: 1.0033\n",
      "Epoch 7/40, Train Loss: 1.1612, Test Loss: 1.0019\n",
      "Epoch 8/40, Train Loss: 1.1652, Test Loss: 0.9985\n",
      "Epoch 9/40, Train Loss: 1.2313, Test Loss: 0.9979\n",
      "Epoch 10/40, Train Loss: 1.1625, Test Loss: 1.0033\n",
      "Epoch 11/40, Train Loss: 1.1606, Test Loss: 1.0037\n",
      "Epoch 12/40, Train Loss: 1.1515, Test Loss: 1.0051\n",
      "Epoch 13/40, Train Loss: 1.1598, Test Loss: 1.0121\n",
      "Epoch 14/40, Train Loss: 1.1446, Test Loss: 1.0120\n",
      "Epoch 15/40, Train Loss: 1.1548, Test Loss: 1.0180\n",
      "Epoch 16/40, Train Loss: 1.1264, Test Loss: 1.0108\n",
      "Epoch 17/40, Train Loss: 1.1643, Test Loss: 1.0086\n",
      "Epoch 18/40, Train Loss: 1.1677, Test Loss: 1.0106\n",
      "Epoch 19/40, Train Loss: 1.1806, Test Loss: 1.0143\n",
      "Epoch 20/40, Train Loss: 1.1198, Test Loss: 1.0149\n",
      "Epoch 21/40, Train Loss: 1.1289, Test Loss: 1.0207\n",
      "Epoch 22/40, Train Loss: 1.1201, Test Loss: 1.0164\n",
      "Epoch 23/40, Train Loss: 1.1405, Test Loss: 1.0157\n",
      "Epoch 24/40, Train Loss: 1.1439, Test Loss: 1.0149\n",
      "Epoch 25/40, Train Loss: 1.1537, Test Loss: 1.0195\n",
      "Epoch 26/40, Train Loss: 1.1162, Test Loss: 1.0125\n",
      "Epoch 27/40, Train Loss: 1.1669, Test Loss: 1.0171\n",
      "Epoch 28/40, Train Loss: 1.1560, Test Loss: 1.0121\n",
      "Epoch 29/40, Train Loss: 1.1235, Test Loss: 1.0088\n",
      "Epoch 30/40, Train Loss: 1.1774, Test Loss: 1.0101\n",
      "Epoch 31/40, Train Loss: 1.0808, Test Loss: 1.0134\n",
      "Epoch 32/40, Train Loss: 1.1203, Test Loss: 1.0111\n",
      "Epoch 33/40, Train Loss: 1.1407, Test Loss: 1.0134\n",
      "Epoch 34/40, Train Loss: 1.1492, Test Loss: 1.0142\n",
      "Epoch 35/40, Train Loss: 1.1671, Test Loss: 1.0112\n",
      "Epoch 36/40, Train Loss: 1.1155, Test Loss: 1.0155\n",
      "Epoch 37/40, Train Loss: 1.1293, Test Loss: 1.0192\n",
      "Epoch 38/40, Train Loss: 1.0967, Test Loss: 1.0129\n",
      "Epoch 39/40, Train Loss: 1.1019, Test Loss: 1.0107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:27:25,206] Trial 94 finished with value: 1.008662834763527 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 98, 'layer_1_size': 204, 'layer_2_size': 215, 'layer_3_size': 73, 'layer_4_size': 99, 'layer_5_size': 223, 'layer_6_size': 156, 'dropout_rate': 0.3576064130371223, 'learning_rate': 0.00011503016324923329, 'batch_size': 64, 'epochs': 40}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/40, Train Loss: 1.1316, Test Loss: 1.0087\n",
      "Epoch 1/21, Train Loss: 1.1338, Test Loss: 0.9324\n",
      "Epoch 2/21, Train Loss: 1.1904, Test Loss: 0.9277\n",
      "Epoch 3/21, Train Loss: 1.1744, Test Loss: 0.9707\n",
      "Epoch 4/21, Train Loss: 1.1468, Test Loss: 0.9324\n",
      "Epoch 5/21, Train Loss: 1.1403, Test Loss: 0.9126\n",
      "Epoch 6/21, Train Loss: 1.1531, Test Loss: 0.9501\n",
      "Epoch 7/21, Train Loss: 1.1509, Test Loss: 0.9305\n",
      "Epoch 8/21, Train Loss: 1.1271, Test Loss: 0.9272\n",
      "Epoch 9/21, Train Loss: 1.1306, Test Loss: 0.8950\n",
      "Epoch 10/21, Train Loss: 1.1385, Test Loss: 0.8865\n",
      "Epoch 11/21, Train Loss: 1.1129, Test Loss: 0.8606\n",
      "Epoch 12/21, Train Loss: 1.0817, Test Loss: 0.8673\n",
      "Epoch 13/21, Train Loss: 1.0430, Test Loss: 0.8657\n",
      "Epoch 14/21, Train Loss: 1.0724, Test Loss: 0.8882\n",
      "Epoch 15/21, Train Loss: 1.0781, Test Loss: 0.8885\n",
      "Epoch 16/21, Train Loss: 1.0938, Test Loss: 0.9139\n",
      "Epoch 17/21, Train Loss: 1.0623, Test Loss: 0.9059\n",
      "Epoch 18/21, Train Loss: 1.0683, Test Loss: 0.9012\n",
      "Epoch 19/21, Train Loss: 1.0518, Test Loss: 0.9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:27:28,913] Trial 95 finished with value: 0.8906306879861015 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 107, 'layer_1_size': 179, 'layer_2_size': 195, 'layer_3_size': 53, 'layer_4_size': 199, 'layer_5_size': 206, 'layer_6_size': 168, 'layer_7_size': 51, 'dropout_rate': 0.40346400701061264, 'learning_rate': 0.00017975909199922338, 'batch_size': 32, 'epochs': 21}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/21, Train Loss: 1.0757, Test Loss: 0.8818\n",
      "Epoch 21/21, Train Loss: 1.0671, Test Loss: 0.8906\n",
      "Epoch 1/60, Train Loss: 1.1625, Test Loss: 0.9015\n",
      "Epoch 2/60, Train Loss: 1.1489, Test Loss: 0.9363\n",
      "Epoch 3/60, Train Loss: 1.1973, Test Loss: 0.9387\n",
      "Epoch 4/60, Train Loss: 1.1101, Test Loss: 0.9519\n",
      "Epoch 5/60, Train Loss: 1.1475, Test Loss: 0.9413\n",
      "Epoch 6/60, Train Loss: 1.1144, Test Loss: 0.9423\n",
      "Epoch 7/60, Train Loss: 1.1605, Test Loss: 0.9321\n",
      "Epoch 8/60, Train Loss: 1.1612, Test Loss: 0.9362\n",
      "Epoch 9/60, Train Loss: 1.1201, Test Loss: 0.9475\n",
      "Epoch 10/60, Train Loss: 1.1425, Test Loss: 0.9276\n",
      "Epoch 11/60, Train Loss: 1.1350, Test Loss: 0.9271\n",
      "Epoch 12/60, Train Loss: 1.0591, Test Loss: 0.9195\n",
      "Epoch 13/60, Train Loss: 1.1046, Test Loss: 0.9097\n",
      "Epoch 14/60, Train Loss: 1.1163, Test Loss: 0.9091\n",
      "Epoch 15/60, Train Loss: 1.0910, Test Loss: 0.9094\n",
      "Epoch 16/60, Train Loss: 1.0992, Test Loss: 0.9094\n",
      "Epoch 17/60, Train Loss: 1.0955, Test Loss: 0.8953\n",
      "Epoch 18/60, Train Loss: 1.0922, Test Loss: 0.8993\n",
      "Epoch 19/60, Train Loss: 1.1354, Test Loss: 0.8978\n",
      "Epoch 20/60, Train Loss: 1.0626, Test Loss: 0.8944\n",
      "Epoch 21/60, Train Loss: 1.1048, Test Loss: 0.8955\n",
      "Epoch 22/60, Train Loss: 1.1190, Test Loss: 0.8959\n",
      "Epoch 23/60, Train Loss: 1.0865, Test Loss: 0.8904\n",
      "Epoch 24/60, Train Loss: 1.1020, Test Loss: 0.8825\n",
      "Epoch 25/60, Train Loss: 1.0911, Test Loss: 0.8831\n",
      "Epoch 26/60, Train Loss: 1.1253, Test Loss: 0.8822\n",
      "Epoch 27/60, Train Loss: 1.0835, Test Loss: 0.8846\n",
      "Epoch 28/60, Train Loss: 1.0729, Test Loss: 0.8782\n",
      "Epoch 29/60, Train Loss: 1.0612, Test Loss: 0.8779\n",
      "Epoch 30/60, Train Loss: 1.0845, Test Loss: 0.8763\n",
      "Epoch 31/60, Train Loss: 1.0730, Test Loss: 0.8794\n",
      "Epoch 32/60, Train Loss: 1.0938, Test Loss: 0.8772\n",
      "Epoch 33/60, Train Loss: 1.0507, Test Loss: 0.8790\n",
      "Epoch 34/60, Train Loss: 1.0657, Test Loss: 0.8817\n",
      "Epoch 35/60, Train Loss: 1.0516, Test Loss: 0.8835\n",
      "Epoch 36/60, Train Loss: 1.0769, Test Loss: 0.8874\n",
      "Epoch 37/60, Train Loss: 1.1045, Test Loss: 0.8818\n",
      "Epoch 38/60, Train Loss: 1.0556, Test Loss: 0.8851\n",
      "Epoch 39/60, Train Loss: 1.0827, Test Loss: 0.8827\n",
      "Epoch 40/60, Train Loss: 1.0917, Test Loss: 0.8810\n",
      "Epoch 41/60, Train Loss: 1.0722, Test Loss: 0.8806\n",
      "Epoch 42/60, Train Loss: 1.0617, Test Loss: 0.8870\n",
      "Epoch 43/60, Train Loss: 1.0775, Test Loss: 0.8864\n",
      "Epoch 44/60, Train Loss: 1.0642, Test Loss: 0.8850\n",
      "Epoch 45/60, Train Loss: 1.0498, Test Loss: 0.8887\n",
      "Epoch 46/60, Train Loss: 1.0195, Test Loss: 0.8909\n",
      "Epoch 47/60, Train Loss: 1.1045, Test Loss: 0.8809\n",
      "Epoch 48/60, Train Loss: 1.1206, Test Loss: 0.8844\n",
      "Epoch 49/60, Train Loss: 1.0640, Test Loss: 0.8810\n",
      "Epoch 50/60, Train Loss: 1.0666, Test Loss: 0.8776\n",
      "Epoch 51/60, Train Loss: 1.0609, Test Loss: 0.8816\n",
      "Epoch 52/60, Train Loss: 1.0498, Test Loss: 0.8812\n",
      "Epoch 53/60, Train Loss: 1.0438, Test Loss: 0.8733\n",
      "Epoch 54/60, Train Loss: 1.0754, Test Loss: 0.8751\n",
      "Epoch 55/60, Train Loss: 1.0603, Test Loss: 0.8751\n",
      "Epoch 56/60, Train Loss: 1.0541, Test Loss: 0.8842\n",
      "Epoch 57/60, Train Loss: 1.1021, Test Loss: 0.8777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:27:33,887] Trial 96 finished with value: 0.8737212866544724 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 120, 'layer_1_size': 32, 'layer_2_size': 211, 'layer_3_size': 55, 'layer_4_size': 210, 'layer_5_size': 215, 'layer_6_size': 143, 'layer_7_size': 114, 'dropout_rate': 0.20261177145730414, 'learning_rate': 4.461058012087704e-05, 'batch_size': 64, 'epochs': 60}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/60, Train Loss: 1.0376, Test Loss: 0.8781\n",
      "Epoch 59/60, Train Loss: 1.0499, Test Loss: 0.8721\n",
      "Epoch 60/60, Train Loss: 1.0582, Test Loss: 0.8737\n",
      "Epoch 1/74, Train Loss: 1.2518, Test Loss: 0.8475\n",
      "Epoch 2/74, Train Loss: 1.2592, Test Loss: 0.8545\n",
      "Epoch 3/74, Train Loss: 1.2410, Test Loss: 0.8566\n",
      "Epoch 4/74, Train Loss: 1.2867, Test Loss: 0.8604\n",
      "Epoch 5/74, Train Loss: 1.2135, Test Loss: 0.8615\n",
      "Epoch 6/74, Train Loss: 1.2002, Test Loss: 0.8562\n",
      "Epoch 7/74, Train Loss: 1.2401, Test Loss: 0.8579\n",
      "Epoch 8/74, Train Loss: 1.2534, Test Loss: 0.8550\n",
      "Epoch 9/74, Train Loss: 1.1478, Test Loss: 0.8540\n",
      "Epoch 10/74, Train Loss: 1.1719, Test Loss: 0.8523\n",
      "Epoch 11/74, Train Loss: 1.2265, Test Loss: 0.8481\n",
      "Epoch 12/74, Train Loss: 1.2090, Test Loss: 0.8395\n",
      "Epoch 13/74, Train Loss: 1.2085, Test Loss: 0.8391\n",
      "Epoch 14/74, Train Loss: 1.1838, Test Loss: 0.8458\n",
      "Epoch 15/74, Train Loss: 1.1452, Test Loss: 0.8511\n",
      "Epoch 16/74, Train Loss: 1.1484, Test Loss: 0.8458\n",
      "Epoch 17/74, Train Loss: 1.1647, Test Loss: 0.8454\n",
      "Epoch 18/74, Train Loss: 1.1600, Test Loss: 0.8479\n",
      "Epoch 19/74, Train Loss: 1.1884, Test Loss: 0.8435\n",
      "Epoch 20/74, Train Loss: 1.1302, Test Loss: 0.8449\n",
      "Epoch 21/74, Train Loss: 1.1642, Test Loss: 0.8432\n",
      "Epoch 22/74, Train Loss: 1.1543, Test Loss: 0.8434\n",
      "Epoch 23/74, Train Loss: 1.1793, Test Loss: 0.8427\n",
      "Epoch 24/74, Train Loss: 1.1658, Test Loss: 0.8462\n",
      "Epoch 25/74, Train Loss: 1.1116, Test Loss: 0.8452\n",
      "Epoch 26/74, Train Loss: 1.1355, Test Loss: 0.8476\n",
      "Epoch 27/74, Train Loss: 1.1506, Test Loss: 0.8464\n",
      "Epoch 28/74, Train Loss: 1.1635, Test Loss: 0.8423\n",
      "Epoch 29/74, Train Loss: 1.1240, Test Loss: 0.8436\n",
      "Epoch 30/74, Train Loss: 1.1397, Test Loss: 0.8429\n",
      "Epoch 31/74, Train Loss: 1.1090, Test Loss: 0.8413\n",
      "Epoch 32/74, Train Loss: 1.1089, Test Loss: 0.8434\n",
      "Epoch 33/74, Train Loss: 1.1279, Test Loss: 0.8446\n",
      "Epoch 34/74, Train Loss: 1.1043, Test Loss: 0.8456\n",
      "Epoch 35/74, Train Loss: 1.1524, Test Loss: 0.8484\n",
      "Epoch 36/74, Train Loss: 1.1386, Test Loss: 0.8433\n",
      "Epoch 37/74, Train Loss: 1.1041, Test Loss: 0.8420\n",
      "Epoch 38/74, Train Loss: 1.1050, Test Loss: 0.8449\n",
      "Epoch 39/74, Train Loss: 1.1303, Test Loss: 0.8451\n",
      "Epoch 40/74, Train Loss: 1.1009, Test Loss: 0.8415\n",
      "Epoch 41/74, Train Loss: 1.1338, Test Loss: 0.8421\n",
      "Epoch 42/74, Train Loss: 1.1273, Test Loss: 0.8416\n",
      "Epoch 43/74, Train Loss: 1.0851, Test Loss: 0.8460\n",
      "Epoch 44/74, Train Loss: 1.1021, Test Loss: 0.8468\n",
      "Epoch 45/74, Train Loss: 1.1406, Test Loss: 0.8455\n",
      "Epoch 46/74, Train Loss: 1.1096, Test Loss: 0.8471\n",
      "Epoch 47/74, Train Loss: 1.1081, Test Loss: 0.8456\n",
      "Epoch 48/74, Train Loss: 1.0864, Test Loss: 0.8455\n",
      "Epoch 49/74, Train Loss: 1.1030, Test Loss: 0.8419\n",
      "Epoch 50/74, Train Loss: 1.1333, Test Loss: 0.8438\n",
      "Epoch 51/74, Train Loss: 1.1016, Test Loss: 0.8442\n",
      "Epoch 52/74, Train Loss: 1.0344, Test Loss: 0.8432\n",
      "Epoch 53/74, Train Loss: 1.0617, Test Loss: 0.8452\n",
      "Epoch 54/74, Train Loss: 1.0749, Test Loss: 0.8455\n",
      "Epoch 55/74, Train Loss: 1.0822, Test Loss: 0.8456\n",
      "Epoch 56/74, Train Loss: 1.0356, Test Loss: 0.8462\n",
      "Epoch 57/74, Train Loss: 1.0980, Test Loss: 0.8467\n",
      "Epoch 58/74, Train Loss: 1.0936, Test Loss: 0.8450\n",
      "Epoch 59/74, Train Loss: 1.0691, Test Loss: 0.8480\n",
      "Epoch 60/74, Train Loss: 1.0408, Test Loss: 0.8479\n",
      "Epoch 61/74, Train Loss: 1.0664, Test Loss: 0.8461\n",
      "Epoch 62/74, Train Loss: 1.0731, Test Loss: 0.8488\n",
      "Epoch 63/74, Train Loss: 1.0952, Test Loss: 0.8447\n",
      "Epoch 64/74, Train Loss: 1.0708, Test Loss: 0.8482\n",
      "Epoch 65/74, Train Loss: 1.0882, Test Loss: 0.8481\n",
      "Epoch 66/74, Train Loss: 1.0277, Test Loss: 0.8479\n",
      "Epoch 67/74, Train Loss: 1.0341, Test Loss: 0.8477\n",
      "Epoch 68/74, Train Loss: 1.0793, Test Loss: 0.8491\n",
      "Epoch 69/74, Train Loss: 1.0963, Test Loss: 0.8436\n",
      "Epoch 70/74, Train Loss: 1.0628, Test Loss: 0.8453\n",
      "Epoch 71/74, Train Loss: 1.0787, Test Loss: 0.8512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:27:40,121] Trial 97 finished with value: 0.8469023704528809 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 112, 'layer_1_size': 191, 'layer_2_size': 201, 'layer_3_size': 62, 'layer_4_size': 230, 'layer_5_size': 226, 'layer_6_size': 156, 'dropout_rate': 0.4821794545418376, 'learning_rate': 9.117002335938326e-05, 'batch_size': 64, 'epochs': 74}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/74, Train Loss: 1.0948, Test Loss: 0.8488\n",
      "Epoch 73/74, Train Loss: 1.0925, Test Loss: 0.8455\n",
      "Epoch 74/74, Train Loss: 1.0695, Test Loss: 0.8469\n",
      "Epoch 1/18, Train Loss: 1.2504, Test Loss: 1.1679\n",
      "Epoch 2/18, Train Loss: 1.2453, Test Loss: 1.1653\n",
      "Epoch 3/18, Train Loss: 1.2286, Test Loss: 1.1657\n",
      "Epoch 4/18, Train Loss: 1.2088, Test Loss: 1.1653\n",
      "Epoch 5/18, Train Loss: 1.2160, Test Loss: 1.1637\n",
      "Epoch 6/18, Train Loss: 1.1917, Test Loss: 1.1699\n",
      "Epoch 7/18, Train Loss: 1.2644, Test Loss: 1.1647\n",
      "Epoch 8/18, Train Loss: 1.1841, Test Loss: 1.1708\n",
      "Epoch 9/18, Train Loss: 1.2076, Test Loss: 1.1673\n",
      "Epoch 10/18, Train Loss: 1.1871, Test Loss: 1.1656\n",
      "Epoch 11/18, Train Loss: 1.2380, Test Loss: 1.1722\n",
      "Epoch 12/18, Train Loss: 1.2053, Test Loss: 1.1669\n",
      "Epoch 13/18, Train Loss: 1.1816, Test Loss: 1.1734\n",
      "Epoch 14/18, Train Loss: 1.1970, Test Loss: 1.1653\n",
      "Epoch 15/18, Train Loss: 1.1726, Test Loss: 1.1623\n",
      "Epoch 16/18, Train Loss: 1.1541, Test Loss: 1.1700\n",
      "Epoch 17/18, Train Loss: 1.1481, Test Loss: 1.1736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:27:41,869] Trial 98 finished with value: 1.1691052466630936 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 129, 'layer_1_size': 164, 'layer_2_size': 207, 'layer_3_size': 84, 'layer_4_size': 151, 'layer_5_size': 240, 'layer_6_size': 194, 'dropout_rate': 0.3926569583905392, 'learning_rate': 6.304487662585618e-05, 'batch_size': 64, 'epochs': 18}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/18, Train Loss: 1.1451, Test Loss: 1.1691\n",
      "Epoch 1/32, Train Loss: 1.4112, Test Loss: 1.1725\n",
      "Epoch 2/32, Train Loss: 1.3523, Test Loss: 1.1722\n",
      "Epoch 3/32, Train Loss: 1.4479, Test Loss: 1.1715\n",
      "Epoch 4/32, Train Loss: 1.4160, Test Loss: 1.1705\n",
      "Epoch 5/32, Train Loss: 1.4455, Test Loss: 1.1696\n",
      "Epoch 6/32, Train Loss: 1.4215, Test Loss: 1.1692\n",
      "Epoch 7/32, Train Loss: 1.4924, Test Loss: 1.1685\n",
      "Epoch 8/32, Train Loss: 1.4166, Test Loss: 1.1687\n",
      "Epoch 9/32, Train Loss: 1.3479, Test Loss: 1.1679\n",
      "Epoch 10/32, Train Loss: 1.4018, Test Loss: 1.1674\n",
      "Epoch 11/32, Train Loss: 1.3375, Test Loss: 1.1674\n",
      "Epoch 12/32, Train Loss: 1.3985, Test Loss: 1.1682\n",
      "Epoch 13/32, Train Loss: 1.3678, Test Loss: 1.1689\n",
      "Epoch 14/32, Train Loss: 1.4263, Test Loss: 1.1680\n",
      "Epoch 15/32, Train Loss: 1.3073, Test Loss: 1.1686\n",
      "Epoch 16/32, Train Loss: 1.3675, Test Loss: 1.1689\n",
      "Epoch 17/32, Train Loss: 1.4001, Test Loss: 1.1696\n",
      "Epoch 18/32, Train Loss: 1.3383, Test Loss: 1.1697\n",
      "Epoch 19/32, Train Loss: 1.4899, Test Loss: 1.1703\n",
      "Epoch 20/32, Train Loss: 1.3534, Test Loss: 1.1713\n",
      "Epoch 21/32, Train Loss: 1.3059, Test Loss: 1.1716\n",
      "Epoch 22/32, Train Loss: 1.3049, Test Loss: 1.1716\n",
      "Epoch 23/32, Train Loss: 1.3560, Test Loss: 1.1724\n",
      "Epoch 24/32, Train Loss: 1.3660, Test Loss: 1.1726\n",
      "Epoch 25/32, Train Loss: 1.3736, Test Loss: 1.1726\n",
      "Epoch 26/32, Train Loss: 1.3515, Test Loss: 1.1720\n",
      "Epoch 27/32, Train Loss: 1.3474, Test Loss: 1.1726\n",
      "Epoch 28/32, Train Loss: 1.3769, Test Loss: 1.1731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:27:43,406] Trial 99 finished with value: 1.1727527379989624 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 142, 'layer_1_size': 171, 'layer_2_size': 230, 'layer_3_size': 42, 'layer_4_size': 191, 'layer_5_size': 247, 'layer_6_size': 175, 'layer_7_size': 98, 'dropout_rate': 0.468451094978283, 'learning_rate': 3.25950172199309e-05, 'batch_size': 256, 'epochs': 32}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/32, Train Loss: 1.3163, Test Loss: 1.1741\n",
      "Epoch 30/32, Train Loss: 1.3273, Test Loss: 1.1739\n",
      "Epoch 31/32, Train Loss: 1.3036, Test Loss: 1.1727\n",
      "Epoch 32/32, Train Loss: 1.3446, Test Loss: 1.1728\n",
      "Epoch 1/69, Train Loss: 1.2006, Test Loss: 0.9124\n",
      "Epoch 2/69, Train Loss: 1.1585, Test Loss: 0.9101\n",
      "Epoch 3/69, Train Loss: 1.1644, Test Loss: 0.9071\n",
      "Epoch 4/69, Train Loss: 1.1025, Test Loss: 0.9145\n",
      "Epoch 5/69, Train Loss: 1.1504, Test Loss: 0.9230\n",
      "Epoch 6/69, Train Loss: 1.1645, Test Loss: 0.9281\n",
      "Epoch 7/69, Train Loss: 1.0929, Test Loss: 0.9243\n",
      "Epoch 8/69, Train Loss: 1.1039, Test Loss: 0.9240\n",
      "Epoch 9/69, Train Loss: 1.1189, Test Loss: 0.9235\n",
      "Epoch 10/69, Train Loss: 1.0839, Test Loss: 0.9231\n",
      "Epoch 11/69, Train Loss: 1.0609, Test Loss: 0.9191\n",
      "Epoch 12/69, Train Loss: 1.1342, Test Loss: 0.9216\n",
      "Epoch 13/69, Train Loss: 1.1145, Test Loss: 0.9234\n",
      "Epoch 14/69, Train Loss: 1.0590, Test Loss: 0.9211\n",
      "Epoch 15/69, Train Loss: 1.0639, Test Loss: 0.9147\n",
      "Epoch 16/69, Train Loss: 1.0778, Test Loss: 0.9199\n",
      "Epoch 17/69, Train Loss: 1.0374, Test Loss: 0.9074\n",
      "Epoch 18/69, Train Loss: 1.0567, Test Loss: 0.9023\n",
      "Epoch 19/69, Train Loss: 1.1324, Test Loss: 0.9035\n",
      "Epoch 20/69, Train Loss: 1.0515, Test Loss: 0.9052\n",
      "Epoch 21/69, Train Loss: 1.0780, Test Loss: 0.9062\n",
      "Epoch 22/69, Train Loss: 1.0564, Test Loss: 0.9107\n",
      "Epoch 23/69, Train Loss: 1.0368, Test Loss: 0.9043\n",
      "Epoch 24/69, Train Loss: 1.0721, Test Loss: 0.9017\n",
      "Epoch 25/69, Train Loss: 1.0319, Test Loss: 0.9071\n",
      "Epoch 26/69, Train Loss: 1.0477, Test Loss: 0.9059\n",
      "Epoch 27/69, Train Loss: 1.0189, Test Loss: 0.9075\n",
      "Epoch 28/69, Train Loss: 1.0066, Test Loss: 0.9136\n",
      "Epoch 29/69, Train Loss: 1.0094, Test Loss: 0.9138\n",
      "Epoch 30/69, Train Loss: 1.0192, Test Loss: 0.9110\n",
      "Epoch 31/69, Train Loss: 1.0229, Test Loss: 0.9147\n",
      "Epoch 32/69, Train Loss: 1.0346, Test Loss: 0.9147\n",
      "Epoch 33/69, Train Loss: 1.0517, Test Loss: 0.9135\n",
      "Epoch 34/69, Train Loss: 1.0317, Test Loss: 0.9070\n",
      "Epoch 35/69, Train Loss: 1.0264, Test Loss: 0.9053\n",
      "Epoch 36/69, Train Loss: 1.0110, Test Loss: 0.9088\n",
      "Epoch 37/69, Train Loss: 0.9909, Test Loss: 0.9129\n",
      "Epoch 38/69, Train Loss: 1.0185, Test Loss: 0.9153\n",
      "Epoch 39/69, Train Loss: 0.9856, Test Loss: 0.9134\n",
      "Epoch 40/69, Train Loss: 1.0489, Test Loss: 0.9144\n",
      "Epoch 41/69, Train Loss: 1.0059, Test Loss: 0.9118\n",
      "Epoch 42/69, Train Loss: 1.0418, Test Loss: 0.9169\n",
      "Epoch 43/69, Train Loss: 1.0162, Test Loss: 0.9234\n",
      "Epoch 44/69, Train Loss: 1.0294, Test Loss: 0.9251\n",
      "Epoch 45/69, Train Loss: 1.0226, Test Loss: 0.9258\n",
      "Epoch 46/69, Train Loss: 0.9809, Test Loss: 0.9193\n",
      "Epoch 47/69, Train Loss: 1.0189, Test Loss: 0.9268\n",
      "Epoch 48/69, Train Loss: 1.0021, Test Loss: 0.9265\n",
      "Epoch 49/69, Train Loss: 1.0040, Test Loss: 0.9282\n",
      "Epoch 50/69, Train Loss: 1.0037, Test Loss: 0.9333\n",
      "Epoch 51/69, Train Loss: 1.0292, Test Loss: 0.9283\n",
      "Epoch 52/69, Train Loss: 1.0196, Test Loss: 0.9224\n",
      "Epoch 53/69, Train Loss: 0.9992, Test Loss: 0.9174\n",
      "Epoch 54/69, Train Loss: 1.0044, Test Loss: 0.9085\n",
      "Epoch 55/69, Train Loss: 0.9810, Test Loss: 0.9067\n",
      "Epoch 56/69, Train Loss: 1.0106, Test Loss: 0.9113\n",
      "Epoch 57/69, Train Loss: 0.9926, Test Loss: 0.9110\n",
      "Epoch 58/69, Train Loss: 0.9832, Test Loss: 0.9125\n",
      "Epoch 59/69, Train Loss: 1.0163, Test Loss: 0.9144\n",
      "Epoch 60/69, Train Loss: 1.0039, Test Loss: 0.9142\n",
      "Epoch 61/69, Train Loss: 0.9882, Test Loss: 0.9232\n",
      "Epoch 62/69, Train Loss: 0.9749, Test Loss: 0.9246\n",
      "Epoch 63/69, Train Loss: 0.9910, Test Loss: 0.9240\n",
      "Epoch 64/69, Train Loss: 0.9818, Test Loss: 0.9231\n",
      "Epoch 65/69, Train Loss: 1.0035, Test Loss: 0.9282\n",
      "Epoch 66/69, Train Loss: 0.9885, Test Loss: 0.9281\n",
      "Epoch 67/69, Train Loss: 0.9843, Test Loss: 0.9245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:27:49,781] Trial 100 finished with value: 0.9244175255298615 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 117, 'layer_1_size': 180, 'layer_2_size': 218, 'layer_3_size': 248, 'layer_4_size': 71, 'layer_5_size': 197, 'layer_6_size': 121, 'dropout_rate': 0.41547297491082313, 'learning_rate': 0.00022800157659249128, 'batch_size': 64, 'epochs': 69}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/69, Train Loss: 1.0190, Test Loss: 0.9285\n",
      "Epoch 69/69, Train Loss: 0.9928, Test Loss: 0.9244\n",
      "Epoch 1/72, Train Loss: 1.2785, Test Loss: 0.8811\n",
      "Epoch 2/72, Train Loss: 1.2876, Test Loss: 0.8822\n",
      "Epoch 3/72, Train Loss: 1.2732, Test Loss: 0.8847\n",
      "Epoch 4/72, Train Loss: 1.2206, Test Loss: 0.8828\n",
      "Epoch 5/72, Train Loss: 1.2344, Test Loss: 0.8840\n",
      "Epoch 6/72, Train Loss: 1.1960, Test Loss: 0.8840\n",
      "Epoch 7/72, Train Loss: 1.2195, Test Loss: 0.8842\n",
      "Epoch 8/72, Train Loss: 1.2246, Test Loss: 0.8821\n",
      "Epoch 9/72, Train Loss: 1.2126, Test Loss: 0.8874\n",
      "Epoch 10/72, Train Loss: 1.1905, Test Loss: 0.8843\n",
      "Epoch 11/72, Train Loss: 1.1675, Test Loss: 0.8874\n",
      "Epoch 12/72, Train Loss: 1.1853, Test Loss: 0.8891\n",
      "Epoch 13/72, Train Loss: 1.1749, Test Loss: 0.8897\n",
      "Epoch 14/72, Train Loss: 1.2229, Test Loss: 0.8914\n",
      "Epoch 15/72, Train Loss: 1.2119, Test Loss: 0.8928\n",
      "Epoch 16/72, Train Loss: 1.2128, Test Loss: 0.8942\n",
      "Epoch 17/72, Train Loss: 1.1698, Test Loss: 0.8945\n",
      "Epoch 18/72, Train Loss: 1.1470, Test Loss: 0.8937\n",
      "Epoch 19/72, Train Loss: 1.2062, Test Loss: 0.8937\n",
      "Epoch 20/72, Train Loss: 1.2083, Test Loss: 0.8970\n",
      "Epoch 21/72, Train Loss: 1.1483, Test Loss: 0.8971\n",
      "Epoch 22/72, Train Loss: 1.1459, Test Loss: 0.9020\n",
      "Epoch 23/72, Train Loss: 1.1308, Test Loss: 0.8968\n",
      "Epoch 24/72, Train Loss: 1.1550, Test Loss: 0.8989\n",
      "Epoch 25/72, Train Loss: 1.1352, Test Loss: 0.8950\n",
      "Epoch 26/72, Train Loss: 1.1405, Test Loss: 0.8952\n",
      "Epoch 27/72, Train Loss: 1.1542, Test Loss: 0.8953\n",
      "Epoch 28/72, Train Loss: 1.1850, Test Loss: 0.8920\n",
      "Epoch 29/72, Train Loss: 1.2222, Test Loss: 0.8940\n",
      "Epoch 30/72, Train Loss: 1.1702, Test Loss: 0.8969\n",
      "Epoch 31/72, Train Loss: 1.1381, Test Loss: 0.8986\n",
      "Epoch 32/72, Train Loss: 1.1282, Test Loss: 0.8948\n",
      "Epoch 33/72, Train Loss: 1.1525, Test Loss: 0.8958\n",
      "Epoch 34/72, Train Loss: 1.1702, Test Loss: 0.8948\n",
      "Epoch 35/72, Train Loss: 1.1297, Test Loss: 0.8917\n",
      "Epoch 36/72, Train Loss: 1.1654, Test Loss: 0.8940\n",
      "Epoch 37/72, Train Loss: 1.1401, Test Loss: 0.8953\n",
      "Epoch 38/72, Train Loss: 1.1323, Test Loss: 0.8951\n",
      "Epoch 39/72, Train Loss: 1.1428, Test Loss: 0.8934\n",
      "Epoch 40/72, Train Loss: 1.1089, Test Loss: 0.8922\n",
      "Epoch 41/72, Train Loss: 1.1165, Test Loss: 0.8917\n",
      "Epoch 42/72, Train Loss: 1.1345, Test Loss: 0.8925\n",
      "Epoch 43/72, Train Loss: 1.1046, Test Loss: 0.8898\n",
      "Epoch 44/72, Train Loss: 1.1315, Test Loss: 0.8883\n",
      "Epoch 45/72, Train Loss: 1.1238, Test Loss: 0.8893\n",
      "Epoch 46/72, Train Loss: 1.1462, Test Loss: 0.8916\n",
      "Epoch 47/72, Train Loss: 1.1040, Test Loss: 0.8917\n",
      "Epoch 48/72, Train Loss: 1.1225, Test Loss: 0.8895\n",
      "Epoch 49/72, Train Loss: 1.1389, Test Loss: 0.8903\n",
      "Epoch 50/72, Train Loss: 1.1498, Test Loss: 0.8860\n",
      "Epoch 51/72, Train Loss: 1.1020, Test Loss: 0.8888\n",
      "Epoch 52/72, Train Loss: 1.1703, Test Loss: 0.8886\n",
      "Epoch 53/72, Train Loss: 1.1362, Test Loss: 0.8876\n",
      "Epoch 54/72, Train Loss: 1.1224, Test Loss: 0.8856\n",
      "Epoch 55/72, Train Loss: 1.1236, Test Loss: 0.8873\n",
      "Epoch 56/72, Train Loss: 1.1260, Test Loss: 0.8897\n",
      "Epoch 57/72, Train Loss: 1.1076, Test Loss: 0.8883\n",
      "Epoch 58/72, Train Loss: 1.1182, Test Loss: 0.8891\n",
      "Epoch 59/72, Train Loss: 1.1145, Test Loss: 0.8882\n",
      "Epoch 60/72, Train Loss: 1.0789, Test Loss: 0.8879\n",
      "Epoch 61/72, Train Loss: 1.1027, Test Loss: 0.8877\n",
      "Epoch 62/72, Train Loss: 1.0965, Test Loss: 0.8881\n",
      "Epoch 63/72, Train Loss: 1.1382, Test Loss: 0.8863\n",
      "Epoch 64/72, Train Loss: 1.0948, Test Loss: 0.8874\n",
      "Epoch 65/72, Train Loss: 1.0931, Test Loss: 0.8883\n",
      "Epoch 66/72, Train Loss: 1.0886, Test Loss: 0.8877\n",
      "Epoch 67/72, Train Loss: 1.1053, Test Loss: 0.8896\n",
      "Epoch 68/72, Train Loss: 1.1240, Test Loss: 0.8887\n",
      "Epoch 69/72, Train Loss: 1.0932, Test Loss: 0.8881\n",
      "Epoch 70/72, Train Loss: 1.1077, Test Loss: 0.8868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:27:56,693] Trial 101 finished with value: 0.8879910707473755 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 112, 'layer_1_size': 192, 'layer_2_size': 202, 'layer_3_size': 64, 'layer_4_size': 230, 'layer_5_size': 228, 'layer_6_size': 159, 'dropout_rate': 0.48700912594376683, 'learning_rate': 8.691710745924171e-05, 'batch_size': 64, 'epochs': 72}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/72, Train Loss: 1.0535, Test Loss: 0.8867\n",
      "Epoch 72/72, Train Loss: 1.1030, Test Loss: 0.8880\n",
      "Epoch 1/66, Train Loss: 1.2794, Test Loss: 1.0696\n",
      "Epoch 2/66, Train Loss: 1.2868, Test Loss: 1.0473\n",
      "Epoch 3/66, Train Loss: 1.2372, Test Loss: 1.0386\n",
      "Epoch 4/66, Train Loss: 1.1890, Test Loss: 1.0352\n",
      "Epoch 5/66, Train Loss: 1.1437, Test Loss: 1.0333\n",
      "Epoch 6/66, Train Loss: 1.2260, Test Loss: 1.0348\n",
      "Epoch 7/66, Train Loss: 1.1891, Test Loss: 1.0424\n",
      "Epoch 8/66, Train Loss: 1.1520, Test Loss: 1.0388\n",
      "Epoch 9/66, Train Loss: 1.1438, Test Loss: 1.0339\n",
      "Epoch 10/66, Train Loss: 1.1718, Test Loss: 1.0346\n",
      "Epoch 11/66, Train Loss: 1.1385, Test Loss: 1.0398\n",
      "Epoch 12/66, Train Loss: 1.1310, Test Loss: 1.0344\n",
      "Epoch 13/66, Train Loss: 1.1499, Test Loss: 1.0296\n",
      "Epoch 14/66, Train Loss: 1.1814, Test Loss: 1.0245\n",
      "Epoch 15/66, Train Loss: 1.1100, Test Loss: 1.0293\n",
      "Epoch 16/66, Train Loss: 1.1104, Test Loss: 1.0247\n",
      "Epoch 17/66, Train Loss: 1.1310, Test Loss: 1.0278\n",
      "Epoch 18/66, Train Loss: 1.1735, Test Loss: 1.0321\n",
      "Epoch 19/66, Train Loss: 1.1017, Test Loss: 1.0240\n",
      "Epoch 20/66, Train Loss: 1.1322, Test Loss: 1.0241\n",
      "Epoch 21/66, Train Loss: 1.1094, Test Loss: 1.0268\n",
      "Epoch 22/66, Train Loss: 1.1040, Test Loss: 1.0238\n",
      "Epoch 23/66, Train Loss: 1.0750, Test Loss: 1.0238\n",
      "Epoch 24/66, Train Loss: 1.0958, Test Loss: 1.0266\n",
      "Epoch 25/66, Train Loss: 1.0806, Test Loss: 1.0300\n",
      "Epoch 26/66, Train Loss: 1.1044, Test Loss: 1.0304\n",
      "Epoch 27/66, Train Loss: 1.0631, Test Loss: 1.0286\n",
      "Epoch 28/66, Train Loss: 1.0563, Test Loss: 1.0281\n",
      "Epoch 29/66, Train Loss: 1.0651, Test Loss: 1.0311\n",
      "Epoch 30/66, Train Loss: 1.0923, Test Loss: 1.0298\n",
      "Epoch 31/66, Train Loss: 1.0562, Test Loss: 1.0280\n",
      "Epoch 32/66, Train Loss: 1.0678, Test Loss: 1.0318\n",
      "Epoch 33/66, Train Loss: 1.0569, Test Loss: 1.0335\n",
      "Epoch 34/66, Train Loss: 1.0801, Test Loss: 1.0383\n",
      "Epoch 35/66, Train Loss: 1.0858, Test Loss: 1.0371\n",
      "Epoch 36/66, Train Loss: 1.0485, Test Loss: 1.0385\n",
      "Epoch 37/66, Train Loss: 1.0509, Test Loss: 1.0431\n",
      "Epoch 38/66, Train Loss: 1.0207, Test Loss: 1.0431\n",
      "Epoch 39/66, Train Loss: 1.0559, Test Loss: 1.0416\n",
      "Epoch 40/66, Train Loss: 1.0890, Test Loss: 1.0454\n",
      "Epoch 41/66, Train Loss: 1.0619, Test Loss: 1.0372\n",
      "Epoch 42/66, Train Loss: 1.1074, Test Loss: 1.0337\n",
      "Epoch 43/66, Train Loss: 1.0290, Test Loss: 1.0295\n",
      "Epoch 44/66, Train Loss: 1.0118, Test Loss: 1.0346\n",
      "Epoch 45/66, Train Loss: 1.0163, Test Loss: 1.0334\n",
      "Epoch 46/66, Train Loss: 1.0296, Test Loss: 1.0320\n",
      "Epoch 47/66, Train Loss: 1.0450, Test Loss: 1.0319\n",
      "Epoch 48/66, Train Loss: 1.0452, Test Loss: 1.0326\n",
      "Epoch 49/66, Train Loss: 1.0209, Test Loss: 1.0344\n",
      "Epoch 50/66, Train Loss: 1.0141, Test Loss: 1.0325\n",
      "Epoch 51/66, Train Loss: 1.0313, Test Loss: 1.0329\n",
      "Epoch 52/66, Train Loss: 0.9882, Test Loss: 1.0291\n",
      "Epoch 53/66, Train Loss: 1.0263, Test Loss: 1.0262\n",
      "Epoch 54/66, Train Loss: 1.0095, Test Loss: 1.0233\n",
      "Epoch 55/66, Train Loss: 1.0377, Test Loss: 1.0231\n",
      "Epoch 56/66, Train Loss: 1.0215, Test Loss: 1.0235\n",
      "Epoch 57/66, Train Loss: 1.0127, Test Loss: 1.0244\n",
      "Epoch 58/66, Train Loss: 1.0515, Test Loss: 1.0233\n",
      "Epoch 59/66, Train Loss: 1.0308, Test Loss: 1.0234\n",
      "Epoch 60/66, Train Loss: 1.0085, Test Loss: 1.0269\n",
      "Epoch 61/66, Train Loss: 1.0276, Test Loss: 1.0244\n",
      "Epoch 62/66, Train Loss: 1.0085, Test Loss: 1.0263\n",
      "Epoch 63/66, Train Loss: 1.0157, Test Loss: 1.0266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:28:01,856] Trial 102 finished with value: 1.0254518389701843 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 90, 'layer_1_size': 190, 'layer_2_size': 177, 'layer_3_size': 58, 'layer_4_size': 139, 'layer_5_size': 235, 'layer_6_size': 135, 'dropout_rate': 0.4779580126706099, 'learning_rate': 0.00014105770977148905, 'batch_size': 64, 'epochs': 66}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/66, Train Loss: 1.0260, Test Loss: 1.0264\n",
      "Epoch 65/66, Train Loss: 1.0092, Test Loss: 1.0253\n",
      "Epoch 66/66, Train Loss: 0.9873, Test Loss: 1.0255\n",
      "Epoch 1/84, Train Loss: 1.1847, Test Loss: 0.9327\n",
      "Epoch 2/84, Train Loss: 1.1682, Test Loss: 0.9367\n",
      "Epoch 3/84, Train Loss: 1.1451, Test Loss: 0.9375\n",
      "Epoch 4/84, Train Loss: 1.1494, Test Loss: 0.9404\n",
      "Epoch 5/84, Train Loss: 1.0892, Test Loss: 0.9469\n",
      "Epoch 6/84, Train Loss: 1.0806, Test Loss: 0.9475\n",
      "Epoch 7/84, Train Loss: 1.1033, Test Loss: 0.9530\n",
      "Epoch 8/84, Train Loss: 1.0612, Test Loss: 0.9496\n",
      "Epoch 9/84, Train Loss: 1.1113, Test Loss: 0.9460\n",
      "Epoch 10/84, Train Loss: 1.1279, Test Loss: 0.9524\n",
      "Epoch 11/84, Train Loss: 1.0940, Test Loss: 0.9493\n",
      "Epoch 12/84, Train Loss: 1.1180, Test Loss: 0.9455\n",
      "Epoch 13/84, Train Loss: 1.1021, Test Loss: 0.9432\n",
      "Epoch 14/84, Train Loss: 1.0536, Test Loss: 0.9410\n",
      "Epoch 15/84, Train Loss: 1.0541, Test Loss: 0.9439\n",
      "Epoch 16/84, Train Loss: 1.0387, Test Loss: 0.9451\n",
      "Epoch 17/84, Train Loss: 1.0762, Test Loss: 0.9416\n",
      "Epoch 18/84, Train Loss: 1.0901, Test Loss: 0.9396\n",
      "Epoch 19/84, Train Loss: 1.0669, Test Loss: 0.9391\n",
      "Epoch 20/84, Train Loss: 1.0487, Test Loss: 0.9381\n",
      "Epoch 21/84, Train Loss: 1.0496, Test Loss: 0.9395\n",
      "Epoch 22/84, Train Loss: 1.0653, Test Loss: 0.9440\n",
      "Epoch 23/84, Train Loss: 1.0325, Test Loss: 0.9415\n",
      "Epoch 24/84, Train Loss: 1.0576, Test Loss: 0.9405\n",
      "Epoch 25/84, Train Loss: 1.0394, Test Loss: 0.9390\n",
      "Epoch 26/84, Train Loss: 1.0093, Test Loss: 0.9390\n",
      "Epoch 27/84, Train Loss: 1.0184, Test Loss: 0.9412\n",
      "Epoch 28/84, Train Loss: 1.0553, Test Loss: 0.9394\n",
      "Epoch 29/84, Train Loss: 1.0440, Test Loss: 0.9424\n",
      "Epoch 30/84, Train Loss: 1.0532, Test Loss: 0.9402\n",
      "Epoch 31/84, Train Loss: 1.0516, Test Loss: 0.9391\n",
      "Epoch 32/84, Train Loss: 1.0320, Test Loss: 0.9369\n",
      "Epoch 33/84, Train Loss: 1.0059, Test Loss: 0.9360\n",
      "Epoch 34/84, Train Loss: 0.9995, Test Loss: 0.9349\n",
      "Epoch 35/84, Train Loss: 1.0364, Test Loss: 0.9367\n",
      "Epoch 36/84, Train Loss: 1.0478, Test Loss: 0.9396\n",
      "Epoch 37/84, Train Loss: 1.0094, Test Loss: 0.9409\n",
      "Epoch 38/84, Train Loss: 1.0204, Test Loss: 0.9386\n",
      "Epoch 39/84, Train Loss: 0.9928, Test Loss: 0.9379\n",
      "Epoch 40/84, Train Loss: 1.0342, Test Loss: 0.9392\n",
      "Epoch 41/84, Train Loss: 1.0539, Test Loss: 0.9370\n",
      "Epoch 42/84, Train Loss: 1.0572, Test Loss: 0.9386\n",
      "Epoch 43/84, Train Loss: 1.0031, Test Loss: 0.9399\n",
      "Epoch 44/84, Train Loss: 0.9988, Test Loss: 0.9415\n",
      "Epoch 45/84, Train Loss: 1.0320, Test Loss: 0.9406\n",
      "Epoch 46/84, Train Loss: 1.0098, Test Loss: 0.9410\n",
      "Epoch 47/84, Train Loss: 1.0000, Test Loss: 0.9418\n",
      "Epoch 48/84, Train Loss: 1.0165, Test Loss: 0.9428\n",
      "Epoch 49/84, Train Loss: 1.0152, Test Loss: 0.9421\n",
      "Epoch 50/84, Train Loss: 1.0020, Test Loss: 0.9410\n",
      "Epoch 51/84, Train Loss: 1.0153, Test Loss: 0.9399\n",
      "Epoch 52/84, Train Loss: 1.0049, Test Loss: 0.9408\n",
      "Epoch 53/84, Train Loss: 0.9709, Test Loss: 0.9418\n",
      "Epoch 54/84, Train Loss: 1.0102, Test Loss: 0.9401\n",
      "Epoch 55/84, Train Loss: 1.0332, Test Loss: 0.9402\n",
      "Epoch 56/84, Train Loss: 1.0235, Test Loss: 0.9422\n",
      "Epoch 57/84, Train Loss: 1.0160, Test Loss: 0.9397\n",
      "Epoch 58/84, Train Loss: 1.0128, Test Loss: 0.9405\n",
      "Epoch 59/84, Train Loss: 0.9966, Test Loss: 0.9392\n",
      "Epoch 60/84, Train Loss: 0.9969, Test Loss: 0.9388\n",
      "Epoch 61/84, Train Loss: 1.0038, Test Loss: 0.9391\n",
      "Epoch 62/84, Train Loss: 1.0224, Test Loss: 0.9429\n",
      "Epoch 63/84, Train Loss: 0.9823, Test Loss: 0.9399\n",
      "Epoch 64/84, Train Loss: 1.0091, Test Loss: 0.9430\n",
      "Epoch 65/84, Train Loss: 1.0075, Test Loss: 0.9435\n",
      "Epoch 66/84, Train Loss: 0.9922, Test Loss: 0.9415\n",
      "Epoch 67/84, Train Loss: 0.9761, Test Loss: 0.9374\n",
      "Epoch 68/84, Train Loss: 0.9721, Test Loss: 0.9373\n",
      "Epoch 69/84, Train Loss: 0.9814, Test Loss: 0.9410\n",
      "Epoch 70/84, Train Loss: 0.9922, Test Loss: 0.9411\n",
      "Epoch 71/84, Train Loss: 0.9783, Test Loss: 0.9407\n",
      "Epoch 72/84, Train Loss: 0.9954, Test Loss: 0.9412\n",
      "Epoch 73/84, Train Loss: 1.0018, Test Loss: 0.9393\n",
      "Epoch 74/84, Train Loss: 0.9824, Test Loss: 0.9384\n",
      "Epoch 75/84, Train Loss: 0.9811, Test Loss: 0.9389\n",
      "Epoch 76/84, Train Loss: 0.9909, Test Loss: 0.9391\n",
      "Epoch 77/84, Train Loss: 0.9892, Test Loss: 0.9374\n",
      "Epoch 78/84, Train Loss: 0.9851, Test Loss: 0.9409\n",
      "Epoch 79/84, Train Loss: 1.0002, Test Loss: 0.9428\n",
      "Epoch 80/84, Train Loss: 0.9784, Test Loss: 0.9429\n",
      "Epoch 81/84, Train Loss: 0.9789, Test Loss: 0.9416\n",
      "Epoch 82/84, Train Loss: 0.9953, Test Loss: 0.9410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:28:08,184] Trial 103 finished with value: 0.938449315726757 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 250, 'layer_1_size': 219, 'layer_2_size': 184, 'layer_3_size': 60, 'layer_4_size': 204, 'layer_5_size': 123, 'layer_6_size': 149, 'dropout_rate': 0.37252822587593626, 'learning_rate': 0.0001020646975672594, 'batch_size': 64, 'epochs': 84}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/84, Train Loss: 0.9779, Test Loss: 0.9410\n",
      "Epoch 84/84, Train Loss: 0.9981, Test Loss: 0.9384\n",
      "Epoch 1/76, Train Loss: 1.1600, Test Loss: 1.2096\n",
      "Epoch 2/76, Train Loss: 1.1260, Test Loss: 1.2216\n",
      "Epoch 3/76, Train Loss: 1.0654, Test Loss: 1.2610\n",
      "Epoch 4/76, Train Loss: 1.0642, Test Loss: 1.2572\n",
      "Epoch 5/76, Train Loss: 1.0344, Test Loss: 1.2649\n",
      "Epoch 6/76, Train Loss: 1.1020, Test Loss: 1.2650\n",
      "Epoch 7/76, Train Loss: 1.0386, Test Loss: 1.2385\n",
      "Epoch 8/76, Train Loss: 1.0262, Test Loss: 1.2555\n",
      "Epoch 9/76, Train Loss: 1.0510, Test Loss: 1.2475\n",
      "Epoch 10/76, Train Loss: 1.0796, Test Loss: 1.2535\n",
      "Epoch 11/76, Train Loss: 1.0642, Test Loss: 1.2475\n",
      "Epoch 12/76, Train Loss: 1.0533, Test Loss: 1.2450\n",
      "Epoch 13/76, Train Loss: 1.0337, Test Loss: 1.2506\n",
      "Epoch 14/76, Train Loss: 1.0666, Test Loss: 1.2483\n",
      "Epoch 15/76, Train Loss: 1.0387, Test Loss: 1.2594\n",
      "Epoch 16/76, Train Loss: 1.0130, Test Loss: 1.2551\n",
      "Epoch 17/76, Train Loss: 1.0668, Test Loss: 1.2449\n",
      "Epoch 18/76, Train Loss: 1.0278, Test Loss: 1.2515\n",
      "Epoch 19/76, Train Loss: 1.0773, Test Loss: 1.2535\n",
      "Epoch 20/76, Train Loss: 1.0427, Test Loss: 1.2481\n",
      "Epoch 21/76, Train Loss: 1.0675, Test Loss: 1.2507\n",
      "Epoch 22/76, Train Loss: 1.0455, Test Loss: 1.2558\n",
      "Epoch 23/76, Train Loss: 1.0134, Test Loss: 1.2597\n",
      "Epoch 24/76, Train Loss: 1.0065, Test Loss: 1.2676\n",
      "Epoch 25/76, Train Loss: 1.0190, Test Loss: 1.2567\n",
      "Epoch 26/76, Train Loss: 1.0063, Test Loss: 1.2597\n",
      "Epoch 27/76, Train Loss: 1.0552, Test Loss: 1.2660\n",
      "Epoch 28/76, Train Loss: 1.0176, Test Loss: 1.2643\n",
      "Epoch 29/76, Train Loss: 0.9884, Test Loss: 1.2848\n",
      "Epoch 30/76, Train Loss: 1.0644, Test Loss: 1.2737\n",
      "Epoch 31/76, Train Loss: 1.0189, Test Loss: 1.2834\n",
      "Epoch 32/76, Train Loss: 0.9981, Test Loss: 1.2835\n",
      "Epoch 33/76, Train Loss: 1.0212, Test Loss: 1.2796\n",
      "Epoch 34/76, Train Loss: 0.9932, Test Loss: 1.2765\n",
      "Epoch 35/76, Train Loss: 0.9983, Test Loss: 1.2560\n",
      "Epoch 36/76, Train Loss: 1.0207, Test Loss: 1.2602\n",
      "Epoch 37/76, Train Loss: 1.0289, Test Loss: 1.2675\n",
      "Epoch 38/76, Train Loss: 1.0130, Test Loss: 1.2735\n",
      "Epoch 39/76, Train Loss: 0.9955, Test Loss: 1.2699\n",
      "Epoch 40/76, Train Loss: 1.0066, Test Loss: 1.2576\n",
      "Epoch 41/76, Train Loss: 0.9965, Test Loss: 1.2647\n",
      "Epoch 42/76, Train Loss: 1.0257, Test Loss: 1.2687\n",
      "Epoch 43/76, Train Loss: 0.9782, Test Loss: 1.2503\n",
      "Epoch 44/76, Train Loss: 1.0228, Test Loss: 1.2676\n",
      "Epoch 45/76, Train Loss: 1.0006, Test Loss: 1.2600\n",
      "Epoch 46/76, Train Loss: 1.0037, Test Loss: 1.2671\n",
      "Epoch 47/76, Train Loss: 1.0153, Test Loss: 1.2550\n",
      "Epoch 48/76, Train Loss: 1.0427, Test Loss: 1.2515\n",
      "Epoch 49/76, Train Loss: 0.9956, Test Loss: 1.2606\n",
      "Epoch 50/76, Train Loss: 0.9917, Test Loss: 1.2652\n",
      "Epoch 51/76, Train Loss: 1.0104, Test Loss: 1.2523\n",
      "Epoch 52/76, Train Loss: 0.9975, Test Loss: 1.2569\n",
      "Epoch 53/76, Train Loss: 0.9995, Test Loss: 1.2403\n",
      "Epoch 54/76, Train Loss: 0.9997, Test Loss: 1.2344\n",
      "Epoch 55/76, Train Loss: 1.0060, Test Loss: 1.2379\n",
      "Epoch 56/76, Train Loss: 0.9880, Test Loss: 1.2428\n",
      "Epoch 57/76, Train Loss: 0.9715, Test Loss: 1.2432\n",
      "Epoch 58/76, Train Loss: 0.9935, Test Loss: 1.2374\n",
      "Epoch 59/76, Train Loss: 0.9924, Test Loss: 1.2388\n",
      "Epoch 60/76, Train Loss: 1.0132, Test Loss: 1.2497\n",
      "Epoch 61/76, Train Loss: 1.0131, Test Loss: 1.2375\n",
      "Epoch 62/76, Train Loss: 0.9920, Test Loss: 1.2354\n",
      "Epoch 63/76, Train Loss: 1.0061, Test Loss: 1.2584\n",
      "Epoch 64/76, Train Loss: 1.0326, Test Loss: 1.2556\n",
      "Epoch 65/76, Train Loss: 0.9699, Test Loss: 1.2306\n",
      "Epoch 66/76, Train Loss: 0.9432, Test Loss: 1.2410\n",
      "Epoch 67/76, Train Loss: 1.0082, Test Loss: 1.2433\n",
      "Epoch 68/76, Train Loss: 0.9896, Test Loss: 1.2453\n",
      "Epoch 69/76, Train Loss: 0.9553, Test Loss: 1.2477\n",
      "Epoch 70/76, Train Loss: 0.9687, Test Loss: 1.2436\n",
      "Epoch 71/76, Train Loss: 0.9744, Test Loss: 1.2488\n",
      "Epoch 72/76, Train Loss: 0.9956, Test Loss: 1.2340\n",
      "Epoch 73/76, Train Loss: 0.9717, Test Loss: 1.2430\n",
      "Epoch 74/76, Train Loss: 0.9687, Test Loss: 1.2369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:28:15,499] Trial 104 finished with value: 1.243070051074028 and parameters: {'num_hidden_layers': 9, 'layer_0_size': 99, 'layer_1_size': 183, 'layer_2_size': 199, 'layer_3_size': 70, 'layer_4_size': 91, 'layer_5_size': 222, 'layer_6_size': 169, 'layer_7_size': 153, 'layer_8_size': 214, 'dropout_rate': 0.11586469625574417, 'learning_rate': 7.00873189329955e-05, 'batch_size': 64, 'epochs': 76}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/76, Train Loss: 0.9920, Test Loss: 1.2327\n",
      "Epoch 76/76, Train Loss: 0.9936, Test Loss: 1.2431\n",
      "Epoch 1/80, Train Loss: 1.2490, Test Loss: 1.2365\n",
      "Epoch 2/80, Train Loss: 1.2668, Test Loss: 1.2454\n",
      "Epoch 3/80, Train Loss: 1.2251, Test Loss: 1.2536\n",
      "Epoch 4/80, Train Loss: 1.2319, Test Loss: 1.2452\n",
      "Epoch 5/80, Train Loss: 1.2599, Test Loss: 1.2411\n",
      "Epoch 6/80, Train Loss: 1.2076, Test Loss: 1.2421\n",
      "Epoch 7/80, Train Loss: 1.2145, Test Loss: 1.2375\n",
      "Epoch 8/80, Train Loss: 1.1862, Test Loss: 1.2301\n",
      "Epoch 9/80, Train Loss: 1.2126, Test Loss: 1.2301\n",
      "Epoch 10/80, Train Loss: 1.1996, Test Loss: 1.2317\n",
      "Epoch 11/80, Train Loss: 1.2043, Test Loss: 1.2325\n",
      "Epoch 12/80, Train Loss: 1.2521, Test Loss: 1.2266\n",
      "Epoch 13/80, Train Loss: 1.1947, Test Loss: 1.2338\n",
      "Epoch 14/80, Train Loss: 1.1789, Test Loss: 1.2319\n",
      "Epoch 15/80, Train Loss: 1.1699, Test Loss: 1.2303\n",
      "Epoch 16/80, Train Loss: 1.1446, Test Loss: 1.2273\n",
      "Epoch 17/80, Train Loss: 1.1657, Test Loss: 1.2271\n",
      "Epoch 18/80, Train Loss: 1.1309, Test Loss: 1.2314\n",
      "Epoch 19/80, Train Loss: 1.1409, Test Loss: 1.2292\n",
      "Epoch 20/80, Train Loss: 1.1489, Test Loss: 1.2296\n",
      "Epoch 21/80, Train Loss: 1.1651, Test Loss: 1.2270\n",
      "Epoch 22/80, Train Loss: 1.1735, Test Loss: 1.2316\n",
      "Epoch 23/80, Train Loss: 1.1779, Test Loss: 1.2309\n",
      "Epoch 24/80, Train Loss: 1.1517, Test Loss: 1.2286\n",
      "Epoch 25/80, Train Loss: 1.1504, Test Loss: 1.2256\n",
      "Epoch 26/80, Train Loss: 1.1804, Test Loss: 1.2203\n",
      "Epoch 27/80, Train Loss: 1.1343, Test Loss: 1.2220\n",
      "Epoch 28/80, Train Loss: 1.1776, Test Loss: 1.2219\n",
      "Epoch 29/80, Train Loss: 1.1489, Test Loss: 1.2233\n",
      "Epoch 30/80, Train Loss: 1.1778, Test Loss: 1.2242\n",
      "Epoch 31/80, Train Loss: 1.1967, Test Loss: 1.2302\n",
      "Epoch 32/80, Train Loss: 1.1367, Test Loss: 1.2269\n",
      "Epoch 33/80, Train Loss: 1.1411, Test Loss: 1.2242\n",
      "Epoch 34/80, Train Loss: 1.1448, Test Loss: 1.2262\n",
      "Epoch 35/80, Train Loss: 1.1270, Test Loss: 1.2249\n",
      "Epoch 36/80, Train Loss: 1.1732, Test Loss: 1.2222\n",
      "Epoch 37/80, Train Loss: 1.1137, Test Loss: 1.2286\n",
      "Epoch 38/80, Train Loss: 1.1490, Test Loss: 1.2237\n",
      "Epoch 39/80, Train Loss: 1.1387, Test Loss: 1.2247\n",
      "Epoch 40/80, Train Loss: 1.1475, Test Loss: 1.2286\n",
      "Epoch 41/80, Train Loss: 1.1189, Test Loss: 1.2287\n",
      "Epoch 42/80, Train Loss: 1.1202, Test Loss: 1.2254\n",
      "Epoch 43/80, Train Loss: 1.1378, Test Loss: 1.2267\n",
      "Epoch 44/80, Train Loss: 1.1153, Test Loss: 1.2295\n",
      "Epoch 45/80, Train Loss: 1.1087, Test Loss: 1.2271\n",
      "Epoch 46/80, Train Loss: 1.1629, Test Loss: 1.2290\n",
      "Epoch 47/80, Train Loss: 1.1389, Test Loss: 1.2211\n",
      "Epoch 48/80, Train Loss: 1.1281, Test Loss: 1.2203\n",
      "Epoch 49/80, Train Loss: 1.1445, Test Loss: 1.2239\n",
      "Epoch 50/80, Train Loss: 1.1538, Test Loss: 1.2247\n",
      "Epoch 51/80, Train Loss: 1.1314, Test Loss: 1.2228\n",
      "Epoch 52/80, Train Loss: 1.1009, Test Loss: 1.2210\n",
      "Epoch 53/80, Train Loss: 1.0909, Test Loss: 1.2227\n",
      "Epoch 54/80, Train Loss: 1.1234, Test Loss: 1.2225\n",
      "Epoch 55/80, Train Loss: 1.1146, Test Loss: 1.2225\n",
      "Epoch 56/80, Train Loss: 1.1201, Test Loss: 1.2230\n",
      "Epoch 57/80, Train Loss: 1.1305, Test Loss: 1.2226\n",
      "Epoch 58/80, Train Loss: 1.1413, Test Loss: 1.2244\n",
      "Epoch 59/80, Train Loss: 1.1198, Test Loss: 1.2208\n",
      "Epoch 60/80, Train Loss: 1.1624, Test Loss: 1.2216\n",
      "Epoch 61/80, Train Loss: 1.0966, Test Loss: 1.2225\n",
      "Epoch 62/80, Train Loss: 1.0750, Test Loss: 1.2205\n",
      "Epoch 63/80, Train Loss: 1.1255, Test Loss: 1.2217\n",
      "Epoch 64/80, Train Loss: 1.1191, Test Loss: 1.2236\n",
      "Epoch 65/80, Train Loss: 1.1177, Test Loss: 1.2230\n",
      "Epoch 66/80, Train Loss: 1.0794, Test Loss: 1.2235\n",
      "Epoch 67/80, Train Loss: 1.0933, Test Loss: 1.2196\n",
      "Epoch 68/80, Train Loss: 1.1123, Test Loss: 1.2231\n",
      "Epoch 69/80, Train Loss: 1.0881, Test Loss: 1.2215\n",
      "Epoch 70/80, Train Loss: 1.0614, Test Loss: 1.2205\n",
      "Epoch 71/80, Train Loss: 1.0892, Test Loss: 1.2198\n",
      "Epoch 72/80, Train Loss: 1.1369, Test Loss: 1.2227\n",
      "Epoch 73/80, Train Loss: 1.1025, Test Loss: 1.2214\n",
      "Epoch 74/80, Train Loss: 1.1151, Test Loss: 1.2204\n",
      "Epoch 75/80, Train Loss: 1.1023, Test Loss: 1.2230\n",
      "Epoch 76/80, Train Loss: 1.0931, Test Loss: 1.2210\n",
      "Epoch 77/80, Train Loss: 1.0791, Test Loss: 1.2224\n",
      "Epoch 78/80, Train Loss: 1.1025, Test Loss: 1.2196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:28:23,033] Trial 105 finished with value: 1.222546100616455 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 110, 'layer_1_size': 201, 'layer_2_size': 225, 'layer_3_size': 105, 'layer_4_size': 180, 'layer_5_size': 229, 'layer_6_size': 151, 'layer_7_size': 81, 'dropout_rate': 0.4934486291986262, 'learning_rate': 4.706468665604014e-05, 'batch_size': 64, 'epochs': 80}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/80, Train Loss: 1.1121, Test Loss: 1.2219\n",
      "Epoch 80/80, Train Loss: 1.1177, Test Loss: 1.2225\n",
      "Epoch 1/15, Train Loss: 1.4128, Test Loss: 0.7535\n",
      "Epoch 2/15, Train Loss: 1.2705, Test Loss: 0.7467\n",
      "Epoch 3/15, Train Loss: 1.2889, Test Loss: 0.7447\n",
      "Epoch 4/15, Train Loss: 1.2503, Test Loss: 0.7452\n",
      "Epoch 5/15, Train Loss: 1.2940, Test Loss: 0.7466\n",
      "Epoch 6/15, Train Loss: 1.2311, Test Loss: 0.7453\n",
      "Epoch 7/15, Train Loss: 1.1521, Test Loss: 0.7483\n",
      "Epoch 8/15, Train Loss: 1.1626, Test Loss: 0.7517\n",
      "Epoch 9/15, Train Loss: 1.2264, Test Loss: 0.7507\n",
      "Epoch 10/15, Train Loss: 1.1907, Test Loss: 0.7639\n",
      "Epoch 11/15, Train Loss: 1.1484, Test Loss: 0.7660\n",
      "Epoch 12/15, Train Loss: 1.1834, Test Loss: 0.7711\n",
      "Epoch 13/15, Train Loss: 1.1573, Test Loss: 0.7734\n",
      "Epoch 14/15, Train Loss: 1.1319, Test Loss: 0.7721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:28:24,517] Trial 106 finished with value: 0.7733706086874008 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 124, 'layer_1_size': 197, 'layer_2_size': 211, 'layer_3_size': 38, 'layer_4_size': 105, 'layer_5_size': 215, 'layer_6_size': 142, 'dropout_rate': 0.47998043328525886, 'learning_rate': 0.00016536915300319427, 'batch_size': 64, 'epochs': 15}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15, Train Loss: 1.1764, Test Loss: 0.7734\n",
      "Epoch 1/14, Train Loss: 1.3683, Test Loss: 0.9842\n",
      "Epoch 2/14, Train Loss: 1.3323, Test Loss: 0.9867\n",
      "Epoch 3/14, Train Loss: 1.2691, Test Loss: 0.9844\n",
      "Epoch 4/14, Train Loss: 1.2382, Test Loss: 0.9820\n",
      "Epoch 5/14, Train Loss: 1.2478, Test Loss: 0.9822\n",
      "Epoch 6/14, Train Loss: 1.2639, Test Loss: 0.9799\n",
      "Epoch 7/14, Train Loss: 1.2273, Test Loss: 0.9799\n",
      "Epoch 8/14, Train Loss: 1.2763, Test Loss: 0.9811\n",
      "Epoch 9/14, Train Loss: 1.2570, Test Loss: 0.9839\n",
      "Epoch 10/14, Train Loss: 1.2139, Test Loss: 0.9817\n",
      "Epoch 11/14, Train Loss: 1.2357, Test Loss: 0.9831\n",
      "Epoch 12/14, Train Loss: 1.2011, Test Loss: 0.9838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:28:25,212] Trial 107 finished with value: 0.9834890961647034 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 122, 'layer_1_size': 154, 'layer_2_size': 210, 'layer_3_size': 33, 'layer_4_size': 83, 'layer_5_size': 214, 'dropout_rate': 0.4591315213367293, 'learning_rate': 0.00027322546718967647, 'batch_size': 128, 'epochs': 14}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/14, Train Loss: 1.2171, Test Loss: 0.9837\n",
      "Epoch 14/14, Train Loss: 1.2113, Test Loss: 0.9835\n",
      "Epoch 1/7, Train Loss: 1.3579, Test Loss: 0.9874\n",
      "Epoch 2/7, Train Loss: 1.3054, Test Loss: 0.9955\n",
      "Epoch 3/7, Train Loss: 1.2455, Test Loss: 0.9720\n",
      "Epoch 4/7, Train Loss: 1.1560, Test Loss: 0.9724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:28:25,931] Trial 108 finished with value: 0.9748131632804871 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 130, 'layer_1_size': 197, 'layer_2_size': 214, 'layer_3_size': 42, 'layer_4_size': 102, 'layer_5_size': 204, 'layer_6_size': 141, 'dropout_rate': 0.3848021812752664, 'learning_rate': 0.00016874259414411156, 'batch_size': 64, 'epochs': 7}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/7, Train Loss: 1.1239, Test Loss: 0.9794\n",
      "Epoch 6/7, Train Loss: 1.1481, Test Loss: 0.9755\n",
      "Epoch 7/7, Train Loss: 1.1414, Test Loss: 0.9748\n",
      "Epoch 1/12, Train Loss: 1.2938, Test Loss: 0.9209\n",
      "Epoch 2/12, Train Loss: 1.2235, Test Loss: 0.9381\n",
      "Epoch 3/12, Train Loss: 1.1651, Test Loss: 0.9257\n",
      "Epoch 4/12, Train Loss: 1.2268, Test Loss: 0.9264\n",
      "Epoch 5/12, Train Loss: 1.1367, Test Loss: 0.9187\n",
      "Epoch 6/12, Train Loss: 1.1640, Test Loss: 0.9164\n",
      "Epoch 7/12, Train Loss: 1.1920, Test Loss: 0.9082\n",
      "Epoch 8/12, Train Loss: 1.1694, Test Loss: 0.9045\n",
      "Epoch 9/12, Train Loss: 1.1560, Test Loss: 0.9039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:28:27,362] Trial 109 finished with value: 0.9066727310419083 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 153, 'layer_1_size': 173, 'layer_2_size': 191, 'layer_3_size': 91, 'layer_4_size': 108, 'layer_5_size': 104, 'layer_6_size': 127, 'layer_7_size': 189, 'dropout_rate': 0.4675249015826623, 'learning_rate': 0.00014150029312092396, 'batch_size': 64, 'epochs': 12}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/12, Train Loss: 1.1677, Test Loss: 0.9055\n",
      "Epoch 11/12, Train Loss: 1.1671, Test Loss: 0.9059\n",
      "Epoch 12/12, Train Loss: 1.1169, Test Loss: 0.9067\n",
      "Epoch 1/6, Train Loss: 1.3116, Test Loss: 0.8617\n",
      "Epoch 2/6, Train Loss: 1.2547, Test Loss: 0.8666\n",
      "Epoch 3/6, Train Loss: 1.2521, Test Loss: 0.8635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:28:28,037] Trial 110 finished with value: 0.8582903146743774 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 136, 'layer_1_size': 160, 'layer_2_size': 239, 'layer_3_size': 242, 'layer_4_size': 91, 'layer_5_size': 251, 'layer_6_size': 178, 'dropout_rate': 0.36059248013045825, 'learning_rate': 0.00030188548499995686, 'batch_size': 64, 'epochs': 6}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/6, Train Loss: 1.2470, Test Loss: 0.8610\n",
      "Epoch 5/6, Train Loss: 1.2278, Test Loss: 0.8632\n",
      "Epoch 6/6, Train Loss: 1.2174, Test Loss: 0.8583\n",
      "Epoch 1/98, Train Loss: 1.2902, Test Loss: 0.9892\n",
      "Epoch 2/98, Train Loss: 1.2868, Test Loss: 0.9986\n",
      "Epoch 3/98, Train Loss: 1.2259, Test Loss: 0.9991\n",
      "Epoch 4/98, Train Loss: 1.2720, Test Loss: 1.0025\n",
      "Epoch 5/98, Train Loss: 1.1506, Test Loss: 1.0070\n",
      "Epoch 6/98, Train Loss: 1.1586, Test Loss: 1.0035\n",
      "Epoch 7/98, Train Loss: 1.1716, Test Loss: 1.0049\n",
      "Epoch 8/98, Train Loss: 1.1680, Test Loss: 0.9955\n",
      "Epoch 9/98, Train Loss: 1.1904, Test Loss: 0.9901\n",
      "Epoch 10/98, Train Loss: 1.1232, Test Loss: 0.9881\n",
      "Epoch 11/98, Train Loss: 1.1435, Test Loss: 0.9871\n",
      "Epoch 12/98, Train Loss: 1.1525, Test Loss: 0.9846\n",
      "Epoch 13/98, Train Loss: 1.1849, Test Loss: 0.9897\n",
      "Epoch 14/98, Train Loss: 1.1226, Test Loss: 0.9902\n",
      "Epoch 15/98, Train Loss: 1.1224, Test Loss: 0.9866\n",
      "Epoch 16/98, Train Loss: 1.1494, Test Loss: 0.9844\n",
      "Epoch 17/98, Train Loss: 1.1247, Test Loss: 0.9860\n",
      "Epoch 18/98, Train Loss: 1.1584, Test Loss: 0.9858\n",
      "Epoch 19/98, Train Loss: 1.1461, Test Loss: 0.9869\n",
      "Epoch 20/98, Train Loss: 1.1111, Test Loss: 0.9844\n",
      "Epoch 21/98, Train Loss: 1.1246, Test Loss: 0.9865\n",
      "Epoch 22/98, Train Loss: 1.1608, Test Loss: 0.9867\n",
      "Epoch 23/98, Train Loss: 1.1125, Test Loss: 0.9918\n",
      "Epoch 24/98, Train Loss: 1.1779, Test Loss: 0.9875\n",
      "Epoch 25/98, Train Loss: 1.1348, Test Loss: 0.9862\n",
      "Epoch 26/98, Train Loss: 1.1040, Test Loss: 0.9855\n",
      "Epoch 27/98, Train Loss: 1.1161, Test Loss: 0.9844\n",
      "Epoch 28/98, Train Loss: 1.0975, Test Loss: 0.9825\n",
      "Epoch 29/98, Train Loss: 1.1074, Test Loss: 0.9846\n",
      "Epoch 30/98, Train Loss: 1.1175, Test Loss: 0.9840\n",
      "Epoch 31/98, Train Loss: 1.1143, Test Loss: 0.9820\n",
      "Epoch 32/98, Train Loss: 1.0848, Test Loss: 0.9826\n",
      "Epoch 33/98, Train Loss: 1.0900, Test Loss: 0.9832\n",
      "Epoch 34/98, Train Loss: 1.0828, Test Loss: 0.9870\n",
      "Epoch 35/98, Train Loss: 1.1126, Test Loss: 0.9846\n",
      "Epoch 36/98, Train Loss: 1.1196, Test Loss: 0.9840\n",
      "Epoch 37/98, Train Loss: 1.0977, Test Loss: 0.9844\n",
      "Epoch 38/98, Train Loss: 1.1088, Test Loss: 0.9853\n",
      "Epoch 39/98, Train Loss: 1.1076, Test Loss: 0.9851\n",
      "Epoch 40/98, Train Loss: 1.0847, Test Loss: 0.9853\n",
      "Epoch 41/98, Train Loss: 1.1036, Test Loss: 0.9850\n",
      "Epoch 42/98, Train Loss: 1.1096, Test Loss: 0.9859\n",
      "Epoch 43/98, Train Loss: 1.0892, Test Loss: 0.9818\n",
      "Epoch 44/98, Train Loss: 1.0675, Test Loss: 0.9811\n",
      "Epoch 45/98, Train Loss: 1.0485, Test Loss: 0.9834\n",
      "Epoch 46/98, Train Loss: 1.0868, Test Loss: 0.9827\n",
      "Epoch 47/98, Train Loss: 1.0911, Test Loss: 0.9805\n",
      "Epoch 48/98, Train Loss: 1.0892, Test Loss: 0.9803\n",
      "Epoch 49/98, Train Loss: 1.0988, Test Loss: 0.9798\n",
      "Epoch 50/98, Train Loss: 1.0762, Test Loss: 0.9808\n",
      "Epoch 51/98, Train Loss: 1.0876, Test Loss: 0.9814\n",
      "Epoch 52/98, Train Loss: 1.0715, Test Loss: 0.9821\n",
      "Epoch 53/98, Train Loss: 1.0762, Test Loss: 0.9813\n",
      "Epoch 54/98, Train Loss: 1.0975, Test Loss: 0.9806\n",
      "Epoch 55/98, Train Loss: 1.0674, Test Loss: 0.9801\n",
      "Epoch 56/98, Train Loss: 1.0733, Test Loss: 0.9803\n",
      "Epoch 57/98, Train Loss: 1.0785, Test Loss: 0.9819\n",
      "Epoch 58/98, Train Loss: 1.0698, Test Loss: 0.9813\n",
      "Epoch 59/98, Train Loss: 1.0613, Test Loss: 0.9803\n",
      "Epoch 60/98, Train Loss: 1.0490, Test Loss: 0.9802\n",
      "Epoch 61/98, Train Loss: 1.0951, Test Loss: 0.9802\n",
      "Epoch 62/98, Train Loss: 1.0533, Test Loss: 0.9796\n",
      "Epoch 63/98, Train Loss: 1.0642, Test Loss: 0.9821\n",
      "Epoch 64/98, Train Loss: 1.0664, Test Loss: 0.9817\n",
      "Epoch 65/98, Train Loss: 1.0783, Test Loss: 0.9833\n",
      "Epoch 66/98, Train Loss: 1.0734, Test Loss: 0.9844\n",
      "Epoch 67/98, Train Loss: 1.0464, Test Loss: 0.9833\n",
      "Epoch 68/98, Train Loss: 1.0757, Test Loss: 0.9824\n",
      "Epoch 69/98, Train Loss: 1.0394, Test Loss: 0.9804\n",
      "Epoch 70/98, Train Loss: 1.0701, Test Loss: 0.9805\n",
      "Epoch 71/98, Train Loss: 1.0725, Test Loss: 0.9823\n",
      "Epoch 72/98, Train Loss: 1.0631, Test Loss: 0.9824\n",
      "Epoch 73/98, Train Loss: 1.0340, Test Loss: 0.9807\n",
      "Epoch 74/98, Train Loss: 1.0536, Test Loss: 0.9806\n",
      "Epoch 75/98, Train Loss: 1.0436, Test Loss: 0.9807\n",
      "Epoch 76/98, Train Loss: 1.0672, Test Loss: 0.9813\n",
      "Epoch 77/98, Train Loss: 1.0440, Test Loss: 0.9808\n",
      "Epoch 78/98, Train Loss: 1.0644, Test Loss: 0.9830\n",
      "Epoch 79/98, Train Loss: 1.0469, Test Loss: 0.9852\n",
      "Epoch 80/98, Train Loss: 1.0656, Test Loss: 0.9883\n",
      "Epoch 81/98, Train Loss: 1.0566, Test Loss: 0.9863\n",
      "Epoch 82/98, Train Loss: 1.0645, Test Loss: 0.9857\n",
      "Epoch 83/98, Train Loss: 1.0590, Test Loss: 0.9848\n",
      "Epoch 84/98, Train Loss: 1.0507, Test Loss: 0.9829\n",
      "Epoch 85/98, Train Loss: 1.0637, Test Loss: 0.9834\n",
      "Epoch 86/98, Train Loss: 1.0563, Test Loss: 0.9811\n",
      "Epoch 87/98, Train Loss: 1.0503, Test Loss: 0.9813\n",
      "Epoch 88/98, Train Loss: 1.0632, Test Loss: 0.9801\n",
      "Epoch 89/98, Train Loss: 1.0427, Test Loss: 0.9796\n",
      "Epoch 90/98, Train Loss: 1.0557, Test Loss: 0.9797\n",
      "Epoch 91/98, Train Loss: 1.0571, Test Loss: 0.9789\n",
      "Epoch 92/98, Train Loss: 1.0433, Test Loss: 0.9793\n",
      "Epoch 93/98, Train Loss: 1.0499, Test Loss: 0.9789\n",
      "Epoch 94/98, Train Loss: 1.0539, Test Loss: 0.9780\n",
      "Epoch 95/98, Train Loss: 1.0672, Test Loss: 0.9786\n",
      "Epoch 96/98, Train Loss: 1.0565, Test Loss: 0.9793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:28:36,171] Trial 111 finished with value: 0.9810013920068741 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 113, 'layer_1_size': 184, 'layer_2_size': 207, 'layer_3_size': 38, 'layer_4_size': 115, 'layer_5_size': 220, 'layer_6_size': 159, 'dropout_rate': 0.4806205585029344, 'learning_rate': 0.00019826240107957035, 'batch_size': 64, 'epochs': 98}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/98, Train Loss: 1.0654, Test Loss: 0.9788\n",
      "Epoch 98/98, Train Loss: 1.0421, Test Loss: 0.9810\n",
      "Epoch 1/9, Train Loss: 1.3863, Test Loss: 1.2347\n",
      "Epoch 2/9, Train Loss: 1.3372, Test Loss: 1.2352\n",
      "Epoch 3/9, Train Loss: 1.3429, Test Loss: 1.2370\n",
      "Epoch 4/9, Train Loss: 1.2951, Test Loss: 1.2326\n",
      "Epoch 5/9, Train Loss: 1.3819, Test Loss: 1.2344\n",
      "Epoch 6/9, Train Loss: 1.2705, Test Loss: 1.2505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:28:36,965] Trial 112 finished with value: 1.2665846794843674 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 105, 'layer_1_size': 166, 'layer_2_size': 221, 'layer_3_size': 51, 'layer_4_size': 196, 'layer_5_size': 157, 'layer_6_size': 153, 'dropout_rate': 0.49942861330732496, 'learning_rate': 8.455305418538505e-05, 'batch_size': 64, 'epochs': 9}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/9, Train Loss: 1.2871, Test Loss: 1.2594\n",
      "Epoch 8/9, Train Loss: 1.3133, Test Loss: 1.2631\n",
      "Epoch 9/9, Train Loss: 1.3028, Test Loss: 1.2666\n",
      "Epoch 1/73, Train Loss: 1.1922, Test Loss: 0.9788\n",
      "Epoch 2/73, Train Loss: 1.1309, Test Loss: 0.9907\n",
      "Epoch 3/73, Train Loss: 1.1858, Test Loss: 0.9897\n",
      "Epoch 4/73, Train Loss: 1.1339, Test Loss: 0.9861\n",
      "Epoch 5/73, Train Loss: 1.1447, Test Loss: 0.9796\n",
      "Epoch 6/73, Train Loss: 1.1247, Test Loss: 0.9821\n",
      "Epoch 7/73, Train Loss: 1.1482, Test Loss: 0.9881\n",
      "Epoch 8/73, Train Loss: 1.1126, Test Loss: 0.9903\n",
      "Epoch 9/73, Train Loss: 1.0980, Test Loss: 0.9878\n",
      "Epoch 10/73, Train Loss: 1.1311, Test Loss: 0.9856\n",
      "Epoch 11/73, Train Loss: 1.1078, Test Loss: 0.9772\n",
      "Epoch 12/73, Train Loss: 1.0798, Test Loss: 0.9771\n",
      "Epoch 13/73, Train Loss: 1.1206, Test Loss: 0.9770\n",
      "Epoch 14/73, Train Loss: 1.0781, Test Loss: 0.9735\n",
      "Epoch 15/73, Train Loss: 1.0978, Test Loss: 0.9731\n",
      "Epoch 16/73, Train Loss: 1.1166, Test Loss: 0.9742\n",
      "Epoch 17/73, Train Loss: 1.0713, Test Loss: 0.9760\n",
      "Epoch 18/73, Train Loss: 1.1157, Test Loss: 0.9790\n",
      "Epoch 19/73, Train Loss: 1.1323, Test Loss: 0.9852\n",
      "Epoch 20/73, Train Loss: 1.0654, Test Loss: 0.9807\n",
      "Epoch 21/73, Train Loss: 1.0887, Test Loss: 0.9788\n",
      "Epoch 22/73, Train Loss: 1.0891, Test Loss: 0.9783\n",
      "Epoch 23/73, Train Loss: 1.0696, Test Loss: 0.9768\n",
      "Epoch 24/73, Train Loss: 1.1106, Test Loss: 0.9778\n",
      "Epoch 25/73, Train Loss: 1.0752, Test Loss: 0.9776\n",
      "Epoch 26/73, Train Loss: 1.0691, Test Loss: 0.9830\n",
      "Epoch 27/73, Train Loss: 1.1097, Test Loss: 0.9837\n",
      "Epoch 28/73, Train Loss: 1.0722, Test Loss: 0.9811\n",
      "Epoch 29/73, Train Loss: 1.0308, Test Loss: 0.9799\n",
      "Epoch 30/73, Train Loss: 1.1077, Test Loss: 0.9832\n",
      "Epoch 31/73, Train Loss: 1.0710, Test Loss: 0.9804\n",
      "Epoch 32/73, Train Loss: 1.0696, Test Loss: 0.9805\n",
      "Epoch 33/73, Train Loss: 1.0509, Test Loss: 0.9831\n",
      "Epoch 34/73, Train Loss: 1.0657, Test Loss: 0.9835\n",
      "Epoch 35/73, Train Loss: 1.0391, Test Loss: 0.9836\n",
      "Epoch 36/73, Train Loss: 1.0763, Test Loss: 0.9781\n",
      "Epoch 37/73, Train Loss: 1.0824, Test Loss: 0.9774\n",
      "Epoch 38/73, Train Loss: 1.0840, Test Loss: 0.9755\n",
      "Epoch 39/73, Train Loss: 1.0789, Test Loss: 0.9770\n",
      "Epoch 40/73, Train Loss: 1.0399, Test Loss: 0.9777\n",
      "Epoch 41/73, Train Loss: 1.0863, Test Loss: 0.9767\n",
      "Epoch 42/73, Train Loss: 1.0520, Test Loss: 0.9775\n",
      "Epoch 43/73, Train Loss: 1.0469, Test Loss: 0.9770\n",
      "Epoch 44/73, Train Loss: 1.1010, Test Loss: 0.9761\n",
      "Epoch 45/73, Train Loss: 1.0681, Test Loss: 0.9754\n",
      "Epoch 46/73, Train Loss: 1.0033, Test Loss: 0.9724\n",
      "Epoch 47/73, Train Loss: 1.0677, Test Loss: 0.9705\n",
      "Epoch 48/73, Train Loss: 1.0494, Test Loss: 0.9712\n",
      "Epoch 49/73, Train Loss: 1.0498, Test Loss: 0.9730\n",
      "Epoch 50/73, Train Loss: 1.0317, Test Loss: 0.9726\n",
      "Epoch 51/73, Train Loss: 1.0621, Test Loss: 0.9742\n",
      "Epoch 52/73, Train Loss: 1.0430, Test Loss: 0.9725\n",
      "Epoch 53/73, Train Loss: 1.0310, Test Loss: 0.9752\n",
      "Epoch 54/73, Train Loss: 1.0457, Test Loss: 0.9749\n",
      "Epoch 55/73, Train Loss: 1.0467, Test Loss: 0.9766\n",
      "Epoch 56/73, Train Loss: 1.0217, Test Loss: 0.9761\n",
      "Epoch 57/73, Train Loss: 1.0242, Test Loss: 0.9754\n",
      "Epoch 58/73, Train Loss: 1.0278, Test Loss: 0.9751\n",
      "Epoch 59/73, Train Loss: 1.0609, Test Loss: 0.9745\n",
      "Epoch 60/73, Train Loss: 1.0419, Test Loss: 0.9749\n",
      "Epoch 61/73, Train Loss: 1.0612, Test Loss: 0.9719\n",
      "Epoch 62/73, Train Loss: 1.0546, Test Loss: 0.9726\n",
      "Epoch 63/73, Train Loss: 1.0356, Test Loss: 0.9719\n",
      "Epoch 64/73, Train Loss: 1.0470, Test Loss: 0.9713\n",
      "Epoch 65/73, Train Loss: 1.0340, Test Loss: 0.9730\n",
      "Epoch 66/73, Train Loss: 1.0240, Test Loss: 0.9736\n",
      "Epoch 67/73, Train Loss: 1.0576, Test Loss: 0.9711\n",
      "Epoch 68/73, Train Loss: 1.0480, Test Loss: 0.9716\n",
      "Epoch 69/73, Train Loss: 1.0517, Test Loss: 0.9700\n",
      "Epoch 70/73, Train Loss: 1.0370, Test Loss: 0.9702\n",
      "Epoch 71/73, Train Loss: 1.0122, Test Loss: 0.9720\n",
      "Epoch 72/73, Train Loss: 1.0470, Test Loss: 0.9717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:28:41,961] Trial 113 finished with value: 0.9725970327854156 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 126, 'layer_1_size': 195, 'layer_2_size': 197, 'layer_3_size': 67, 'layer_4_size': 123, 'layer_5_size': 234, 'dropout_rate': 0.34312452440473323, 'learning_rate': 0.0001262140684535398, 'batch_size': 64, 'epochs': 73}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/73, Train Loss: 1.0241, Test Loss: 0.9726\n",
      "Epoch 1/28, Train Loss: 1.2956, Test Loss: 1.1141\n",
      "Epoch 2/28, Train Loss: 1.2287, Test Loss: 1.1181\n",
      "Epoch 3/28, Train Loss: 1.2687, Test Loss: 1.1133\n",
      "Epoch 4/28, Train Loss: 1.2663, Test Loss: 1.1195\n",
      "Epoch 5/28, Train Loss: 1.2634, Test Loss: 1.1275\n",
      "Epoch 6/28, Train Loss: 1.1414, Test Loss: 1.1199\n",
      "Epoch 7/28, Train Loss: 1.1513, Test Loss: 1.1115\n",
      "Epoch 8/28, Train Loss: 1.2067, Test Loss: 1.1162\n",
      "Epoch 9/28, Train Loss: 1.1708, Test Loss: 1.1206\n",
      "Epoch 10/28, Train Loss: 1.1933, Test Loss: 1.1244\n",
      "Epoch 11/28, Train Loss: 1.1565, Test Loss: 1.1139\n",
      "Epoch 12/28, Train Loss: 1.1773, Test Loss: 1.1206\n",
      "Epoch 13/28, Train Loss: 1.1527, Test Loss: 1.1165\n",
      "Epoch 14/28, Train Loss: 1.1181, Test Loss: 1.1172\n",
      "Epoch 15/28, Train Loss: 1.1716, Test Loss: 1.1207\n",
      "Epoch 16/28, Train Loss: 1.1159, Test Loss: 1.1177\n",
      "Epoch 17/28, Train Loss: 1.1228, Test Loss: 1.1204\n",
      "Epoch 18/28, Train Loss: 1.1280, Test Loss: 1.1248\n",
      "Epoch 19/28, Train Loss: 1.1524, Test Loss: 1.1220\n",
      "Epoch 20/28, Train Loss: 1.1022, Test Loss: 1.1263\n",
      "Epoch 21/28, Train Loss: 1.0883, Test Loss: 1.1227\n",
      "Epoch 22/28, Train Loss: 1.1065, Test Loss: 1.1160\n",
      "Epoch 23/28, Train Loss: 1.1664, Test Loss: 1.1140\n",
      "Epoch 24/28, Train Loss: 1.0925, Test Loss: 1.1153\n",
      "Epoch 25/28, Train Loss: 1.1010, Test Loss: 1.1093\n",
      "Epoch 26/28, Train Loss: 1.1187, Test Loss: 1.1086\n",
      "Epoch 27/28, Train Loss: 1.1340, Test Loss: 1.1087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:28:46,687] Trial 114 finished with value: 1.1082955258233207 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 147, 'layer_1_size': 211, 'layer_2_size': 203, 'layer_3_size': 45, 'layer_4_size': 209, 'layer_5_size': 225, 'layer_6_size': 171, 'dropout_rate': 0.44848476518491603, 'learning_rate': 0.00011232430707417885, 'batch_size': 32, 'epochs': 28}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/28, Train Loss: 1.1184, Test Loss: 1.1083\n",
      "Epoch 1/100, Train Loss: 1.3346, Test Loss: 1.0060\n",
      "Epoch 2/100, Train Loss: 1.2453, Test Loss: 1.0148\n",
      "Epoch 3/100, Train Loss: 1.2180, Test Loss: 1.0053\n",
      "Epoch 4/100, Train Loss: 1.2269, Test Loss: 0.9921\n",
      "Epoch 5/100, Train Loss: 1.2420, Test Loss: 0.9912\n",
      "Epoch 6/100, Train Loss: 1.2423, Test Loss: 0.9840\n",
      "Epoch 7/100, Train Loss: 1.1872, Test Loss: 0.9735\n",
      "Epoch 8/100, Train Loss: 1.2846, Test Loss: 0.9794\n",
      "Epoch 9/100, Train Loss: 1.1698, Test Loss: 0.9770\n",
      "Epoch 10/100, Train Loss: 1.1971, Test Loss: 0.9783\n",
      "Epoch 11/100, Train Loss: 1.2181, Test Loss: 0.9784\n",
      "Epoch 12/100, Train Loss: 1.1660, Test Loss: 0.9739\n",
      "Epoch 13/100, Train Loss: 1.1724, Test Loss: 0.9757\n",
      "Epoch 14/100, Train Loss: 1.1806, Test Loss: 0.9765\n",
      "Epoch 15/100, Train Loss: 1.1910, Test Loss: 0.9788\n",
      "Epoch 16/100, Train Loss: 1.1687, Test Loss: 0.9738\n",
      "Epoch 17/100, Train Loss: 1.2277, Test Loss: 0.9716\n",
      "Epoch 18/100, Train Loss: 1.1942, Test Loss: 0.9727\n",
      "Epoch 19/100, Train Loss: 1.1915, Test Loss: 0.9725\n",
      "Epoch 20/100, Train Loss: 1.1656, Test Loss: 0.9696\n",
      "Epoch 21/100, Train Loss: 1.2173, Test Loss: 0.9686\n",
      "Epoch 22/100, Train Loss: 1.1803, Test Loss: 0.9699\n",
      "Epoch 23/100, Train Loss: 1.2180, Test Loss: 0.9695\n",
      "Epoch 24/100, Train Loss: 1.2054, Test Loss: 0.9727\n",
      "Epoch 25/100, Train Loss: 1.1508, Test Loss: 0.9735\n",
      "Epoch 26/100, Train Loss: 1.1193, Test Loss: 0.9707\n",
      "Epoch 27/100, Train Loss: 1.1297, Test Loss: 0.9710\n",
      "Epoch 28/100, Train Loss: 1.1296, Test Loss: 0.9697\n",
      "Epoch 29/100, Train Loss: 1.1426, Test Loss: 0.9726\n",
      "Epoch 30/100, Train Loss: 1.1420, Test Loss: 0.9702\n",
      "Epoch 31/100, Train Loss: 1.1401, Test Loss: 0.9710\n",
      "Epoch 32/100, Train Loss: 1.1540, Test Loss: 0.9679\n",
      "Epoch 33/100, Train Loss: 1.1681, Test Loss: 0.9678\n",
      "Epoch 34/100, Train Loss: 1.1445, Test Loss: 0.9700\n",
      "Epoch 35/100, Train Loss: 1.1488, Test Loss: 0.9693\n",
      "Epoch 36/100, Train Loss: 1.1460, Test Loss: 0.9703\n",
      "Epoch 37/100, Train Loss: 1.1061, Test Loss: 0.9708\n",
      "Epoch 38/100, Train Loss: 1.1958, Test Loss: 0.9707\n",
      "Epoch 39/100, Train Loss: 1.1754, Test Loss: 0.9712\n",
      "Epoch 40/100, Train Loss: 1.1347, Test Loss: 0.9704\n",
      "Epoch 41/100, Train Loss: 1.1762, Test Loss: 0.9686\n",
      "Epoch 42/100, Train Loss: 1.1397, Test Loss: 0.9697\n",
      "Epoch 43/100, Train Loss: 1.1000, Test Loss: 0.9718\n",
      "Epoch 44/100, Train Loss: 1.1192, Test Loss: 0.9718\n",
      "Epoch 45/100, Train Loss: 1.1150, Test Loss: 0.9707\n",
      "Epoch 46/100, Train Loss: 1.1256, Test Loss: 0.9689\n",
      "Epoch 47/100, Train Loss: 1.1442, Test Loss: 0.9690\n",
      "Epoch 48/100, Train Loss: 1.1067, Test Loss: 0.9700\n",
      "Epoch 49/100, Train Loss: 1.1328, Test Loss: 0.9708\n",
      "Epoch 50/100, Train Loss: 1.1481, Test Loss: 0.9694\n",
      "Epoch 51/100, Train Loss: 1.1161, Test Loss: 0.9713\n",
      "Epoch 52/100, Train Loss: 1.0967, Test Loss: 0.9684\n",
      "Epoch 53/100, Train Loss: 1.1146, Test Loss: 0.9706\n",
      "Epoch 54/100, Train Loss: 1.1345, Test Loss: 0.9704\n",
      "Epoch 55/100, Train Loss: 1.1126, Test Loss: 0.9698\n",
      "Epoch 56/100, Train Loss: 1.1220, Test Loss: 0.9703\n",
      "Epoch 57/100, Train Loss: 1.1500, Test Loss: 0.9716\n",
      "Epoch 58/100, Train Loss: 1.0881, Test Loss: 0.9729\n",
      "Epoch 59/100, Train Loss: 1.1045, Test Loss: 0.9697\n",
      "Epoch 60/100, Train Loss: 1.1225, Test Loss: 0.9706\n",
      "Epoch 61/100, Train Loss: 1.0939, Test Loss: 0.9702\n",
      "Epoch 62/100, Train Loss: 1.1348, Test Loss: 0.9692\n",
      "Epoch 63/100, Train Loss: 1.0911, Test Loss: 0.9709\n",
      "Epoch 64/100, Train Loss: 1.0841, Test Loss: 0.9697\n",
      "Epoch 65/100, Train Loss: 1.0767, Test Loss: 0.9702\n",
      "Epoch 66/100, Train Loss: 1.0854, Test Loss: 0.9706\n",
      "Epoch 67/100, Train Loss: 1.1163, Test Loss: 0.9711\n",
      "Epoch 68/100, Train Loss: 1.0814, Test Loss: 0.9703\n",
      "Epoch 69/100, Train Loss: 1.0847, Test Loss: 0.9697\n",
      "Epoch 70/100, Train Loss: 1.0992, Test Loss: 0.9714\n",
      "Epoch 71/100, Train Loss: 1.1476, Test Loss: 0.9714\n",
      "Epoch 72/100, Train Loss: 1.0657, Test Loss: 0.9708\n",
      "Epoch 73/100, Train Loss: 1.0912, Test Loss: 0.9725\n",
      "Epoch 74/100, Train Loss: 1.0603, Test Loss: 0.9710\n",
      "Epoch 75/100, Train Loss: 1.0898, Test Loss: 0.9711\n",
      "Epoch 76/100, Train Loss: 1.0947, Test Loss: 0.9697\n",
      "Epoch 77/100, Train Loss: 1.0691, Test Loss: 0.9723\n",
      "Epoch 78/100, Train Loss: 1.0830, Test Loss: 0.9723\n",
      "Epoch 79/100, Train Loss: 1.0799, Test Loss: 0.9699\n",
      "Epoch 80/100, Train Loss: 1.0974, Test Loss: 0.9698\n",
      "Epoch 81/100, Train Loss: 1.0718, Test Loss: 0.9704\n",
      "Epoch 82/100, Train Loss: 1.1314, Test Loss: 0.9698\n",
      "Epoch 83/100, Train Loss: 1.0412, Test Loss: 0.9699\n",
      "Epoch 84/100, Train Loss: 1.0734, Test Loss: 0.9700\n",
      "Epoch 85/100, Train Loss: 1.0932, Test Loss: 0.9701\n",
      "Epoch 86/100, Train Loss: 1.0591, Test Loss: 0.9708\n",
      "Epoch 87/100, Train Loss: 1.0414, Test Loss: 0.9720\n",
      "Epoch 88/100, Train Loss: 1.0617, Test Loss: 0.9692\n",
      "Epoch 89/100, Train Loss: 1.0946, Test Loss: 0.9713\n",
      "Epoch 90/100, Train Loss: 1.0953, Test Loss: 0.9700\n",
      "Epoch 91/100, Train Loss: 1.1110, Test Loss: 0.9702\n",
      "Epoch 92/100, Train Loss: 1.0705, Test Loss: 0.9688\n",
      "Epoch 93/100, Train Loss: 1.0753, Test Loss: 0.9695\n",
      "Epoch 94/100, Train Loss: 1.1029, Test Loss: 0.9710\n",
      "Epoch 95/100, Train Loss: 1.0820, Test Loss: 0.9703\n",
      "Epoch 96/100, Train Loss: 1.0884, Test Loss: 0.9713\n",
      "Epoch 97/100, Train Loss: 1.0773, Test Loss: 0.9709\n",
      "Epoch 98/100, Train Loss: 1.0888, Test Loss: 0.9692\n",
      "Epoch 99/100, Train Loss: 1.0567, Test Loss: 0.9696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:28:54,864] Trial 115 finished with value: 0.9695234894752502 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 119, 'layer_1_size': 188, 'layer_2_size': 229, 'layer_3_size': 60, 'layer_4_size': 220, 'layer_5_size': 137, 'layer_6_size': 141, 'layer_7_size': 128, 'dropout_rate': 0.4710636358063504, 'learning_rate': 5.06810285208516e-05, 'batch_size': 64, 'epochs': 100}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100, Train Loss: 1.0540, Test Loss: 0.9695\n",
      "Epoch 1/26, Train Loss: 1.2664, Test Loss: 0.9652\n",
      "Epoch 2/26, Train Loss: 1.2142, Test Loss: 0.9630\n",
      "Epoch 3/26, Train Loss: 1.1194, Test Loss: 0.9718\n",
      "Epoch 4/26, Train Loss: 1.0744, Test Loss: 0.9615\n",
      "Epoch 5/26, Train Loss: 1.1311, Test Loss: 0.9569\n",
      "Epoch 6/26, Train Loss: 1.1566, Test Loss: 0.9531\n",
      "Epoch 7/26, Train Loss: 1.1243, Test Loss: 0.9507\n",
      "Epoch 8/26, Train Loss: 1.1161, Test Loss: 0.9523\n",
      "Epoch 9/26, Train Loss: 1.0967, Test Loss: 0.9488\n",
      "Epoch 10/26, Train Loss: 1.0954, Test Loss: 0.9468\n",
      "Epoch 11/26, Train Loss: 1.0752, Test Loss: 0.9470\n",
      "Epoch 12/26, Train Loss: 1.1363, Test Loss: 0.9405\n",
      "Epoch 13/26, Train Loss: 1.0640, Test Loss: 0.9381\n",
      "Epoch 14/26, Train Loss: 1.0721, Test Loss: 0.9416\n",
      "Epoch 15/26, Train Loss: 1.0976, Test Loss: 0.9409\n",
      "Epoch 16/26, Train Loss: 1.0687, Test Loss: 0.9416\n",
      "Epoch 17/26, Train Loss: 1.0557, Test Loss: 0.9437\n",
      "Epoch 18/26, Train Loss: 1.1003, Test Loss: 0.9446\n",
      "Epoch 19/26, Train Loss: 1.0331, Test Loss: 0.9459\n",
      "Epoch 20/26, Train Loss: 1.0957, Test Loss: 0.9492\n",
      "Epoch 21/26, Train Loss: 1.0746, Test Loss: 0.9496\n",
      "Epoch 22/26, Train Loss: 1.1032, Test Loss: 0.9493\n",
      "Epoch 23/26, Train Loss: 1.0724, Test Loss: 0.9457\n",
      "Epoch 24/26, Train Loss: 1.0900, Test Loss: 0.9440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:28:57,145] Trial 116 finished with value: 0.9518527388572693 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 95, 'layer_1_size': 203, 'layer_2_size': 220, 'layer_3_size': 80, 'layer_4_size': 241, 'layer_5_size': 239, 'layer_6_size': 186, 'dropout_rate': 0.41020698774000275, 'learning_rate': 0.00015845165216066815, 'batch_size': 64, 'epochs': 26}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/26, Train Loss: 1.0802, Test Loss: 0.9495\n",
      "Epoch 26/26, Train Loss: 1.0530, Test Loss: 0.9519\n",
      "Epoch 1/92, Train Loss: 1.1562, Test Loss: 1.1307\n",
      "Epoch 2/92, Train Loss: 1.1965, Test Loss: 1.1478\n",
      "Epoch 3/92, Train Loss: 1.1971, Test Loss: 1.1500\n",
      "Epoch 4/92, Train Loss: 1.2550, Test Loss: 1.1461\n",
      "Epoch 5/92, Train Loss: 1.1902, Test Loss: 1.1417\n",
      "Epoch 6/92, Train Loss: 1.1603, Test Loss: 1.1406\n",
      "Epoch 7/92, Train Loss: 1.2356, Test Loss: 1.1310\n",
      "Epoch 8/92, Train Loss: 1.1666, Test Loss: 1.1269\n",
      "Epoch 9/92, Train Loss: 1.2022, Test Loss: 1.1218\n",
      "Epoch 10/92, Train Loss: 1.1304, Test Loss: 1.1218\n",
      "Epoch 11/92, Train Loss: 1.1504, Test Loss: 1.1169\n",
      "Epoch 12/92, Train Loss: 1.1701, Test Loss: 1.1127\n",
      "Epoch 13/92, Train Loss: 1.1404, Test Loss: 1.1123\n",
      "Epoch 14/92, Train Loss: 1.1373, Test Loss: 1.1143\n",
      "Epoch 15/92, Train Loss: 1.1582, Test Loss: 1.1151\n",
      "Epoch 16/92, Train Loss: 1.1135, Test Loss: 1.1193\n",
      "Epoch 17/92, Train Loss: 1.1763, Test Loss: 1.1130\n",
      "Epoch 18/92, Train Loss: 1.1143, Test Loss: 1.1095\n",
      "Epoch 19/92, Train Loss: 1.0762, Test Loss: 1.1079\n",
      "Epoch 20/92, Train Loss: 1.0983, Test Loss: 1.1057\n",
      "Epoch 21/92, Train Loss: 1.1080, Test Loss: 1.1048\n",
      "Epoch 22/92, Train Loss: 1.0711, Test Loss: 1.1054\n",
      "Epoch 23/92, Train Loss: 1.1180, Test Loss: 1.1006\n",
      "Epoch 24/92, Train Loss: 1.1219, Test Loss: 1.1032\n",
      "Epoch 25/92, Train Loss: 1.0371, Test Loss: 1.1006\n",
      "Epoch 26/92, Train Loss: 1.0701, Test Loss: 1.1032\n",
      "Epoch 27/92, Train Loss: 1.0940, Test Loss: 1.1002\n",
      "Epoch 28/92, Train Loss: 1.0573, Test Loss: 1.0967\n",
      "Epoch 29/92, Train Loss: 1.1030, Test Loss: 1.0978\n",
      "Epoch 30/92, Train Loss: 1.1139, Test Loss: 1.0989\n",
      "Epoch 31/92, Train Loss: 1.0336, Test Loss: 1.0976\n",
      "Epoch 32/92, Train Loss: 1.0981, Test Loss: 1.0968\n",
      "Epoch 33/92, Train Loss: 1.0581, Test Loss: 1.0923\n",
      "Epoch 34/92, Train Loss: 1.0644, Test Loss: 1.0929\n",
      "Epoch 35/92, Train Loss: 1.0378, Test Loss: 1.0945\n",
      "Epoch 36/92, Train Loss: 1.0446, Test Loss: 1.0964\n",
      "Epoch 37/92, Train Loss: 1.0721, Test Loss: 1.0946\n",
      "Epoch 38/92, Train Loss: 1.0674, Test Loss: 1.0945\n",
      "Epoch 39/92, Train Loss: 1.0549, Test Loss: 1.0962\n",
      "Epoch 40/92, Train Loss: 1.0308, Test Loss: 1.0945\n",
      "Epoch 41/92, Train Loss: 1.0384, Test Loss: 1.0948\n",
      "Epoch 42/92, Train Loss: 1.0860, Test Loss: 1.0957\n",
      "Epoch 43/92, Train Loss: 1.0738, Test Loss: 1.0917\n",
      "Epoch 44/92, Train Loss: 1.0092, Test Loss: 1.0908\n",
      "Epoch 45/92, Train Loss: 1.0628, Test Loss: 1.0913\n",
      "Epoch 46/92, Train Loss: 1.0491, Test Loss: 1.0919\n",
      "Epoch 47/92, Train Loss: 1.0835, Test Loss: 1.0905\n",
      "Epoch 48/92, Train Loss: 1.0148, Test Loss: 1.0883\n",
      "Epoch 49/92, Train Loss: 1.0614, Test Loss: 1.0931\n",
      "Epoch 50/92, Train Loss: 1.0365, Test Loss: 1.0909\n",
      "Epoch 51/92, Train Loss: 1.0255, Test Loss: 1.0897\n",
      "Epoch 52/92, Train Loss: 1.0490, Test Loss: 1.0897\n",
      "Epoch 53/92, Train Loss: 1.0111, Test Loss: 1.0909\n",
      "Epoch 54/92, Train Loss: 1.0009, Test Loss: 1.0933\n",
      "Epoch 55/92, Train Loss: 1.0455, Test Loss: 1.0885\n",
      "Epoch 56/92, Train Loss: 1.0088, Test Loss: 1.0909\n",
      "Epoch 57/92, Train Loss: 1.0314, Test Loss: 1.0924\n",
      "Epoch 58/92, Train Loss: 1.0055, Test Loss: 1.0924\n",
      "Epoch 59/92, Train Loss: 0.9823, Test Loss: 1.0897\n",
      "Epoch 60/92, Train Loss: 1.0492, Test Loss: 1.0935\n",
      "Epoch 61/92, Train Loss: 1.0466, Test Loss: 1.0936\n",
      "Epoch 62/92, Train Loss: 1.0466, Test Loss: 1.0958\n",
      "Epoch 63/92, Train Loss: 0.9973, Test Loss: 1.0978\n",
      "Epoch 64/92, Train Loss: 1.0061, Test Loss: 1.0969\n",
      "Epoch 65/92, Train Loss: 1.0477, Test Loss: 1.0988\n",
      "Epoch 66/92, Train Loss: 0.9719, Test Loss: 1.0998\n",
      "Epoch 67/92, Train Loss: 1.0021, Test Loss: 1.0931\n",
      "Epoch 68/92, Train Loss: 1.0257, Test Loss: 1.0936\n",
      "Epoch 69/92, Train Loss: 1.0348, Test Loss: 1.0926\n",
      "Epoch 70/92, Train Loss: 1.0001, Test Loss: 1.0957\n",
      "Epoch 71/92, Train Loss: 1.0242, Test Loss: 1.0989\n",
      "Epoch 72/92, Train Loss: 0.9900, Test Loss: 1.0920\n",
      "Epoch 73/92, Train Loss: 0.9922, Test Loss: 1.0962\n",
      "Epoch 74/92, Train Loss: 1.0082, Test Loss: 1.0894\n",
      "Epoch 75/92, Train Loss: 1.0234, Test Loss: 1.0940\n",
      "Epoch 76/92, Train Loss: 0.9856, Test Loss: 1.0958\n",
      "Epoch 77/92, Train Loss: 1.0179, Test Loss: 1.0970\n",
      "Epoch 78/92, Train Loss: 0.9698, Test Loss: 1.0919\n",
      "Epoch 79/92, Train Loss: 0.9789, Test Loss: 1.0875\n",
      "Epoch 80/92, Train Loss: 0.9933, Test Loss: 1.0938\n",
      "Epoch 81/92, Train Loss: 1.0454, Test Loss: 1.0873\n",
      "Epoch 82/92, Train Loss: 1.0005, Test Loss: 1.0889\n",
      "Epoch 83/92, Train Loss: 0.9971, Test Loss: 1.0892\n",
      "Epoch 84/92, Train Loss: 0.9663, Test Loss: 1.0897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:28:59,314] Trial 117 finished with value: 1.0910706669092178 and parameters: {'num_hidden_layers': 2, 'layer_0_size': 131, 'layer_1_size': 137, 'dropout_rate': 0.4919348005347692, 'learning_rate': 5.891734841790546e-05, 'batch_size': 64, 'epochs': 92}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/92, Train Loss: 0.9844, Test Loss: 1.0911\n",
      "Epoch 86/92, Train Loss: 0.9986, Test Loss: 1.0908\n",
      "Epoch 87/92, Train Loss: 0.9555, Test Loss: 1.0901\n",
      "Epoch 88/92, Train Loss: 1.0245, Test Loss: 1.0912\n",
      "Epoch 89/92, Train Loss: 1.0286, Test Loss: 1.0905\n",
      "Epoch 90/92, Train Loss: 0.9712, Test Loss: 1.0892\n",
      "Epoch 91/92, Train Loss: 0.9836, Test Loss: 1.0915\n",
      "Epoch 92/92, Train Loss: 1.0016, Test Loss: 1.0911\n",
      "Epoch 1/57, Train Loss: 1.3649, Test Loss: 1.0941\n",
      "Epoch 2/57, Train Loss: 1.4118, Test Loss: 1.1049\n",
      "Epoch 3/57, Train Loss: 1.4333, Test Loss: 1.1155\n",
      "Epoch 4/57, Train Loss: 1.4869, Test Loss: 1.1254\n",
      "Epoch 5/57, Train Loss: 1.4482, Test Loss: 1.1351\n",
      "Epoch 6/57, Train Loss: 1.3754, Test Loss: 1.1433\n",
      "Epoch 7/57, Train Loss: 1.3906, Test Loss: 1.1467\n",
      "Epoch 8/57, Train Loss: 1.4812, Test Loss: 1.1555\n",
      "Epoch 9/57, Train Loss: 1.4384, Test Loss: 1.1609\n",
      "Epoch 10/57, Train Loss: 1.4270, Test Loss: 1.1634\n",
      "Epoch 11/57, Train Loss: 1.3737, Test Loss: 1.1639\n",
      "Epoch 12/57, Train Loss: 1.4267, Test Loss: 1.1722\n",
      "Epoch 13/57, Train Loss: 1.3690, Test Loss: 1.1763\n",
      "Epoch 14/57, Train Loss: 1.3212, Test Loss: 1.1788\n",
      "Epoch 15/57, Train Loss: 1.3963, Test Loss: 1.1830\n",
      "Epoch 16/57, Train Loss: 1.4052, Test Loss: 1.1812\n",
      "Epoch 17/57, Train Loss: 1.3602, Test Loss: 1.1868\n",
      "Epoch 18/57, Train Loss: 1.3018, Test Loss: 1.1876\n",
      "Epoch 19/57, Train Loss: 1.3136, Test Loss: 1.1877\n",
      "Epoch 20/57, Train Loss: 1.3327, Test Loss: 1.1840\n",
      "Epoch 21/57, Train Loss: 1.3685, Test Loss: 1.1918\n",
      "Epoch 22/57, Train Loss: 1.2874, Test Loss: 1.1934\n",
      "Epoch 23/57, Train Loss: 1.3661, Test Loss: 1.1936\n",
      "Epoch 24/57, Train Loss: 1.3444, Test Loss: 1.1931\n",
      "Epoch 25/57, Train Loss: 1.3590, Test Loss: 1.1990\n",
      "Epoch 26/57, Train Loss: 1.3440, Test Loss: 1.2025\n",
      "Epoch 27/57, Train Loss: 1.2527, Test Loss: 1.2024\n",
      "Epoch 28/57, Train Loss: 1.3596, Test Loss: 1.2036\n",
      "Epoch 29/57, Train Loss: 1.2921, Test Loss: 1.2000\n",
      "Epoch 30/57, Train Loss: 1.3464, Test Loss: 1.2004\n",
      "Epoch 31/57, Train Loss: 1.3232, Test Loss: 1.2030\n",
      "Epoch 32/57, Train Loss: 1.3571, Test Loss: 1.2058\n",
      "Epoch 33/57, Train Loss: 1.2550, Test Loss: 1.2041\n",
      "Epoch 34/57, Train Loss: 1.3184, Test Loss: 1.2121\n",
      "Epoch 35/57, Train Loss: 1.3096, Test Loss: 1.2161\n",
      "Epoch 36/57, Train Loss: 1.3218, Test Loss: 1.2172\n",
      "Epoch 37/57, Train Loss: 1.3004, Test Loss: 1.2157\n",
      "Epoch 38/57, Train Loss: 1.3255, Test Loss: 1.2182\n",
      "Epoch 39/57, Train Loss: 1.3249, Test Loss: 1.2141\n",
      "Epoch 40/57, Train Loss: 1.3709, Test Loss: 1.2216\n",
      "Epoch 41/57, Train Loss: 1.2871, Test Loss: 1.2144\n",
      "Epoch 42/57, Train Loss: 1.3288, Test Loss: 1.2149\n",
      "Epoch 43/57, Train Loss: 1.2898, Test Loss: 1.2144\n",
      "Epoch 44/57, Train Loss: 1.3408, Test Loss: 1.2121\n",
      "Epoch 45/57, Train Loss: 1.2703, Test Loss: 1.2108\n",
      "Epoch 46/57, Train Loss: 1.3643, Test Loss: 1.2159\n",
      "Epoch 47/57, Train Loss: 1.3064, Test Loss: 1.2134\n",
      "Epoch 48/57, Train Loss: 1.3039, Test Loss: 1.2104\n",
      "Epoch 49/57, Train Loss: 1.3080, Test Loss: 1.2159\n",
      "Epoch 50/57, Train Loss: 1.3253, Test Loss: 1.2175\n",
      "Epoch 51/57, Train Loss: 1.3262, Test Loss: 1.2211\n",
      "Epoch 52/57, Train Loss: 1.3221, Test Loss: 1.2212\n",
      "Epoch 53/57, Train Loss: 1.3076, Test Loss: 1.2207\n",
      "Epoch 54/57, Train Loss: 1.3694, Test Loss: 1.2261\n",
      "Epoch 55/57, Train Loss: 1.2957, Test Loss: 1.2233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:29:02,179] Trial 118 finished with value: 1.2234892845153809 and parameters: {'num_hidden_layers': 9, 'layer_0_size': 116, 'layer_1_size': 176, 'layer_2_size': 171, 'layer_3_size': 177, 'layer_4_size': 231, 'layer_5_size': 193, 'layer_6_size': 146, 'layer_7_size': 163, 'layer_8_size': 71, 'dropout_rate': 0.4555409358928552, 'learning_rate': 2.17303869143426e-05, 'batch_size': 256, 'epochs': 57}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/57, Train Loss: 1.3189, Test Loss: 1.2229\n",
      "Epoch 57/57, Train Loss: 1.3012, Test Loss: 1.2235\n",
      "Epoch 1/81, Train Loss: 1.1604, Test Loss: 0.8373\n",
      "Epoch 2/81, Train Loss: 1.1950, Test Loss: 0.8325\n",
      "Epoch 3/81, Train Loss: 1.2045, Test Loss: 0.8280\n",
      "Epoch 4/81, Train Loss: 1.1980, Test Loss: 0.8272\n",
      "Epoch 5/81, Train Loss: 1.2304, Test Loss: 0.8268\n",
      "Epoch 6/81, Train Loss: 1.1187, Test Loss: 0.8263\n",
      "Epoch 7/81, Train Loss: 1.1388, Test Loss: 0.8267\n",
      "Epoch 8/81, Train Loss: 1.1416, Test Loss: 0.8291\n",
      "Epoch 9/81, Train Loss: 1.1576, Test Loss: 0.8289\n",
      "Epoch 10/81, Train Loss: 1.1612, Test Loss: 0.8289\n",
      "Epoch 11/81, Train Loss: 1.0972, Test Loss: 0.8281\n",
      "Epoch 12/81, Train Loss: 1.1569, Test Loss: 0.8262\n",
      "Epoch 13/81, Train Loss: 1.1281, Test Loss: 0.8276\n",
      "Epoch 14/81, Train Loss: 1.1468, Test Loss: 0.8295\n",
      "Epoch 15/81, Train Loss: 1.1251, Test Loss: 0.8285\n",
      "Epoch 16/81, Train Loss: 1.1026, Test Loss: 0.8285\n",
      "Epoch 17/81, Train Loss: 1.1584, Test Loss: 0.8276\n",
      "Epoch 18/81, Train Loss: 1.1225, Test Loss: 0.8266\n",
      "Epoch 19/81, Train Loss: 1.1255, Test Loss: 0.8276\n",
      "Epoch 20/81, Train Loss: 1.0873, Test Loss: 0.8272\n",
      "Epoch 21/81, Train Loss: 1.1353, Test Loss: 0.8290\n",
      "Epoch 22/81, Train Loss: 1.1138, Test Loss: 0.8282\n",
      "Epoch 23/81, Train Loss: 1.1422, Test Loss: 0.8299\n",
      "Epoch 24/81, Train Loss: 1.1061, Test Loss: 0.8304\n",
      "Epoch 25/81, Train Loss: 1.0835, Test Loss: 0.8291\n",
      "Epoch 26/81, Train Loss: 1.0975, Test Loss: 0.8279\n",
      "Epoch 27/81, Train Loss: 1.0680, Test Loss: 0.8287\n",
      "Epoch 28/81, Train Loss: 1.1041, Test Loss: 0.8289\n",
      "Epoch 29/81, Train Loss: 1.0661, Test Loss: 0.8300\n",
      "Epoch 30/81, Train Loss: 1.1381, Test Loss: 0.8298\n",
      "Epoch 31/81, Train Loss: 1.0904, Test Loss: 0.8294\n",
      "Epoch 32/81, Train Loss: 1.0793, Test Loss: 0.8289\n",
      "Epoch 33/81, Train Loss: 1.0962, Test Loss: 0.8278\n",
      "Epoch 34/81, Train Loss: 1.0920, Test Loss: 0.8271\n",
      "Epoch 35/81, Train Loss: 1.1389, Test Loss: 0.8289\n",
      "Epoch 36/81, Train Loss: 1.1018, Test Loss: 0.8284\n",
      "Epoch 37/81, Train Loss: 1.1222, Test Loss: 0.8293\n",
      "Epoch 38/81, Train Loss: 1.0843, Test Loss: 0.8300\n",
      "Epoch 39/81, Train Loss: 1.0647, Test Loss: 0.8285\n",
      "Epoch 40/81, Train Loss: 1.0905, Test Loss: 0.8287\n",
      "Epoch 41/81, Train Loss: 1.0815, Test Loss: 0.8287\n",
      "Epoch 42/81, Train Loss: 1.1079, Test Loss: 0.8283\n",
      "Epoch 43/81, Train Loss: 1.0856, Test Loss: 0.8285\n",
      "Epoch 44/81, Train Loss: 1.0781, Test Loss: 0.8282\n",
      "Epoch 45/81, Train Loss: 1.0796, Test Loss: 0.8260\n",
      "Epoch 46/81, Train Loss: 1.0814, Test Loss: 0.8280\n",
      "Epoch 47/81, Train Loss: 1.1134, Test Loss: 0.8267\n",
      "Epoch 48/81, Train Loss: 1.0716, Test Loss: 0.8290\n",
      "Epoch 49/81, Train Loss: 1.1026, Test Loss: 0.8297\n",
      "Epoch 50/81, Train Loss: 1.0642, Test Loss: 0.8286\n",
      "Epoch 51/81, Train Loss: 1.0756, Test Loss: 0.8281\n",
      "Epoch 52/81, Train Loss: 1.0810, Test Loss: 0.8268\n",
      "Epoch 53/81, Train Loss: 1.0871, Test Loss: 0.8265\n",
      "Epoch 54/81, Train Loss: 1.0667, Test Loss: 0.8261\n",
      "Epoch 55/81, Train Loss: 1.1120, Test Loss: 0.8268\n",
      "Epoch 56/81, Train Loss: 1.0845, Test Loss: 0.8273\n",
      "Epoch 57/81, Train Loss: 1.0529, Test Loss: 0.8282\n",
      "Epoch 58/81, Train Loss: 1.0777, Test Loss: 0.8277\n",
      "Epoch 59/81, Train Loss: 1.0893, Test Loss: 0.8274\n",
      "Epoch 60/81, Train Loss: 1.0776, Test Loss: 0.8271\n",
      "Epoch 61/81, Train Loss: 1.0636, Test Loss: 0.8283\n",
      "Epoch 62/81, Train Loss: 1.0670, Test Loss: 0.8289\n",
      "Epoch 63/81, Train Loss: 1.0670, Test Loss: 0.8290\n",
      "Epoch 64/81, Train Loss: 1.0825, Test Loss: 0.8298\n",
      "Epoch 65/81, Train Loss: 1.0670, Test Loss: 0.8278\n",
      "Epoch 66/81, Train Loss: 1.0556, Test Loss: 0.8278\n",
      "Epoch 67/81, Train Loss: 1.0592, Test Loss: 0.8279\n",
      "Epoch 68/81, Train Loss: 1.0634, Test Loss: 0.8287\n",
      "Epoch 69/81, Train Loss: 1.0712, Test Loss: 0.8280\n",
      "Epoch 70/81, Train Loss: 1.0801, Test Loss: 0.8292\n",
      "Epoch 71/81, Train Loss: 1.0581, Test Loss: 0.8299\n",
      "Epoch 72/81, Train Loss: 1.0214, Test Loss: 0.8295\n",
      "Epoch 73/81, Train Loss: 1.0677, Test Loss: 0.8301\n",
      "Epoch 74/81, Train Loss: 1.0772, Test Loss: 0.8301\n",
      "Epoch 75/81, Train Loss: 1.0609, Test Loss: 0.8325\n",
      "Epoch 76/81, Train Loss: 1.0860, Test Loss: 0.8324\n",
      "Epoch 77/81, Train Loss: 1.0691, Test Loss: 0.8318\n",
      "Epoch 78/81, Train Loss: 1.0670, Test Loss: 0.8314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:29:05,471] Trial 119 finished with value: 0.8311890363693237 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 101, 'layer_1_size': 218, 'layer_2_size': 213, 'layer_3_size': 252, 'layer_4_size': 96, 'layer_5_size': 117, 'dropout_rate': 0.30676415680443836, 'learning_rate': 7.913481693679927e-05, 'batch_size': 128, 'epochs': 81}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/81, Train Loss: 1.0413, Test Loss: 0.8316\n",
      "Epoch 80/81, Train Loss: 1.0556, Test Loss: 0.8314\n",
      "Epoch 81/81, Train Loss: 1.0693, Test Loss: 0.8312\n",
      "Epoch 1/95, Train Loss: 1.1841, Test Loss: 0.9927\n",
      "Epoch 2/95, Train Loss: 1.2371, Test Loss: 0.9928\n",
      "Epoch 3/95, Train Loss: 1.1994, Test Loss: 0.9966\n",
      "Epoch 4/95, Train Loss: 1.1987, Test Loss: 1.0001\n",
      "Epoch 5/95, Train Loss: 1.2552, Test Loss: 1.0062\n",
      "Epoch 6/95, Train Loss: 1.2154, Test Loss: 1.0113\n",
      "Epoch 7/95, Train Loss: 1.1978, Test Loss: 1.0129\n",
      "Epoch 8/95, Train Loss: 1.2225, Test Loss: 1.0137\n",
      "Epoch 9/95, Train Loss: 1.1519, Test Loss: 1.0160\n",
      "Epoch 10/95, Train Loss: 1.2027, Test Loss: 1.0160\n",
      "Epoch 11/95, Train Loss: 1.2247, Test Loss: 1.0210\n",
      "Epoch 12/95, Train Loss: 1.1817, Test Loss: 1.0241\n",
      "Epoch 13/95, Train Loss: 1.1404, Test Loss: 1.0238\n",
      "Epoch 14/95, Train Loss: 1.1945, Test Loss: 1.0232\n",
      "Epoch 15/95, Train Loss: 1.1486, Test Loss: 1.0242\n",
      "Epoch 16/95, Train Loss: 1.1954, Test Loss: 1.0223\n",
      "Epoch 17/95, Train Loss: 1.1617, Test Loss: 1.0215\n",
      "Epoch 18/95, Train Loss: 1.1163, Test Loss: 1.0268\n",
      "Epoch 19/95, Train Loss: 1.1300, Test Loss: 1.0268\n",
      "Epoch 20/95, Train Loss: 1.1438, Test Loss: 1.0229\n",
      "Epoch 21/95, Train Loss: 1.1461, Test Loss: 1.0199\n",
      "Epoch 22/95, Train Loss: 1.1697, Test Loss: 1.0216\n",
      "Epoch 23/95, Train Loss: 1.1353, Test Loss: 1.0240\n",
      "Epoch 24/95, Train Loss: 1.1972, Test Loss: 1.0256\n",
      "Epoch 25/95, Train Loss: 1.1944, Test Loss: 1.0332\n",
      "Epoch 26/95, Train Loss: 1.1250, Test Loss: 1.0324\n",
      "Epoch 27/95, Train Loss: 1.1705, Test Loss: 1.0300\n",
      "Epoch 28/95, Train Loss: 1.1950, Test Loss: 1.0277\n",
      "Epoch 29/95, Train Loss: 1.1691, Test Loss: 1.0292\n",
      "Epoch 30/95, Train Loss: 1.1277, Test Loss: 1.0319\n",
      "Epoch 31/95, Train Loss: 1.1309, Test Loss: 1.0316\n",
      "Epoch 32/95, Train Loss: 1.1626, Test Loss: 1.0338\n",
      "Epoch 33/95, Train Loss: 1.1756, Test Loss: 1.0312\n",
      "Epoch 34/95, Train Loss: 1.1217, Test Loss: 1.0344\n",
      "Epoch 35/95, Train Loss: 1.1899, Test Loss: 1.0268\n",
      "Epoch 36/95, Train Loss: 1.1368, Test Loss: 1.0280\n",
      "Epoch 37/95, Train Loss: 1.1671, Test Loss: 1.0310\n",
      "Epoch 38/95, Train Loss: 1.0924, Test Loss: 1.0292\n",
      "Epoch 39/95, Train Loss: 1.1641, Test Loss: 1.0319\n",
      "Epoch 40/95, Train Loss: 1.2149, Test Loss: 1.0325\n",
      "Epoch 41/95, Train Loss: 1.1319, Test Loss: 1.0301\n",
      "Epoch 42/95, Train Loss: 1.1141, Test Loss: 1.0310\n",
      "Epoch 43/95, Train Loss: 1.1072, Test Loss: 1.0332\n",
      "Epoch 44/95, Train Loss: 1.0956, Test Loss: 1.0342\n",
      "Epoch 45/95, Train Loss: 1.1593, Test Loss: 1.0337\n",
      "Epoch 46/95, Train Loss: 1.1384, Test Loss: 1.0343\n",
      "Epoch 47/95, Train Loss: 1.1157, Test Loss: 1.0334\n",
      "Epoch 48/95, Train Loss: 1.1496, Test Loss: 1.0357\n",
      "Epoch 49/95, Train Loss: 1.1772, Test Loss: 1.0363\n",
      "Epoch 50/95, Train Loss: 1.1732, Test Loss: 1.0373\n",
      "Epoch 51/95, Train Loss: 1.1600, Test Loss: 1.0402\n",
      "Epoch 52/95, Train Loss: 1.1688, Test Loss: 1.0407\n",
      "Epoch 53/95, Train Loss: 1.0972, Test Loss: 1.0407\n",
      "Epoch 54/95, Train Loss: 1.1668, Test Loss: 1.0435\n",
      "Epoch 55/95, Train Loss: 1.1012, Test Loss: 1.0458\n",
      "Epoch 56/95, Train Loss: 1.1685, Test Loss: 1.0453\n",
      "Epoch 57/95, Train Loss: 1.1001, Test Loss: 1.0450\n",
      "Epoch 58/95, Train Loss: 1.1324, Test Loss: 1.0462\n",
      "Epoch 59/95, Train Loss: 1.1124, Test Loss: 1.0434\n",
      "Epoch 60/95, Train Loss: 1.1226, Test Loss: 1.0454\n",
      "Epoch 61/95, Train Loss: 1.1020, Test Loss: 1.0483\n",
      "Epoch 62/95, Train Loss: 1.1252, Test Loss: 1.0462\n",
      "Epoch 63/95, Train Loss: 1.0351, Test Loss: 1.0484\n",
      "Epoch 64/95, Train Loss: 1.1279, Test Loss: 1.0400\n",
      "Epoch 65/95, Train Loss: 1.1260, Test Loss: 1.0425\n",
      "Epoch 66/95, Train Loss: 1.1522, Test Loss: 1.0493\n",
      "Epoch 67/95, Train Loss: 1.1653, Test Loss: 1.0469\n",
      "Epoch 68/95, Train Loss: 1.1371, Test Loss: 1.0419\n",
      "Epoch 69/95, Train Loss: 1.0953, Test Loss: 1.0456\n",
      "Epoch 70/95, Train Loss: 1.1112, Test Loss: 1.0508\n",
      "Epoch 71/95, Train Loss: 1.1479, Test Loss: 1.0490\n",
      "Epoch 72/95, Train Loss: 1.1336, Test Loss: 1.0451\n",
      "Epoch 73/95, Train Loss: 1.1375, Test Loss: 1.0473\n",
      "Epoch 74/95, Train Loss: 1.1301, Test Loss: 1.0445\n",
      "Epoch 75/95, Train Loss: 1.1197, Test Loss: 1.0459\n",
      "Epoch 76/95, Train Loss: 1.1141, Test Loss: 1.0431\n",
      "Epoch 77/95, Train Loss: 1.1470, Test Loss: 1.0406\n",
      "Epoch 78/95, Train Loss: 1.1102, Test Loss: 1.0420\n",
      "Epoch 79/95, Train Loss: 1.0674, Test Loss: 1.0452\n",
      "Epoch 80/95, Train Loss: 1.0964, Test Loss: 1.0490\n",
      "Epoch 81/95, Train Loss: 1.1300, Test Loss: 1.0486\n",
      "Epoch 82/95, Train Loss: 1.1082, Test Loss: 1.0453\n",
      "Epoch 83/95, Train Loss: 1.1221, Test Loss: 1.0461\n",
      "Epoch 84/95, Train Loss: 1.0902, Test Loss: 1.0436\n",
      "Epoch 85/95, Train Loss: 1.1256, Test Loss: 1.0472\n",
      "Epoch 86/95, Train Loss: 1.0687, Test Loss: 1.0449\n",
      "Epoch 87/95, Train Loss: 1.1254, Test Loss: 1.0461\n",
      "Epoch 88/95, Train Loss: 1.0707, Test Loss: 1.0450\n",
      "Epoch 89/95, Train Loss: 1.0619, Test Loss: 1.0434\n",
      "Epoch 90/95, Train Loss: 1.1341, Test Loss: 1.0426\n",
      "Epoch 91/95, Train Loss: 1.1136, Test Loss: 1.0402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:29:09,050] Trial 120 finished with value: 1.0451612770557404 and parameters: {'num_hidden_layers': 5, 'layer_0_size': 86, 'layer_1_size': 248, 'layer_2_size': 214, 'layer_3_size': 251, 'layer_4_size': 97, 'dropout_rate': 0.39827656112636484, 'learning_rate': 3.83306919972883e-05, 'batch_size': 128, 'epochs': 95}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/95, Train Loss: 1.0977, Test Loss: 1.0402\n",
      "Epoch 93/95, Train Loss: 1.0954, Test Loss: 1.0415\n",
      "Epoch 94/95, Train Loss: 1.1058, Test Loss: 1.0444\n",
      "Epoch 95/95, Train Loss: 1.0539, Test Loss: 1.0452\n",
      "Epoch 1/75, Train Loss: 1.1150, Test Loss: 0.8675\n",
      "Epoch 2/75, Train Loss: 1.1396, Test Loss: 0.8758\n",
      "Epoch 3/75, Train Loss: 1.1892, Test Loss: 0.8783\n",
      "Epoch 4/75, Train Loss: 1.1412, Test Loss: 0.8776\n",
      "Epoch 5/75, Train Loss: 1.1099, Test Loss: 0.8764\n",
      "Epoch 6/75, Train Loss: 1.1170, Test Loss: 0.8798\n",
      "Epoch 7/75, Train Loss: 1.1918, Test Loss: 0.8798\n",
      "Epoch 8/75, Train Loss: 1.1371, Test Loss: 0.8798\n",
      "Epoch 9/75, Train Loss: 1.0576, Test Loss: 0.8795\n",
      "Epoch 10/75, Train Loss: 1.1093, Test Loss: 0.8818\n",
      "Epoch 11/75, Train Loss: 1.0863, Test Loss: 0.8775\n",
      "Epoch 12/75, Train Loss: 1.0984, Test Loss: 0.8766\n",
      "Epoch 13/75, Train Loss: 1.0747, Test Loss: 0.8739\n",
      "Epoch 14/75, Train Loss: 1.0906, Test Loss: 0.8753\n",
      "Epoch 15/75, Train Loss: 1.1029, Test Loss: 0.8737\n",
      "Epoch 16/75, Train Loss: 1.0793, Test Loss: 0.8765\n",
      "Epoch 17/75, Train Loss: 1.1026, Test Loss: 0.8790\n",
      "Epoch 18/75, Train Loss: 1.0481, Test Loss: 0.8772\n",
      "Epoch 19/75, Train Loss: 1.1202, Test Loss: 0.8742\n",
      "Epoch 20/75, Train Loss: 0.9974, Test Loss: 0.8759\n",
      "Epoch 21/75, Train Loss: 1.1219, Test Loss: 0.8747\n",
      "Epoch 22/75, Train Loss: 1.0452, Test Loss: 0.8760\n",
      "Epoch 23/75, Train Loss: 1.0799, Test Loss: 0.8721\n",
      "Epoch 24/75, Train Loss: 1.1068, Test Loss: 0.8734\n",
      "Epoch 25/75, Train Loss: 1.0658, Test Loss: 0.8700\n",
      "Epoch 26/75, Train Loss: 1.0429, Test Loss: 0.8746\n",
      "Epoch 27/75, Train Loss: 1.0330, Test Loss: 0.8725\n",
      "Epoch 28/75, Train Loss: 1.0132, Test Loss: 0.8711\n",
      "Epoch 29/75, Train Loss: 1.0580, Test Loss: 0.8695\n",
      "Epoch 30/75, Train Loss: 1.0605, Test Loss: 0.8712\n",
      "Epoch 31/75, Train Loss: 1.0642, Test Loss: 0.8711\n",
      "Epoch 32/75, Train Loss: 1.0303, Test Loss: 0.8698\n",
      "Epoch 33/75, Train Loss: 0.9876, Test Loss: 0.8738\n",
      "Epoch 34/75, Train Loss: 1.0729, Test Loss: 0.8723\n",
      "Epoch 35/75, Train Loss: 1.0175, Test Loss: 0.8720\n",
      "Epoch 36/75, Train Loss: 1.0534, Test Loss: 0.8733\n",
      "Epoch 37/75, Train Loss: 1.0401, Test Loss: 0.8755\n",
      "Epoch 38/75, Train Loss: 1.0585, Test Loss: 0.8763\n",
      "Epoch 39/75, Train Loss: 1.0148, Test Loss: 0.8776\n",
      "Epoch 40/75, Train Loss: 1.0311, Test Loss: 0.8742\n",
      "Epoch 41/75, Train Loss: 1.0270, Test Loss: 0.8756\n",
      "Epoch 42/75, Train Loss: 1.0452, Test Loss: 0.8745\n",
      "Epoch 43/75, Train Loss: 1.0401, Test Loss: 0.8730\n",
      "Epoch 44/75, Train Loss: 1.0280, Test Loss: 0.8734\n",
      "Epoch 45/75, Train Loss: 1.0508, Test Loss: 0.8686\n",
      "Epoch 46/75, Train Loss: 1.0431, Test Loss: 0.8698\n",
      "Epoch 47/75, Train Loss: 1.0475, Test Loss: 0.8697\n",
      "Epoch 48/75, Train Loss: 1.0190, Test Loss: 0.8688\n",
      "Epoch 49/75, Train Loss: 1.0268, Test Loss: 0.8678\n",
      "Epoch 50/75, Train Loss: 1.0037, Test Loss: 0.8687\n",
      "Epoch 51/75, Train Loss: 1.0606, Test Loss: 0.8683\n",
      "Epoch 52/75, Train Loss: 1.0283, Test Loss: 0.8677\n",
      "Epoch 53/75, Train Loss: 1.0168, Test Loss: 0.8677\n",
      "Epoch 54/75, Train Loss: 1.0183, Test Loss: 0.8703\n",
      "Epoch 55/75, Train Loss: 1.0287, Test Loss: 0.8686\n",
      "Epoch 56/75, Train Loss: 1.0329, Test Loss: 0.8687\n",
      "Epoch 57/75, Train Loss: 1.0255, Test Loss: 0.8697\n",
      "Epoch 58/75, Train Loss: 1.0231, Test Loss: 0.8708\n",
      "Epoch 59/75, Train Loss: 1.0028, Test Loss: 0.8723\n",
      "Epoch 60/75, Train Loss: 1.0203, Test Loss: 0.8726\n",
      "Epoch 61/75, Train Loss: 1.0052, Test Loss: 0.8711\n",
      "Epoch 62/75, Train Loss: 0.9976, Test Loss: 0.8699\n",
      "Epoch 63/75, Train Loss: 1.0159, Test Loss: 0.8720\n",
      "Epoch 64/75, Train Loss: 0.9914, Test Loss: 0.8721\n",
      "Epoch 65/75, Train Loss: 1.0128, Test Loss: 0.8722\n",
      "Epoch 66/75, Train Loss: 1.0028, Test Loss: 0.8710\n",
      "Epoch 67/75, Train Loss: 1.0110, Test Loss: 0.8700\n",
      "Epoch 68/75, Train Loss: 1.0185, Test Loss: 0.8687\n",
      "Epoch 69/75, Train Loss: 1.0543, Test Loss: 0.8693\n",
      "Epoch 70/75, Train Loss: 1.0260, Test Loss: 0.8699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:29:11,762] Trial 121 finished with value: 0.871998518705368 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 108, 'layer_1_size': 143, 'layer_2_size': 208, 'layer_3_size': 246, 'layer_4_size': 88, 'layer_5_size': 116, 'dropout_rate': 0.3050516014108404, 'learning_rate': 9.705504054130578e-05, 'batch_size': 128, 'epochs': 75}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/75, Train Loss: 1.0110, Test Loss: 0.8700\n",
      "Epoch 72/75, Train Loss: 0.9818, Test Loss: 0.8708\n",
      "Epoch 73/75, Train Loss: 1.0202, Test Loss: 0.8713\n",
      "Epoch 74/75, Train Loss: 1.0628, Test Loss: 0.8703\n",
      "Epoch 75/75, Train Loss: 0.9944, Test Loss: 0.8720\n",
      "Epoch 1/82, Train Loss: 1.2199, Test Loss: 1.0288\n",
      "Epoch 2/82, Train Loss: 1.2671, Test Loss: 1.0356\n",
      "Epoch 3/82, Train Loss: 1.3088, Test Loss: 1.0405\n",
      "Epoch 4/82, Train Loss: 1.2158, Test Loss: 1.0452\n",
      "Epoch 5/82, Train Loss: 1.2066, Test Loss: 1.0462\n",
      "Epoch 6/82, Train Loss: 1.2343, Test Loss: 1.0498\n",
      "Epoch 7/82, Train Loss: 1.2354, Test Loss: 1.0452\n",
      "Epoch 8/82, Train Loss: 1.2253, Test Loss: 1.0488\n",
      "Epoch 9/82, Train Loss: 1.2241, Test Loss: 1.0521\n",
      "Epoch 10/82, Train Loss: 1.1461, Test Loss: 1.0514\n",
      "Epoch 11/82, Train Loss: 1.2545, Test Loss: 1.0481\n",
      "Epoch 12/82, Train Loss: 1.2442, Test Loss: 1.0477\n",
      "Epoch 13/82, Train Loss: 1.1982, Test Loss: 1.0455\n",
      "Epoch 14/82, Train Loss: 1.1996, Test Loss: 1.0436\n",
      "Epoch 15/82, Train Loss: 1.2100, Test Loss: 1.0446\n",
      "Epoch 16/82, Train Loss: 1.2075, Test Loss: 1.0422\n",
      "Epoch 17/82, Train Loss: 1.2386, Test Loss: 1.0417\n",
      "Epoch 18/82, Train Loss: 1.1997, Test Loss: 1.0428\n",
      "Epoch 19/82, Train Loss: 1.2194, Test Loss: 1.0419\n",
      "Epoch 20/82, Train Loss: 1.1550, Test Loss: 1.0446\n",
      "Epoch 21/82, Train Loss: 1.1665, Test Loss: 1.0446\n",
      "Epoch 22/82, Train Loss: 1.1584, Test Loss: 1.0431\n",
      "Epoch 23/82, Train Loss: 1.1823, Test Loss: 1.0411\n",
      "Epoch 24/82, Train Loss: 1.1738, Test Loss: 1.0421\n",
      "Epoch 25/82, Train Loss: 1.1921, Test Loss: 1.0415\n",
      "Epoch 26/82, Train Loss: 1.1755, Test Loss: 1.0430\n",
      "Epoch 27/82, Train Loss: 1.1990, Test Loss: 1.0438\n",
      "Epoch 28/82, Train Loss: 1.1872, Test Loss: 1.0434\n",
      "Epoch 29/82, Train Loss: 1.0772, Test Loss: 1.0425\n",
      "Epoch 30/82, Train Loss: 1.1540, Test Loss: 1.0414\n",
      "Epoch 31/82, Train Loss: 1.1559, Test Loss: 1.0398\n",
      "Epoch 32/82, Train Loss: 1.1380, Test Loss: 1.0395\n",
      "Epoch 33/82, Train Loss: 1.1208, Test Loss: 1.0410\n",
      "Epoch 34/82, Train Loss: 1.1672, Test Loss: 1.0406\n",
      "Epoch 35/82, Train Loss: 1.1453, Test Loss: 1.0426\n",
      "Epoch 36/82, Train Loss: 1.1401, Test Loss: 1.0425\n",
      "Epoch 37/82, Train Loss: 1.1328, Test Loss: 1.0383\n",
      "Epoch 38/82, Train Loss: 1.1678, Test Loss: 1.0404\n",
      "Epoch 39/82, Train Loss: 1.1091, Test Loss: 1.0418\n",
      "Epoch 40/82, Train Loss: 1.0789, Test Loss: 1.0396\n",
      "Epoch 41/82, Train Loss: 1.1517, Test Loss: 1.0378\n",
      "Epoch 42/82, Train Loss: 1.1449, Test Loss: 1.0376\n",
      "Epoch 43/82, Train Loss: 1.1444, Test Loss: 1.0382\n",
      "Epoch 44/82, Train Loss: 1.1425, Test Loss: 1.0397\n",
      "Epoch 45/82, Train Loss: 1.1115, Test Loss: 1.0404\n",
      "Epoch 46/82, Train Loss: 1.1473, Test Loss: 1.0404\n",
      "Epoch 47/82, Train Loss: 1.0797, Test Loss: 1.0397\n",
      "Epoch 48/82, Train Loss: 1.1778, Test Loss: 1.0394\n",
      "Epoch 49/82, Train Loss: 1.1171, Test Loss: 1.0410\n",
      "Epoch 50/82, Train Loss: 1.0807, Test Loss: 1.0402\n",
      "Epoch 51/82, Train Loss: 1.1273, Test Loss: 1.0419\n",
      "Epoch 52/82, Train Loss: 1.0784, Test Loss: 1.0405\n",
      "Epoch 53/82, Train Loss: 1.1218, Test Loss: 1.0397\n",
      "Epoch 54/82, Train Loss: 1.1061, Test Loss: 1.0385\n",
      "Epoch 55/82, Train Loss: 1.1778, Test Loss: 1.0374\n",
      "Epoch 56/82, Train Loss: 1.1305, Test Loss: 1.0382\n",
      "Epoch 57/82, Train Loss: 1.1657, Test Loss: 1.0395\n",
      "Epoch 58/82, Train Loss: 1.1440, Test Loss: 1.0378\n",
      "Epoch 59/82, Train Loss: 1.1310, Test Loss: 1.0355\n",
      "Epoch 60/82, Train Loss: 1.1706, Test Loss: 1.0356\n",
      "Epoch 61/82, Train Loss: 1.1246, Test Loss: 1.0355\n",
      "Epoch 62/82, Train Loss: 1.0854, Test Loss: 1.0361\n",
      "Epoch 63/82, Train Loss: 1.1360, Test Loss: 1.0381\n",
      "Epoch 64/82, Train Loss: 1.1045, Test Loss: 1.0377\n",
      "Epoch 65/82, Train Loss: 1.0864, Test Loss: 1.0385\n",
      "Epoch 66/82, Train Loss: 1.1050, Test Loss: 1.0378\n",
      "Epoch 67/82, Train Loss: 1.0856, Test Loss: 1.0370\n",
      "Epoch 68/82, Train Loss: 1.0773, Test Loss: 1.0381\n",
      "Epoch 69/82, Train Loss: 1.0696, Test Loss: 1.0364\n",
      "Epoch 70/82, Train Loss: 1.1095, Test Loss: 1.0340\n",
      "Epoch 71/82, Train Loss: 1.1211, Test Loss: 1.0362\n",
      "Epoch 72/82, Train Loss: 1.1227, Test Loss: 1.0371\n",
      "Epoch 73/82, Train Loss: 1.1159, Test Loss: 1.0349\n",
      "Epoch 74/82, Train Loss: 1.0927, Test Loss: 1.0345\n",
      "Epoch 75/82, Train Loss: 1.0601, Test Loss: 1.0329\n",
      "Epoch 76/82, Train Loss: 1.0472, Test Loss: 1.0335\n",
      "Epoch 77/82, Train Loss: 1.0880, Test Loss: 1.0334\n",
      "Epoch 78/82, Train Loss: 1.0882, Test Loss: 1.0332\n",
      "Epoch 79/82, Train Loss: 1.0926, Test Loss: 1.0350\n",
      "Epoch 80/82, Train Loss: 1.1057, Test Loss: 1.0355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:29:14,545] Trial 122 finished with value: 1.0361477732658386 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 102, 'layer_1_size': 216, 'layer_2_size': 103, 'layer_3_size': 254, 'layer_4_size': 76, 'layer_5_size': 131, 'dropout_rate': 0.4831740405288829, 'learning_rate': 7.93947674972373e-05, 'batch_size': 128, 'epochs': 82}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/82, Train Loss: 1.0612, Test Loss: 1.0364\n",
      "Epoch 82/82, Train Loss: 1.0903, Test Loss: 1.0361\n",
      "Epoch 1/77, Train Loss: 1.2417, Test Loss: 0.9152\n",
      "Epoch 2/77, Train Loss: 1.1918, Test Loss: 0.9348\n",
      "Epoch 3/77, Train Loss: 1.1345, Test Loss: 0.9407\n",
      "Epoch 4/77, Train Loss: 1.1711, Test Loss: 0.9406\n",
      "Epoch 5/77, Train Loss: 1.1701, Test Loss: 0.9366\n",
      "Epoch 6/77, Train Loss: 1.1182, Test Loss: 0.9340\n",
      "Epoch 7/77, Train Loss: 1.1305, Test Loss: 0.9300\n",
      "Epoch 8/77, Train Loss: 1.1099, Test Loss: 0.9239\n",
      "Epoch 9/77, Train Loss: 1.1161, Test Loss: 0.9218\n",
      "Epoch 10/77, Train Loss: 1.0865, Test Loss: 0.9179\n",
      "Epoch 11/77, Train Loss: 1.1139, Test Loss: 0.9205\n",
      "Epoch 12/77, Train Loss: 1.0897, Test Loss: 0.9158\n",
      "Epoch 13/77, Train Loss: 1.1211, Test Loss: 0.9093\n",
      "Epoch 14/77, Train Loss: 1.0955, Test Loss: 0.9071\n",
      "Epoch 15/77, Train Loss: 1.0932, Test Loss: 0.9089\n",
      "Epoch 16/77, Train Loss: 1.1218, Test Loss: 0.9104\n",
      "Epoch 17/77, Train Loss: 1.0841, Test Loss: 0.9122\n",
      "Epoch 18/77, Train Loss: 1.1335, Test Loss: 0.9098\n",
      "Epoch 19/77, Train Loss: 1.0574, Test Loss: 0.9114\n",
      "Epoch 20/77, Train Loss: 1.0967, Test Loss: 0.9126\n",
      "Epoch 21/77, Train Loss: 1.0871, Test Loss: 0.9132\n",
      "Epoch 22/77, Train Loss: 1.1168, Test Loss: 0.9116\n",
      "Epoch 23/77, Train Loss: 1.0923, Test Loss: 0.9115\n",
      "Epoch 24/77, Train Loss: 1.1107, Test Loss: 0.9117\n",
      "Epoch 25/77, Train Loss: 1.1008, Test Loss: 0.9131\n",
      "Epoch 26/77, Train Loss: 1.0546, Test Loss: 0.9137\n",
      "Epoch 27/77, Train Loss: 1.0662, Test Loss: 0.9126\n",
      "Epoch 28/77, Train Loss: 1.0767, Test Loss: 0.9140\n",
      "Epoch 29/77, Train Loss: 1.0946, Test Loss: 0.9128\n",
      "Epoch 30/77, Train Loss: 1.0812, Test Loss: 0.9124\n",
      "Epoch 31/77, Train Loss: 1.1013, Test Loss: 0.9130\n",
      "Epoch 32/77, Train Loss: 1.0657, Test Loss: 0.9143\n",
      "Epoch 33/77, Train Loss: 1.0665, Test Loss: 0.9121\n",
      "Epoch 34/77, Train Loss: 1.0979, Test Loss: 0.9070\n",
      "Epoch 35/77, Train Loss: 1.0910, Test Loss: 0.9033\n",
      "Epoch 36/77, Train Loss: 1.1047, Test Loss: 0.9048\n",
      "Epoch 37/77, Train Loss: 1.0712, Test Loss: 0.9051\n",
      "Epoch 38/77, Train Loss: 1.0545, Test Loss: 0.9057\n",
      "Epoch 39/77, Train Loss: 1.0438, Test Loss: 0.9052\n",
      "Epoch 40/77, Train Loss: 1.0536, Test Loss: 0.9029\n",
      "Epoch 41/77, Train Loss: 1.0459, Test Loss: 0.9027\n",
      "Epoch 42/77, Train Loss: 1.1029, Test Loss: 0.9027\n",
      "Epoch 43/77, Train Loss: 1.0764, Test Loss: 0.9033\n",
      "Epoch 44/77, Train Loss: 1.0641, Test Loss: 0.9025\n",
      "Epoch 45/77, Train Loss: 1.0738, Test Loss: 0.9022\n",
      "Epoch 46/77, Train Loss: 1.0545, Test Loss: 0.8997\n",
      "Epoch 47/77, Train Loss: 1.0601, Test Loss: 0.8986\n",
      "Epoch 48/77, Train Loss: 1.0660, Test Loss: 0.8962\n",
      "Epoch 49/77, Train Loss: 1.0880, Test Loss: 0.8971\n",
      "Epoch 50/77, Train Loss: 1.0635, Test Loss: 0.8973\n",
      "Epoch 51/77, Train Loss: 1.0974, Test Loss: 0.8984\n",
      "Epoch 52/77, Train Loss: 1.0363, Test Loss: 0.8979\n",
      "Epoch 53/77, Train Loss: 1.0831, Test Loss: 0.8980\n",
      "Epoch 54/77, Train Loss: 1.0294, Test Loss: 0.8949\n",
      "Epoch 55/77, Train Loss: 1.0532, Test Loss: 0.8930\n",
      "Epoch 56/77, Train Loss: 1.0636, Test Loss: 0.8934\n",
      "Epoch 57/77, Train Loss: 1.0653, Test Loss: 0.8938\n",
      "Epoch 58/77, Train Loss: 1.0529, Test Loss: 0.8961\n",
      "Epoch 59/77, Train Loss: 1.0560, Test Loss: 0.8963\n",
      "Epoch 60/77, Train Loss: 1.0423, Test Loss: 0.8942\n",
      "Epoch 61/77, Train Loss: 1.0461, Test Loss: 0.8921\n",
      "Epoch 62/77, Train Loss: 1.0535, Test Loss: 0.8921\n",
      "Epoch 63/77, Train Loss: 1.0462, Test Loss: 0.8931\n",
      "Epoch 64/77, Train Loss: 1.0451, Test Loss: 0.8940\n",
      "Epoch 65/77, Train Loss: 1.0536, Test Loss: 0.8941\n",
      "Epoch 66/77, Train Loss: 1.0805, Test Loss: 0.8970\n",
      "Epoch 67/77, Train Loss: 1.0325, Test Loss: 0.8993\n",
      "Epoch 68/77, Train Loss: 1.0586, Test Loss: 0.8971\n",
      "Epoch 69/77, Train Loss: 1.0620, Test Loss: 0.8983\n",
      "Epoch 70/77, Train Loss: 1.0572, Test Loss: 0.9002\n",
      "Epoch 71/77, Train Loss: 1.0378, Test Loss: 0.8984\n",
      "Epoch 72/77, Train Loss: 1.0629, Test Loss: 0.8988\n",
      "Epoch 73/77, Train Loss: 1.0460, Test Loss: 0.9012\n",
      "Epoch 74/77, Train Loss: 1.0559, Test Loss: 0.9033\n",
      "Epoch 75/77, Train Loss: 1.0571, Test Loss: 0.9024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:29:18,369] Trial 123 finished with value: 0.9027500152587891 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 138, 'layer_1_size': 228, 'layer_2_size': 201, 'layer_3_size': 237, 'layer_4_size': 109, 'layer_5_size': 245, 'dropout_rate': 0.2626661081625886, 'learning_rate': 0.00012229038260080544, 'batch_size': 128, 'epochs': 77}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/77, Train Loss: 1.0291, Test Loss: 0.9031\n",
      "Epoch 77/77, Train Loss: 1.0281, Test Loss: 0.9028\n",
      "Epoch 1/70, Train Loss: 1.1999, Test Loss: 1.0011\n",
      "Epoch 2/70, Train Loss: 1.1808, Test Loss: 1.0011\n",
      "Epoch 3/70, Train Loss: 1.2014, Test Loss: 1.0000\n",
      "Epoch 4/70, Train Loss: 1.1474, Test Loss: 1.0011\n",
      "Epoch 5/70, Train Loss: 1.2159, Test Loss: 1.0007\n",
      "Epoch 6/70, Train Loss: 1.1909, Test Loss: 0.9985\n",
      "Epoch 7/70, Train Loss: 1.1880, Test Loss: 0.9987\n",
      "Epoch 8/70, Train Loss: 1.2207, Test Loss: 0.9985\n",
      "Epoch 9/70, Train Loss: 1.1911, Test Loss: 1.0031\n",
      "Epoch 10/70, Train Loss: 1.1500, Test Loss: 1.0041\n",
      "Epoch 11/70, Train Loss: 1.1580, Test Loss: 1.0043\n",
      "Epoch 12/70, Train Loss: 1.1407, Test Loss: 1.0005\n",
      "Epoch 13/70, Train Loss: 1.1329, Test Loss: 0.9987\n",
      "Epoch 14/70, Train Loss: 1.1136, Test Loss: 1.0007\n",
      "Epoch 15/70, Train Loss: 1.1399, Test Loss: 0.9996\n",
      "Epoch 16/70, Train Loss: 1.1577, Test Loss: 0.9995\n",
      "Epoch 17/70, Train Loss: 1.1266, Test Loss: 1.0010\n",
      "Epoch 18/70, Train Loss: 1.1546, Test Loss: 1.0003\n",
      "Epoch 19/70, Train Loss: 1.1521, Test Loss: 1.0021\n",
      "Epoch 20/70, Train Loss: 1.1400, Test Loss: 1.0016\n",
      "Epoch 21/70, Train Loss: 1.1701, Test Loss: 1.0024\n",
      "Epoch 22/70, Train Loss: 1.1428, Test Loss: 1.0018\n",
      "Epoch 23/70, Train Loss: 1.1324, Test Loss: 1.0037\n",
      "Epoch 24/70, Train Loss: 1.1698, Test Loss: 1.0051\n",
      "Epoch 25/70, Train Loss: 1.1120, Test Loss: 1.0067\n",
      "Epoch 26/70, Train Loss: 1.1121, Test Loss: 1.0073\n",
      "Epoch 27/70, Train Loss: 1.0824, Test Loss: 1.0066\n",
      "Epoch 28/70, Train Loss: 1.0944, Test Loss: 1.0077\n",
      "Epoch 29/70, Train Loss: 1.1149, Test Loss: 1.0069\n",
      "Epoch 30/70, Train Loss: 1.1244, Test Loss: 1.0078\n",
      "Epoch 31/70, Train Loss: 1.1225, Test Loss: 1.0092\n",
      "Epoch 32/70, Train Loss: 1.1180, Test Loss: 1.0096\n",
      "Epoch 33/70, Train Loss: 1.1156, Test Loss: 1.0097\n",
      "Epoch 34/70, Train Loss: 1.0994, Test Loss: 1.0069\n",
      "Epoch 35/70, Train Loss: 1.0895, Test Loss: 1.0040\n",
      "Epoch 36/70, Train Loss: 1.1202, Test Loss: 1.0035\n",
      "Epoch 37/70, Train Loss: 1.0964, Test Loss: 1.0035\n",
      "Epoch 38/70, Train Loss: 1.1122, Test Loss: 1.0034\n",
      "Epoch 39/70, Train Loss: 1.0565, Test Loss: 1.0071\n",
      "Epoch 40/70, Train Loss: 1.1291, Test Loss: 1.0049\n",
      "Epoch 41/70, Train Loss: 1.0909, Test Loss: 1.0048\n",
      "Epoch 42/70, Train Loss: 1.1237, Test Loss: 1.0070\n",
      "Epoch 43/70, Train Loss: 1.1337, Test Loss: 1.0036\n",
      "Epoch 44/70, Train Loss: 1.1106, Test Loss: 1.0032\n",
      "Epoch 45/70, Train Loss: 1.1204, Test Loss: 1.0027\n",
      "Epoch 46/70, Train Loss: 1.0990, Test Loss: 1.0007\n",
      "Epoch 47/70, Train Loss: 1.1327, Test Loss: 1.0029\n",
      "Epoch 48/70, Train Loss: 1.1104, Test Loss: 1.0040\n",
      "Epoch 49/70, Train Loss: 1.1094, Test Loss: 1.0038\n",
      "Epoch 50/70, Train Loss: 1.0671, Test Loss: 1.0041\n",
      "Epoch 51/70, Train Loss: 1.0739, Test Loss: 1.0031\n",
      "Epoch 52/70, Train Loss: 1.0770, Test Loss: 1.0021\n",
      "Epoch 53/70, Train Loss: 1.1030, Test Loss: 1.0015\n",
      "Epoch 54/70, Train Loss: 1.1162, Test Loss: 1.0012\n",
      "Epoch 55/70, Train Loss: 1.0882, Test Loss: 1.0001\n",
      "Epoch 56/70, Train Loss: 1.0769, Test Loss: 1.0015\n",
      "Epoch 57/70, Train Loss: 1.0900, Test Loss: 1.0007\n",
      "Epoch 58/70, Train Loss: 1.1293, Test Loss: 1.0003\n",
      "Epoch 59/70, Train Loss: 1.0801, Test Loss: 1.0009\n",
      "Epoch 60/70, Train Loss: 1.0370, Test Loss: 1.0006\n",
      "Epoch 61/70, Train Loss: 1.1082, Test Loss: 0.9991\n",
      "Epoch 62/70, Train Loss: 1.1151, Test Loss: 0.9997\n",
      "Epoch 63/70, Train Loss: 1.1061, Test Loss: 1.0004\n",
      "Epoch 64/70, Train Loss: 1.0827, Test Loss: 0.9985\n",
      "Epoch 65/70, Train Loss: 1.1328, Test Loss: 0.9999\n",
      "Epoch 66/70, Train Loss: 1.0668, Test Loss: 0.9984\n",
      "Epoch 67/70, Train Loss: 1.0830, Test Loss: 0.9966\n",
      "Epoch 68/70, Train Loss: 1.0581, Test Loss: 0.9978\n",
      "Epoch 69/70, Train Loss: 1.0821, Test Loss: 0.9976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:29:21,623] Trial 124 finished with value: 0.9970135390758514 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 144, 'layer_1_size': 181, 'layer_2_size': 217, 'layer_3_size': 232, 'layer_4_size': 170, 'layer_5_size': 113, 'layer_6_size': 164, 'dropout_rate': 0.38904549364187974, 'learning_rate': 6.449250025249285e-05, 'batch_size': 128, 'epochs': 70}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/70, Train Loss: 1.0709, Test Loss: 0.9970\n",
      "Epoch 1/10, Train Loss: 1.2274, Test Loss: 1.2128\n",
      "Epoch 2/10, Train Loss: 1.1888, Test Loss: 1.2276\n",
      "Epoch 3/10, Train Loss: 1.1796, Test Loss: 1.2159\n",
      "Epoch 4/10, Train Loss: 1.1709, Test Loss: 1.2081\n",
      "Epoch 5/10, Train Loss: 1.1548, Test Loss: 1.1943\n",
      "Epoch 6/10, Train Loss: 1.1397, Test Loss: 1.2063\n",
      "Epoch 7/10, Train Loss: 1.1677, Test Loss: 1.1981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:29:22,501] Trial 125 finished with value: 1.202903002500534 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 124, 'layer_1_size': 223, 'layer_2_size': 226, 'layer_3_size': 72, 'layer_4_size': 188, 'layer_5_size': 97, 'layer_6_size': 135, 'dropout_rate': 0.24691480414028744, 'learning_rate': 0.00014720039448659386, 'batch_size': 64, 'epochs': 10}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Loss: 1.1244, Test Loss: 1.2068\n",
      "Epoch 9/10, Train Loss: 1.1640, Test Loss: 1.2025\n",
      "Epoch 10/10, Train Loss: 1.1197, Test Loss: 1.2029\n",
      "Epoch 1/94, Train Loss: 1.1537, Test Loss: 1.0429\n",
      "Epoch 2/94, Train Loss: 1.1772, Test Loss: 1.0506\n",
      "Epoch 3/94, Train Loss: 1.1342, Test Loss: 1.0466\n",
      "Epoch 4/94, Train Loss: 1.1056, Test Loss: 1.0494\n",
      "Epoch 5/94, Train Loss: 1.1277, Test Loss: 1.0464\n",
      "Epoch 6/94, Train Loss: 1.0800, Test Loss: 1.0459\n",
      "Epoch 7/94, Train Loss: 1.0584, Test Loss: 1.0436\n",
      "Epoch 8/94, Train Loss: 1.0646, Test Loss: 1.0457\n",
      "Epoch 9/94, Train Loss: 1.0766, Test Loss: 1.0471\n",
      "Epoch 10/94, Train Loss: 1.0853, Test Loss: 1.0496\n",
      "Epoch 11/94, Train Loss: 1.0878, Test Loss: 1.0483\n",
      "Epoch 12/94, Train Loss: 1.1030, Test Loss: 1.0475\n",
      "Epoch 13/94, Train Loss: 1.0857, Test Loss: 1.0467\n",
      "Epoch 14/94, Train Loss: 1.0833, Test Loss: 1.0450\n",
      "Epoch 15/94, Train Loss: 1.0389, Test Loss: 1.0412\n",
      "Epoch 16/94, Train Loss: 1.0517, Test Loss: 1.0442\n",
      "Epoch 17/94, Train Loss: 1.0862, Test Loss: 1.0443\n",
      "Epoch 18/94, Train Loss: 1.0626, Test Loss: 1.0448\n",
      "Epoch 19/94, Train Loss: 1.0700, Test Loss: 1.0469\n",
      "Epoch 20/94, Train Loss: 1.0573, Test Loss: 1.0463\n",
      "Epoch 21/94, Train Loss: 1.0674, Test Loss: 1.0454\n",
      "Epoch 22/94, Train Loss: 1.0049, Test Loss: 1.0451\n",
      "Epoch 23/94, Train Loss: 1.0451, Test Loss: 1.0448\n",
      "Epoch 24/94, Train Loss: 1.0750, Test Loss: 1.0451\n",
      "Epoch 25/94, Train Loss: 1.0336, Test Loss: 1.0455\n",
      "Epoch 26/94, Train Loss: 1.0183, Test Loss: 1.0459\n",
      "Epoch 27/94, Train Loss: 1.0425, Test Loss: 1.0441\n",
      "Epoch 28/94, Train Loss: 1.0108, Test Loss: 1.0446\n",
      "Epoch 29/94, Train Loss: 1.0421, Test Loss: 1.0455\n",
      "Epoch 30/94, Train Loss: 1.0153, Test Loss: 1.0454\n",
      "Epoch 31/94, Train Loss: 1.0184, Test Loss: 1.0432\n",
      "Epoch 32/94, Train Loss: 1.0437, Test Loss: 1.0461\n",
      "Epoch 33/94, Train Loss: 1.0090, Test Loss: 1.0448\n",
      "Epoch 34/94, Train Loss: 1.0270, Test Loss: 1.0449\n",
      "Epoch 35/94, Train Loss: 0.9810, Test Loss: 1.0438\n",
      "Epoch 36/94, Train Loss: 1.0102, Test Loss: 1.0479\n",
      "Epoch 37/94, Train Loss: 1.0276, Test Loss: 1.0481\n",
      "Epoch 38/94, Train Loss: 1.0087, Test Loss: 1.0476\n",
      "Epoch 39/94, Train Loss: 1.0033, Test Loss: 1.0477\n",
      "Epoch 40/94, Train Loss: 1.0119, Test Loss: 1.0455\n",
      "Epoch 41/94, Train Loss: 1.0035, Test Loss: 1.0435\n",
      "Epoch 42/94, Train Loss: 1.0104, Test Loss: 1.0427\n",
      "Epoch 43/94, Train Loss: 1.0005, Test Loss: 1.0442\n",
      "Epoch 44/94, Train Loss: 1.0283, Test Loss: 1.0427\n",
      "Epoch 45/94, Train Loss: 0.9960, Test Loss: 1.0429\n",
      "Epoch 46/94, Train Loss: 1.0003, Test Loss: 1.0418\n",
      "Epoch 47/94, Train Loss: 1.0095, Test Loss: 1.0427\n",
      "Epoch 48/94, Train Loss: 1.0109, Test Loss: 1.0431\n",
      "Epoch 49/94, Train Loss: 0.9962, Test Loss: 1.0419\n",
      "Epoch 50/94, Train Loss: 1.0054, Test Loss: 1.0416\n",
      "Epoch 51/94, Train Loss: 0.9902, Test Loss: 1.0423\n",
      "Epoch 52/94, Train Loss: 1.0154, Test Loss: 1.0410\n",
      "Epoch 53/94, Train Loss: 0.9917, Test Loss: 1.0415\n",
      "Epoch 54/94, Train Loss: 0.9827, Test Loss: 1.0416\n",
      "Epoch 55/94, Train Loss: 0.9655, Test Loss: 1.0390\n",
      "Epoch 56/94, Train Loss: 0.9956, Test Loss: 1.0402\n",
      "Epoch 57/94, Train Loss: 0.9792, Test Loss: 1.0400\n",
      "Epoch 58/94, Train Loss: 0.9807, Test Loss: 1.0397\n",
      "Epoch 59/94, Train Loss: 0.9831, Test Loss: 1.0376\n",
      "Epoch 60/94, Train Loss: 1.0084, Test Loss: 1.0381\n",
      "Epoch 61/94, Train Loss: 0.9867, Test Loss: 1.0380\n",
      "Epoch 62/94, Train Loss: 0.9707, Test Loss: 1.0369\n",
      "Epoch 63/94, Train Loss: 0.9744, Test Loss: 1.0361\n",
      "Epoch 64/94, Train Loss: 0.9935, Test Loss: 1.0351\n",
      "Epoch 65/94, Train Loss: 0.9841, Test Loss: 1.0377\n",
      "Epoch 66/94, Train Loss: 0.9925, Test Loss: 1.0375\n",
      "Epoch 67/94, Train Loss: 0.9962, Test Loss: 1.0379\n",
      "Epoch 68/94, Train Loss: 0.9901, Test Loss: 1.0392\n",
      "Epoch 69/94, Train Loss: 1.0028, Test Loss: 1.0404\n",
      "Epoch 70/94, Train Loss: 0.9991, Test Loss: 1.0402\n",
      "Epoch 71/94, Train Loss: 0.9838, Test Loss: 1.0402\n",
      "Epoch 72/94, Train Loss: 0.9722, Test Loss: 1.0397\n",
      "Epoch 73/94, Train Loss: 0.9863, Test Loss: 1.0393\n",
      "Epoch 74/94, Train Loss: 0.9885, Test Loss: 1.0385\n",
      "Epoch 75/94, Train Loss: 0.9666, Test Loss: 1.0402\n",
      "Epoch 76/94, Train Loss: 0.9954, Test Loss: 1.0398\n",
      "Epoch 77/94, Train Loss: 0.9814, Test Loss: 1.0411\n",
      "Epoch 78/94, Train Loss: 0.9833, Test Loss: 1.0377\n",
      "Epoch 79/94, Train Loss: 0.9655, Test Loss: 1.0384\n",
      "Epoch 80/94, Train Loss: 0.9906, Test Loss: 1.0374\n",
      "Epoch 81/94, Train Loss: 0.9766, Test Loss: 1.0363\n",
      "Epoch 82/94, Train Loss: 0.9749, Test Loss: 1.0364\n",
      "Epoch 83/94, Train Loss: 0.9800, Test Loss: 1.0364\n",
      "Epoch 84/94, Train Loss: 0.9771, Test Loss: 1.0373\n",
      "Epoch 85/94, Train Loss: 0.9837, Test Loss: 1.0371\n",
      "Epoch 86/94, Train Loss: 0.9700, Test Loss: 1.0359\n",
      "Epoch 87/94, Train Loss: 0.9703, Test Loss: 1.0358\n",
      "Epoch 88/94, Train Loss: 0.9522, Test Loss: 1.0362\n",
      "Epoch 89/94, Train Loss: 0.9728, Test Loss: 1.0360\n",
      "Epoch 90/94, Train Loss: 0.9787, Test Loss: 1.0349\n",
      "Epoch 91/94, Train Loss: 0.9731, Test Loss: 1.0353\n",
      "Epoch 92/94, Train Loss: 0.9700, Test Loss: 1.0349\n",
      "Epoch 93/94, Train Loss: 0.9594, Test Loss: 1.0358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:29:29,143] Trial 126 finished with value: 1.0351300537586212 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 114, 'layer_1_size': 168, 'layer_2_size': 188, 'layer_3_size': 137, 'layer_4_size': 200, 'layer_5_size': 121, 'dropout_rate': 0.4233525599175807, 'learning_rate': 0.0002221275213344235, 'batch_size': 64, 'epochs': 94}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/94, Train Loss: 0.9627, Test Loss: 1.0351\n",
      "Epoch 1/52, Train Loss: 1.1769, Test Loss: 1.1223\n",
      "Epoch 2/52, Train Loss: 1.1279, Test Loss: 1.1326\n",
      "Epoch 3/52, Train Loss: 1.1683, Test Loss: 1.1147\n",
      "Epoch 4/52, Train Loss: 1.1907, Test Loss: 1.1242\n",
      "Epoch 5/52, Train Loss: 1.1229, Test Loss: 1.1187\n",
      "Epoch 6/52, Train Loss: 1.1559, Test Loss: 1.1227\n",
      "Epoch 7/52, Train Loss: 1.0926, Test Loss: 1.1206\n",
      "Epoch 8/52, Train Loss: 1.1416, Test Loss: 1.1234\n",
      "Epoch 9/52, Train Loss: 1.0762, Test Loss: 1.1300\n",
      "Epoch 10/52, Train Loss: 1.0496, Test Loss: 1.1257\n",
      "Epoch 11/52, Train Loss: 1.1197, Test Loss: 1.1250\n",
      "Epoch 12/52, Train Loss: 1.0632, Test Loss: 1.1255\n",
      "Epoch 13/52, Train Loss: 1.0824, Test Loss: 1.1241\n",
      "Epoch 14/52, Train Loss: 1.0799, Test Loss: 1.1147\n",
      "Epoch 15/52, Train Loss: 1.0843, Test Loss: 1.1161\n",
      "Epoch 16/52, Train Loss: 1.0771, Test Loss: 1.1217\n",
      "Epoch 17/52, Train Loss: 1.0587, Test Loss: 1.1306\n",
      "Epoch 18/52, Train Loss: 1.0543, Test Loss: 1.1282\n",
      "Epoch 19/52, Train Loss: 1.1036, Test Loss: 1.1235\n",
      "Epoch 20/52, Train Loss: 1.0543, Test Loss: 1.1192\n",
      "Epoch 21/52, Train Loss: 1.0838, Test Loss: 1.1270\n",
      "Epoch 22/52, Train Loss: 1.0740, Test Loss: 1.1235\n",
      "Epoch 23/52, Train Loss: 1.0754, Test Loss: 1.1212\n",
      "Epoch 24/52, Train Loss: 1.0739, Test Loss: 1.1194\n",
      "Epoch 25/52, Train Loss: 1.0598, Test Loss: 1.1162\n",
      "Epoch 26/52, Train Loss: 1.0867, Test Loss: 1.1152\n",
      "Epoch 27/52, Train Loss: 1.0486, Test Loss: 1.1153\n",
      "Epoch 28/52, Train Loss: 1.0874, Test Loss: 1.1201\n",
      "Epoch 29/52, Train Loss: 1.0986, Test Loss: 1.1194\n",
      "Epoch 30/52, Train Loss: 1.0571, Test Loss: 1.1220\n",
      "Epoch 31/52, Train Loss: 1.0400, Test Loss: 1.1227\n",
      "Epoch 32/52, Train Loss: 1.0817, Test Loss: 1.1191\n",
      "Epoch 33/52, Train Loss: 1.0490, Test Loss: 1.1175\n",
      "Epoch 34/52, Train Loss: 1.0274, Test Loss: 1.1147\n",
      "Epoch 35/52, Train Loss: 1.0274, Test Loss: 1.1127\n",
      "Epoch 36/52, Train Loss: 1.0627, Test Loss: 1.1124\n",
      "Epoch 37/52, Train Loss: 1.0392, Test Loss: 1.1107\n",
      "Epoch 38/52, Train Loss: 1.0470, Test Loss: 1.1181\n",
      "Epoch 39/52, Train Loss: 1.0487, Test Loss: 1.1155\n",
      "Epoch 40/52, Train Loss: 1.0348, Test Loss: 1.1129\n",
      "Epoch 41/52, Train Loss: 1.0623, Test Loss: 1.1177\n",
      "Epoch 42/52, Train Loss: 1.0264, Test Loss: 1.1149\n",
      "Epoch 43/52, Train Loss: 1.0362, Test Loss: 1.1183\n",
      "Epoch 44/52, Train Loss: 1.0313, Test Loss: 1.1213\n",
      "Epoch 45/52, Train Loss: 1.0437, Test Loss: 1.1255\n",
      "Epoch 46/52, Train Loss: 1.0555, Test Loss: 1.1215\n",
      "Epoch 47/52, Train Loss: 1.0474, Test Loss: 1.1300\n",
      "Epoch 48/52, Train Loss: 1.0351, Test Loss: 1.1324\n",
      "Epoch 49/52, Train Loss: 1.0175, Test Loss: 1.1301\n",
      "Epoch 50/52, Train Loss: 1.0257, Test Loss: 1.1237\n",
      "Epoch 51/52, Train Loss: 1.0272, Test Loss: 1.1271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:29:34,610] Trial 127 finished with value: 1.129079893231392 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 168, 'layer_1_size': 187, 'layer_2_size': 245, 'layer_3_size': 256, 'layer_4_size': 219, 'layer_5_size': 232, 'layer_6_size': 113, 'dropout_rate': 0.2859529384866859, 'learning_rate': 0.00010532265221641554, 'batch_size': 64, 'epochs': 52}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/52, Train Loss: 1.0404, Test Loss: 1.1291\n",
      "Epoch 1/66, Train Loss: 1.2739, Test Loss: 0.8874\n",
      "Epoch 2/66, Train Loss: 1.1960, Test Loss: 0.8896\n",
      "Epoch 3/66, Train Loss: 1.2227, Test Loss: 0.8946\n",
      "Epoch 4/66, Train Loss: 1.2071, Test Loss: 0.8946\n",
      "Epoch 5/66, Train Loss: 1.1970, Test Loss: 0.8938\n",
      "Epoch 6/66, Train Loss: 1.1580, Test Loss: 0.8948\n",
      "Epoch 7/66, Train Loss: 1.1448, Test Loss: 0.8964\n",
      "Epoch 8/66, Train Loss: 1.1162, Test Loss: 0.8925\n",
      "Epoch 9/66, Train Loss: 1.0866, Test Loss: 0.8935\n",
      "Epoch 10/66, Train Loss: 1.1462, Test Loss: 0.8915\n",
      "Epoch 11/66, Train Loss: 1.1110, Test Loss: 0.8909\n",
      "Epoch 12/66, Train Loss: 1.1940, Test Loss: 0.8923\n",
      "Epoch 13/66, Train Loss: 1.1679, Test Loss: 0.8968\n",
      "Epoch 14/66, Train Loss: 1.1217, Test Loss: 0.8953\n",
      "Epoch 15/66, Train Loss: 1.1456, Test Loss: 0.8939\n",
      "Epoch 16/66, Train Loss: 1.1308, Test Loss: 0.8917\n",
      "Epoch 17/66, Train Loss: 1.0805, Test Loss: 0.8943\n",
      "Epoch 18/66, Train Loss: 1.1370, Test Loss: 0.8935\n",
      "Epoch 19/66, Train Loss: 1.0730, Test Loss: 0.8961\n",
      "Epoch 20/66, Train Loss: 1.1340, Test Loss: 0.8975\n",
      "Epoch 21/66, Train Loss: 1.1110, Test Loss: 0.8961\n",
      "Epoch 22/66, Train Loss: 1.0902, Test Loss: 0.8930\n",
      "Epoch 23/66, Train Loss: 1.0915, Test Loss: 0.8956\n",
      "Epoch 24/66, Train Loss: 1.1369, Test Loss: 0.8966\n",
      "Epoch 25/66, Train Loss: 1.1240, Test Loss: 0.8927\n",
      "Epoch 26/66, Train Loss: 1.0996, Test Loss: 0.8923\n",
      "Epoch 27/66, Train Loss: 1.1025, Test Loss: 0.8922\n",
      "Epoch 28/66, Train Loss: 1.0412, Test Loss: 0.8894\n",
      "Epoch 29/66, Train Loss: 1.1124, Test Loss: 0.8873\n",
      "Epoch 30/66, Train Loss: 1.0806, Test Loss: 0.8903\n",
      "Epoch 31/66, Train Loss: 1.1223, Test Loss: 0.8923\n",
      "Epoch 32/66, Train Loss: 1.1030, Test Loss: 0.8908\n",
      "Epoch 33/66, Train Loss: 1.0856, Test Loss: 0.8882\n",
      "Epoch 34/66, Train Loss: 1.1176, Test Loss: 0.8882\n",
      "Epoch 35/66, Train Loss: 1.0923, Test Loss: 0.8876\n",
      "Epoch 36/66, Train Loss: 1.1176, Test Loss: 0.8898\n",
      "Epoch 37/66, Train Loss: 1.0595, Test Loss: 0.8855\n",
      "Epoch 38/66, Train Loss: 1.0845, Test Loss: 0.8920\n",
      "Epoch 39/66, Train Loss: 1.0420, Test Loss: 0.8928\n",
      "Epoch 40/66, Train Loss: 1.0766, Test Loss: 0.8937\n",
      "Epoch 41/66, Train Loss: 1.0466, Test Loss: 0.8928\n",
      "Epoch 42/66, Train Loss: 1.0848, Test Loss: 0.8907\n",
      "Epoch 43/66, Train Loss: 1.0850, Test Loss: 0.8916\n",
      "Epoch 44/66, Train Loss: 1.0934, Test Loss: 0.8876\n",
      "Epoch 45/66, Train Loss: 1.0939, Test Loss: 0.8877\n",
      "Epoch 46/66, Train Loss: 1.0764, Test Loss: 0.8879\n",
      "Epoch 47/66, Train Loss: 1.0869, Test Loss: 0.8876\n",
      "Epoch 48/66, Train Loss: 1.0634, Test Loss: 0.8870\n",
      "Epoch 49/66, Train Loss: 1.0869, Test Loss: 0.8875\n",
      "Epoch 50/66, Train Loss: 1.0638, Test Loss: 0.8878\n",
      "Epoch 51/66, Train Loss: 1.0784, Test Loss: 0.8881\n",
      "Epoch 52/66, Train Loss: 1.0906, Test Loss: 0.8868\n",
      "Epoch 53/66, Train Loss: 1.0865, Test Loss: 0.8883\n",
      "Epoch 54/66, Train Loss: 1.0610, Test Loss: 0.8887\n",
      "Epoch 55/66, Train Loss: 1.0957, Test Loss: 0.8890\n",
      "Epoch 56/66, Train Loss: 1.0525, Test Loss: 0.8884\n",
      "Epoch 57/66, Train Loss: 1.0671, Test Loss: 0.8867\n",
      "Epoch 58/66, Train Loss: 1.0390, Test Loss: 0.8849\n",
      "Epoch 59/66, Train Loss: 1.0677, Test Loss: 0.8890\n",
      "Epoch 60/66, Train Loss: 1.0578, Test Loss: 0.8891\n",
      "Epoch 61/66, Train Loss: 1.0584, Test Loss: 0.8866\n",
      "Epoch 62/66, Train Loss: 1.0720, Test Loss: 0.8831\n",
      "Epoch 63/66, Train Loss: 1.0730, Test Loss: 0.8860\n",
      "Epoch 64/66, Train Loss: 1.1130, Test Loss: 0.8861\n",
      "Epoch 65/66, Train Loss: 1.0406, Test Loss: 0.8865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:29:39,694] Trial 128 finished with value: 0.8867997080087662 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 133, 'layer_1_size': 147, 'layer_2_size': 193, 'layer_3_size': 238, 'layer_4_size': 95, 'layer_5_size': 209, 'dropout_rate': 0.36497115831832677, 'learning_rate': 7.342975759058573e-05, 'batch_size': 64, 'epochs': 66}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/66, Train Loss: 1.0681, Test Loss: 0.8868\n",
      "Epoch 1/87, Train Loss: 1.1760, Test Loss: 0.9766\n",
      "Epoch 2/87, Train Loss: 1.1507, Test Loss: 0.9872\n",
      "Epoch 3/87, Train Loss: 1.1616, Test Loss: 0.9964\n",
      "Epoch 4/87, Train Loss: 1.0977, Test Loss: 0.9980\n",
      "Epoch 5/87, Train Loss: 1.1216, Test Loss: 0.9966\n",
      "Epoch 6/87, Train Loss: 1.1322, Test Loss: 0.9928\n",
      "Epoch 7/87, Train Loss: 1.1778, Test Loss: 0.9906\n",
      "Epoch 8/87, Train Loss: 1.0533, Test Loss: 0.9864\n",
      "Epoch 9/87, Train Loss: 1.0770, Test Loss: 0.9854\n",
      "Epoch 10/87, Train Loss: 1.0900, Test Loss: 0.9843\n",
      "Epoch 11/87, Train Loss: 1.0792, Test Loss: 0.9848\n",
      "Epoch 12/87, Train Loss: 1.0679, Test Loss: 0.9850\n",
      "Epoch 13/87, Train Loss: 1.0537, Test Loss: 0.9841\n",
      "Epoch 14/87, Train Loss: 1.0833, Test Loss: 0.9824\n",
      "Epoch 15/87, Train Loss: 1.0624, Test Loss: 0.9791\n",
      "Epoch 16/87, Train Loss: 1.0946, Test Loss: 0.9806\n",
      "Epoch 17/87, Train Loss: 1.0772, Test Loss: 0.9792\n",
      "Epoch 18/87, Train Loss: 1.0386, Test Loss: 0.9791\n",
      "Epoch 19/87, Train Loss: 1.0768, Test Loss: 0.9784\n",
      "Epoch 20/87, Train Loss: 1.0646, Test Loss: 0.9774\n",
      "Epoch 21/87, Train Loss: 1.0486, Test Loss: 0.9754\n",
      "Epoch 22/87, Train Loss: 1.0737, Test Loss: 0.9733\n",
      "Epoch 23/87, Train Loss: 1.0594, Test Loss: 0.9734\n",
      "Epoch 24/87, Train Loss: 1.0299, Test Loss: 0.9740\n",
      "Epoch 25/87, Train Loss: 1.0477, Test Loss: 0.9772\n",
      "Epoch 26/87, Train Loss: 1.0377, Test Loss: 0.9746\n",
      "Epoch 27/87, Train Loss: 1.0328, Test Loss: 0.9743\n",
      "Epoch 28/87, Train Loss: 1.0343, Test Loss: 0.9735\n",
      "Epoch 29/87, Train Loss: 1.0389, Test Loss: 0.9724\n",
      "Epoch 30/87, Train Loss: 1.0149, Test Loss: 0.9710\n",
      "Epoch 31/87, Train Loss: 1.0403, Test Loss: 0.9696\n",
      "Epoch 32/87, Train Loss: 1.0549, Test Loss: 0.9700\n",
      "Epoch 33/87, Train Loss: 1.0642, Test Loss: 0.9683\n",
      "Epoch 34/87, Train Loss: 0.9996, Test Loss: 0.9685\n",
      "Epoch 35/87, Train Loss: 1.0394, Test Loss: 0.9699\n",
      "Epoch 36/87, Train Loss: 1.0438, Test Loss: 0.9690\n",
      "Epoch 37/87, Train Loss: 1.0527, Test Loss: 0.9683\n",
      "Epoch 38/87, Train Loss: 1.0510, Test Loss: 0.9698\n",
      "Epoch 39/87, Train Loss: 1.0431, Test Loss: 0.9681\n",
      "Epoch 40/87, Train Loss: 1.0144, Test Loss: 0.9680\n",
      "Epoch 41/87, Train Loss: 1.0335, Test Loss: 0.9695\n",
      "Epoch 42/87, Train Loss: 1.0042, Test Loss: 0.9711\n",
      "Epoch 43/87, Train Loss: 1.0206, Test Loss: 0.9701\n",
      "Epoch 44/87, Train Loss: 0.9985, Test Loss: 0.9705\n",
      "Epoch 45/87, Train Loss: 1.0395, Test Loss: 0.9719\n",
      "Epoch 46/87, Train Loss: 1.0401, Test Loss: 0.9701\n",
      "Epoch 47/87, Train Loss: 1.0327, Test Loss: 0.9733\n",
      "Epoch 48/87, Train Loss: 1.0154, Test Loss: 0.9743\n",
      "Epoch 49/87, Train Loss: 1.0213, Test Loss: 0.9718\n",
      "Epoch 50/87, Train Loss: 1.0062, Test Loss: 0.9722\n",
      "Epoch 51/87, Train Loss: 1.0082, Test Loss: 0.9706\n",
      "Epoch 52/87, Train Loss: 1.0413, Test Loss: 0.9718\n",
      "Epoch 53/87, Train Loss: 1.0406, Test Loss: 0.9734\n",
      "Epoch 54/87, Train Loss: 1.0443, Test Loss: 0.9739\n",
      "Epoch 55/87, Train Loss: 1.0003, Test Loss: 0.9730\n",
      "Epoch 56/87, Train Loss: 0.9902, Test Loss: 0.9729\n",
      "Epoch 57/87, Train Loss: 1.0100, Test Loss: 0.9734\n",
      "Epoch 58/87, Train Loss: 1.0234, Test Loss: 0.9761\n",
      "Epoch 59/87, Train Loss: 1.0266, Test Loss: 0.9766\n",
      "Epoch 60/87, Train Loss: 0.9938, Test Loss: 0.9777\n",
      "Epoch 61/87, Train Loss: 1.0084, Test Loss: 0.9777\n",
      "Epoch 62/87, Train Loss: 1.0192, Test Loss: 0.9770\n",
      "Epoch 63/87, Train Loss: 0.9928, Test Loss: 0.9769\n",
      "Epoch 64/87, Train Loss: 1.0092, Test Loss: 0.9745\n",
      "Epoch 65/87, Train Loss: 0.9925, Test Loss: 0.9745\n",
      "Epoch 66/87, Train Loss: 0.9788, Test Loss: 0.9758\n",
      "Epoch 67/87, Train Loss: 1.0070, Test Loss: 0.9752\n",
      "Epoch 68/87, Train Loss: 0.9970, Test Loss: 0.9762\n",
      "Epoch 69/87, Train Loss: 0.9778, Test Loss: 0.9744\n",
      "Epoch 70/87, Train Loss: 1.0128, Test Loss: 0.9745\n",
      "Epoch 71/87, Train Loss: 0.9757, Test Loss: 0.9730\n",
      "Epoch 72/87, Train Loss: 0.9908, Test Loss: 0.9729\n",
      "Epoch 73/87, Train Loss: 1.0047, Test Loss: 0.9747\n",
      "Epoch 74/87, Train Loss: 1.0116, Test Loss: 0.9753\n",
      "Epoch 75/87, Train Loss: 0.9891, Test Loss: 0.9736\n",
      "Epoch 76/87, Train Loss: 1.0076, Test Loss: 0.9744\n",
      "Epoch 77/87, Train Loss: 0.9724, Test Loss: 0.9736\n",
      "Epoch 78/87, Train Loss: 1.0057, Test Loss: 0.9736\n",
      "Epoch 79/87, Train Loss: 0.9975, Test Loss: 0.9731\n",
      "Epoch 80/87, Train Loss: 0.9988, Test Loss: 0.9735\n",
      "Epoch 81/87, Train Loss: 0.9993, Test Loss: 0.9736\n",
      "Epoch 82/87, Train Loss: 1.0129, Test Loss: 0.9743\n",
      "Epoch 83/87, Train Loss: 0.9988, Test Loss: 0.9729\n",
      "Epoch 84/87, Train Loss: 1.0154, Test Loss: 0.9745\n",
      "Epoch 85/87, Train Loss: 0.9968, Test Loss: 0.9729\n",
      "Epoch 86/87, Train Loss: 1.0185, Test Loss: 0.9734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:29:44,775] Trial 129 finished with value: 0.9733547866344452 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 158, 'layer_1_size': 197, 'layer_2_size': 212, 'layer_3_size': 115, 'layer_4_size': 225, 'layer_5_size': 172, 'layer_6_size': 40, 'layer_7_size': 53, 'dropout_rate': 0.35397560608673884, 'learning_rate': 0.00017925601573614045, 'batch_size': 128, 'epochs': 87}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/87, Train Loss: 0.9740, Test Loss: 0.9734\n",
      "Epoch 1/96, Train Loss: 1.2462, Test Loss: 0.8998\n",
      "Epoch 2/96, Train Loss: 1.2406, Test Loss: 0.9082\n",
      "Epoch 3/96, Train Loss: 1.2824, Test Loss: 0.9137\n",
      "Epoch 4/96, Train Loss: 1.2123, Test Loss: 0.9162\n",
      "Epoch 5/96, Train Loss: 1.2835, Test Loss: 0.9123\n",
      "Epoch 6/96, Train Loss: 1.2352, Test Loss: 0.9095\n",
      "Epoch 7/96, Train Loss: 1.2558, Test Loss: 0.9101\n",
      "Epoch 8/96, Train Loss: 1.1867, Test Loss: 0.9090\n",
      "Epoch 9/96, Train Loss: 1.2398, Test Loss: 0.9112\n",
      "Epoch 10/96, Train Loss: 1.1752, Test Loss: 0.9090\n",
      "Epoch 11/96, Train Loss: 1.2810, Test Loss: 0.9113\n",
      "Epoch 12/96, Train Loss: 1.1876, Test Loss: 0.9069\n",
      "Epoch 13/96, Train Loss: 1.2605, Test Loss: 0.9089\n",
      "Epoch 14/96, Train Loss: 1.2256, Test Loss: 0.9091\n",
      "Epoch 15/96, Train Loss: 1.2435, Test Loss: 0.9070\n",
      "Epoch 16/96, Train Loss: 1.1660, Test Loss: 0.9093\n",
      "Epoch 17/96, Train Loss: 1.2248, Test Loss: 0.9050\n",
      "Epoch 18/96, Train Loss: 1.2309, Test Loss: 0.9039\n",
      "Epoch 19/96, Train Loss: 1.2070, Test Loss: 0.9072\n",
      "Epoch 20/96, Train Loss: 1.2152, Test Loss: 0.9060\n",
      "Epoch 21/96, Train Loss: 1.2605, Test Loss: 0.9020\n",
      "Epoch 22/96, Train Loss: 1.2218, Test Loss: 0.9001\n",
      "Epoch 23/96, Train Loss: 1.2110, Test Loss: 0.8995\n",
      "Epoch 24/96, Train Loss: 1.2032, Test Loss: 0.8981\n",
      "Epoch 25/96, Train Loss: 1.2082, Test Loss: 0.8997\n",
      "Epoch 26/96, Train Loss: 1.2179, Test Loss: 0.8990\n",
      "Epoch 27/96, Train Loss: 1.1653, Test Loss: 0.8983\n",
      "Epoch 28/96, Train Loss: 1.1580, Test Loss: 0.9000\n",
      "Epoch 29/96, Train Loss: 1.1555, Test Loss: 0.8957\n",
      "Epoch 30/96, Train Loss: 1.1916, Test Loss: 0.8963\n",
      "Epoch 31/96, Train Loss: 1.2005, Test Loss: 0.8952\n",
      "Epoch 32/96, Train Loss: 1.1579, Test Loss: 0.8975\n",
      "Epoch 33/96, Train Loss: 1.2162, Test Loss: 0.8950\n",
      "Epoch 34/96, Train Loss: 1.1786, Test Loss: 0.8984\n",
      "Epoch 35/96, Train Loss: 1.1659, Test Loss: 0.8993\n",
      "Epoch 36/96, Train Loss: 1.2101, Test Loss: 0.8946\n",
      "Epoch 37/96, Train Loss: 1.1659, Test Loss: 0.8964\n",
      "Epoch 38/96, Train Loss: 1.1757, Test Loss: 0.8972\n",
      "Epoch 39/96, Train Loss: 1.1542, Test Loss: 0.8978\n",
      "Epoch 40/96, Train Loss: 1.1353, Test Loss: 0.8939\n",
      "Epoch 41/96, Train Loss: 1.1295, Test Loss: 0.8934\n",
      "Epoch 42/96, Train Loss: 1.1822, Test Loss: 0.8936\n",
      "Epoch 43/96, Train Loss: 1.1635, Test Loss: 0.8940\n",
      "Epoch 44/96, Train Loss: 1.1984, Test Loss: 0.8956\n",
      "Epoch 45/96, Train Loss: 1.1689, Test Loss: 0.8935\n",
      "Epoch 46/96, Train Loss: 1.1520, Test Loss: 0.8934\n",
      "Epoch 47/96, Train Loss: 1.1452, Test Loss: 0.8952\n",
      "Epoch 48/96, Train Loss: 1.1218, Test Loss: 0.8966\n",
      "Epoch 49/96, Train Loss: 1.1708, Test Loss: 0.8946\n",
      "Epoch 50/96, Train Loss: 1.1912, Test Loss: 0.8960\n",
      "Epoch 51/96, Train Loss: 1.1834, Test Loss: 0.8930\n",
      "Epoch 52/96, Train Loss: 1.1626, Test Loss: 0.8927\n",
      "Epoch 53/96, Train Loss: 1.1362, Test Loss: 0.8933\n",
      "Epoch 54/96, Train Loss: 1.1695, Test Loss: 0.8963\n",
      "Epoch 55/96, Train Loss: 1.1339, Test Loss: 0.8932\n",
      "Epoch 56/96, Train Loss: 1.1692, Test Loss: 0.8921\n",
      "Epoch 57/96, Train Loss: 1.1517, Test Loss: 0.8903\n",
      "Epoch 58/96, Train Loss: 1.1691, Test Loss: 0.8952\n",
      "Epoch 59/96, Train Loss: 1.1009, Test Loss: 0.8954\n",
      "Epoch 60/96, Train Loss: 1.1457, Test Loss: 0.8929\n",
      "Epoch 61/96, Train Loss: 1.1142, Test Loss: 0.8908\n",
      "Epoch 62/96, Train Loss: 1.1020, Test Loss: 0.8912\n",
      "Epoch 63/96, Train Loss: 1.1663, Test Loss: 0.8915\n",
      "Epoch 64/96, Train Loss: 1.1568, Test Loss: 0.8936\n",
      "Epoch 65/96, Train Loss: 1.1047, Test Loss: 0.8921\n",
      "Epoch 66/96, Train Loss: 1.1356, Test Loss: 0.8911\n",
      "Epoch 67/96, Train Loss: 1.1371, Test Loss: 0.8911\n",
      "Epoch 68/96, Train Loss: 1.1244, Test Loss: 0.8880\n",
      "Epoch 69/96, Train Loss: 1.1439, Test Loss: 0.8882\n",
      "Epoch 70/96, Train Loss: 1.1501, Test Loss: 0.8923\n",
      "Epoch 71/96, Train Loss: 1.1245, Test Loss: 0.8877\n",
      "Epoch 72/96, Train Loss: 1.1563, Test Loss: 0.8901\n",
      "Epoch 73/96, Train Loss: 1.1320, Test Loss: 0.8910\n",
      "Epoch 74/96, Train Loss: 1.1494, Test Loss: 0.8909\n",
      "Epoch 75/96, Train Loss: 1.1519, Test Loss: 0.8896\n",
      "Epoch 76/96, Train Loss: 1.1076, Test Loss: 0.8892\n",
      "Epoch 77/96, Train Loss: 1.1379, Test Loss: 0.8909\n",
      "Epoch 78/96, Train Loss: 1.1200, Test Loss: 0.8907\n",
      "Epoch 79/96, Train Loss: 1.1252, Test Loss: 0.8883\n",
      "Epoch 80/96, Train Loss: 1.0943, Test Loss: 0.8890\n",
      "Epoch 81/96, Train Loss: 1.1205, Test Loss: 0.8867\n",
      "Epoch 82/96, Train Loss: 1.1488, Test Loss: 0.8888\n",
      "Epoch 83/96, Train Loss: 1.1371, Test Loss: 0.8873\n",
      "Epoch 84/96, Train Loss: 1.1312, Test Loss: 0.8870\n",
      "Epoch 85/96, Train Loss: 1.1521, Test Loss: 0.8882\n",
      "Epoch 86/96, Train Loss: 1.1128, Test Loss: 0.8899\n",
      "Epoch 87/96, Train Loss: 1.0771, Test Loss: 0.8924\n",
      "Epoch 88/96, Train Loss: 1.1298, Test Loss: 0.8873\n",
      "Epoch 89/96, Train Loss: 1.1079, Test Loss: 0.8900\n",
      "Epoch 90/96, Train Loss: 1.1014, Test Loss: 0.8895\n",
      "Epoch 91/96, Train Loss: 1.1731, Test Loss: 0.8882\n",
      "Epoch 92/96, Train Loss: 1.1055, Test Loss: 0.8850\n",
      "Epoch 93/96, Train Loss: 1.1496, Test Loss: 0.8904\n",
      "Epoch 94/96, Train Loss: 1.0891, Test Loss: 0.8888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:29:49,701] Trial 130 finished with value: 0.8863866627216339 and parameters: {'num_hidden_layers': 5, 'layer_0_size': 109, 'layer_1_size': 206, 'layer_2_size': 204, 'layer_3_size': 88, 'layer_4_size': 103, 'dropout_rate': 0.37604457584073747, 'learning_rate': 2.8366822858972098e-05, 'batch_size': 64, 'epochs': 96}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/96, Train Loss: 1.0978, Test Loss: 0.8879\n",
      "Epoch 96/96, Train Loss: 1.1406, Test Loss: 0.8864\n",
      "Epoch 1/80, Train Loss: 1.2302, Test Loss: 0.9012\n",
      "Epoch 2/80, Train Loss: 1.2549, Test Loss: 0.9144\n",
      "Epoch 3/80, Train Loss: 1.1629, Test Loss: 0.9125\n",
      "Epoch 4/80, Train Loss: 1.2406, Test Loss: 0.9090\n",
      "Epoch 5/80, Train Loss: 1.1875, Test Loss: 0.9148\n",
      "Epoch 6/80, Train Loss: 1.2286, Test Loss: 0.9154\n",
      "Epoch 7/80, Train Loss: 1.1452, Test Loss: 0.9071\n",
      "Epoch 8/80, Train Loss: 1.1232, Test Loss: 0.9040\n",
      "Epoch 9/80, Train Loss: 1.1409, Test Loss: 0.8998\n",
      "Epoch 10/80, Train Loss: 1.1608, Test Loss: 0.8996\n",
      "Epoch 11/80, Train Loss: 1.1537, Test Loss: 0.8982\n",
      "Epoch 12/80, Train Loss: 1.1414, Test Loss: 0.8965\n",
      "Epoch 13/80, Train Loss: 1.1786, Test Loss: 0.8967\n",
      "Epoch 14/80, Train Loss: 1.1593, Test Loss: 0.8955\n",
      "Epoch 15/80, Train Loss: 1.1550, Test Loss: 0.8970\n",
      "Epoch 16/80, Train Loss: 1.1324, Test Loss: 0.8952\n",
      "Epoch 17/80, Train Loss: 1.1318, Test Loss: 0.8950\n",
      "Epoch 18/80, Train Loss: 1.1779, Test Loss: 0.8974\n",
      "Epoch 19/80, Train Loss: 1.1267, Test Loss: 0.8967\n",
      "Epoch 20/80, Train Loss: 1.1343, Test Loss: 0.8929\n",
      "Epoch 21/80, Train Loss: 1.1289, Test Loss: 0.8957\n",
      "Epoch 22/80, Train Loss: 1.1371, Test Loss: 0.8939\n",
      "Epoch 23/80, Train Loss: 1.1337, Test Loss: 0.8943\n",
      "Epoch 24/80, Train Loss: 1.1157, Test Loss: 0.8942\n",
      "Epoch 25/80, Train Loss: 1.1436, Test Loss: 0.8937\n",
      "Epoch 26/80, Train Loss: 1.0922, Test Loss: 0.8930\n",
      "Epoch 27/80, Train Loss: 1.0933, Test Loss: 0.8927\n",
      "Epoch 28/80, Train Loss: 1.0981, Test Loss: 0.8923\n",
      "Epoch 29/80, Train Loss: 1.1644, Test Loss: 0.8948\n",
      "Epoch 30/80, Train Loss: 1.0819, Test Loss: 0.8917\n",
      "Epoch 31/80, Train Loss: 1.1164, Test Loss: 0.8923\n",
      "Epoch 32/80, Train Loss: 1.0424, Test Loss: 0.8928\n",
      "Epoch 33/80, Train Loss: 1.1513, Test Loss: 0.8927\n",
      "Epoch 34/80, Train Loss: 1.0990, Test Loss: 0.8934\n",
      "Epoch 35/80, Train Loss: 1.1416, Test Loss: 0.8922\n",
      "Epoch 36/80, Train Loss: 1.1321, Test Loss: 0.8918\n",
      "Epoch 37/80, Train Loss: 1.1178, Test Loss: 0.8921\n",
      "Epoch 38/80, Train Loss: 1.1076, Test Loss: 0.8903\n",
      "Epoch 39/80, Train Loss: 1.1125, Test Loss: 0.8894\n",
      "Epoch 40/80, Train Loss: 1.1051, Test Loss: 0.8911\n",
      "Epoch 41/80, Train Loss: 1.0783, Test Loss: 0.8905\n",
      "Epoch 42/80, Train Loss: 1.0882, Test Loss: 0.8922\n",
      "Epoch 43/80, Train Loss: 1.1164, Test Loss: 0.8905\n",
      "Epoch 44/80, Train Loss: 1.0884, Test Loss: 0.8911\n",
      "Epoch 45/80, Train Loss: 1.0838, Test Loss: 0.8918\n",
      "Epoch 46/80, Train Loss: 1.0936, Test Loss: 0.8906\n",
      "Epoch 47/80, Train Loss: 1.0976, Test Loss: 0.8911\n",
      "Epoch 48/80, Train Loss: 1.1259, Test Loss: 0.8908\n",
      "Epoch 49/80, Train Loss: 1.1171, Test Loss: 0.8897\n",
      "Epoch 50/80, Train Loss: 1.0970, Test Loss: 0.8893\n",
      "Epoch 51/80, Train Loss: 1.0931, Test Loss: 0.8899\n",
      "Epoch 52/80, Train Loss: 1.0720, Test Loss: 0.8914\n",
      "Epoch 53/80, Train Loss: 1.0846, Test Loss: 0.8907\n",
      "Epoch 54/80, Train Loss: 1.0867, Test Loss: 0.8892\n",
      "Epoch 55/80, Train Loss: 1.1096, Test Loss: 0.8882\n",
      "Epoch 56/80, Train Loss: 1.0778, Test Loss: 0.8886\n",
      "Epoch 57/80, Train Loss: 1.0865, Test Loss: 0.8871\n",
      "Epoch 58/80, Train Loss: 1.0837, Test Loss: 0.8897\n",
      "Epoch 59/80, Train Loss: 1.0776, Test Loss: 0.8901\n",
      "Epoch 60/80, Train Loss: 1.0788, Test Loss: 0.8889\n",
      "Epoch 61/80, Train Loss: 1.0903, Test Loss: 0.8882\n",
      "Epoch 62/80, Train Loss: 1.0962, Test Loss: 0.8882\n",
      "Epoch 63/80, Train Loss: 1.0767, Test Loss: 0.8901\n",
      "Epoch 64/80, Train Loss: 1.0824, Test Loss: 0.8894\n",
      "Epoch 65/80, Train Loss: 1.0517, Test Loss: 0.8894\n",
      "Epoch 66/80, Train Loss: 1.0708, Test Loss: 0.8907\n",
      "Epoch 67/80, Train Loss: 1.0741, Test Loss: 0.8894\n",
      "Epoch 68/80, Train Loss: 1.0615, Test Loss: 0.8910\n",
      "Epoch 69/80, Train Loss: 1.0745, Test Loss: 0.8898\n",
      "Epoch 70/80, Train Loss: 1.0730, Test Loss: 0.8913\n",
      "Epoch 71/80, Train Loss: 1.0602, Test Loss: 0.8920\n",
      "Epoch 72/80, Train Loss: 1.0983, Test Loss: 0.8903\n",
      "Epoch 73/80, Train Loss: 1.0854, Test Loss: 0.8902\n",
      "Epoch 74/80, Train Loss: 1.0691, Test Loss: 0.8912\n",
      "Epoch 75/80, Train Loss: 1.1128, Test Loss: 0.8903\n",
      "Epoch 76/80, Train Loss: 1.0636, Test Loss: 0.8916\n",
      "Epoch 77/80, Train Loss: 1.0732, Test Loss: 0.8897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:29:56,688] Trial 131 finished with value: 0.8881663531064987 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 164, 'layer_1_size': 234, 'layer_2_size': 234, 'layer_3_size': 187, 'layer_4_size': 228, 'layer_5_size': 253, 'dropout_rate': 0.4394114071130169, 'learning_rate': 4.8782601032920144e-05, 'batch_size': 64, 'epochs': 80}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/80, Train Loss: 1.0788, Test Loss: 0.8900\n",
      "Epoch 79/80, Train Loss: 1.0738, Test Loss: 0.8911\n",
      "Epoch 80/80, Train Loss: 1.0980, Test Loss: 0.8882\n",
      "Epoch 1/74, Train Loss: 1.1759, Test Loss: 0.8490\n",
      "Epoch 2/74, Train Loss: 1.1354, Test Loss: 0.8567\n",
      "Epoch 3/74, Train Loss: 1.1355, Test Loss: 0.8602\n",
      "Epoch 4/74, Train Loss: 1.0697, Test Loss: 0.8582\n",
      "Epoch 5/74, Train Loss: 1.0986, Test Loss: 0.8642\n",
      "Epoch 6/74, Train Loss: 1.0874, Test Loss: 0.8589\n",
      "Epoch 7/74, Train Loss: 1.0462, Test Loss: 0.8600\n",
      "Epoch 8/74, Train Loss: 1.0801, Test Loss: 0.8595\n",
      "Epoch 9/74, Train Loss: 1.0581, Test Loss: 0.8593\n",
      "Epoch 10/74, Train Loss: 1.0255, Test Loss: 0.8592\n",
      "Epoch 11/74, Train Loss: 1.0761, Test Loss: 0.8585\n",
      "Epoch 12/74, Train Loss: 1.0187, Test Loss: 0.8579\n",
      "Epoch 13/74, Train Loss: 1.0806, Test Loss: 0.8614\n",
      "Epoch 14/74, Train Loss: 1.0333, Test Loss: 0.8623\n",
      "Epoch 15/74, Train Loss: 1.1220, Test Loss: 0.8576\n",
      "Epoch 16/74, Train Loss: 1.0674, Test Loss: 0.8597\n",
      "Epoch 17/74, Train Loss: 1.0093, Test Loss: 0.8640\n",
      "Epoch 18/74, Train Loss: 1.0672, Test Loss: 0.8626\n",
      "Epoch 19/74, Train Loss: 0.9830, Test Loss: 0.8624\n",
      "Epoch 20/74, Train Loss: 1.0499, Test Loss: 0.8613\n",
      "Epoch 21/74, Train Loss: 1.0112, Test Loss: 0.8635\n",
      "Epoch 22/74, Train Loss: 1.0342, Test Loss: 0.8661\n",
      "Epoch 23/74, Train Loss: 1.0400, Test Loss: 0.8669\n",
      "Epoch 24/74, Train Loss: 1.0675, Test Loss: 0.8684\n",
      "Epoch 25/74, Train Loss: 1.0521, Test Loss: 0.8662\n",
      "Epoch 26/74, Train Loss: 1.0477, Test Loss: 0.8689\n",
      "Epoch 27/74, Train Loss: 1.0549, Test Loss: 0.8629\n",
      "Epoch 28/74, Train Loss: 1.0321, Test Loss: 0.8695\n",
      "Epoch 29/74, Train Loss: 1.0218, Test Loss: 0.8744\n",
      "Epoch 30/74, Train Loss: 1.0164, Test Loss: 0.8701\n",
      "Epoch 31/74, Train Loss: 1.0879, Test Loss: 0.8690\n",
      "Epoch 32/74, Train Loss: 1.0716, Test Loss: 0.8671\n",
      "Epoch 33/74, Train Loss: 1.0030, Test Loss: 0.8692\n",
      "Epoch 34/74, Train Loss: 1.0219, Test Loss: 0.8659\n",
      "Epoch 35/74, Train Loss: 1.0206, Test Loss: 0.8646\n",
      "Epoch 36/74, Train Loss: 1.0166, Test Loss: 0.8657\n",
      "Epoch 37/74, Train Loss: 0.9855, Test Loss: 0.8679\n",
      "Epoch 38/74, Train Loss: 0.9871, Test Loss: 0.8666\n",
      "Epoch 39/74, Train Loss: 1.0152, Test Loss: 0.8653\n",
      "Epoch 40/74, Train Loss: 1.0136, Test Loss: 0.8682\n",
      "Epoch 41/74, Train Loss: 1.0226, Test Loss: 0.8658\n",
      "Epoch 42/74, Train Loss: 1.0014, Test Loss: 0.8730\n",
      "Epoch 43/74, Train Loss: 0.9926, Test Loss: 0.8725\n",
      "Epoch 44/74, Train Loss: 1.0419, Test Loss: 0.8767\n",
      "Epoch 45/74, Train Loss: 1.0554, Test Loss: 0.8776\n",
      "Epoch 46/74, Train Loss: 1.0335, Test Loss: 0.8790\n",
      "Epoch 47/74, Train Loss: 0.9928, Test Loss: 0.8745\n",
      "Epoch 48/74, Train Loss: 0.9653, Test Loss: 0.8739\n",
      "Epoch 49/74, Train Loss: 1.0222, Test Loss: 0.8726\n",
      "Epoch 50/74, Train Loss: 1.0089, Test Loss: 0.8722\n",
      "Epoch 51/74, Train Loss: 0.9909, Test Loss: 0.8764\n",
      "Epoch 52/74, Train Loss: 1.0225, Test Loss: 0.8737\n",
      "Epoch 53/74, Train Loss: 1.0576, Test Loss: 0.8739\n",
      "Epoch 54/74, Train Loss: 1.0186, Test Loss: 0.8728\n",
      "Epoch 55/74, Train Loss: 1.0117, Test Loss: 0.8726\n",
      "Epoch 56/74, Train Loss: 1.0176, Test Loss: 0.8733\n",
      "Epoch 57/74, Train Loss: 0.9958, Test Loss: 0.8759\n",
      "Epoch 58/74, Train Loss: 0.9800, Test Loss: 0.8737\n",
      "Epoch 59/74, Train Loss: 0.9614, Test Loss: 0.8737\n",
      "Epoch 60/74, Train Loss: 1.0120, Test Loss: 0.8709\n",
      "Epoch 61/74, Train Loss: 1.0190, Test Loss: 0.8745\n",
      "Epoch 62/74, Train Loss: 0.9875, Test Loss: 0.8727\n",
      "Epoch 63/74, Train Loss: 0.9988, Test Loss: 0.8669\n",
      "Epoch 64/74, Train Loss: 0.9779, Test Loss: 0.8656\n",
      "Epoch 65/74, Train Loss: 0.9864, Test Loss: 0.8621\n",
      "Epoch 66/74, Train Loss: 0.9842, Test Loss: 0.8660\n",
      "Epoch 67/74, Train Loss: 0.9744, Test Loss: 0.8649\n",
      "Epoch 68/74, Train Loss: 0.9874, Test Loss: 0.8641\n",
      "Epoch 69/74, Train Loss: 0.9831, Test Loss: 0.8629\n",
      "Epoch 70/74, Train Loss: 0.9538, Test Loss: 0.8627\n",
      "Epoch 71/74, Train Loss: 0.9836, Test Loss: 0.8643\n",
      "Epoch 72/74, Train Loss: 1.0116, Test Loss: 0.8630\n",
      "Epoch 73/74, Train Loss: 0.9783, Test Loss: 0.8666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:30:02,734] Trial 132 finished with value: 0.8685236424207687 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 149, 'layer_1_size': 72, 'layer_2_size': 237, 'layer_3_size': 216, 'layer_4_size': 214, 'layer_5_size': 256, 'dropout_rate': 0.39864256768955403, 'learning_rate': 5.641929451146618e-05, 'batch_size': 64, 'epochs': 74}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/74, Train Loss: 0.9660, Test Loss: 0.8685\n",
      "Epoch 1/78, Train Loss: 1.2182, Test Loss: 0.8879\n",
      "Epoch 2/78, Train Loss: 1.1365, Test Loss: 0.8908\n",
      "Epoch 3/78, Train Loss: 1.1821, Test Loss: 0.8944\n",
      "Epoch 4/78, Train Loss: 1.2120, Test Loss: 0.8895\n",
      "Epoch 5/78, Train Loss: 1.1923, Test Loss: 0.8939\n",
      "Epoch 6/78, Train Loss: 1.1789, Test Loss: 0.8929\n",
      "Epoch 7/78, Train Loss: 1.1772, Test Loss: 0.8888\n",
      "Epoch 8/78, Train Loss: 1.1946, Test Loss: 0.8908\n",
      "Epoch 9/78, Train Loss: 1.1473, Test Loss: 0.8909\n",
      "Epoch 10/78, Train Loss: 1.2007, Test Loss: 0.8877\n",
      "Epoch 11/78, Train Loss: 1.1402, Test Loss: 0.8888\n",
      "Epoch 12/78, Train Loss: 1.1440, Test Loss: 0.8890\n",
      "Epoch 13/78, Train Loss: 1.1403, Test Loss: 0.8908\n",
      "Epoch 14/78, Train Loss: 1.1350, Test Loss: 0.8905\n",
      "Epoch 15/78, Train Loss: 1.1180, Test Loss: 0.8932\n",
      "Epoch 16/78, Train Loss: 1.1243, Test Loss: 0.8920\n",
      "Epoch 17/78, Train Loss: 1.1144, Test Loss: 0.8903\n",
      "Epoch 18/78, Train Loss: 1.1235, Test Loss: 0.8902\n",
      "Epoch 19/78, Train Loss: 1.1066, Test Loss: 0.8891\n",
      "Epoch 20/78, Train Loss: 1.1159, Test Loss: 0.8913\n",
      "Epoch 21/78, Train Loss: 1.1328, Test Loss: 0.8910\n",
      "Epoch 22/78, Train Loss: 1.1638, Test Loss: 0.8925\n",
      "Epoch 23/78, Train Loss: 1.1210, Test Loss: 0.8896\n",
      "Epoch 24/78, Train Loss: 1.1210, Test Loss: 0.8895\n",
      "Epoch 25/78, Train Loss: 1.1233, Test Loss: 0.8895\n",
      "Epoch 26/78, Train Loss: 1.1314, Test Loss: 0.8916\n",
      "Epoch 27/78, Train Loss: 1.1231, Test Loss: 0.8917\n",
      "Epoch 28/78, Train Loss: 1.1138, Test Loss: 0.8901\n",
      "Epoch 29/78, Train Loss: 1.1287, Test Loss: 0.8930\n",
      "Epoch 30/78, Train Loss: 1.0840, Test Loss: 0.8932\n",
      "Epoch 31/78, Train Loss: 1.1191, Test Loss: 0.8910\n",
      "Epoch 32/78, Train Loss: 1.1310, Test Loss: 0.8900\n",
      "Epoch 33/78, Train Loss: 1.1057, Test Loss: 0.8908\n",
      "Epoch 34/78, Train Loss: 1.0822, Test Loss: 0.8896\n",
      "Epoch 35/78, Train Loss: 1.1173, Test Loss: 0.8896\n",
      "Epoch 36/78, Train Loss: 1.1250, Test Loss: 0.8915\n",
      "Epoch 37/78, Train Loss: 1.0825, Test Loss: 0.8908\n",
      "Epoch 38/78, Train Loss: 1.0939, Test Loss: 0.8925\n",
      "Epoch 39/78, Train Loss: 1.1210, Test Loss: 0.8938\n",
      "Epoch 40/78, Train Loss: 1.1162, Test Loss: 0.8913\n",
      "Epoch 41/78, Train Loss: 1.1030, Test Loss: 0.8939\n",
      "Epoch 42/78, Train Loss: 1.0631, Test Loss: 0.8920\n",
      "Epoch 43/78, Train Loss: 1.0495, Test Loss: 0.8909\n",
      "Epoch 44/78, Train Loss: 1.0777, Test Loss: 0.8912\n",
      "Epoch 45/78, Train Loss: 1.1026, Test Loss: 0.8910\n",
      "Epoch 46/78, Train Loss: 1.1582, Test Loss: 0.8906\n",
      "Epoch 47/78, Train Loss: 1.0845, Test Loss: 0.8918\n",
      "Epoch 48/78, Train Loss: 1.0611, Test Loss: 0.8908\n",
      "Epoch 49/78, Train Loss: 1.1432, Test Loss: 0.8935\n",
      "Epoch 50/78, Train Loss: 1.1333, Test Loss: 0.8924\n",
      "Epoch 51/78, Train Loss: 1.0680, Test Loss: 0.8920\n",
      "Epoch 52/78, Train Loss: 1.0883, Test Loss: 0.8911\n",
      "Epoch 53/78, Train Loss: 1.0791, Test Loss: 0.8934\n",
      "Epoch 54/78, Train Loss: 1.1186, Test Loss: 0.8935\n",
      "Epoch 55/78, Train Loss: 1.0429, Test Loss: 0.8914\n",
      "Epoch 56/78, Train Loss: 1.0618, Test Loss: 0.8920\n",
      "Epoch 57/78, Train Loss: 1.0495, Test Loss: 0.8918\n",
      "Epoch 58/78, Train Loss: 1.0869, Test Loss: 0.8942\n",
      "Epoch 59/78, Train Loss: 1.0472, Test Loss: 0.8920\n",
      "Epoch 60/78, Train Loss: 1.0964, Test Loss: 0.8913\n",
      "Epoch 61/78, Train Loss: 1.1059, Test Loss: 0.8910\n",
      "Epoch 62/78, Train Loss: 1.0693, Test Loss: 0.8897\n",
      "Epoch 63/78, Train Loss: 1.1011, Test Loss: 0.8918\n",
      "Epoch 64/78, Train Loss: 1.0502, Test Loss: 0.8915\n",
      "Epoch 65/78, Train Loss: 1.0843, Test Loss: 0.8890\n",
      "Epoch 66/78, Train Loss: 1.0621, Test Loss: 0.8906\n",
      "Epoch 67/78, Train Loss: 1.0394, Test Loss: 0.8899\n",
      "Epoch 68/78, Train Loss: 1.0362, Test Loss: 0.8916\n",
      "Epoch 69/78, Train Loss: 1.0639, Test Loss: 0.8911\n",
      "Epoch 70/78, Train Loss: 1.0796, Test Loss: 0.8912\n",
      "Epoch 71/78, Train Loss: 1.1060, Test Loss: 0.8890\n",
      "Epoch 72/78, Train Loss: 1.0701, Test Loss: 0.8882\n",
      "Epoch 73/78, Train Loss: 1.0905, Test Loss: 0.8906\n",
      "Epoch 74/78, Train Loss: 1.0878, Test Loss: 0.8922\n",
      "Epoch 75/78, Train Loss: 1.0876, Test Loss: 0.8906\n",
      "Epoch 76/78, Train Loss: 1.0909, Test Loss: 0.8911\n",
      "Epoch 77/78, Train Loss: 1.0468, Test Loss: 0.8913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:30:10,489] Trial 133 finished with value: 0.8902827650308609 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 217, 'layer_1_size': 176, 'layer_2_size': 223, 'layer_3_size': 95, 'layer_4_size': 207, 'layer_5_size': 248, 'layer_6_size': 199, 'dropout_rate': 0.43567077958537626, 'learning_rate': 4.297010067155875e-05, 'batch_size': 64, 'epochs': 78}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/78, Train Loss: 1.1216, Test Loss: 0.8903\n",
      "Epoch 1/71, Train Loss: 1.3122, Test Loss: 0.8994\n",
      "Epoch 2/71, Train Loss: 1.2731, Test Loss: 0.9050\n",
      "Epoch 3/71, Train Loss: 1.2389, Test Loss: 0.9057\n",
      "Epoch 4/71, Train Loss: 1.2877, Test Loss: 0.9078\n",
      "Epoch 5/71, Train Loss: 1.3022, Test Loss: 0.9075\n",
      "Epoch 6/71, Train Loss: 1.3167, Test Loss: 0.9063\n",
      "Epoch 7/71, Train Loss: 1.2500, Test Loss: 0.9084\n",
      "Epoch 8/71, Train Loss: 1.2586, Test Loss: 0.9060\n",
      "Epoch 9/71, Train Loss: 1.2760, Test Loss: 0.9076\n",
      "Epoch 10/71, Train Loss: 1.2494, Test Loss: 0.9086\n",
      "Epoch 11/71, Train Loss: 1.1878, Test Loss: 0.9072\n",
      "Epoch 12/71, Train Loss: 1.2183, Test Loss: 0.9032\n",
      "Epoch 13/71, Train Loss: 1.1770, Test Loss: 0.9043\n",
      "Epoch 14/71, Train Loss: 1.2982, Test Loss: 0.9060\n",
      "Epoch 15/71, Train Loss: 1.1618, Test Loss: 0.9077\n",
      "Epoch 16/71, Train Loss: 1.2902, Test Loss: 0.9086\n",
      "Epoch 17/71, Train Loss: 1.2509, Test Loss: 0.9111\n",
      "Epoch 18/71, Train Loss: 1.2226, Test Loss: 0.9067\n",
      "Epoch 19/71, Train Loss: 1.2547, Test Loss: 0.9046\n",
      "Epoch 20/71, Train Loss: 1.2333, Test Loss: 0.9054\n",
      "Epoch 21/71, Train Loss: 1.1989, Test Loss: 0.9044\n",
      "Epoch 22/71, Train Loss: 1.1812, Test Loss: 0.9079\n",
      "Epoch 23/71, Train Loss: 1.2553, Test Loss: 0.9084\n",
      "Epoch 24/71, Train Loss: 1.2732, Test Loss: 0.9094\n",
      "Epoch 25/71, Train Loss: 1.1901, Test Loss: 0.9065\n",
      "Epoch 26/71, Train Loss: 1.2347, Test Loss: 0.9078\n",
      "Epoch 27/71, Train Loss: 1.1738, Test Loss: 0.9093\n",
      "Epoch 28/71, Train Loss: 1.2381, Test Loss: 0.9088\n",
      "Epoch 29/71, Train Loss: 1.2163, Test Loss: 0.9115\n",
      "Epoch 30/71, Train Loss: 1.2201, Test Loss: 0.9089\n",
      "Epoch 31/71, Train Loss: 1.1955, Test Loss: 0.9107\n",
      "Epoch 32/71, Train Loss: 1.1296, Test Loss: 0.9144\n",
      "Epoch 33/71, Train Loss: 1.2263, Test Loss: 0.9178\n",
      "Epoch 34/71, Train Loss: 1.1962, Test Loss: 0.9123\n",
      "Epoch 35/71, Train Loss: 1.1623, Test Loss: 0.9171\n",
      "Epoch 36/71, Train Loss: 1.0953, Test Loss: 0.9135\n",
      "Epoch 37/71, Train Loss: 1.2031, Test Loss: 0.9185\n",
      "Epoch 38/71, Train Loss: 1.1894, Test Loss: 0.9202\n",
      "Epoch 39/71, Train Loss: 1.1377, Test Loss: 0.9148\n",
      "Epoch 40/71, Train Loss: 1.1493, Test Loss: 0.9188\n",
      "Epoch 41/71, Train Loss: 1.1923, Test Loss: 0.9240\n",
      "Epoch 42/71, Train Loss: 1.1678, Test Loss: 0.9219\n",
      "Epoch 43/71, Train Loss: 1.1686, Test Loss: 0.9168\n",
      "Epoch 44/71, Train Loss: 1.1633, Test Loss: 0.9144\n",
      "Epoch 45/71, Train Loss: 1.1596, Test Loss: 0.9124\n",
      "Epoch 46/71, Train Loss: 1.1422, Test Loss: 0.9146\n",
      "Epoch 47/71, Train Loss: 1.1992, Test Loss: 0.9179\n",
      "Epoch 48/71, Train Loss: 1.1274, Test Loss: 0.9188\n",
      "Epoch 49/71, Train Loss: 1.1620, Test Loss: 0.9175\n",
      "Epoch 50/71, Train Loss: 1.2074, Test Loss: 0.9218\n",
      "Epoch 51/71, Train Loss: 1.1644, Test Loss: 0.9203\n",
      "Epoch 52/71, Train Loss: 1.1022, Test Loss: 0.9190\n",
      "Epoch 53/71, Train Loss: 1.2187, Test Loss: 0.9184\n",
      "Epoch 54/71, Train Loss: 1.1247, Test Loss: 0.9176\n",
      "Epoch 55/71, Train Loss: 1.1856, Test Loss: 0.9195\n",
      "Epoch 56/71, Train Loss: 1.2092, Test Loss: 0.9210\n",
      "Epoch 57/71, Train Loss: 1.0768, Test Loss: 0.9173\n",
      "Epoch 58/71, Train Loss: 1.1993, Test Loss: 0.9194\n",
      "Epoch 59/71, Train Loss: 1.1331, Test Loss: 0.9184\n",
      "Epoch 60/71, Train Loss: 1.1696, Test Loss: 0.9213\n",
      "Epoch 61/71, Train Loss: 1.1289, Test Loss: 0.9202\n",
      "Epoch 62/71, Train Loss: 1.1600, Test Loss: 0.9234\n",
      "Epoch 63/71, Train Loss: 1.1425, Test Loss: 0.9200\n",
      "Epoch 64/71, Train Loss: 1.1345, Test Loss: 0.9186\n",
      "Epoch 65/71, Train Loss: 1.1543, Test Loss: 0.9183\n",
      "Epoch 66/71, Train Loss: 1.1625, Test Loss: 0.9199\n",
      "Epoch 67/71, Train Loss: 1.1121, Test Loss: 0.9187\n",
      "Epoch 68/71, Train Loss: 1.1252, Test Loss: 0.9194\n",
      "Epoch 69/71, Train Loss: 1.1224, Test Loss: 0.9192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:30:17,865] Trial 134 finished with value: 0.9208903089165688 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 154, 'layer_1_size': 117, 'layer_2_size': 231, 'layer_3_size': 223, 'layer_4_size': 247, 'layer_5_size': 244, 'layer_6_size': 218, 'dropout_rate': 0.4613335844836168, 'learning_rate': 3.481949330225365e-05, 'batch_size': 64, 'epochs': 71}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/71, Train Loss: 1.1128, Test Loss: 0.9194\n",
      "Epoch 71/71, Train Loss: 1.1029, Test Loss: 0.9209\n",
      "Epoch 1/77, Train Loss: 1.3970, Test Loss: 0.9409\n",
      "Epoch 2/77, Train Loss: 1.2513, Test Loss: 0.9502\n",
      "Epoch 3/77, Train Loss: 1.2226, Test Loss: 0.9435\n",
      "Epoch 4/77, Train Loss: 1.1626, Test Loss: 0.9342\n",
      "Epoch 5/77, Train Loss: 1.1334, Test Loss: 0.9272\n",
      "Epoch 6/77, Train Loss: 1.0858, Test Loss: 0.9301\n",
      "Epoch 7/77, Train Loss: 1.1035, Test Loss: 0.9355\n",
      "Epoch 8/77, Train Loss: 1.0849, Test Loss: 0.9368\n",
      "Epoch 9/77, Train Loss: 1.1102, Test Loss: 0.9368\n",
      "Epoch 10/77, Train Loss: 1.0865, Test Loss: 0.9372\n",
      "Epoch 11/77, Train Loss: 1.0929, Test Loss: 0.9457\n",
      "Epoch 12/77, Train Loss: 1.1209, Test Loss: 0.9501\n",
      "Epoch 13/77, Train Loss: 1.0672, Test Loss: 0.9634\n",
      "Epoch 14/77, Train Loss: 1.0944, Test Loss: 0.9612\n",
      "Epoch 15/77, Train Loss: 1.1255, Test Loss: 0.9522\n",
      "Epoch 16/77, Train Loss: 1.1144, Test Loss: 0.9511\n",
      "Epoch 17/77, Train Loss: 1.0701, Test Loss: 0.9529\n",
      "Epoch 18/77, Train Loss: 1.0855, Test Loss: 0.9507\n",
      "Epoch 19/77, Train Loss: 1.0534, Test Loss: 0.9629\n",
      "Epoch 20/77, Train Loss: 1.0692, Test Loss: 0.9653\n",
      "Epoch 21/77, Train Loss: 1.0560, Test Loss: 0.9656\n",
      "Epoch 22/77, Train Loss: 1.0822, Test Loss: 0.9682\n",
      "Epoch 23/77, Train Loss: 1.0566, Test Loss: 0.9632\n",
      "Epoch 24/77, Train Loss: 1.0641, Test Loss: 0.9693\n",
      "Epoch 25/77, Train Loss: 1.0524, Test Loss: 0.9638\n",
      "Epoch 26/77, Train Loss: 1.0344, Test Loss: 0.9610\n",
      "Epoch 27/77, Train Loss: 1.0563, Test Loss: 0.9589\n",
      "Epoch 28/77, Train Loss: 1.0779, Test Loss: 0.9604\n",
      "Epoch 29/77, Train Loss: 1.0488, Test Loss: 0.9551\n",
      "Epoch 30/77, Train Loss: 1.0723, Test Loss: 0.9557\n",
      "Epoch 31/77, Train Loss: 1.0515, Test Loss: 0.9507\n",
      "Epoch 32/77, Train Loss: 1.0292, Test Loss: 0.9433\n",
      "Epoch 33/77, Train Loss: 1.0441, Test Loss: 0.9527\n",
      "Epoch 34/77, Train Loss: 1.0141, Test Loss: 0.9450\n",
      "Epoch 35/77, Train Loss: 1.0453, Test Loss: 0.9471\n",
      "Epoch 36/77, Train Loss: 1.0769, Test Loss: 0.9490\n",
      "Epoch 37/77, Train Loss: 1.0408, Test Loss: 0.9502\n",
      "Epoch 38/77, Train Loss: 1.0417, Test Loss: 0.9484\n",
      "Epoch 39/77, Train Loss: 1.0366, Test Loss: 0.9433\n",
      "Epoch 40/77, Train Loss: 1.0250, Test Loss: 0.9431\n",
      "Epoch 41/77, Train Loss: 1.0493, Test Loss: 0.9364\n",
      "Epoch 42/77, Train Loss: 1.0603, Test Loss: 0.9398\n",
      "Epoch 43/77, Train Loss: 1.0366, Test Loss: 0.9397\n",
      "Epoch 44/77, Train Loss: 1.0353, Test Loss: 0.9467\n",
      "Epoch 45/77, Train Loss: 1.0627, Test Loss: 0.9481\n",
      "Epoch 46/77, Train Loss: 1.0646, Test Loss: 0.9435\n",
      "Epoch 47/77, Train Loss: 0.9991, Test Loss: 0.9402\n",
      "Epoch 48/77, Train Loss: 1.0522, Test Loss: 0.9403\n",
      "Epoch 49/77, Train Loss: 1.0368, Test Loss: 0.9428\n",
      "Epoch 50/77, Train Loss: 1.0039, Test Loss: 0.9455\n",
      "Epoch 51/77, Train Loss: 1.0265, Test Loss: 0.9452\n",
      "Epoch 52/77, Train Loss: 1.0303, Test Loss: 0.9504\n",
      "Epoch 53/77, Train Loss: 1.0464, Test Loss: 0.9556\n",
      "Epoch 54/77, Train Loss: 1.0571, Test Loss: 0.9498\n",
      "Epoch 55/77, Train Loss: 1.0460, Test Loss: 0.9551\n",
      "Epoch 56/77, Train Loss: 1.0319, Test Loss: 0.9542\n",
      "Epoch 57/77, Train Loss: 1.0396, Test Loss: 0.9611\n",
      "Epoch 58/77, Train Loss: 1.0118, Test Loss: 0.9629\n",
      "Epoch 59/77, Train Loss: 1.0332, Test Loss: 0.9617\n",
      "Epoch 60/77, Train Loss: 0.9978, Test Loss: 0.9539\n",
      "Epoch 61/77, Train Loss: 1.0297, Test Loss: 0.9453\n",
      "Epoch 62/77, Train Loss: 1.0111, Test Loss: 0.9451\n",
      "Epoch 63/77, Train Loss: 1.0186, Test Loss: 0.9378\n",
      "Epoch 64/77, Train Loss: 1.0492, Test Loss: 0.9403\n",
      "Epoch 65/77, Train Loss: 1.0187, Test Loss: 0.9425\n",
      "Epoch 66/77, Train Loss: 1.0193, Test Loss: 0.9461\n",
      "Epoch 67/77, Train Loss: 1.0354, Test Loss: 0.9423\n",
      "Epoch 68/77, Train Loss: 1.0160, Test Loss: 0.9500\n",
      "Epoch 69/77, Train Loss: 1.0296, Test Loss: 0.9519\n",
      "Epoch 70/77, Train Loss: 1.0177, Test Loss: 0.9525\n",
      "Epoch 71/77, Train Loss: 1.0318, Test Loss: 0.9493\n",
      "Epoch 72/77, Train Loss: 1.0044, Test Loss: 0.9462\n",
      "Epoch 73/77, Train Loss: 1.0215, Test Loss: 0.9516\n",
      "Epoch 74/77, Train Loss: 1.0104, Test Loss: 0.9547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:30:24,581] Trial 135 finished with value: 0.9424747377634048 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 204, 'layer_1_size': 95, 'layer_2_size': 217, 'layer_3_size': 242, 'layer_4_size': 237, 'layer_5_size': 240, 'dropout_rate': 0.3248400257336878, 'learning_rate': 9.417471843840317e-05, 'batch_size': 64, 'epochs': 77}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/77, Train Loss: 1.0154, Test Loss: 0.9491\n",
      "Epoch 76/77, Train Loss: 1.0187, Test Loss: 0.9472\n",
      "Epoch 77/77, Train Loss: 1.0404, Test Loss: 0.9425\n",
      "Epoch 1/45, Train Loss: 1.3444, Test Loss: 0.8066\n",
      "Epoch 2/45, Train Loss: 1.3230, Test Loss: 0.8119\n",
      "Epoch 3/45, Train Loss: 1.3217, Test Loss: 0.8098\n",
      "Epoch 4/45, Train Loss: 1.3052, Test Loss: 0.8139\n",
      "Epoch 5/45, Train Loss: 1.2835, Test Loss: 0.8083\n",
      "Epoch 6/45, Train Loss: 1.3297, Test Loss: 0.8083\n",
      "Epoch 7/45, Train Loss: 1.2714, Test Loss: 0.8020\n",
      "Epoch 8/45, Train Loss: 1.2612, Test Loss: 0.8006\n",
      "Epoch 9/45, Train Loss: 1.2640, Test Loss: 0.8017\n",
      "Epoch 10/45, Train Loss: 1.2677, Test Loss: 0.7987\n",
      "Epoch 11/45, Train Loss: 1.2242, Test Loss: 0.7977\n",
      "Epoch 12/45, Train Loss: 1.2022, Test Loss: 0.7990\n",
      "Epoch 13/45, Train Loss: 1.2353, Test Loss: 0.7988\n",
      "Epoch 14/45, Train Loss: 1.2224, Test Loss: 0.7965\n",
      "Epoch 15/45, Train Loss: 1.2143, Test Loss: 0.7937\n",
      "Epoch 16/45, Train Loss: 1.2576, Test Loss: 0.7931\n",
      "Epoch 17/45, Train Loss: 1.2577, Test Loss: 0.7946\n",
      "Epoch 18/45, Train Loss: 1.2466, Test Loss: 0.7933\n",
      "Epoch 19/45, Train Loss: 1.1765, Test Loss: 0.7934\n",
      "Epoch 20/45, Train Loss: 1.2274, Test Loss: 0.7949\n",
      "Epoch 21/45, Train Loss: 1.2206, Test Loss: 0.7928\n",
      "Epoch 22/45, Train Loss: 1.2103, Test Loss: 0.7906\n",
      "Epoch 23/45, Train Loss: 1.1608, Test Loss: 0.7910\n",
      "Epoch 24/45, Train Loss: 1.1945, Test Loss: 0.7924\n",
      "Epoch 25/45, Train Loss: 1.1925, Test Loss: 0.7915\n",
      "Epoch 26/45, Train Loss: 1.1878, Test Loss: 0.7919\n",
      "Epoch 27/45, Train Loss: 1.1783, Test Loss: 0.7911\n",
      "Epoch 28/45, Train Loss: 1.1885, Test Loss: 0.7922\n",
      "Epoch 29/45, Train Loss: 1.1949, Test Loss: 0.7910\n",
      "Epoch 30/45, Train Loss: 1.2015, Test Loss: 0.7930\n",
      "Epoch 31/45, Train Loss: 1.1891, Test Loss: 0.7927\n",
      "Epoch 32/45, Train Loss: 1.2005, Test Loss: 0.7911\n",
      "Epoch 33/45, Train Loss: 1.1957, Test Loss: 0.7932\n",
      "Epoch 34/45, Train Loss: 1.1645, Test Loss: 0.7925\n",
      "Epoch 35/45, Train Loss: 1.1663, Test Loss: 0.7928\n",
      "Epoch 36/45, Train Loss: 1.1477, Test Loss: 0.7910\n",
      "Epoch 37/45, Train Loss: 1.1541, Test Loss: 0.7922\n",
      "Epoch 38/45, Train Loss: 1.1830, Test Loss: 0.7921\n",
      "Epoch 39/45, Train Loss: 1.1667, Test Loss: 0.7922\n",
      "Epoch 40/45, Train Loss: 1.1642, Test Loss: 0.7931\n",
      "Epoch 41/45, Train Loss: 1.1633, Test Loss: 0.7937\n",
      "Epoch 42/45, Train Loss: 1.1883, Test Loss: 0.7909\n",
      "Epoch 43/45, Train Loss: 1.1513, Test Loss: 0.7914\n",
      "Epoch 44/45, Train Loss: 1.1627, Test Loss: 0.7920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:30:28,611] Trial 136 finished with value: 0.7902699112892151 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 140, 'layer_1_size': 128, 'layer_2_size': 197, 'layer_3_size': 208, 'layer_4_size': 65, 'layer_5_size': 145, 'dropout_rate': 0.4283728593158682, 'learning_rate': 5.2515799930962786e-05, 'batch_size': 64, 'epochs': 45}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/45, Train Loss: 1.1500, Test Loss: 0.7903\n",
      "Epoch 1/44, Train Loss: 1.3048, Test Loss: 1.1097\n",
      "Epoch 2/44, Train Loss: 1.2534, Test Loss: 1.1097\n",
      "Epoch 3/44, Train Loss: 1.2691, Test Loss: 1.1056\n",
      "Epoch 4/44, Train Loss: 1.2573, Test Loss: 1.1144\n",
      "Epoch 5/44, Train Loss: 1.2805, Test Loss: 1.1166\n",
      "Epoch 6/44, Train Loss: 1.2048, Test Loss: 1.1173\n",
      "Epoch 7/44, Train Loss: 1.2598, Test Loss: 1.1321\n",
      "Epoch 8/44, Train Loss: 1.2205, Test Loss: 1.1246\n",
      "Epoch 9/44, Train Loss: 1.2538, Test Loss: 1.1287\n",
      "Epoch 10/44, Train Loss: 1.2916, Test Loss: 1.1336\n",
      "Epoch 11/44, Train Loss: 1.1670, Test Loss: 1.1359\n",
      "Epoch 12/44, Train Loss: 1.2121, Test Loss: 1.1316\n",
      "Epoch 13/44, Train Loss: 1.2129, Test Loss: 1.1328\n",
      "Epoch 14/44, Train Loss: 1.1617, Test Loss: 1.1348\n",
      "Epoch 15/44, Train Loss: 1.1851, Test Loss: 1.1355\n",
      "Epoch 16/44, Train Loss: 1.1918, Test Loss: 1.1386\n",
      "Epoch 17/44, Train Loss: 1.1693, Test Loss: 1.1240\n",
      "Epoch 18/44, Train Loss: 1.1462, Test Loss: 1.1344\n",
      "Epoch 19/44, Train Loss: 1.2003, Test Loss: 1.1410\n",
      "Epoch 20/44, Train Loss: 1.1193, Test Loss: 1.1342\n",
      "Epoch 21/44, Train Loss: 1.1420, Test Loss: 1.1409\n",
      "Epoch 22/44, Train Loss: 1.1881, Test Loss: 1.1367\n",
      "Epoch 23/44, Train Loss: 1.1398, Test Loss: 1.1374\n",
      "Epoch 24/44, Train Loss: 1.1855, Test Loss: 1.1430\n",
      "Epoch 25/44, Train Loss: 1.1676, Test Loss: 1.1333\n",
      "Epoch 26/44, Train Loss: 1.1914, Test Loss: 1.1449\n",
      "Epoch 27/44, Train Loss: 1.1960, Test Loss: 1.1456\n",
      "Epoch 28/44, Train Loss: 1.1814, Test Loss: 1.1382\n",
      "Epoch 29/44, Train Loss: 1.1787, Test Loss: 1.1395\n",
      "Epoch 30/44, Train Loss: 1.2164, Test Loss: 1.1411\n",
      "Epoch 31/44, Train Loss: 1.1589, Test Loss: 1.1378\n",
      "Epoch 32/44, Train Loss: 1.1353, Test Loss: 1.1451\n",
      "Epoch 33/44, Train Loss: 1.1364, Test Loss: 1.1443\n",
      "Epoch 34/44, Train Loss: 1.2142, Test Loss: 1.1448\n",
      "Epoch 35/44, Train Loss: 1.1807, Test Loss: 1.1441\n",
      "Epoch 36/44, Train Loss: 1.1989, Test Loss: 1.1537\n",
      "Epoch 37/44, Train Loss: 1.1357, Test Loss: 1.1446\n",
      "Epoch 38/44, Train Loss: 1.1384, Test Loss: 1.1450\n",
      "Epoch 39/44, Train Loss: 1.1199, Test Loss: 1.1489\n",
      "Epoch 40/44, Train Loss: 1.1759, Test Loss: 1.1512\n",
      "Epoch 41/44, Train Loss: 1.1603, Test Loss: 1.1468\n",
      "Epoch 42/44, Train Loss: 1.1562, Test Loss: 1.1451\n",
      "Epoch 43/44, Train Loss: 1.1557, Test Loss: 1.1476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:30:31,974] Trial 137 finished with value: 1.1505859345197678 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 141, 'layer_1_size': 161, 'layer_2_size': 197, 'layer_3_size': 38, 'layer_4_size': 41, 'layer_5_size': 143, 'layer_6_size': 126, 'dropout_rate': 0.4139622845549401, 'learning_rate': 6.693834544408202e-05, 'batch_size': 64, 'epochs': 44}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/44, Train Loss: 1.1400, Test Loss: 1.1506\n",
      "Epoch 1/38, Train Loss: 1.8137, Test Loss: 1.0518\n",
      "Epoch 2/38, Train Loss: 1.7573, Test Loss: 1.0614\n",
      "Epoch 3/38, Train Loss: 1.7289, Test Loss: 1.0530\n",
      "Epoch 4/38, Train Loss: 1.6615, Test Loss: 1.0468\n",
      "Epoch 5/38, Train Loss: 1.5658, Test Loss: 1.0602\n",
      "Epoch 6/38, Train Loss: 1.5566, Test Loss: 1.0420\n",
      "Epoch 7/38, Train Loss: 1.5561, Test Loss: 1.0453\n",
      "Epoch 8/38, Train Loss: 1.5082, Test Loss: 1.0419\n",
      "Epoch 9/38, Train Loss: 1.3728, Test Loss: 1.0348\n",
      "Epoch 10/38, Train Loss: 1.3832, Test Loss: 1.0449\n",
      "Epoch 11/38, Train Loss: 1.4340, Test Loss: 1.0351\n",
      "Epoch 12/38, Train Loss: 1.3865, Test Loss: 1.0347\n",
      "Epoch 13/38, Train Loss: 1.3771, Test Loss: 1.0368\n",
      "Epoch 14/38, Train Loss: 1.3521, Test Loss: 1.0348\n",
      "Epoch 15/38, Train Loss: 1.3352, Test Loss: 1.0367\n",
      "Epoch 16/38, Train Loss: 1.3053, Test Loss: 1.0404\n",
      "Epoch 17/38, Train Loss: 1.2636, Test Loss: 1.0326\n",
      "Epoch 18/38, Train Loss: 1.2342, Test Loss: 1.0304\n",
      "Epoch 19/38, Train Loss: 1.3081, Test Loss: 1.0275\n",
      "Epoch 20/38, Train Loss: 1.1802, Test Loss: 1.0316\n",
      "Epoch 21/38, Train Loss: 1.1959, Test Loss: 1.0296\n",
      "Epoch 22/38, Train Loss: 1.2219, Test Loss: 1.0307\n",
      "Epoch 23/38, Train Loss: 1.1896, Test Loss: 1.0326\n",
      "Epoch 24/38, Train Loss: 1.1716, Test Loss: 1.0332\n",
      "Epoch 25/38, Train Loss: 1.2011, Test Loss: 1.0318\n",
      "Epoch 26/38, Train Loss: 1.1439, Test Loss: 1.0320\n",
      "Epoch 27/38, Train Loss: 1.2154, Test Loss: 1.0273\n",
      "Epoch 28/38, Train Loss: 1.2062, Test Loss: 1.0275\n",
      "Epoch 29/38, Train Loss: 1.1665, Test Loss: 1.0296\n",
      "Epoch 30/38, Train Loss: 1.1451, Test Loss: 1.0268\n",
      "Epoch 31/38, Train Loss: 1.2521, Test Loss: 1.0290\n",
      "Epoch 32/38, Train Loss: 1.1148, Test Loss: 1.0313\n",
      "Epoch 33/38, Train Loss: 1.1295, Test Loss: 1.0288\n",
      "Epoch 34/38, Train Loss: 1.1661, Test Loss: 1.0304\n",
      "Epoch 35/38, Train Loss: 1.1763, Test Loss: 1.0279\n",
      "Epoch 36/38, Train Loss: 1.1449, Test Loss: 1.0282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:30:38,174] Trial 138 finished with value: 1.0306891288076128 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 127, 'layer_1_size': 131, 'layer_2_size': 205, 'layer_3_size': 198, 'layer_4_size': 60, 'layer_5_size': 143, 'layer_6_size': 159, 'layer_7_size': 243, 'dropout_rate': 0.42621544399517475, 'learning_rate': 1.520107724611832e-05, 'batch_size': 32, 'epochs': 38}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/38, Train Loss: 1.1597, Test Loss: 1.0296\n",
      "Epoch 38/38, Train Loss: 1.1616, Test Loss: 1.0307\n",
      "Epoch 1/42, Train Loss: 1.1392, Test Loss: 1.0827\n",
      "Epoch 2/42, Train Loss: 1.0376, Test Loss: 1.0806\n",
      "Epoch 3/42, Train Loss: 1.0570, Test Loss: 1.0796\n",
      "Epoch 4/42, Train Loss: 1.0766, Test Loss: 1.0842\n",
      "Epoch 5/42, Train Loss: 1.0744, Test Loss: 1.0806\n",
      "Epoch 6/42, Train Loss: 1.0738, Test Loss: 1.0825\n",
      "Epoch 7/42, Train Loss: 1.0025, Test Loss: 1.0786\n",
      "Epoch 8/42, Train Loss: 0.9905, Test Loss: 1.0824\n",
      "Epoch 9/42, Train Loss: 1.0312, Test Loss: 1.0779\n",
      "Epoch 10/42, Train Loss: 1.0050, Test Loss: 1.0821\n",
      "Epoch 11/42, Train Loss: 0.9975, Test Loss: 1.0798\n",
      "Epoch 12/42, Train Loss: 0.9931, Test Loss: 1.0772\n",
      "Epoch 13/42, Train Loss: 0.9838, Test Loss: 1.0751\n",
      "Epoch 14/42, Train Loss: 0.9943, Test Loss: 1.0781\n",
      "Epoch 15/42, Train Loss: 0.9991, Test Loss: 1.0747\n",
      "Epoch 16/42, Train Loss: 0.9934, Test Loss: 1.0745\n",
      "Epoch 17/42, Train Loss: 1.0010, Test Loss: 1.0734\n",
      "Epoch 18/42, Train Loss: 1.0212, Test Loss: 1.0719\n",
      "Epoch 19/42, Train Loss: 0.9477, Test Loss: 1.0683\n",
      "Epoch 20/42, Train Loss: 0.9744, Test Loss: 1.0692\n",
      "Epoch 21/42, Train Loss: 0.9672, Test Loss: 1.0702\n",
      "Epoch 22/42, Train Loss: 0.9822, Test Loss: 1.0699\n",
      "Epoch 23/42, Train Loss: 0.9977, Test Loss: 1.0685\n",
      "Epoch 24/42, Train Loss: 0.9925, Test Loss: 1.0700\n",
      "Epoch 25/42, Train Loss: 0.9581, Test Loss: 1.0715\n",
      "Epoch 26/42, Train Loss: 0.9363, Test Loss: 1.0708\n",
      "Epoch 27/42, Train Loss: 0.9605, Test Loss: 1.0691\n",
      "Epoch 28/42, Train Loss: 0.9731, Test Loss: 1.0694\n",
      "Epoch 29/42, Train Loss: 0.9675, Test Loss: 1.0679\n",
      "Epoch 30/42, Train Loss: 0.9555, Test Loss: 1.0677\n",
      "Epoch 31/42, Train Loss: 0.9710, Test Loss: 1.0690\n",
      "Epoch 32/42, Train Loss: 0.9548, Test Loss: 1.0675\n",
      "Epoch 33/42, Train Loss: 0.9638, Test Loss: 1.0696\n",
      "Epoch 34/42, Train Loss: 0.9396, Test Loss: 1.0681\n",
      "Epoch 35/42, Train Loss: 0.9518, Test Loss: 1.0685\n",
      "Epoch 36/42, Train Loss: 0.9634, Test Loss: 1.0654\n",
      "Epoch 37/42, Train Loss: 0.9435, Test Loss: 1.0663\n",
      "Epoch 38/42, Train Loss: 0.9821, Test Loss: 1.0679\n",
      "Epoch 39/42, Train Loss: 0.9613, Test Loss: 1.0693\n",
      "Epoch 40/42, Train Loss: 0.9432, Test Loss: 1.0700\n",
      "Epoch 41/42, Train Loss: 0.9598, Test Loss: 1.0719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:30:40,988] Trial 139 finished with value: 1.071152150630951 and parameters: {'num_hidden_layers': 5, 'layer_0_size': 102, 'layer_1_size': 171, 'layer_2_size': 211, 'layer_3_size': 212, 'layer_4_size': 84, 'dropout_rate': 0.3831090987094694, 'learning_rate': 0.00020272768735071214, 'batch_size': 64, 'epochs': 42}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/42, Train Loss: 0.9395, Test Loss: 1.0712\n",
      "Epoch 1/64, Train Loss: 1.3843, Test Loss: 0.8751\n",
      "Epoch 2/64, Train Loss: 1.3825, Test Loss: 0.8681\n",
      "Epoch 3/64, Train Loss: 1.3754, Test Loss: 0.8655\n",
      "Epoch 4/64, Train Loss: 1.4542, Test Loss: 0.8661\n",
      "Epoch 5/64, Train Loss: 1.3017, Test Loss: 0.8661\n",
      "Epoch 6/64, Train Loss: 1.3189, Test Loss: 0.8650\n",
      "Epoch 7/64, Train Loss: 1.3209, Test Loss: 0.8672\n",
      "Epoch 8/64, Train Loss: 1.3477, Test Loss: 0.8687\n",
      "Epoch 9/64, Train Loss: 1.3454, Test Loss: 0.8693\n",
      "Epoch 10/64, Train Loss: 1.3663, Test Loss: 0.8687\n",
      "Epoch 11/64, Train Loss: 1.3149, Test Loss: 0.8738\n",
      "Epoch 12/64, Train Loss: 1.3568, Test Loss: 0.8740\n",
      "Epoch 13/64, Train Loss: 1.3027, Test Loss: 0.8706\n",
      "Epoch 14/64, Train Loss: 1.3890, Test Loss: 0.8741\n",
      "Epoch 15/64, Train Loss: 1.3108, Test Loss: 0.8740\n",
      "Epoch 16/64, Train Loss: 1.3241, Test Loss: 0.8739\n",
      "Epoch 17/64, Train Loss: 1.3289, Test Loss: 0.8779\n",
      "Epoch 18/64, Train Loss: 1.2402, Test Loss: 0.8759\n",
      "Epoch 19/64, Train Loss: 1.3302, Test Loss: 0.8774\n",
      "Epoch 20/64, Train Loss: 1.2756, Test Loss: 0.8741\n",
      "Epoch 21/64, Train Loss: 1.3094, Test Loss: 0.8753\n",
      "Epoch 22/64, Train Loss: 1.2915, Test Loss: 0.8763\n",
      "Epoch 23/64, Train Loss: 1.2694, Test Loss: 0.8736\n",
      "Epoch 24/64, Train Loss: 1.2501, Test Loss: 0.8715\n",
      "Epoch 25/64, Train Loss: 1.3265, Test Loss: 0.8740\n",
      "Epoch 26/64, Train Loss: 1.2941, Test Loss: 0.8778\n",
      "Epoch 27/64, Train Loss: 1.2791, Test Loss: 0.8761\n",
      "Epoch 28/64, Train Loss: 1.2817, Test Loss: 0.8742\n",
      "Epoch 29/64, Train Loss: 1.3078, Test Loss: 0.8742\n",
      "Epoch 30/64, Train Loss: 1.2847, Test Loss: 0.8736\n",
      "Epoch 31/64, Train Loss: 1.2379, Test Loss: 0.8778\n",
      "Epoch 32/64, Train Loss: 1.2890, Test Loss: 0.8771\n",
      "Epoch 33/64, Train Loss: 1.3353, Test Loss: 0.8789\n",
      "Epoch 34/64, Train Loss: 1.2283, Test Loss: 0.8784\n",
      "Epoch 35/64, Train Loss: 1.2192, Test Loss: 0.8781\n",
      "Epoch 36/64, Train Loss: 1.2462, Test Loss: 0.8777\n",
      "Epoch 37/64, Train Loss: 1.2740, Test Loss: 0.8784\n",
      "Epoch 38/64, Train Loss: 1.2545, Test Loss: 0.8771\n",
      "Epoch 39/64, Train Loss: 1.2542, Test Loss: 0.8776\n",
      "Epoch 40/64, Train Loss: 1.2525, Test Loss: 0.8804\n",
      "Epoch 41/64, Train Loss: 1.2443, Test Loss: 0.8793\n",
      "Epoch 42/64, Train Loss: 1.2441, Test Loss: 0.8795\n",
      "Epoch 43/64, Train Loss: 1.2314, Test Loss: 0.8798\n",
      "Epoch 44/64, Train Loss: 1.2340, Test Loss: 0.8790\n",
      "Epoch 45/64, Train Loss: 1.3080, Test Loss: 0.8785\n",
      "Epoch 46/64, Train Loss: 1.2466, Test Loss: 0.8765\n",
      "Epoch 47/64, Train Loss: 1.2455, Test Loss: 0.8788\n",
      "Epoch 48/64, Train Loss: 1.2358, Test Loss: 0.8780\n",
      "Epoch 49/64, Train Loss: 1.2319, Test Loss: 0.8780\n",
      "Epoch 50/64, Train Loss: 1.2591, Test Loss: 0.8792\n",
      "Epoch 51/64, Train Loss: 1.2502, Test Loss: 0.8787\n",
      "Epoch 52/64, Train Loss: 1.2434, Test Loss: 0.8809\n",
      "Epoch 53/64, Train Loss: 1.2322, Test Loss: 0.8813\n",
      "Epoch 54/64, Train Loss: 1.2250, Test Loss: 0.8836\n",
      "Epoch 55/64, Train Loss: 1.2272, Test Loss: 0.8803\n",
      "Epoch 56/64, Train Loss: 1.2227, Test Loss: 0.8825\n",
      "Epoch 57/64, Train Loss: 1.2329, Test Loss: 0.8798\n",
      "Epoch 58/64, Train Loss: 1.1981, Test Loss: 0.8822\n",
      "Epoch 59/64, Train Loss: 1.2354, Test Loss: 0.8816\n",
      "Epoch 60/64, Train Loss: 1.2718, Test Loss: 0.8807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:30:44,969] Trial 140 finished with value: 0.8825024664402008 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 122, 'layer_1_size': 128, 'layer_2_size': 185, 'layer_3_size': 48, 'layer_4_size': 65, 'layer_5_size': 127, 'layer_6_size': 152, 'dropout_rate': 0.4762559183723969, 'learning_rate': 8.634164393950472e-05, 'batch_size': 64, 'epochs': 64}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/64, Train Loss: 1.2240, Test Loss: 0.8807\n",
      "Epoch 62/64, Train Loss: 1.2069, Test Loss: 0.8807\n",
      "Epoch 63/64, Train Loss: 1.2535, Test Loss: 0.8823\n",
      "Epoch 64/64, Train Loss: 1.1810, Test Loss: 0.8825\n",
      "Epoch 1/85, Train Loss: 1.2063, Test Loss: 0.9298\n",
      "Epoch 2/85, Train Loss: 1.2282, Test Loss: 0.9341\n",
      "Epoch 3/85, Train Loss: 1.2071, Test Loss: 0.9339\n",
      "Epoch 4/85, Train Loss: 1.1977, Test Loss: 0.9302\n",
      "Epoch 5/85, Train Loss: 1.2013, Test Loss: 0.9335\n",
      "Epoch 6/85, Train Loss: 1.1810, Test Loss: 0.9319\n",
      "Epoch 7/85, Train Loss: 1.2049, Test Loss: 0.9312\n",
      "Epoch 8/85, Train Loss: 1.1320, Test Loss: 0.9327\n",
      "Epoch 9/85, Train Loss: 1.2140, Test Loss: 0.9326\n",
      "Epoch 10/85, Train Loss: 1.2143, Test Loss: 0.9356\n",
      "Epoch 11/85, Train Loss: 1.1330, Test Loss: 0.9316\n",
      "Epoch 12/85, Train Loss: 1.1732, Test Loss: 0.9367\n",
      "Epoch 13/85, Train Loss: 1.1973, Test Loss: 0.9369\n",
      "Epoch 14/85, Train Loss: 1.1359, Test Loss: 0.9343\n",
      "Epoch 15/85, Train Loss: 1.1922, Test Loss: 0.9315\n",
      "Epoch 16/85, Train Loss: 1.1648, Test Loss: 0.9333\n",
      "Epoch 17/85, Train Loss: 1.1542, Test Loss: 0.9325\n",
      "Epoch 18/85, Train Loss: 1.1564, Test Loss: 0.9354\n",
      "Epoch 19/85, Train Loss: 1.1656, Test Loss: 0.9326\n",
      "Epoch 20/85, Train Loss: 1.1213, Test Loss: 0.9311\n",
      "Epoch 21/85, Train Loss: 1.1559, Test Loss: 0.9303\n",
      "Epoch 22/85, Train Loss: 1.1732, Test Loss: 0.9308\n",
      "Epoch 23/85, Train Loss: 1.1562, Test Loss: 0.9313\n",
      "Epoch 24/85, Train Loss: 1.1186, Test Loss: 0.9290\n",
      "Epoch 25/85, Train Loss: 1.1185, Test Loss: 0.9300\n",
      "Epoch 26/85, Train Loss: 1.1172, Test Loss: 0.9299\n",
      "Epoch 27/85, Train Loss: 1.1361, Test Loss: 0.9332\n",
      "Epoch 28/85, Train Loss: 1.1259, Test Loss: 0.9321\n",
      "Epoch 29/85, Train Loss: 1.1460, Test Loss: 0.9313\n",
      "Epoch 30/85, Train Loss: 1.1406, Test Loss: 0.9309\n",
      "Epoch 31/85, Train Loss: 1.1475, Test Loss: 0.9301\n",
      "Epoch 32/85, Train Loss: 1.1334, Test Loss: 0.9321\n",
      "Epoch 33/85, Train Loss: 1.0941, Test Loss: 0.9321\n",
      "Epoch 34/85, Train Loss: 1.1399, Test Loss: 0.9317\n",
      "Epoch 35/85, Train Loss: 1.0930, Test Loss: 0.9334\n",
      "Epoch 36/85, Train Loss: 1.1259, Test Loss: 0.9322\n",
      "Epoch 37/85, Train Loss: 1.1362, Test Loss: 0.9311\n",
      "Epoch 38/85, Train Loss: 1.1192, Test Loss: 0.9315\n",
      "Epoch 39/85, Train Loss: 1.0897, Test Loss: 0.9304\n",
      "Epoch 40/85, Train Loss: 1.1032, Test Loss: 0.9307\n",
      "Epoch 41/85, Train Loss: 1.0775, Test Loss: 0.9314\n",
      "Epoch 42/85, Train Loss: 1.1292, Test Loss: 0.9320\n",
      "Epoch 43/85, Train Loss: 1.0832, Test Loss: 0.9306\n",
      "Epoch 44/85, Train Loss: 1.0635, Test Loss: 0.9307\n",
      "Epoch 45/85, Train Loss: 1.1105, Test Loss: 0.9282\n",
      "Epoch 46/85, Train Loss: 1.0750, Test Loss: 0.9327\n",
      "Epoch 47/85, Train Loss: 1.1105, Test Loss: 0.9304\n",
      "Epoch 48/85, Train Loss: 1.0610, Test Loss: 0.9296\n",
      "Epoch 49/85, Train Loss: 1.0896, Test Loss: 0.9304\n",
      "Epoch 50/85, Train Loss: 1.0901, Test Loss: 0.9313\n",
      "Epoch 51/85, Train Loss: 1.1133, Test Loss: 0.9310\n",
      "Epoch 52/85, Train Loss: 1.1208, Test Loss: 0.9318\n",
      "Epoch 53/85, Train Loss: 1.0925, Test Loss: 0.9323\n",
      "Epoch 54/85, Train Loss: 1.1208, Test Loss: 0.9324\n",
      "Epoch 55/85, Train Loss: 1.0965, Test Loss: 0.9296\n",
      "Epoch 56/85, Train Loss: 1.0871, Test Loss: 0.9297\n",
      "Epoch 57/85, Train Loss: 1.1043, Test Loss: 0.9312\n",
      "Epoch 58/85, Train Loss: 1.0767, Test Loss: 0.9309\n",
      "Epoch 59/85, Train Loss: 1.1424, Test Loss: 0.9291\n",
      "Epoch 60/85, Train Loss: 1.0879, Test Loss: 0.9286\n",
      "Epoch 61/85, Train Loss: 1.0931, Test Loss: 0.9276\n",
      "Epoch 62/85, Train Loss: 1.1547, Test Loss: 0.9275\n",
      "Epoch 63/85, Train Loss: 1.0839, Test Loss: 0.9300\n",
      "Epoch 64/85, Train Loss: 1.1342, Test Loss: 0.9298\n",
      "Epoch 65/85, Train Loss: 1.0955, Test Loss: 0.9302\n",
      "Epoch 66/85, Train Loss: 1.1095, Test Loss: 0.9296\n",
      "Epoch 67/85, Train Loss: 1.1141, Test Loss: 0.9282\n",
      "Epoch 68/85, Train Loss: 1.1220, Test Loss: 0.9302\n",
      "Epoch 69/85, Train Loss: 1.0910, Test Loss: 0.9302\n",
      "Epoch 70/85, Train Loss: 1.0776, Test Loss: 0.9302\n",
      "Epoch 71/85, Train Loss: 1.1134, Test Loss: 0.9324\n",
      "Epoch 72/85, Train Loss: 1.0939, Test Loss: 0.9324\n",
      "Epoch 73/85, Train Loss: 1.0951, Test Loss: 0.9301\n",
      "Epoch 74/85, Train Loss: 1.0536, Test Loss: 0.9310\n",
      "Epoch 75/85, Train Loss: 1.0443, Test Loss: 0.9324\n",
      "Epoch 76/85, Train Loss: 1.0825, Test Loss: 0.9298\n",
      "Epoch 77/85, Train Loss: 1.0495, Test Loss: 0.9298\n",
      "Epoch 78/85, Train Loss: 1.0756, Test Loss: 0.9287\n",
      "Epoch 79/85, Train Loss: 1.1009, Test Loss: 0.9303\n",
      "Epoch 80/85, Train Loss: 1.0529, Test Loss: 0.9301\n",
      "Epoch 81/85, Train Loss: 1.0597, Test Loss: 0.9316\n",
      "Epoch 82/85, Train Loss: 1.0383, Test Loss: 0.9299\n",
      "Epoch 83/85, Train Loss: 1.0475, Test Loss: 0.9313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:30:51,922] Trial 141 finished with value: 0.9312921166419983 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 235, 'layer_1_size': 113, 'layer_2_size': 199, 'layer_3_size': 207, 'layer_4_size': 234, 'layer_5_size': 250, 'dropout_rate': 0.44693604331866815, 'learning_rate': 5.1326389309888935e-05, 'batch_size': 64, 'epochs': 85}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/85, Train Loss: 1.0432, Test Loss: 0.9305\n",
      "Epoch 85/85, Train Loss: 1.0479, Test Loss: 0.9313\n",
      "Epoch 1/46, Train Loss: 1.2408, Test Loss: 1.0172\n",
      "Epoch 2/46, Train Loss: 1.1468, Test Loss: 1.0473\n",
      "Epoch 3/46, Train Loss: 1.1672, Test Loss: 1.0437\n",
      "Epoch 4/46, Train Loss: 1.2040, Test Loss: 1.0404\n",
      "Epoch 5/46, Train Loss: 1.0645, Test Loss: 1.0334\n",
      "Epoch 6/46, Train Loss: 1.0892, Test Loss: 1.0276\n",
      "Epoch 7/46, Train Loss: 1.0844, Test Loss: 1.0258\n",
      "Epoch 8/46, Train Loss: 1.1174, Test Loss: 1.0182\n",
      "Epoch 9/46, Train Loss: 1.0820, Test Loss: 1.0199\n",
      "Epoch 10/46, Train Loss: 1.0509, Test Loss: 1.0206\n",
      "Epoch 11/46, Train Loss: 1.1239, Test Loss: 1.0204\n",
      "Epoch 12/46, Train Loss: 1.1355, Test Loss: 1.0193\n",
      "Epoch 13/46, Train Loss: 1.1221, Test Loss: 1.0175\n",
      "Epoch 14/46, Train Loss: 1.0660, Test Loss: 1.0161\n",
      "Epoch 15/46, Train Loss: 1.0666, Test Loss: 1.0131\n",
      "Epoch 16/46, Train Loss: 1.1043, Test Loss: 1.0118\n",
      "Epoch 17/46, Train Loss: 1.0869, Test Loss: 1.0097\n",
      "Epoch 18/46, Train Loss: 1.0410, Test Loss: 1.0107\n",
      "Epoch 19/46, Train Loss: 1.0449, Test Loss: 1.0147\n",
      "Epoch 20/46, Train Loss: 1.0818, Test Loss: 1.0079\n",
      "Epoch 21/46, Train Loss: 1.0690, Test Loss: 1.0077\n",
      "Epoch 22/46, Train Loss: 1.0661, Test Loss: 1.0141\n",
      "Epoch 23/46, Train Loss: 1.0166, Test Loss: 1.0092\n",
      "Epoch 24/46, Train Loss: 1.0335, Test Loss: 1.0102\n",
      "Epoch 25/46, Train Loss: 1.0139, Test Loss: 1.0093\n",
      "Epoch 26/46, Train Loss: 1.0164, Test Loss: 1.0080\n",
      "Epoch 27/46, Train Loss: 1.0179, Test Loss: 1.0088\n",
      "Epoch 28/46, Train Loss: 1.0520, Test Loss: 1.0069\n",
      "Epoch 29/46, Train Loss: 1.0854, Test Loss: 1.0090\n",
      "Epoch 30/46, Train Loss: 1.0142, Test Loss: 1.0088\n",
      "Epoch 31/46, Train Loss: 1.0622, Test Loss: 1.0128\n",
      "Epoch 32/46, Train Loss: 1.0541, Test Loss: 1.0084\n",
      "Epoch 33/46, Train Loss: 1.0195, Test Loss: 1.0080\n",
      "Epoch 34/46, Train Loss: 1.0404, Test Loss: 1.0062\n",
      "Epoch 35/46, Train Loss: 1.0357, Test Loss: 1.0059\n",
      "Epoch 36/46, Train Loss: 0.9911, Test Loss: 1.0068\n",
      "Epoch 37/46, Train Loss: 1.0119, Test Loss: 1.0072\n",
      "Epoch 38/46, Train Loss: 1.0253, Test Loss: 1.0067\n",
      "Epoch 39/46, Train Loss: 0.9781, Test Loss: 1.0063\n",
      "Epoch 40/46, Train Loss: 1.0232, Test Loss: 1.0043\n",
      "Epoch 41/46, Train Loss: 1.0598, Test Loss: 1.0066\n",
      "Epoch 42/46, Train Loss: 0.9892, Test Loss: 1.0077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:30:55,143] Trial 142 finished with value: 1.0046185404062271 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 145, 'layer_1_size': 122, 'layer_2_size': 209, 'layer_3_size': 203, 'layer_4_size': 79, 'layer_5_size': 225, 'dropout_rate': 0.4285724707012848, 'learning_rate': 5.721273902412809e-05, 'batch_size': 64, 'epochs': 46}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/46, Train Loss: 1.0216, Test Loss: 1.0065\n",
      "Epoch 44/46, Train Loss: 1.0528, Test Loss: 1.0065\n",
      "Epoch 45/46, Train Loss: 1.0127, Test Loss: 1.0058\n",
      "Epoch 46/46, Train Loss: 0.9949, Test Loss: 1.0046\n",
      "Epoch 1/50, Train Loss: 1.4928, Test Loss: 0.9479\n",
      "Epoch 2/50, Train Loss: 1.3390, Test Loss: 0.9639\n",
      "Epoch 3/50, Train Loss: 1.3851, Test Loss: 0.9466\n",
      "Epoch 4/50, Train Loss: 1.2040, Test Loss: 0.9345\n",
      "Epoch 5/50, Train Loss: 1.3138, Test Loss: 0.9264\n",
      "Epoch 6/50, Train Loss: 1.2376, Test Loss: 0.9214\n",
      "Epoch 7/50, Train Loss: 1.2218, Test Loss: 0.9200\n",
      "Epoch 8/50, Train Loss: 1.2020, Test Loss: 0.9145\n",
      "Epoch 9/50, Train Loss: 1.2106, Test Loss: 0.9139\n",
      "Epoch 10/50, Train Loss: 1.2310, Test Loss: 0.9175\n",
      "Epoch 11/50, Train Loss: 1.2336, Test Loss: 0.9181\n",
      "Epoch 12/50, Train Loss: 1.2113, Test Loss: 0.9202\n",
      "Epoch 13/50, Train Loss: 1.1743, Test Loss: 0.9215\n",
      "Epoch 14/50, Train Loss: 1.2473, Test Loss: 0.9178\n",
      "Epoch 15/50, Train Loss: 1.1470, Test Loss: 0.9207\n",
      "Epoch 16/50, Train Loss: 1.2196, Test Loss: 0.9171\n",
      "Epoch 17/50, Train Loss: 1.2487, Test Loss: 0.9166\n",
      "Epoch 18/50, Train Loss: 1.2301, Test Loss: 0.9174\n",
      "Epoch 19/50, Train Loss: 1.1542, Test Loss: 0.9133\n",
      "Epoch 20/50, Train Loss: 1.1798, Test Loss: 0.9112\n",
      "Epoch 21/50, Train Loss: 1.1579, Test Loss: 0.9164\n",
      "Epoch 22/50, Train Loss: 1.1865, Test Loss: 0.9182\n",
      "Epoch 23/50, Train Loss: 1.1833, Test Loss: 0.9156\n",
      "Epoch 24/50, Train Loss: 1.1582, Test Loss: 0.9184\n",
      "Epoch 25/50, Train Loss: 1.1635, Test Loss: 0.9210\n",
      "Epoch 26/50, Train Loss: 1.1412, Test Loss: 0.9245\n",
      "Epoch 27/50, Train Loss: 1.2277, Test Loss: 0.9194\n",
      "Epoch 28/50, Train Loss: 1.1876, Test Loss: 0.9146\n",
      "Epoch 29/50, Train Loss: 1.1577, Test Loss: 0.9147\n",
      "Epoch 30/50, Train Loss: 1.1395, Test Loss: 0.9190\n",
      "Epoch 31/50, Train Loss: 1.1737, Test Loss: 0.9209\n",
      "Epoch 32/50, Train Loss: 1.1517, Test Loss: 0.9247\n",
      "Epoch 33/50, Train Loss: 1.1429, Test Loss: 0.9265\n",
      "Epoch 34/50, Train Loss: 1.1541, Test Loss: 0.9262\n",
      "Epoch 35/50, Train Loss: 1.1233, Test Loss: 0.9241\n",
      "Epoch 36/50, Train Loss: 1.1448, Test Loss: 0.9203\n",
      "Epoch 37/50, Train Loss: 1.2029, Test Loss: 0.9237\n",
      "Epoch 38/50, Train Loss: 1.1369, Test Loss: 0.9243\n",
      "Epoch 39/50, Train Loss: 1.1421, Test Loss: 0.9262\n",
      "Epoch 40/50, Train Loss: 1.0975, Test Loss: 0.9257\n",
      "Epoch 41/50, Train Loss: 1.1425, Test Loss: 0.9274\n",
      "Epoch 42/50, Train Loss: 1.1194, Test Loss: 0.9310\n",
      "Epoch 43/50, Train Loss: 1.1355, Test Loss: 0.9280\n",
      "Epoch 44/50, Train Loss: 1.1738, Test Loss: 0.9258\n",
      "Epoch 45/50, Train Loss: 1.1320, Test Loss: 0.9263\n",
      "Epoch 46/50, Train Loss: 1.1444, Test Loss: 0.9233\n",
      "Epoch 47/50, Train Loss: 1.1349, Test Loss: 0.9268\n",
      "Epoch 48/50, Train Loss: 1.1274, Test Loss: 0.9222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:30:59,547] Trial 143 finished with value: 0.9190860390663147 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 161, 'layer_1_size': 105, 'layer_2_size': 222, 'layer_3_size': 229, 'layer_4_size': 181, 'layer_5_size': 215, 'dropout_rate': 0.4074455970833377, 'learning_rate': 0.00012432662651144886, 'batch_size': 64, 'epochs': 50}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50, Train Loss: 1.1246, Test Loss: 0.9193\n",
      "Epoch 50/50, Train Loss: 1.1599, Test Loss: 0.9191\n",
      "Epoch 1/81, Train Loss: 1.2394, Test Loss: 1.0888\n",
      "Epoch 2/81, Train Loss: 1.2114, Test Loss: 1.1012\n",
      "Epoch 3/81, Train Loss: 1.2775, Test Loss: 1.1154\n",
      "Epoch 4/81, Train Loss: 1.1751, Test Loss: 1.1196\n",
      "Epoch 5/81, Train Loss: 1.2267, Test Loss: 1.1246\n",
      "Epoch 6/81, Train Loss: 1.2162, Test Loss: 1.1185\n",
      "Epoch 7/81, Train Loss: 1.2023, Test Loss: 1.1192\n",
      "Epoch 8/81, Train Loss: 1.2153, Test Loss: 1.1201\n",
      "Epoch 9/81, Train Loss: 1.2352, Test Loss: 1.1195\n",
      "Epoch 10/81, Train Loss: 1.2061, Test Loss: 1.1129\n",
      "Epoch 11/81, Train Loss: 1.2456, Test Loss: 1.1205\n",
      "Epoch 12/81, Train Loss: 1.2048, Test Loss: 1.1151\n",
      "Epoch 13/81, Train Loss: 1.1434, Test Loss: 1.1169\n",
      "Epoch 14/81, Train Loss: 1.1921, Test Loss: 1.1256\n",
      "Epoch 15/81, Train Loss: 1.1354, Test Loss: 1.1156\n",
      "Epoch 16/81, Train Loss: 1.1703, Test Loss: 1.1129\n",
      "Epoch 17/81, Train Loss: 1.1631, Test Loss: 1.1132\n",
      "Epoch 18/81, Train Loss: 1.2148, Test Loss: 1.1180\n",
      "Epoch 19/81, Train Loss: 1.1543, Test Loss: 1.1169\n",
      "Epoch 20/81, Train Loss: 1.1702, Test Loss: 1.1175\n",
      "Epoch 21/81, Train Loss: 1.1686, Test Loss: 1.1156\n",
      "Epoch 22/81, Train Loss: 1.1741, Test Loss: 1.1119\n",
      "Epoch 23/81, Train Loss: 1.1881, Test Loss: 1.1132\n",
      "Epoch 24/81, Train Loss: 1.2003, Test Loss: 1.1163\n",
      "Epoch 25/81, Train Loss: 1.1905, Test Loss: 1.1193\n",
      "Epoch 26/81, Train Loss: 1.1800, Test Loss: 1.1173\n",
      "Epoch 27/81, Train Loss: 1.1218, Test Loss: 1.1141\n",
      "Epoch 28/81, Train Loss: 1.1809, Test Loss: 1.1135\n",
      "Epoch 29/81, Train Loss: 1.1894, Test Loss: 1.1127\n",
      "Epoch 30/81, Train Loss: 1.1213, Test Loss: 1.1197\n",
      "Epoch 31/81, Train Loss: 1.1471, Test Loss: 1.1183\n",
      "Epoch 32/81, Train Loss: 1.1400, Test Loss: 1.1118\n",
      "Epoch 33/81, Train Loss: 1.1553, Test Loss: 1.1156\n",
      "Epoch 34/81, Train Loss: 1.1774, Test Loss: 1.1153\n",
      "Epoch 35/81, Train Loss: 1.1228, Test Loss: 1.1135\n",
      "Epoch 36/81, Train Loss: 1.2025, Test Loss: 1.1146\n",
      "Epoch 37/81, Train Loss: 1.2319, Test Loss: 1.1056\n",
      "Epoch 38/81, Train Loss: 1.1408, Test Loss: 1.1081\n",
      "Epoch 39/81, Train Loss: 1.2096, Test Loss: 1.1057\n",
      "Epoch 40/81, Train Loss: 1.1776, Test Loss: 1.1117\n",
      "Epoch 41/81, Train Loss: 1.1387, Test Loss: 1.1119\n",
      "Epoch 42/81, Train Loss: 1.1950, Test Loss: 1.1137\n",
      "Epoch 43/81, Train Loss: 1.1094, Test Loss: 1.1104\n",
      "Epoch 44/81, Train Loss: 1.1495, Test Loss: 1.1111\n",
      "Epoch 45/81, Train Loss: 1.1422, Test Loss: 1.1068\n",
      "Epoch 46/81, Train Loss: 1.1837, Test Loss: 1.1037\n",
      "Epoch 47/81, Train Loss: 1.1702, Test Loss: 1.1035\n",
      "Epoch 48/81, Train Loss: 1.1319, Test Loss: 1.1106\n",
      "Epoch 49/81, Train Loss: 1.1446, Test Loss: 1.1056\n",
      "Epoch 50/81, Train Loss: 1.1619, Test Loss: 1.1054\n",
      "Epoch 51/81, Train Loss: 1.1296, Test Loss: 1.1103\n",
      "Epoch 52/81, Train Loss: 1.1394, Test Loss: 1.1088\n",
      "Epoch 53/81, Train Loss: 1.1599, Test Loss: 1.1117\n",
      "Epoch 54/81, Train Loss: 1.1612, Test Loss: 1.1097\n",
      "Epoch 55/81, Train Loss: 1.1435, Test Loss: 1.1059\n",
      "Epoch 56/81, Train Loss: 1.1400, Test Loss: 1.1061\n",
      "Epoch 57/81, Train Loss: 1.1387, Test Loss: 1.1026\n",
      "Epoch 58/81, Train Loss: 1.1619, Test Loss: 1.1059\n",
      "Epoch 59/81, Train Loss: 1.1373, Test Loss: 1.1038\n",
      "Epoch 60/81, Train Loss: 1.1140, Test Loss: 1.1065\n",
      "Epoch 61/81, Train Loss: 1.1204, Test Loss: 1.1082\n",
      "Epoch 62/81, Train Loss: 1.0998, Test Loss: 1.1095\n",
      "Epoch 63/81, Train Loss: 1.1457, Test Loss: 1.1069\n",
      "Epoch 64/81, Train Loss: 1.1166, Test Loss: 1.1103\n",
      "Epoch 65/81, Train Loss: 1.0875, Test Loss: 1.1115\n",
      "Epoch 66/81, Train Loss: 1.1112, Test Loss: 1.1055\n",
      "Epoch 67/81, Train Loss: 1.1300, Test Loss: 1.1036\n",
      "Epoch 68/81, Train Loss: 1.1386, Test Loss: 1.1069\n",
      "Epoch 69/81, Train Loss: 1.1328, Test Loss: 1.1049\n",
      "Epoch 70/81, Train Loss: 1.1089, Test Loss: 1.1072\n",
      "Epoch 71/81, Train Loss: 1.0915, Test Loss: 1.1040\n",
      "Epoch 72/81, Train Loss: 1.1111, Test Loss: 1.1021\n",
      "Epoch 73/81, Train Loss: 1.1640, Test Loss: 1.1046\n",
      "Epoch 74/81, Train Loss: 1.1060, Test Loss: 1.1042\n",
      "Epoch 75/81, Train Loss: 1.1087, Test Loss: 1.1057\n",
      "Epoch 76/81, Train Loss: 1.1098, Test Loss: 1.1073\n",
      "Epoch 77/81, Train Loss: 1.0960, Test Loss: 1.1026\n",
      "Epoch 78/81, Train Loss: 1.1140, Test Loss: 1.1040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:07,635] Trial 144 finished with value: 1.1041750013828278 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 134, 'layer_1_size': 140, 'layer_2_size': 74, 'layer_3_size': 192, 'layer_4_size': 217, 'layer_5_size': 235, 'layer_6_size': 164, 'dropout_rate': 0.36679897286284313, 'learning_rate': 4.032839206374094e-05, 'batch_size': 64, 'epochs': 81}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/81, Train Loss: 1.1199, Test Loss: 1.1028\n",
      "Epoch 80/81, Train Loss: 1.1046, Test Loss: 1.1024\n",
      "Epoch 81/81, Train Loss: 1.1553, Test Loss: 1.1042\n",
      "Epoch 1/75, Train Loss: 1.1662, Test Loss: 0.9754\n",
      "Epoch 2/75, Train Loss: 1.2301, Test Loss: 0.9682\n",
      "Epoch 3/75, Train Loss: 1.1835, Test Loss: 0.9753\n",
      "Epoch 4/75, Train Loss: 1.2583, Test Loss: 0.9681\n",
      "Epoch 5/75, Train Loss: 1.1678, Test Loss: 0.9668\n",
      "Epoch 6/75, Train Loss: 1.1924, Test Loss: 0.9685\n",
      "Epoch 7/75, Train Loss: 1.1959, Test Loss: 0.9715\n",
      "Epoch 8/75, Train Loss: 1.2111, Test Loss: 0.9673\n",
      "Epoch 9/75, Train Loss: 1.2183, Test Loss: 0.9780\n",
      "Epoch 10/75, Train Loss: 1.1745, Test Loss: 0.9729\n",
      "Epoch 11/75, Train Loss: 1.1598, Test Loss: 0.9726\n",
      "Epoch 12/75, Train Loss: 1.1692, Test Loss: 0.9761\n",
      "Epoch 13/75, Train Loss: 1.2071, Test Loss: 0.9767\n",
      "Epoch 14/75, Train Loss: 1.1590, Test Loss: 0.9714\n",
      "Epoch 15/75, Train Loss: 1.1457, Test Loss: 0.9677\n",
      "Epoch 16/75, Train Loss: 1.1209, Test Loss: 0.9680\n",
      "Epoch 17/75, Train Loss: 1.1287, Test Loss: 0.9629\n",
      "Epoch 18/75, Train Loss: 1.1396, Test Loss: 0.9684\n",
      "Epoch 19/75, Train Loss: 1.1835, Test Loss: 0.9624\n",
      "Epoch 20/75, Train Loss: 1.1352, Test Loss: 0.9625\n",
      "Epoch 21/75, Train Loss: 1.1636, Test Loss: 0.9622\n",
      "Epoch 22/75, Train Loss: 1.1461, Test Loss: 0.9635\n",
      "Epoch 23/75, Train Loss: 1.1655, Test Loss: 0.9651\n",
      "Epoch 24/75, Train Loss: 1.1308, Test Loss: 0.9647\n",
      "Epoch 25/75, Train Loss: 1.0839, Test Loss: 0.9633\n",
      "Epoch 26/75, Train Loss: 1.0973, Test Loss: 0.9598\n",
      "Epoch 27/75, Train Loss: 1.1023, Test Loss: 0.9644\n",
      "Epoch 28/75, Train Loss: 1.1658, Test Loss: 0.9614\n",
      "Epoch 29/75, Train Loss: 1.1498, Test Loss: 0.9618\n",
      "Epoch 30/75, Train Loss: 1.1449, Test Loss: 0.9672\n",
      "Epoch 31/75, Train Loss: 1.1347, Test Loss: 0.9650\n",
      "Epoch 32/75, Train Loss: 1.1161, Test Loss: 0.9626\n",
      "Epoch 33/75, Train Loss: 1.1019, Test Loss: 0.9614\n",
      "Epoch 34/75, Train Loss: 1.1277, Test Loss: 0.9614\n",
      "Epoch 35/75, Train Loss: 1.1350, Test Loss: 0.9600\n",
      "Epoch 36/75, Train Loss: 1.1445, Test Loss: 0.9581\n",
      "Epoch 37/75, Train Loss: 1.1322, Test Loss: 0.9575\n",
      "Epoch 38/75, Train Loss: 1.0951, Test Loss: 0.9601\n",
      "Epoch 39/75, Train Loss: 1.1052, Test Loss: 0.9656\n",
      "Epoch 40/75, Train Loss: 1.1150, Test Loss: 0.9600\n",
      "Epoch 41/75, Train Loss: 1.1182, Test Loss: 0.9594\n",
      "Epoch 42/75, Train Loss: 1.1161, Test Loss: 0.9586\n",
      "Epoch 43/75, Train Loss: 1.1167, Test Loss: 0.9603\n",
      "Epoch 44/75, Train Loss: 1.1217, Test Loss: 0.9592\n",
      "Epoch 45/75, Train Loss: 1.0983, Test Loss: 0.9579\n",
      "Epoch 46/75, Train Loss: 1.1052, Test Loss: 0.9572\n",
      "Epoch 47/75, Train Loss: 1.1120, Test Loss: 0.9572\n",
      "Epoch 48/75, Train Loss: 1.1097, Test Loss: 0.9601\n",
      "Epoch 49/75, Train Loss: 1.1070, Test Loss: 0.9554\n",
      "Epoch 50/75, Train Loss: 1.1098, Test Loss: 0.9561\n",
      "Epoch 51/75, Train Loss: 1.0820, Test Loss: 0.9555\n",
      "Epoch 52/75, Train Loss: 1.1173, Test Loss: 0.9589\n",
      "Epoch 53/75, Train Loss: 1.1123, Test Loss: 0.9552\n",
      "Epoch 54/75, Train Loss: 1.1290, Test Loss: 0.9529\n",
      "Epoch 55/75, Train Loss: 1.1060, Test Loss: 0.9548\n",
      "Epoch 56/75, Train Loss: 1.0876, Test Loss: 0.9542\n",
      "Epoch 57/75, Train Loss: 1.1031, Test Loss: 0.9542\n",
      "Epoch 58/75, Train Loss: 1.0568, Test Loss: 0.9546\n",
      "Epoch 59/75, Train Loss: 1.0788, Test Loss: 0.9553\n",
      "Epoch 60/75, Train Loss: 1.0559, Test Loss: 0.9536\n",
      "Epoch 61/75, Train Loss: 1.1094, Test Loss: 0.9528\n",
      "Epoch 62/75, Train Loss: 1.0945, Test Loss: 0.9543\n",
      "Epoch 63/75, Train Loss: 1.0495, Test Loss: 0.9545\n",
      "Epoch 64/75, Train Loss: 1.0828, Test Loss: 0.9539\n",
      "Epoch 65/75, Train Loss: 1.0653, Test Loss: 0.9555\n",
      "Epoch 66/75, Train Loss: 1.0959, Test Loss: 0.9540\n",
      "Epoch 67/75, Train Loss: 1.0984, Test Loss: 0.9538\n",
      "Epoch 68/75, Train Loss: 1.0915, Test Loss: 0.9547\n",
      "Epoch 69/75, Train Loss: 1.0651, Test Loss: 0.9546\n",
      "Epoch 70/75, Train Loss: 1.0600, Test Loss: 0.9526\n",
      "Epoch 71/75, Train Loss: 1.0993, Test Loss: 0.9543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:13,080] Trial 145 finished with value: 0.9549714922904968 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 152, 'layer_1_size': 91, 'layer_2_size': 131, 'layer_3_size': 212, 'layer_4_size': 226, 'layer_5_size': 106, 'dropout_rate': 0.45234051676545717, 'learning_rate': 6.695296941865e-05, 'batch_size': 64, 'epochs': 75}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/75, Train Loss: 1.1078, Test Loss: 0.9545\n",
      "Epoch 73/75, Train Loss: 1.0970, Test Loss: 0.9529\n",
      "Epoch 74/75, Train Loss: 1.0278, Test Loss: 0.9559\n",
      "Epoch 75/75, Train Loss: 1.0766, Test Loss: 0.9550\n",
      "Epoch 1/68, Train Loss: 1.2914, Test Loss: 0.9930\n",
      "Epoch 2/68, Train Loss: 1.2714, Test Loss: 0.9956\n",
      "Epoch 3/68, Train Loss: 1.2234, Test Loss: 0.9979\n",
      "Epoch 4/68, Train Loss: 1.2677, Test Loss: 0.9992\n",
      "Epoch 5/68, Train Loss: 1.1858, Test Loss: 1.0029\n",
      "Epoch 6/68, Train Loss: 1.1908, Test Loss: 1.0029\n",
      "Epoch 7/68, Train Loss: 1.1934, Test Loss: 1.0043\n",
      "Epoch 8/68, Train Loss: 1.2014, Test Loss: 1.0041\n",
      "Epoch 9/68, Train Loss: 1.1820, Test Loss: 1.0049\n",
      "Epoch 10/68, Train Loss: 1.1489, Test Loss: 1.0045\n",
      "Epoch 11/68, Train Loss: 1.1898, Test Loss: 1.0045\n",
      "Epoch 12/68, Train Loss: 1.2329, Test Loss: 1.0027\n",
      "Epoch 13/68, Train Loss: 1.2271, Test Loss: 1.0027\n",
      "Epoch 14/68, Train Loss: 1.1333, Test Loss: 1.0021\n",
      "Epoch 15/68, Train Loss: 1.1111, Test Loss: 1.0000\n",
      "Epoch 16/68, Train Loss: 1.1780, Test Loss: 0.9996\n",
      "Epoch 17/68, Train Loss: 1.1460, Test Loss: 0.9989\n",
      "Epoch 18/68, Train Loss: 1.1909, Test Loss: 0.9987\n",
      "Epoch 19/68, Train Loss: 1.1687, Test Loss: 0.9978\n",
      "Epoch 20/68, Train Loss: 1.1171, Test Loss: 0.9973\n",
      "Epoch 21/68, Train Loss: 1.1417, Test Loss: 0.9965\n",
      "Epoch 22/68, Train Loss: 1.1191, Test Loss: 0.9959\n",
      "Epoch 23/68, Train Loss: 1.1431, Test Loss: 0.9959\n",
      "Epoch 24/68, Train Loss: 1.1169, Test Loss: 0.9952\n",
      "Epoch 25/68, Train Loss: 1.1409, Test Loss: 0.9949\n",
      "Epoch 26/68, Train Loss: 1.1745, Test Loss: 0.9937\n",
      "Epoch 27/68, Train Loss: 1.1374, Test Loss: 0.9923\n",
      "Epoch 28/68, Train Loss: 1.0851, Test Loss: 0.9920\n",
      "Epoch 29/68, Train Loss: 1.1797, Test Loss: 0.9922\n",
      "Epoch 30/68, Train Loss: 1.1135, Test Loss: 0.9929\n",
      "Epoch 31/68, Train Loss: 1.1046, Test Loss: 0.9923\n",
      "Epoch 32/68, Train Loss: 1.1263, Test Loss: 0.9928\n",
      "Epoch 33/68, Train Loss: 1.1352, Test Loss: 0.9924\n",
      "Epoch 34/68, Train Loss: 1.1163, Test Loss: 0.9919\n",
      "Epoch 35/68, Train Loss: 1.1775, Test Loss: 0.9927\n",
      "Epoch 36/68, Train Loss: 1.1360, Test Loss: 0.9926\n",
      "Epoch 37/68, Train Loss: 1.1191, Test Loss: 0.9918\n",
      "Epoch 38/68, Train Loss: 1.1202, Test Loss: 0.9916\n",
      "Epoch 39/68, Train Loss: 1.1277, Test Loss: 0.9921\n",
      "Epoch 40/68, Train Loss: 1.1630, Test Loss: 0.9927\n",
      "Epoch 41/68, Train Loss: 1.1619, Test Loss: 0.9920\n",
      "Epoch 42/68, Train Loss: 1.1186, Test Loss: 0.9923\n",
      "Epoch 43/68, Train Loss: 1.1505, Test Loss: 0.9910\n",
      "Epoch 44/68, Train Loss: 1.0810, Test Loss: 0.9905\n",
      "Epoch 45/68, Train Loss: 1.1034, Test Loss: 0.9896\n",
      "Epoch 46/68, Train Loss: 1.1037, Test Loss: 0.9898\n",
      "Epoch 47/68, Train Loss: 1.0988, Test Loss: 0.9903\n",
      "Epoch 48/68, Train Loss: 1.0780, Test Loss: 0.9901\n",
      "Epoch 49/68, Train Loss: 1.1032, Test Loss: 0.9894\n",
      "Epoch 50/68, Train Loss: 1.1431, Test Loss: 0.9901\n",
      "Epoch 51/68, Train Loss: 1.1020, Test Loss: 0.9891\n",
      "Epoch 52/68, Train Loss: 1.1093, Test Loss: 0.9888\n",
      "Epoch 53/68, Train Loss: 1.1448, Test Loss: 0.9889\n",
      "Epoch 54/68, Train Loss: 1.1100, Test Loss: 0.9892\n",
      "Epoch 55/68, Train Loss: 1.0956, Test Loss: 0.9898\n",
      "Epoch 56/68, Train Loss: 1.0819, Test Loss: 0.9896\n",
      "Epoch 57/68, Train Loss: 1.0774, Test Loss: 0.9891\n",
      "Epoch 58/68, Train Loss: 1.0956, Test Loss: 0.9891\n",
      "Epoch 59/68, Train Loss: 1.0809, Test Loss: 0.9889\n",
      "Epoch 60/68, Train Loss: 1.0933, Test Loss: 0.9895\n",
      "Epoch 61/68, Train Loss: 1.0549, Test Loss: 0.9892\n",
      "Epoch 62/68, Train Loss: 1.1094, Test Loss: 0.9899\n",
      "Epoch 63/68, Train Loss: 1.0860, Test Loss: 0.9895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:15,820] Trial 146 finished with value: 0.9901765584945679 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 138, 'layer_1_size': 152, 'layer_2_size': 226, 'layer_3_size': 251, 'layer_4_size': 125, 'layer_5_size': 220, 'layer_6_size': 173, 'dropout_rate': 0.4435598014822092, 'learning_rate': 7.673082571535231e-05, 'batch_size': 256, 'epochs': 68}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/68, Train Loss: 1.0595, Test Loss: 0.9893\n",
      "Epoch 65/68, Train Loss: 1.1042, Test Loss: 0.9902\n",
      "Epoch 66/68, Train Loss: 1.1400, Test Loss: 0.9899\n",
      "Epoch 67/68, Train Loss: 1.1128, Test Loss: 0.9900\n",
      "Epoch 68/68, Train Loss: 1.0884, Test Loss: 0.9902\n",
      "Epoch 1/22, Train Loss: 1.2331, Test Loss: 0.7890\n",
      "Epoch 2/22, Train Loss: 1.1765, Test Loss: 0.7881\n",
      "Epoch 3/22, Train Loss: 1.1703, Test Loss: 0.7890\n",
      "Epoch 4/22, Train Loss: 1.1671, Test Loss: 0.7865\n",
      "Epoch 5/22, Train Loss: 1.1701, Test Loss: 0.7862\n",
      "Epoch 6/22, Train Loss: 1.1597, Test Loss: 0.7840\n",
      "Epoch 7/22, Train Loss: 1.1476, Test Loss: 0.7840\n",
      "Epoch 8/22, Train Loss: 1.1588, Test Loss: 0.7845\n",
      "Epoch 9/22, Train Loss: 1.1940, Test Loss: 0.7848\n",
      "Epoch 10/22, Train Loss: 1.1433, Test Loss: 0.7827\n",
      "Epoch 11/22, Train Loss: 1.1287, Test Loss: 0.7849\n",
      "Epoch 12/22, Train Loss: 1.1070, Test Loss: 0.7891\n",
      "Epoch 13/22, Train Loss: 1.0951, Test Loss: 0.7881\n",
      "Epoch 14/22, Train Loss: 1.1190, Test Loss: 0.7917\n",
      "Epoch 15/22, Train Loss: 1.1168, Test Loss: 0.7925\n",
      "Epoch 16/22, Train Loss: 1.0562, Test Loss: 0.7908\n",
      "Epoch 17/22, Train Loss: 1.1081, Test Loss: 0.7926\n",
      "Epoch 18/22, Train Loss: 1.1193, Test Loss: 0.7948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:17,294] Trial 147 finished with value: 0.785415530204773 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 117, 'layer_1_size': 190, 'layer_2_size': 243, 'layer_3_size': 75, 'layer_4_size': 213, 'layer_5_size': 229, 'dropout_rate': 0.4312924532003094, 'learning_rate': 0.0001647134731342103, 'batch_size': 64, 'epochs': 22}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/22, Train Loss: 1.1088, Test Loss: 0.7927\n",
      "Epoch 20/22, Train Loss: 1.0815, Test Loss: 0.7889\n",
      "Epoch 21/22, Train Loss: 1.0946, Test Loss: 0.7893\n",
      "Epoch 22/22, Train Loss: 1.0972, Test Loss: 0.7854\n",
      "Epoch 1/15, Train Loss: 1.1795, Test Loss: 0.8916\n",
      "Epoch 2/15, Train Loss: 1.2192, Test Loss: 0.8899\n",
      "Epoch 3/15, Train Loss: 1.2102, Test Loss: 0.8881\n",
      "Epoch 4/15, Train Loss: 1.1881, Test Loss: 0.8873\n",
      "Epoch 5/15, Train Loss: 1.1903, Test Loss: 0.8856\n",
      "Epoch 6/15, Train Loss: 1.1572, Test Loss: 0.8857\n",
      "Epoch 7/15, Train Loss: 1.1639, Test Loss: 0.8845\n",
      "Epoch 8/15, Train Loss: 1.1513, Test Loss: 0.8847\n",
      "Epoch 9/15, Train Loss: 1.1724, Test Loss: 0.8822\n",
      "Epoch 10/15, Train Loss: 1.0877, Test Loss: 0.8820\n",
      "Epoch 11/15, Train Loss: 1.1642, Test Loss: 0.8843\n",
      "Epoch 12/15, Train Loss: 1.1365, Test Loss: 0.8811\n",
      "Epoch 13/15, Train Loss: 1.1762, Test Loss: 0.8827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:19,497] Trial 148 finished with value: 0.8844669163227081 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 116, 'layer_1_size': 183, 'layer_2_size': 254, 'layer_3_size': 80, 'layer_4_size': 212, 'layer_5_size': 161, 'layer_6_size': 180, 'dropout_rate': 0.4182132155836343, 'learning_rate': 0.000152212180310779, 'batch_size': 128, 'epochs': 15}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15, Train Loss: 1.1989, Test Loss: 0.8849\n",
      "Epoch 15/15, Train Loss: 1.0828, Test Loss: 0.8845\n",
      "Epoch 1/20, Train Loss: 1.4289, Test Loss: 0.8292\n",
      "Epoch 2/20, Train Loss: 1.2899, Test Loss: 0.8294\n",
      "Epoch 3/20, Train Loss: 1.2933, Test Loss: 0.8279\n",
      "Epoch 4/20, Train Loss: 1.2571, Test Loss: 0.8268\n",
      "Epoch 5/20, Train Loss: 1.3378, Test Loss: 0.8283\n",
      "Epoch 6/20, Train Loss: 1.2477, Test Loss: 0.8262\n",
      "Epoch 7/20, Train Loss: 1.2174, Test Loss: 0.8247\n",
      "Epoch 8/20, Train Loss: 1.2603, Test Loss: 0.8269\n",
      "Epoch 9/20, Train Loss: 1.2448, Test Loss: 0.8262\n",
      "Epoch 10/20, Train Loss: 1.2108, Test Loss: 0.8262\n",
      "Epoch 11/20, Train Loss: 1.1934, Test Loss: 0.8258\n",
      "Epoch 12/20, Train Loss: 1.2092, Test Loss: 0.8243\n",
      "Epoch 13/20, Train Loss: 1.2044, Test Loss: 0.8242\n",
      "Epoch 14/20, Train Loss: 1.2295, Test Loss: 0.8259\n",
      "Epoch 15/20, Train Loss: 1.1937, Test Loss: 0.8227\n",
      "Epoch 16/20, Train Loss: 1.1830, Test Loss: 0.8223\n",
      "Epoch 17/20, Train Loss: 1.2150, Test Loss: 0.8221\n",
      "Epoch 18/20, Train Loss: 1.1578, Test Loss: 0.8226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:21,585] Trial 149 finished with value: 0.8228830918669701 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 106, 'layer_1_size': 60, 'layer_2_size': 244, 'layer_3_size': 76, 'layer_4_size': 197, 'layer_5_size': 231, 'dropout_rate': 0.46404811715704186, 'learning_rate': 0.0002485583459210904, 'batch_size': 64, 'epochs': 20}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Train Loss: 1.2085, Test Loss: 0.8209\n",
      "Epoch 20/20, Train Loss: 1.1934, Test Loss: 0.8229\n",
      "Epoch 1/21, Train Loss: 1.2022, Test Loss: 0.9065\n",
      "Epoch 2/21, Train Loss: 1.1872, Test Loss: 0.9075\n",
      "Epoch 3/21, Train Loss: 1.1827, Test Loss: 0.9081\n",
      "Epoch 4/21, Train Loss: 1.1625, Test Loss: 0.9035\n",
      "Epoch 5/21, Train Loss: 1.1164, Test Loss: 0.9077\n",
      "Epoch 6/21, Train Loss: 1.1299, Test Loss: 0.9101\n",
      "Epoch 7/21, Train Loss: 1.1309, Test Loss: 0.9075\n",
      "Epoch 8/21, Train Loss: 1.1192, Test Loss: 0.9147\n",
      "Epoch 9/21, Train Loss: 1.0712, Test Loss: 0.9172\n",
      "Epoch 10/21, Train Loss: 1.1216, Test Loss: 0.9181\n",
      "Epoch 11/21, Train Loss: 1.1526, Test Loss: 0.9254\n",
      "Epoch 12/21, Train Loss: 1.0757, Test Loss: 0.9266\n",
      "Epoch 13/21, Train Loss: 1.0927, Test Loss: 0.9236\n",
      "Epoch 14/21, Train Loss: 1.0819, Test Loss: 0.9254\n",
      "Epoch 15/21, Train Loss: 1.1111, Test Loss: 0.9295\n",
      "Epoch 16/21, Train Loss: 1.0473, Test Loss: 0.9306\n",
      "Epoch 17/21, Train Loss: 1.1035, Test Loss: 0.9281\n",
      "Epoch 18/21, Train Loss: 1.0361, Test Loss: 0.9202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:22,946] Trial 150 finished with value: 0.9158424884080887 and parameters: {'num_hidden_layers': 5, 'layer_0_size': 99, 'layer_1_size': 58, 'layer_2_size': 245, 'layer_3_size': 83, 'layer_4_size': 196, 'dropout_rate': 0.4649366330445591, 'learning_rate': 0.00039873882762782266, 'batch_size': 64, 'epochs': 21}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/21, Train Loss: 1.0841, Test Loss: 0.9209\n",
      "Epoch 20/21, Train Loss: 1.0654, Test Loss: 0.9183\n",
      "Epoch 21/21, Train Loss: 1.0345, Test Loss: 0.9158\n",
      "Epoch 1/21, Train Loss: 1.2012, Test Loss: 1.2700\n",
      "Epoch 2/21, Train Loss: 1.1508, Test Loss: 1.2555\n",
      "Epoch 3/21, Train Loss: 1.1000, Test Loss: 1.2646\n",
      "Epoch 4/21, Train Loss: 1.1438, Test Loss: 1.2651\n",
      "Epoch 5/21, Train Loss: 1.1565, Test Loss: 1.2657\n",
      "Epoch 6/21, Train Loss: 1.0999, Test Loss: 1.2500\n",
      "Epoch 7/21, Train Loss: 1.0397, Test Loss: 1.2483\n",
      "Epoch 8/21, Train Loss: 1.1069, Test Loss: 1.2505\n",
      "Epoch 9/21, Train Loss: 1.0402, Test Loss: 1.2660\n",
      "Epoch 10/21, Train Loss: 1.0911, Test Loss: 1.2769\n",
      "Epoch 11/21, Train Loss: 1.0572, Test Loss: 1.2792\n",
      "Epoch 12/21, Train Loss: 1.0528, Test Loss: 1.2905\n",
      "Epoch 13/21, Train Loss: 1.0685, Test Loss: 1.2917\n",
      "Epoch 14/21, Train Loss: 1.0178, Test Loss: 1.2967\n",
      "Epoch 15/21, Train Loss: 1.1138, Test Loss: 1.2940\n",
      "Epoch 16/21, Train Loss: 1.0309, Test Loss: 1.2862\n",
      "Epoch 17/21, Train Loss: 1.0496, Test Loss: 1.2768\n",
      "Epoch 18/21, Train Loss: 1.0579, Test Loss: 1.2765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:24,897] Trial 151 finished with value: 1.2972292602062225 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 107, 'layer_1_size': 65, 'layer_2_size': 242, 'layer_3_size': 73, 'layer_4_size': 206, 'layer_5_size': 229, 'dropout_rate': 0.48679529600369265, 'learning_rate': 0.0002505286688030092, 'batch_size': 64, 'epochs': 21}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/21, Train Loss: 1.0145, Test Loss: 1.2800\n",
      "Epoch 20/21, Train Loss: 1.0705, Test Loss: 1.2879\n",
      "Epoch 21/21, Train Loss: 1.0339, Test Loss: 1.2972\n",
      "Epoch 1/24, Train Loss: 1.2139, Test Loss: 1.1264\n",
      "Epoch 2/24, Train Loss: 1.1874, Test Loss: 1.1248\n",
      "Epoch 3/24, Train Loss: 1.0940, Test Loss: 1.1103\n",
      "Epoch 4/24, Train Loss: 1.1212, Test Loss: 1.1050\n",
      "Epoch 5/24, Train Loss: 1.1404, Test Loss: 1.0948\n",
      "Epoch 6/24, Train Loss: 1.0778, Test Loss: 1.0854\n",
      "Epoch 7/24, Train Loss: 1.0986, Test Loss: 1.0840\n",
      "Epoch 8/24, Train Loss: 1.1063, Test Loss: 1.0794\n",
      "Epoch 9/24, Train Loss: 1.1352, Test Loss: 1.0797\n",
      "Epoch 10/24, Train Loss: 1.0884, Test Loss: 1.0804\n",
      "Epoch 11/24, Train Loss: 1.0276, Test Loss: 1.0738\n",
      "Epoch 12/24, Train Loss: 1.0216, Test Loss: 1.0744\n",
      "Epoch 13/24, Train Loss: 1.0468, Test Loss: 1.0738\n",
      "Epoch 14/24, Train Loss: 1.0486, Test Loss: 1.0696\n",
      "Epoch 15/24, Train Loss: 1.0516, Test Loss: 1.0694\n",
      "Epoch 16/24, Train Loss: 1.0791, Test Loss: 1.0718\n",
      "Epoch 17/24, Train Loss: 1.0870, Test Loss: 1.0706\n",
      "Epoch 18/24, Train Loss: 1.0226, Test Loss: 1.0757\n",
      "Epoch 19/24, Train Loss: 1.0278, Test Loss: 1.0717\n",
      "Epoch 20/24, Train Loss: 1.0584, Test Loss: 1.0702\n",
      "Epoch 21/24, Train Loss: 1.0313, Test Loss: 1.0734\n",
      "Epoch 22/24, Train Loss: 1.0461, Test Loss: 1.0729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:27,093] Trial 152 finished with value: 1.067834734916687 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 93, 'layer_1_size': 50, 'layer_2_size': 192, 'layer_3_size': 61, 'layer_4_size': 32, 'layer_5_size': 237, 'dropout_rate': 0.4747649153763481, 'learning_rate': 0.0001675203072245358, 'batch_size': 64, 'epochs': 24}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/24, Train Loss: 1.0198, Test Loss: 1.0719\n",
      "Epoch 24/24, Train Loss: 1.0055, Test Loss: 1.0678\n",
      "Epoch 1/17, Train Loss: 1.2727, Test Loss: 0.9951\n",
      "Epoch 2/17, Train Loss: 1.1398, Test Loss: 1.0032\n",
      "Epoch 3/17, Train Loss: 1.1349, Test Loss: 1.0007\n",
      "Epoch 4/17, Train Loss: 1.1969, Test Loss: 0.9993\n",
      "Epoch 5/17, Train Loss: 1.1049, Test Loss: 0.9951\n",
      "Epoch 6/17, Train Loss: 1.1037, Test Loss: 0.9952\n",
      "Epoch 7/17, Train Loss: 1.0877, Test Loss: 0.9969\n",
      "Epoch 8/17, Train Loss: 1.0627, Test Loss: 0.9943\n",
      "Epoch 9/17, Train Loss: 1.0958, Test Loss: 0.9956\n",
      "Epoch 10/17, Train Loss: 1.0717, Test Loss: 0.9881\n",
      "Epoch 11/17, Train Loss: 1.0702, Test Loss: 0.9881\n",
      "Epoch 12/17, Train Loss: 1.0976, Test Loss: 0.9810\n",
      "Epoch 13/17, Train Loss: 1.0843, Test Loss: 0.9815\n",
      "Epoch 14/17, Train Loss: 1.0529, Test Loss: 0.9810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:28,773] Trial 153 finished with value: 0.9889347106218338 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 110, 'layer_1_size': 190, 'layer_2_size': 216, 'layer_3_size': 67, 'layer_4_size': 203, 'layer_5_size': 225, 'dropout_rate': 0.43271631818462786, 'learning_rate': 0.00024465334984646414, 'batch_size': 64, 'epochs': 17}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/17, Train Loss: 1.0410, Test Loss: 0.9801\n",
      "Epoch 16/17, Train Loss: 1.0395, Test Loss: 0.9836\n",
      "Epoch 17/17, Train Loss: 1.0666, Test Loss: 0.9889\n",
      "Epoch 1/36, Train Loss: 1.4050, Test Loss: 0.7683\n",
      "Epoch 2/36, Train Loss: 1.3212, Test Loss: 0.7603\n",
      "Epoch 3/36, Train Loss: 1.3366, Test Loss: 0.7604\n",
      "Epoch 4/36, Train Loss: 1.3352, Test Loss: 0.7599\n",
      "Epoch 5/36, Train Loss: 1.3153, Test Loss: 0.7610\n",
      "Epoch 6/36, Train Loss: 1.2908, Test Loss: 0.7596\n",
      "Epoch 7/36, Train Loss: 1.3616, Test Loss: 0.7608\n",
      "Epoch 8/36, Train Loss: 1.2863, Test Loss: 0.7616\n",
      "Epoch 9/36, Train Loss: 1.3058, Test Loss: 0.7647\n",
      "Epoch 10/36, Train Loss: 1.2493, Test Loss: 0.7657\n",
      "Epoch 11/36, Train Loss: 1.2924, Test Loss: 0.7643\n",
      "Epoch 12/36, Train Loss: 1.2565, Test Loss: 0.7653\n",
      "Epoch 13/36, Train Loss: 1.2115, Test Loss: 0.7665\n",
      "Epoch 14/36, Train Loss: 1.2207, Test Loss: 0.7682\n",
      "Epoch 15/36, Train Loss: 1.1987, Test Loss: 0.7665\n",
      "Epoch 16/36, Train Loss: 1.2229, Test Loss: 0.7620\n",
      "Epoch 17/36, Train Loss: 1.2219, Test Loss: 0.7637\n",
      "Epoch 18/36, Train Loss: 1.2289, Test Loss: 0.7627\n",
      "Epoch 19/36, Train Loss: 1.2051, Test Loss: 0.7626\n",
      "Epoch 20/36, Train Loss: 1.1980, Test Loss: 0.7627\n",
      "Epoch 21/36, Train Loss: 1.2186, Test Loss: 0.7609\n",
      "Epoch 22/36, Train Loss: 1.1540, Test Loss: 0.7611\n",
      "Epoch 23/36, Train Loss: 1.2051, Test Loss: 0.7631\n",
      "Epoch 24/36, Train Loss: 1.2249, Test Loss: 0.7618\n",
      "Epoch 25/36, Train Loss: 1.2029, Test Loss: 0.7636\n",
      "Epoch 26/36, Train Loss: 1.1784, Test Loss: 0.7687\n",
      "Epoch 27/36, Train Loss: 1.1984, Test Loss: 0.7689\n",
      "Epoch 28/36, Train Loss: 1.1987, Test Loss: 0.7677\n",
      "Epoch 29/36, Train Loss: 1.1689, Test Loss: 0.7659\n",
      "Epoch 30/36, Train Loss: 1.2248, Test Loss: 0.7667\n",
      "Epoch 31/36, Train Loss: 1.2061, Test Loss: 0.7653\n",
      "Epoch 32/36, Train Loss: 1.1994, Test Loss: 0.7655\n",
      "Epoch 33/36, Train Loss: 1.2000, Test Loss: 0.7677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:32,131] Trial 154 finished with value: 0.7762987911701202 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 113, 'layer_1_size': 193, 'layer_2_size': 204, 'layer_3_size': 54, 'layer_4_size': 52, 'layer_5_size': 230, 'layer_6_size': 191, 'dropout_rate': 0.4639690847394457, 'learning_rate': 0.00020578216320229687, 'batch_size': 64, 'epochs': 36}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/36, Train Loss: 1.1810, Test Loss: 0.7684\n",
      "Epoch 35/36, Train Loss: 1.2191, Test Loss: 0.7725\n",
      "Epoch 36/36, Train Loss: 1.1831, Test Loss: 0.7763\n",
      "Epoch 1/32, Train Loss: 1.2841, Test Loss: 1.2688\n",
      "Epoch 2/32, Train Loss: 1.2918, Test Loss: 1.2429\n",
      "Epoch 3/32, Train Loss: 1.2567, Test Loss: 1.2051\n",
      "Epoch 4/32, Train Loss: 1.1906, Test Loss: 1.1951\n",
      "Epoch 5/32, Train Loss: 1.2062, Test Loss: 1.1899\n",
      "Epoch 6/32, Train Loss: 1.1805, Test Loss: 1.2034\n",
      "Epoch 7/32, Train Loss: 1.1311, Test Loss: 1.2001\n",
      "Epoch 8/32, Train Loss: 1.1978, Test Loss: 1.1887\n",
      "Epoch 9/32, Train Loss: 1.1437, Test Loss: 1.1994\n",
      "Epoch 10/32, Train Loss: 1.1193, Test Loss: 1.1994\n",
      "Epoch 11/32, Train Loss: 1.1317, Test Loss: 1.1937\n",
      "Epoch 12/32, Train Loss: 1.1240, Test Loss: 1.1969\n",
      "Epoch 13/32, Train Loss: 1.1154, Test Loss: 1.1996\n",
      "Epoch 14/32, Train Loss: 1.1264, Test Loss: 1.2124\n",
      "Epoch 15/32, Train Loss: 1.1276, Test Loss: 1.2177\n",
      "Epoch 16/32, Train Loss: 1.1295, Test Loss: 1.2237\n",
      "Epoch 17/32, Train Loss: 1.1022, Test Loss: 1.2247\n",
      "Epoch 18/32, Train Loss: 1.0768, Test Loss: 1.2376\n",
      "Epoch 19/32, Train Loss: 1.1385, Test Loss: 1.2287\n",
      "Epoch 20/32, Train Loss: 1.1050, Test Loss: 1.2213\n",
      "Epoch 21/32, Train Loss: 1.1146, Test Loss: 1.2077\n",
      "Epoch 22/32, Train Loss: 1.0564, Test Loss: 1.1966\n",
      "Epoch 23/32, Train Loss: 1.0733, Test Loss: 1.2069\n",
      "Epoch 24/32, Train Loss: 1.0664, Test Loss: 1.2135\n",
      "Epoch 25/32, Train Loss: 1.1255, Test Loss: 1.2128\n",
      "Epoch 26/32, Train Loss: 1.0976, Test Loss: 1.2164\n",
      "Epoch 27/32, Train Loss: 1.0738, Test Loss: 1.2087\n",
      "Epoch 28/32, Train Loss: 1.0474, Test Loss: 1.1998\n",
      "Epoch 29/32, Train Loss: 1.0848, Test Loss: 1.1949\n",
      "Epoch 30/32, Train Loss: 1.0670, Test Loss: 1.1991\n",
      "Epoch 31/32, Train Loss: 1.0657, Test Loss: 1.2008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:34,565] Trial 155 finished with value: 1.1996906697750092 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 120, 'layer_1_size': 41, 'layer_2_size': 206, 'layer_3_size': 52, 'layer_4_size': 56, 'layer_5_size': 231, 'layer_6_size': 211, 'layer_7_size': 136, 'dropout_rate': 0.457151526423624, 'learning_rate': 0.00029253602113082, 'batch_size': 64, 'epochs': 32}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/32, Train Loss: 1.0767, Test Loss: 1.1997\n",
      "Epoch 1/35, Train Loss: 1.3282, Test Loss: 0.9356\n",
      "Epoch 2/35, Train Loss: 1.2635, Test Loss: 0.9371\n",
      "Epoch 3/35, Train Loss: 1.3294, Test Loss: 0.9364\n",
      "Epoch 4/35, Train Loss: 1.2563, Test Loss: 0.9340\n",
      "Epoch 5/35, Train Loss: 1.1832, Test Loss: 0.9356\n",
      "Epoch 6/35, Train Loss: 1.2600, Test Loss: 0.9357\n",
      "Epoch 7/35, Train Loss: 1.2340, Test Loss: 0.9380\n",
      "Epoch 8/35, Train Loss: 1.1730, Test Loss: 0.9360\n",
      "Epoch 9/35, Train Loss: 1.1664, Test Loss: 0.9316\n",
      "Epoch 10/35, Train Loss: 1.2308, Test Loss: 0.9325\n",
      "Epoch 11/35, Train Loss: 1.1683, Test Loss: 0.9329\n",
      "Epoch 12/35, Train Loss: 1.1960, Test Loss: 0.9330\n",
      "Epoch 13/35, Train Loss: 1.1724, Test Loss: 0.9356\n",
      "Epoch 14/35, Train Loss: 1.1540, Test Loss: 0.9400\n",
      "Epoch 15/35, Train Loss: 1.1650, Test Loss: 0.9358\n",
      "Epoch 16/35, Train Loss: 1.1850, Test Loss: 0.9343\n",
      "Epoch 17/35, Train Loss: 1.1606, Test Loss: 0.9354\n",
      "Epoch 18/35, Train Loss: 1.1736, Test Loss: 0.9344\n",
      "Epoch 19/35, Train Loss: 1.1667, Test Loss: 0.9337\n",
      "Epoch 20/35, Train Loss: 1.1744, Test Loss: 0.9384\n",
      "Epoch 21/35, Train Loss: 1.1232, Test Loss: 0.9360\n",
      "Epoch 22/35, Train Loss: 1.1588, Test Loss: 0.9330\n",
      "Epoch 23/35, Train Loss: 1.1195, Test Loss: 0.9333\n",
      "Epoch 24/35, Train Loss: 1.1228, Test Loss: 0.9339\n",
      "Epoch 25/35, Train Loss: 1.1396, Test Loss: 0.9362\n",
      "Epoch 26/35, Train Loss: 1.1557, Test Loss: 0.9358\n",
      "Epoch 27/35, Train Loss: 1.1117, Test Loss: 0.9373\n",
      "Epoch 28/35, Train Loss: 1.1416, Test Loss: 0.9334\n",
      "Epoch 29/35, Train Loss: 1.0840, Test Loss: 0.9332\n",
      "Epoch 30/35, Train Loss: 1.1158, Test Loss: 0.9334\n",
      "Epoch 31/35, Train Loss: 1.1256, Test Loss: 0.9338\n",
      "Epoch 32/35, Train Loss: 1.0930, Test Loss: 0.9301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:37,492] Trial 156 finished with value: 0.9336085468530655 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 128, 'layer_1_size': 81, 'layer_2_size': 230, 'layer_3_size': 161, 'layer_4_size': 71, 'layer_5_size': 219, 'layer_6_size': 188, 'dropout_rate': 0.465954338133948, 'learning_rate': 0.0001886648217544478, 'batch_size': 64, 'epochs': 35}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/35, Train Loss: 1.1573, Test Loss: 0.9324\n",
      "Epoch 34/35, Train Loss: 1.1120, Test Loss: 0.9340\n",
      "Epoch 35/35, Train Loss: 1.1443, Test Loss: 0.9336\n",
      "Epoch 1/19, Train Loss: 1.2514, Test Loss: 0.8271\n",
      "Epoch 2/19, Train Loss: 1.1812, Test Loss: 0.8233\n",
      "Epoch 3/19, Train Loss: 1.1387, Test Loss: 0.8208\n",
      "Epoch 4/19, Train Loss: 1.1733, Test Loss: 0.8285\n",
      "Epoch 5/19, Train Loss: 1.1689, Test Loss: 0.8296\n",
      "Epoch 6/19, Train Loss: 1.1338, Test Loss: 0.8339\n",
      "Epoch 7/19, Train Loss: 1.1456, Test Loss: 0.8353\n",
      "Epoch 8/19, Train Loss: 1.1826, Test Loss: 0.8404\n",
      "Epoch 9/19, Train Loss: 1.1213, Test Loss: 0.8476\n",
      "Epoch 10/19, Train Loss: 1.1449, Test Loss: 0.8482\n",
      "Epoch 11/19, Train Loss: 1.1187, Test Loss: 0.8473\n",
      "Epoch 12/19, Train Loss: 1.1013, Test Loss: 0.8473\n",
      "Epoch 13/19, Train Loss: 1.1304, Test Loss: 0.8494\n",
      "Epoch 14/19, Train Loss: 1.0937, Test Loss: 0.8465\n",
      "Epoch 15/19, Train Loss: 1.0891, Test Loss: 0.8445\n",
      "Epoch 16/19, Train Loss: 1.1015, Test Loss: 0.8485\n",
      "Epoch 17/19, Train Loss: 1.1021, Test Loss: 0.8528\n",
      "Epoch 18/19, Train Loss: 1.1159, Test Loss: 0.8459\n",
      "Epoch 19/19, Train Loss: 1.1711, Test Loss: 0.8439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:38,595] Trial 157 finished with value: 0.8439081683754921 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 104, 'layer_1_size': 61, 'layer_2_size': 248, 'layer_3_size': 42, 'layer_4_size': 48, 'layer_5_size': 210, 'dropout_rate': 0.3918856559865802, 'learning_rate': 0.00019762701712013303, 'batch_size': 64, 'epochs': 19}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18, Train Loss: 1.1767, Test Loss: 0.8248\n",
      "Epoch 2/18, Train Loss: 1.1642, Test Loss: 0.8115\n",
      "Epoch 3/18, Train Loss: 1.1469, Test Loss: 0.8103\n",
      "Epoch 4/18, Train Loss: 1.1841, Test Loss: 0.8085\n",
      "Epoch 5/18, Train Loss: 1.1091, Test Loss: 0.8115\n",
      "Epoch 6/18, Train Loss: 1.1251, Test Loss: 0.8145\n",
      "Epoch 7/18, Train Loss: 1.1342, Test Loss: 0.8203\n",
      "Epoch 8/18, Train Loss: 1.1417, Test Loss: 0.8187\n",
      "Epoch 9/18, Train Loss: 1.0955, Test Loss: 0.8236\n",
      "Epoch 10/18, Train Loss: 1.0912, Test Loss: 0.8226\n",
      "Epoch 11/18, Train Loss: 1.0759, Test Loss: 0.8283\n",
      "Epoch 12/18, Train Loss: 1.0984, Test Loss: 0.8234\n",
      "Epoch 13/18, Train Loss: 1.1441, Test Loss: 0.8332\n",
      "Epoch 14/18, Train Loss: 1.0929, Test Loss: 0.8363\n",
      "Epoch 15/18, Train Loss: 1.0479, Test Loss: 0.8355\n",
      "Epoch 16/18, Train Loss: 1.0841, Test Loss: 0.8378\n",
      "Epoch 17/18, Train Loss: 1.1178, Test Loss: 0.8401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:40,152] Trial 158 finished with value: 0.8369950950145721 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 105, 'layer_1_size': 178, 'layer_2_size': 253, 'layer_3_size': 57, 'layer_4_size': 49, 'layer_5_size': 211, 'dropout_rate': 0.3937447253859384, 'learning_rate': 0.00020421059581758591, 'batch_size': 64, 'epochs': 18}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/18, Train Loss: 1.0965, Test Loss: 0.8370\n",
      "Epoch 1/13, Train Loss: 1.2151, Test Loss: 1.0271\n",
      "Epoch 2/13, Train Loss: 1.2656, Test Loss: 1.0356\n",
      "Epoch 3/13, Train Loss: 1.1999, Test Loss: 1.0399\n",
      "Epoch 4/13, Train Loss: 1.2205, Test Loss: 1.0416\n",
      "Epoch 5/13, Train Loss: 1.2459, Test Loss: 1.0398\n",
      "Epoch 6/13, Train Loss: 1.1703, Test Loss: 1.0408\n",
      "Epoch 7/13, Train Loss: 1.2638, Test Loss: 1.0407\n",
      "Epoch 8/13, Train Loss: 1.2154, Test Loss: 1.0397\n",
      "Epoch 9/13, Train Loss: 1.2163, Test Loss: 1.0412\n",
      "Epoch 10/13, Train Loss: 1.2192, Test Loss: 1.0413\n",
      "Epoch 11/13, Train Loss: 1.2049, Test Loss: 1.0374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:40,601] Trial 159 finished with value: 1.0341547429561615 and parameters: {'num_hidden_layers': 5, 'layer_0_size': 89, 'layer_1_size': 176, 'layer_2_size': 256, 'layer_3_size': 57, 'layer_4_size': 48, 'dropout_rate': 0.4317851869692309, 'learning_rate': 0.00013513213239501102, 'batch_size': 128, 'epochs': 13}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/13, Train Loss: 1.1550, Test Loss: 1.0349\n",
      "Epoch 13/13, Train Loss: 1.1772, Test Loss: 1.0342\n",
      "Epoch 1/8, Train Loss: 1.1569, Test Loss: 1.1606\n",
      "Epoch 2/8, Train Loss: 1.1396, Test Loss: 1.1603\n",
      "Epoch 3/8, Train Loss: 1.1656, Test Loss: 1.1543\n",
      "Epoch 4/8, Train Loss: 1.1095, Test Loss: 1.1546\n",
      "Epoch 5/8, Train Loss: 1.1121, Test Loss: 1.1506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:41,113] Trial 160 finished with value: 1.1475182920694351 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 117, 'layer_1_size': 75, 'layer_2_size': 241, 'layer_3_size': 99, 'layer_4_size': 50, 'layer_5_size': 207, 'dropout_rate': 0.3996173102172854, 'learning_rate': 0.00021979952262026576, 'batch_size': 64, 'epochs': 8}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/8, Train Loss: 1.1058, Test Loss: 1.1450\n",
      "Epoch 7/8, Train Loss: 1.0755, Test Loss: 1.1385\n",
      "Epoch 8/8, Train Loss: 1.1266, Test Loss: 1.1475\n",
      "Epoch 1/26, Train Loss: 1.2272, Test Loss: 0.9397\n",
      "Epoch 2/26, Train Loss: 1.2013, Test Loss: 0.9429\n",
      "Epoch 3/26, Train Loss: 1.1726, Test Loss: 0.9474\n",
      "Epoch 4/26, Train Loss: 1.1486, Test Loss: 0.9449\n",
      "Epoch 5/26, Train Loss: 1.1397, Test Loss: 0.9457\n",
      "Epoch 6/26, Train Loss: 1.1211, Test Loss: 0.9439\n",
      "Epoch 7/26, Train Loss: 1.1097, Test Loss: 0.9455\n",
      "Epoch 8/26, Train Loss: 1.1304, Test Loss: 0.9445\n",
      "Epoch 9/26, Train Loss: 1.0953, Test Loss: 0.9440\n",
      "Epoch 10/26, Train Loss: 1.0922, Test Loss: 0.9416\n",
      "Epoch 11/26, Train Loss: 1.0742, Test Loss: 0.9397\n",
      "Epoch 12/26, Train Loss: 1.1053, Test Loss: 0.9386\n",
      "Epoch 13/26, Train Loss: 1.0825, Test Loss: 0.9382\n",
      "Epoch 14/26, Train Loss: 1.1171, Test Loss: 0.9391\n",
      "Epoch 15/26, Train Loss: 1.0799, Test Loss: 0.9384\n",
      "Epoch 16/26, Train Loss: 1.1036, Test Loss: 0.9361\n",
      "Epoch 17/26, Train Loss: 1.0912, Test Loss: 0.9353\n",
      "Epoch 18/26, Train Loss: 1.0519, Test Loss: 0.9366\n",
      "Epoch 19/26, Train Loss: 1.0585, Test Loss: 0.9359\n",
      "Epoch 20/26, Train Loss: 1.0817, Test Loss: 0.9342\n",
      "Epoch 21/26, Train Loss: 1.0248, Test Loss: 0.9318\n",
      "Epoch 22/26, Train Loss: 1.0877, Test Loss: 0.9322\n",
      "Epoch 23/26, Train Loss: 1.0550, Test Loss: 0.9309\n",
      "Epoch 24/26, Train Loss: 1.0909, Test Loss: 0.9315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:42,912] Trial 161 finished with value: 0.9329230338335037 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 104, 'layer_1_size': 62, 'layer_2_size': 251, 'layer_3_size': 77, 'layer_4_size': 39, 'layer_5_size': 212, 'dropout_rate': 0.39127854431295134, 'learning_rate': 0.0001959963577353595, 'batch_size': 64, 'epochs': 26}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/26, Train Loss: 1.0546, Test Loss: 0.9307\n",
      "Epoch 26/26, Train Loss: 1.0521, Test Loss: 0.9329\n",
      "Epoch 1/19, Train Loss: 1.4747, Test Loss: 0.9550\n",
      "Epoch 2/19, Train Loss: 1.3384, Test Loss: 0.9631\n",
      "Epoch 3/19, Train Loss: 1.2612, Test Loss: 0.9558\n",
      "Epoch 4/19, Train Loss: 1.1905, Test Loss: 0.9527\n",
      "Epoch 5/19, Train Loss: 1.1999, Test Loss: 0.9550\n",
      "Epoch 6/19, Train Loss: 1.2400, Test Loss: 0.9521\n",
      "Epoch 7/19, Train Loss: 1.1952, Test Loss: 0.9474\n",
      "Epoch 8/19, Train Loss: 1.1190, Test Loss: 0.9472\n",
      "Epoch 9/19, Train Loss: 1.1603, Test Loss: 0.9500\n",
      "Epoch 10/19, Train Loss: 1.1904, Test Loss: 0.9477\n",
      "Epoch 11/19, Train Loss: 1.1618, Test Loss: 0.9460\n",
      "Epoch 12/19, Train Loss: 1.1357, Test Loss: 0.9418\n",
      "Epoch 13/19, Train Loss: 1.1373, Test Loss: 0.9430\n",
      "Epoch 14/19, Train Loss: 1.1811, Test Loss: 0.9448\n",
      "Epoch 15/19, Train Loss: 1.1465, Test Loss: 0.9450\n",
      "Epoch 16/19, Train Loss: 1.1848, Test Loss: 0.9394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:44,203] Trial 162 finished with value: 0.9441517218947411 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 97, 'layer_1_size': 71, 'layer_2_size': 250, 'layer_3_size': 44, 'layer_4_size': 52, 'layer_5_size': 202, 'dropout_rate': 0.40733849455787896, 'learning_rate': 0.00017723646006556742, 'batch_size': 64, 'epochs': 19}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/19, Train Loss: 1.1326, Test Loss: 0.9408\n",
      "Epoch 18/19, Train Loss: 1.1278, Test Loss: 0.9444\n",
      "Epoch 19/19, Train Loss: 1.1480, Test Loss: 0.9442\n",
      "Epoch 1/24, Train Loss: 1.2201, Test Loss: 0.9933\n",
      "Epoch 2/24, Train Loss: 1.2665, Test Loss: 0.9895\n",
      "Epoch 3/24, Train Loss: 1.1303, Test Loss: 0.9910\n",
      "Epoch 4/24, Train Loss: 1.1399, Test Loss: 0.9910\n",
      "Epoch 5/24, Train Loss: 1.1015, Test Loss: 0.9900\n",
      "Epoch 6/24, Train Loss: 1.1096, Test Loss: 0.9926\n",
      "Epoch 7/24, Train Loss: 1.1342, Test Loss: 0.9903\n",
      "Epoch 8/24, Train Loss: 1.0752, Test Loss: 0.9926\n",
      "Epoch 9/24, Train Loss: 1.0653, Test Loss: 0.9902\n",
      "Epoch 10/24, Train Loss: 1.0926, Test Loss: 0.9916\n",
      "Epoch 11/24, Train Loss: 1.0824, Test Loss: 0.9918\n",
      "Epoch 12/24, Train Loss: 1.0569, Test Loss: 0.9900\n",
      "Epoch 13/24, Train Loss: 1.0494, Test Loss: 0.9878\n",
      "Epoch 14/24, Train Loss: 1.0372, Test Loss: 0.9886\n",
      "Epoch 15/24, Train Loss: 1.0408, Test Loss: 0.9896\n",
      "Epoch 16/24, Train Loss: 1.0112, Test Loss: 0.9894\n",
      "Epoch 17/24, Train Loss: 1.0729, Test Loss: 0.9915\n",
      "Epoch 18/24, Train Loss: 1.0623, Test Loss: 0.9913\n",
      "Epoch 19/24, Train Loss: 1.0418, Test Loss: 0.9918\n",
      "Epoch 20/24, Train Loss: 1.0504, Test Loss: 0.9919\n",
      "Epoch 21/24, Train Loss: 1.0354, Test Loss: 0.9912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:46,249] Trial 163 finished with value: 0.9930341094732285 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 101, 'layer_1_size': 55, 'layer_2_size': 247, 'layer_3_size': 52, 'layer_4_size': 60, 'layer_5_size': 215, 'dropout_rate': 0.44254584493441557, 'learning_rate': 0.0002810386642114328, 'batch_size': 64, 'epochs': 24}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/24, Train Loss: 1.0597, Test Loss: 0.9933\n",
      "Epoch 23/24, Train Loss: 1.0193, Test Loss: 0.9898\n",
      "Epoch 24/24, Train Loss: 1.0164, Test Loss: 0.9930\n",
      "Epoch 1/17, Train Loss: 1.1878, Test Loss: 1.0297\n",
      "Epoch 2/17, Train Loss: 1.1898, Test Loss: 1.0382\n",
      "Epoch 3/17, Train Loss: 1.1882, Test Loss: 1.0473\n",
      "Epoch 4/17, Train Loss: 1.1752, Test Loss: 1.0433\n",
      "Epoch 5/17, Train Loss: 1.1972, Test Loss: 1.0404\n",
      "Epoch 6/17, Train Loss: 1.1164, Test Loss: 1.0390\n",
      "Epoch 7/17, Train Loss: 1.2143, Test Loss: 1.0404\n",
      "Epoch 8/17, Train Loss: 1.1136, Test Loss: 1.0367\n",
      "Epoch 9/17, Train Loss: 1.0874, Test Loss: 1.0386\n",
      "Epoch 10/17, Train Loss: 1.1283, Test Loss: 1.0410\n",
      "Epoch 11/17, Train Loss: 1.1587, Test Loss: 1.0354\n",
      "Epoch 12/17, Train Loss: 1.1422, Test Loss: 1.0344\n",
      "Epoch 13/17, Train Loss: 1.1198, Test Loss: 1.0341\n",
      "Epoch 14/17, Train Loss: 1.1322, Test Loss: 1.0310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:47,510] Trial 164 finished with value: 1.0326821953058243 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 55, 'layer_1_size': 68, 'layer_2_size': 252, 'layer_3_size': 245, 'layer_4_size': 65, 'layer_5_size': 148, 'dropout_rate': 0.3850310565834701, 'learning_rate': 0.00015847870999774325, 'batch_size': 64, 'epochs': 17}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/17, Train Loss: 1.1385, Test Loss: 1.0335\n",
      "Epoch 16/17, Train Loss: 1.1462, Test Loss: 1.0337\n",
      "Epoch 17/17, Train Loss: 1.0864, Test Loss: 1.0327\n",
      "Epoch 1/22, Train Loss: 1.4218, Test Loss: 0.9497\n",
      "Epoch 2/22, Train Loss: 1.2785, Test Loss: 0.9389\n",
      "Epoch 3/22, Train Loss: 1.2628, Test Loss: 0.9365\n",
      "Epoch 4/22, Train Loss: 1.2398, Test Loss: 0.9336\n",
      "Epoch 5/22, Train Loss: 1.2369, Test Loss: 0.9313\n",
      "Epoch 6/22, Train Loss: 1.2173, Test Loss: 0.9380\n",
      "Epoch 7/22, Train Loss: 1.1529, Test Loss: 0.9268\n",
      "Epoch 8/22, Train Loss: 1.1928, Test Loss: 0.9284\n",
      "Epoch 9/22, Train Loss: 1.1749, Test Loss: 0.9263\n",
      "Epoch 10/22, Train Loss: 1.2025, Test Loss: 0.9218\n",
      "Epoch 11/22, Train Loss: 1.1705, Test Loss: 0.9194\n",
      "Epoch 12/22, Train Loss: 1.1588, Test Loss: 0.9197\n",
      "Epoch 13/22, Train Loss: 1.1619, Test Loss: 0.9218\n",
      "Epoch 14/22, Train Loss: 1.1555, Test Loss: 0.9197\n",
      "Epoch 15/22, Train Loss: 1.1382, Test Loss: 0.9203\n",
      "Epoch 16/22, Train Loss: 1.1469, Test Loss: 0.9131\n",
      "Epoch 17/22, Train Loss: 1.1550, Test Loss: 0.9173\n",
      "Epoch 18/22, Train Loss: 1.1428, Test Loss: 0.9212\n",
      "Epoch 19/22, Train Loss: 1.1515, Test Loss: 0.9287\n",
      "Epoch 20/22, Train Loss: 1.1462, Test Loss: 0.9319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:48,928] Trial 165 finished with value: 0.9324400424957275 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 106, 'layer_1_size': 158, 'layer_2_size': 237, 'layer_3_size': 56, 'layer_4_size': 43, 'layer_5_size': 221, 'dropout_rate': 0.4143505321868867, 'learning_rate': 0.0003350374483392592, 'batch_size': 64, 'epochs': 22}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/22, Train Loss: 1.1352, Test Loss: 0.9321\n",
      "Epoch 22/22, Train Loss: 1.1521, Test Loss: 0.9324\n",
      "Epoch 1/19, Train Loss: 1.2086, Test Loss: 0.9223\n",
      "Epoch 2/19, Train Loss: 1.1772, Test Loss: 0.9299\n",
      "Epoch 3/19, Train Loss: 1.1620, Test Loss: 0.9266\n",
      "Epoch 4/19, Train Loss: 1.1972, Test Loss: 0.9269\n",
      "Epoch 5/19, Train Loss: 1.1167, Test Loss: 0.9249\n",
      "Epoch 6/19, Train Loss: 1.1231, Test Loss: 0.9244\n",
      "Epoch 7/19, Train Loss: 1.0918, Test Loss: 0.9238\n",
      "Epoch 8/19, Train Loss: 1.0780, Test Loss: 0.9223\n",
      "Epoch 9/19, Train Loss: 1.0789, Test Loss: 0.9214\n",
      "Epoch 10/19, Train Loss: 1.0653, Test Loss: 0.9201\n",
      "Epoch 11/19, Train Loss: 1.0748, Test Loss: 0.9186\n",
      "Epoch 12/19, Train Loss: 1.0579, Test Loss: 0.9152\n",
      "Epoch 13/19, Train Loss: 1.0622, Test Loss: 0.9188\n",
      "Epoch 14/19, Train Loss: 1.0578, Test Loss: 0.9169\n",
      "Epoch 15/19, Train Loss: 1.0592, Test Loss: 0.9220\n",
      "Epoch 16/19, Train Loss: 1.0329, Test Loss: 0.9190\n",
      "Epoch 17/19, Train Loss: 1.0708, Test Loss: 0.9175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:50,120] Trial 166 finished with value: 0.9165178686380386 and parameters: {'num_hidden_layers': 5, 'layer_0_size': 111, 'layer_1_size': 62, 'layer_2_size': 248, 'layer_3_size': 33, 'layer_4_size': 45, 'dropout_rate': 0.39600464686396514, 'learning_rate': 0.0002132050706285427, 'batch_size': 64, 'epochs': 19}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/19, Train Loss: 1.0748, Test Loss: 0.9167\n",
      "Epoch 19/19, Train Loss: 1.0443, Test Loss: 0.9165\n",
      "Epoch 1/15, Train Loss: 1.0604, Test Loss: 0.8892\n",
      "Epoch 2/15, Train Loss: 1.0524, Test Loss: 0.8974\n",
      "Epoch 3/15, Train Loss: 1.1194, Test Loss: 0.9043\n",
      "Epoch 4/15, Train Loss: 1.0681, Test Loss: 0.9178\n",
      "Epoch 5/15, Train Loss: 1.0179, Test Loss: 0.9237\n",
      "Epoch 6/15, Train Loss: 1.0466, Test Loss: 0.9117\n",
      "Epoch 7/15, Train Loss: 1.0565, Test Loss: 0.9123\n",
      "Epoch 8/15, Train Loss: 1.0216, Test Loss: 0.9022\n",
      "Epoch 9/15, Train Loss: 1.0513, Test Loss: 0.8989\n",
      "Epoch 10/15, Train Loss: 1.0720, Test Loss: 0.8950\n",
      "Epoch 11/15, Train Loss: 1.0236, Test Loss: 0.8931\n",
      "Epoch 12/15, Train Loss: 1.0345, Test Loss: 0.8984\n",
      "Epoch 13/15, Train Loss: 1.0022, Test Loss: 0.9050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:51,360] Trial 167 finished with value: 0.9147719144821167 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 123, 'layer_1_size': 180, 'layer_2_size': 245, 'layer_3_size': 66, 'layer_4_size': 195, 'layer_5_size': 182, 'layer_6_size': 192, 'dropout_rate': 0.3709122445203804, 'learning_rate': 0.00025616135977079364, 'batch_size': 64, 'epochs': 15}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15, Train Loss: 1.0070, Test Loss: 0.9139\n",
      "Epoch 15/15, Train Loss: 1.0013, Test Loss: 0.9148\n",
      "Epoch 1/5, Train Loss: 1.3404, Test Loss: 0.9089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:51,723] Trial 168 finished with value: 0.9287630617618561 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 114, 'layer_1_size': 49, 'layer_2_size': 213, 'layer_3_size': 48, 'layer_4_size': 190, 'layer_5_size': 210, 'dropout_rate': 0.4937367988903965, 'learning_rate': 0.00013698952044480497, 'batch_size': 64, 'epochs': 5}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Train Loss: 1.2048, Test Loss: 0.9115\n",
      "Epoch 3/5, Train Loss: 1.2968, Test Loss: 0.9185\n",
      "Epoch 4/5, Train Loss: 1.2624, Test Loss: 0.9249\n",
      "Epoch 5/5, Train Loss: 1.1782, Test Loss: 0.9288\n",
      "Epoch 1/11, Train Loss: 1.1965, Test Loss: 0.8722\n",
      "Epoch 2/11, Train Loss: 1.0807, Test Loss: 0.8640\n",
      "Epoch 3/11, Train Loss: 1.0642, Test Loss: 0.8431\n",
      "Epoch 4/11, Train Loss: 1.0581, Test Loss: 0.8270\n",
      "Epoch 5/11, Train Loss: 1.0456, Test Loss: 0.8262\n",
      "Epoch 6/11, Train Loss: 1.0533, Test Loss: 0.8287\n",
      "Epoch 7/11, Train Loss: 1.0431, Test Loss: 0.8271\n",
      "Epoch 8/11, Train Loss: 1.0123, Test Loss: 0.8285\n",
      "Epoch 9/11, Train Loss: 0.9958, Test Loss: 0.8290\n",
      "Epoch 10/11, Train Loss: 1.0130, Test Loss: 0.8345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:53,101] Trial 169 finished with value: 0.8283654962267194 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 191, 'layer_1_size': 171, 'layer_2_size': 157, 'layer_3_size': 146, 'layer_4_size': 54, 'layer_5_size': 120, 'layer_6_size': 202, 'dropout_rate': 0.42006199275592143, 'learning_rate': 0.00011541373750607379, 'batch_size': 32, 'epochs': 11}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/11, Train Loss: 1.0098, Test Loss: 0.8284\n",
      "Epoch 1/9, Train Loss: 1.1865, Test Loss: 1.1802\n",
      "Epoch 2/9, Train Loss: 1.1377, Test Loss: 1.2164\n",
      "Epoch 3/9, Train Loss: 1.1420, Test Loss: 1.2140\n",
      "Epoch 4/9, Train Loss: 1.1232, Test Loss: 1.2174\n",
      "Epoch 5/9, Train Loss: 1.1123, Test Loss: 1.2209\n",
      "Epoch 6/9, Train Loss: 1.1275, Test Loss: 1.2040\n",
      "Epoch 7/9, Train Loss: 1.0810, Test Loss: 1.2125\n",
      "Epoch 8/9, Train Loss: 1.0624, Test Loss: 1.2366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:54,269] Trial 170 finished with value: 1.2253390891211373 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 194, 'layer_1_size': 170, 'layer_2_size': 160, 'layer_3_size': 149, 'layer_4_size': 37, 'layer_5_size': 121, 'layer_6_size': 203, 'dropout_rate': 0.4501158801469933, 'learning_rate': 0.00016324126901748142, 'batch_size': 32, 'epochs': 9}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/9, Train Loss: 1.1003, Test Loss: 1.2253\n",
      "Epoch 1/12, Train Loss: 1.2112, Test Loss: 0.9584\n",
      "Epoch 2/12, Train Loss: 1.1560, Test Loss: 0.9570\n",
      "Epoch 3/12, Train Loss: 1.1570, Test Loss: 0.9344\n",
      "Epoch 4/12, Train Loss: 1.1504, Test Loss: 0.9330\n",
      "Epoch 5/12, Train Loss: 1.1179, Test Loss: 0.9328\n",
      "Epoch 6/12, Train Loss: 1.1214, Test Loss: 0.9383\n",
      "Epoch 7/12, Train Loss: 1.1308, Test Loss: 0.9418\n",
      "Epoch 8/12, Train Loss: 1.1419, Test Loss: 0.9397\n",
      "Epoch 9/12, Train Loss: 1.1365, Test Loss: 0.9311\n",
      "Epoch 10/12, Train Loss: 1.1235, Test Loss: 0.9374\n",
      "Epoch 11/12, Train Loss: 1.0818, Test Loss: 0.9418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:56,278] Trial 171 finished with value: 0.9422243663242885 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 93, 'layer_1_size': 165, 'layer_2_size': 154, 'layer_3_size': 173, 'layer_4_size': 54, 'layer_5_size': 117, 'layer_6_size': 199, 'dropout_rate': 0.42384778752492563, 'learning_rate': 0.00011427076440859816, 'batch_size': 32, 'epochs': 12}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/12, Train Loss: 1.1536, Test Loss: 0.9422\n",
      "Epoch 1/14, Train Loss: 1.2904, Test Loss: 1.4108\n",
      "Epoch 2/14, Train Loss: 1.1897, Test Loss: 1.3816\n",
      "Epoch 3/14, Train Loss: 1.1660, Test Loss: 1.3528\n",
      "Epoch 4/14, Train Loss: 1.1775, Test Loss: 1.3556\n",
      "Epoch 5/14, Train Loss: 1.1734, Test Loss: 1.3521\n",
      "Epoch 6/14, Train Loss: 1.1022, Test Loss: 1.3529\n",
      "Epoch 7/14, Train Loss: 1.1148, Test Loss: 1.3511\n",
      "Epoch 8/14, Train Loss: 1.1137, Test Loss: 1.3516\n",
      "Epoch 9/14, Train Loss: 1.1024, Test Loss: 1.3508\n",
      "Epoch 10/14, Train Loss: 1.1029, Test Loss: 1.3539\n",
      "Epoch 11/14, Train Loss: 1.0990, Test Loss: 1.3553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:31:58,012] Trial 172 finished with value: 1.3556836417743139 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 105, 'layer_1_size': 186, 'layer_2_size': 147, 'layer_3_size': 74, 'layer_4_size': 47, 'layer_5_size': 109, 'layer_6_size': 210, 'dropout_rate': 0.4099861314683046, 'learning_rate': 0.00013934908490715773, 'batch_size': 32, 'epochs': 14}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/14, Train Loss: 1.1147, Test Loss: 1.3543\n",
      "Epoch 13/14, Train Loss: 1.0780, Test Loss: 1.3553\n",
      "Epoch 14/14, Train Loss: 1.0815, Test Loss: 1.3557\n",
      "Epoch 1/16, Train Loss: 1.3100, Test Loss: 1.1334\n",
      "Epoch 2/16, Train Loss: 1.2308, Test Loss: 1.1439\n",
      "Epoch 3/16, Train Loss: 1.2413, Test Loss: 1.1522\n",
      "Epoch 4/16, Train Loss: 1.2411, Test Loss: 1.1570\n",
      "Epoch 5/16, Train Loss: 1.2428, Test Loss: 1.1677\n",
      "Epoch 6/16, Train Loss: 1.2031, Test Loss: 1.1877\n",
      "Epoch 7/16, Train Loss: 1.2738, Test Loss: 1.1689\n",
      "Epoch 8/16, Train Loss: 1.1630, Test Loss: 1.1611\n",
      "Epoch 9/16, Train Loss: 1.1589, Test Loss: 1.1610\n",
      "Epoch 10/16, Train Loss: 1.1503, Test Loss: 1.1667\n",
      "Epoch 11/16, Train Loss: 1.1836, Test Loss: 1.1643\n",
      "Epoch 12/16, Train Loss: 1.1808, Test Loss: 1.1582\n",
      "Epoch 13/16, Train Loss: 1.1464, Test Loss: 1.1469\n",
      "Epoch 14/16, Train Loss: 1.1812, Test Loss: 1.1415\n",
      "Epoch 15/16, Train Loss: 1.0954, Test Loss: 1.1450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:32:00,656] Trial 173 finished with value: 1.1413488728659493 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 222, 'layer_1_size': 180, 'layer_2_size': 137, 'layer_3_size': 167, 'layer_4_size': 59, 'layer_5_size': 231, 'layer_6_size': 215, 'dropout_rate': 0.42034787073038266, 'learning_rate': 0.00011326366874486197, 'batch_size': 32, 'epochs': 16}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/16, Train Loss: 1.1595, Test Loss: 1.1413\n",
      "Epoch 1/11, Train Loss: 1.1846, Test Loss: 1.0840\n",
      "Epoch 2/11, Train Loss: 1.1215, Test Loss: 1.0948\n",
      "Epoch 3/11, Train Loss: 1.1332, Test Loss: 1.0810\n",
      "Epoch 4/11, Train Loss: 1.1236, Test Loss: 1.0815\n",
      "Epoch 5/11, Train Loss: 1.1151, Test Loss: 1.0836\n",
      "Epoch 6/11, Train Loss: 1.1256, Test Loss: 1.0833\n",
      "Epoch 7/11, Train Loss: 1.1188, Test Loss: 1.0824\n",
      "Epoch 8/11, Train Loss: 1.0778, Test Loss: 1.0827\n",
      "Epoch 9/11, Train Loss: 1.0652, Test Loss: 1.0771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:32:01,767] Trial 174 finished with value: 1.0820690904344832 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 208, 'layer_1_size': 175, 'layer_2_size': 45, 'layer_3_size': 69, 'layer_4_size': 200, 'layer_5_size': 131, 'dropout_rate': 0.40177947696137717, 'learning_rate': 0.00021613787546209624, 'batch_size': 32, 'epochs': 11}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/11, Train Loss: 1.0534, Test Loss: 1.0825\n",
      "Epoch 11/11, Train Loss: 1.0606, Test Loss: 1.0821\n",
      "Epoch 1/47, Train Loss: 1.5347, Test Loss: 0.8822\n",
      "Epoch 2/47, Train Loss: 1.4155, Test Loss: 0.8832\n",
      "Epoch 3/47, Train Loss: 1.3508, Test Loss: 0.8847\n",
      "Epoch 4/47, Train Loss: 1.2690, Test Loss: 0.8864\n",
      "Epoch 5/47, Train Loss: 1.2974, Test Loss: 0.8865\n",
      "Epoch 6/47, Train Loss: 1.1919, Test Loss: 0.8877\n",
      "Epoch 7/47, Train Loss: 1.2231, Test Loss: 0.8883\n",
      "Epoch 8/47, Train Loss: 1.1853, Test Loss: 0.8898\n",
      "Epoch 9/47, Train Loss: 1.1608, Test Loss: 0.8915\n",
      "Epoch 10/47, Train Loss: 1.1870, Test Loss: 0.8905\n",
      "Epoch 11/47, Train Loss: 1.1148, Test Loss: 0.8937\n",
      "Epoch 12/47, Train Loss: 1.1253, Test Loss: 0.8941\n",
      "Epoch 13/47, Train Loss: 1.1324, Test Loss: 0.8958\n",
      "Epoch 14/47, Train Loss: 1.1487, Test Loss: 0.9015\n",
      "Epoch 15/47, Train Loss: 1.1059, Test Loss: 0.9057\n",
      "Epoch 16/47, Train Loss: 1.1479, Test Loss: 0.9052\n",
      "Epoch 17/47, Train Loss: 1.1060, Test Loss: 0.9070\n",
      "Epoch 18/47, Train Loss: 1.1028, Test Loss: 0.9024\n",
      "Epoch 19/47, Train Loss: 1.1146, Test Loss: 0.9043\n",
      "Epoch 20/47, Train Loss: 1.1161, Test Loss: 0.9083\n",
      "Epoch 21/47, Train Loss: 1.1177, Test Loss: 0.9099\n",
      "Epoch 22/47, Train Loss: 1.1249, Test Loss: 0.9099\n",
      "Epoch 23/47, Train Loss: 1.1220, Test Loss: 0.9115\n",
      "Epoch 24/47, Train Loss: 1.1174, Test Loss: 0.9102\n",
      "Epoch 25/47, Train Loss: 1.1280, Test Loss: 0.9101\n",
      "Epoch 26/47, Train Loss: 1.0919, Test Loss: 0.9091\n",
      "Epoch 27/47, Train Loss: 1.1172, Test Loss: 0.9106\n",
      "Epoch 28/47, Train Loss: 1.0953, Test Loss: 0.9099\n",
      "Epoch 29/47, Train Loss: 1.1150, Test Loss: 0.9080\n",
      "Epoch 30/47, Train Loss: 1.0739, Test Loss: 0.9069\n",
      "Epoch 31/47, Train Loss: 1.1321, Test Loss: 0.9082\n",
      "Epoch 32/47, Train Loss: 1.1216, Test Loss: 0.9089\n",
      "Epoch 33/47, Train Loss: 1.1189, Test Loss: 0.9115\n",
      "Epoch 34/47, Train Loss: 1.0844, Test Loss: 0.9145\n",
      "Epoch 35/47, Train Loss: 1.1181, Test Loss: 0.9112\n",
      "Epoch 36/47, Train Loss: 1.0956, Test Loss: 0.9103\n",
      "Epoch 37/47, Train Loss: 1.1235, Test Loss: 0.9106\n",
      "Epoch 38/47, Train Loss: 1.0659, Test Loss: 0.9106\n",
      "Epoch 39/47, Train Loss: 1.1070, Test Loss: 0.9125\n",
      "Epoch 40/47, Train Loss: 1.1080, Test Loss: 0.9133\n",
      "Epoch 41/47, Train Loss: 1.0999, Test Loss: 0.9137\n",
      "Epoch 42/47, Train Loss: 1.1088, Test Loss: 0.9119\n",
      "Epoch 43/47, Train Loss: 1.0630, Test Loss: 0.9147\n",
      "Epoch 44/47, Train Loss: 1.0728, Test Loss: 0.9133\n",
      "Epoch 45/47, Train Loss: 1.0394, Test Loss: 0.9132\n",
      "Epoch 46/47, Train Loss: 1.0729, Test Loss: 0.9126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:32:03,402] Trial 175 finished with value: 0.9122559428215027 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 227, 'layer_1_size': 83, 'layer_2_size': 196, 'layer_3_size': 125, 'layer_4_size': 51, 'layer_5_size': 136, 'dropout_rate': 0.38663228899950947, 'learning_rate': 0.00017348744810718355, 'batch_size': 128, 'epochs': 47}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/47, Train Loss: 1.0724, Test Loss: 0.9123\n",
      "Epoch 1/54, Train Loss: 1.1490, Test Loss: 0.8801\n",
      "Epoch 2/54, Train Loss: 1.0623, Test Loss: 0.9087\n",
      "Epoch 3/54, Train Loss: 1.0705, Test Loss: 0.9070\n",
      "Epoch 4/54, Train Loss: 1.0728, Test Loss: 0.8996\n",
      "Epoch 5/54, Train Loss: 1.0517, Test Loss: 0.8961\n",
      "Epoch 6/54, Train Loss: 1.0499, Test Loss: 0.9068\n",
      "Epoch 7/54, Train Loss: 1.0250, Test Loss: 0.8943\n",
      "Epoch 8/54, Train Loss: 1.0337, Test Loss: 0.8891\n",
      "Epoch 9/54, Train Loss: 1.0342, Test Loss: 0.8864\n",
      "Epoch 10/54, Train Loss: 0.9818, Test Loss: 0.8936\n",
      "Epoch 11/54, Train Loss: 1.0309, Test Loss: 0.8916\n",
      "Epoch 12/54, Train Loss: 1.0086, Test Loss: 0.8922\n",
      "Epoch 13/54, Train Loss: 1.0297, Test Loss: 0.8917\n",
      "Epoch 14/54, Train Loss: 0.9993, Test Loss: 0.8951\n",
      "Epoch 15/54, Train Loss: 1.0088, Test Loss: 0.8939\n",
      "Epoch 16/54, Train Loss: 1.0147, Test Loss: 0.8977\n",
      "Epoch 17/54, Train Loss: 1.0349, Test Loss: 0.9022\n",
      "Epoch 18/54, Train Loss: 1.0143, Test Loss: 0.9030\n",
      "Epoch 19/54, Train Loss: 1.0001, Test Loss: 0.8934\n",
      "Epoch 20/54, Train Loss: 1.0023, Test Loss: 0.8891\n",
      "Epoch 21/54, Train Loss: 1.0183, Test Loss: 0.8915\n",
      "Epoch 22/54, Train Loss: 0.9899, Test Loss: 0.8999\n",
      "Epoch 23/54, Train Loss: 0.9822, Test Loss: 0.9044\n",
      "Epoch 24/54, Train Loss: 1.0255, Test Loss: 0.9155\n",
      "Epoch 25/54, Train Loss: 1.0323, Test Loss: 0.9173\n",
      "Epoch 26/54, Train Loss: 1.0368, Test Loss: 0.9107\n",
      "Epoch 27/54, Train Loss: 1.0123, Test Loss: 0.9103\n",
      "Epoch 28/54, Train Loss: 0.9910, Test Loss: 0.9137\n",
      "Epoch 29/54, Train Loss: 0.9948, Test Loss: 0.9138\n",
      "Epoch 30/54, Train Loss: 1.0475, Test Loss: 0.9128\n",
      "Epoch 31/54, Train Loss: 0.9891, Test Loss: 0.9062\n",
      "Epoch 32/54, Train Loss: 0.9877, Test Loss: 0.9059\n",
      "Epoch 33/54, Train Loss: 0.9729, Test Loss: 0.9123\n",
      "Epoch 34/54, Train Loss: 0.9869, Test Loss: 0.9124\n",
      "Epoch 35/54, Train Loss: 1.0050, Test Loss: 0.9125\n",
      "Epoch 36/54, Train Loss: 0.9656, Test Loss: 0.9142\n",
      "Epoch 37/54, Train Loss: 0.9847, Test Loss: 0.9078\n",
      "Epoch 38/54, Train Loss: 0.9967, Test Loss: 0.9131\n",
      "Epoch 39/54, Train Loss: 0.9641, Test Loss: 0.9107\n",
      "Epoch 40/54, Train Loss: 0.9853, Test Loss: 0.9069\n",
      "Epoch 41/54, Train Loss: 0.9718, Test Loss: 0.9050\n",
      "Epoch 42/54, Train Loss: 0.9744, Test Loss: 0.9048\n",
      "Epoch 43/54, Train Loss: 0.9963, Test Loss: 0.9100\n",
      "Epoch 44/54, Train Loss: 0.9691, Test Loss: 0.9066\n",
      "Epoch 45/54, Train Loss: 0.9886, Test Loss: 0.9130\n",
      "Epoch 46/54, Train Loss: 0.9849, Test Loss: 0.9078\n",
      "Epoch 47/54, Train Loss: 0.9917, Test Loss: 0.9074\n",
      "Epoch 48/54, Train Loss: 0.9642, Test Loss: 0.9095\n",
      "Epoch 49/54, Train Loss: 0.9311, Test Loss: 0.9138\n",
      "Epoch 50/54, Train Loss: 0.9767, Test Loss: 0.9183\n",
      "Epoch 51/54, Train Loss: 0.9630, Test Loss: 0.9170\n",
      "Epoch 52/54, Train Loss: 0.9650, Test Loss: 0.9174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:32:08,021] Trial 176 finished with value: 0.9176561534404755 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 117, 'layer_1_size': 183, 'layer_2_size': 208, 'layer_3_size': 181, 'layer_4_size': 74, 'layer_5_size': 188, 'layer_6_size': 241, 'dropout_rate': 0.17737710995811712, 'learning_rate': 0.00010254573031900665, 'batch_size': 64, 'epochs': 54}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/54, Train Loss: 0.9758, Test Loss: 0.9167\n",
      "Epoch 54/54, Train Loss: 0.9660, Test Loss: 0.9177\n",
      "Epoch 1/7, Train Loss: 1.2492, Test Loss: 0.9821\n",
      "Epoch 2/7, Train Loss: 1.1809, Test Loss: 0.9857\n",
      "Epoch 3/7, Train Loss: 1.1754, Test Loss: 0.9813\n",
      "Epoch 4/7, Train Loss: 1.1831, Test Loss: 0.9878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:32:08,528] Trial 177 finished with value: 0.9952109903097153 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 110, 'layer_1_size': 199, 'layer_2_size': 203, 'layer_3_size': 142, 'layer_4_size': 66, 'layer_5_size': 98, 'dropout_rate': 0.43448144575816233, 'learning_rate': 0.0001947215741307285, 'batch_size': 64, 'epochs': 7}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/7, Train Loss: 1.1416, Test Loss: 0.9889\n",
      "Epoch 6/7, Train Loss: 1.1266, Test Loss: 0.9940\n",
      "Epoch 7/7, Train Loss: 1.1195, Test Loss: 0.9952\n",
      "Epoch 1/99, Train Loss: 1.4397, Test Loss: 1.1219\n",
      "Epoch 2/99, Train Loss: 1.3706, Test Loss: 1.1281\n",
      "Epoch 3/99, Train Loss: 1.2810, Test Loss: 1.1292\n",
      "Epoch 4/99, Train Loss: 1.3307, Test Loss: 1.1326\n",
      "Epoch 5/99, Train Loss: 1.2497, Test Loss: 1.1423\n",
      "Epoch 6/99, Train Loss: 1.2630, Test Loss: 1.1528\n",
      "Epoch 7/99, Train Loss: 1.2371, Test Loss: 1.1569\n",
      "Epoch 8/99, Train Loss: 1.2308, Test Loss: 1.1614\n",
      "Epoch 9/99, Train Loss: 1.2392, Test Loss: 1.1684\n",
      "Epoch 10/99, Train Loss: 1.1861, Test Loss: 1.1803\n",
      "Epoch 11/99, Train Loss: 1.2565, Test Loss: 1.1805\n",
      "Epoch 12/99, Train Loss: 1.1694, Test Loss: 1.1826\n",
      "Epoch 13/99, Train Loss: 1.2106, Test Loss: 1.1747\n",
      "Epoch 14/99, Train Loss: 1.2087, Test Loss: 1.1786\n",
      "Epoch 15/99, Train Loss: 1.1218, Test Loss: 1.1804\n",
      "Epoch 16/99, Train Loss: 1.1631, Test Loss: 1.1847\n",
      "Epoch 17/99, Train Loss: 1.1657, Test Loss: 1.1828\n",
      "Epoch 18/99, Train Loss: 1.1302, Test Loss: 1.1749\n",
      "Epoch 19/99, Train Loss: 1.1559, Test Loss: 1.1697\n",
      "Epoch 20/99, Train Loss: 1.1764, Test Loss: 1.1725\n",
      "Epoch 21/99, Train Loss: 1.1692, Test Loss: 1.1719\n",
      "Epoch 22/99, Train Loss: 1.1486, Test Loss: 1.1740\n",
      "Epoch 23/99, Train Loss: 1.1511, Test Loss: 1.1665\n",
      "Epoch 24/99, Train Loss: 1.1240, Test Loss: 1.1702\n",
      "Epoch 25/99, Train Loss: 1.1477, Test Loss: 1.1632\n",
      "Epoch 26/99, Train Loss: 1.1494, Test Loss: 1.1716\n",
      "Epoch 27/99, Train Loss: 1.1107, Test Loss: 1.1808\n",
      "Epoch 28/99, Train Loss: 1.1402, Test Loss: 1.1819\n",
      "Epoch 29/99, Train Loss: 1.1149, Test Loss: 1.1823\n",
      "Epoch 30/99, Train Loss: 1.1324, Test Loss: 1.1919\n",
      "Epoch 31/99, Train Loss: 1.1079, Test Loss: 1.1919\n",
      "Epoch 32/99, Train Loss: 1.1293, Test Loss: 1.1789\n",
      "Epoch 33/99, Train Loss: 1.1028, Test Loss: 1.1831\n",
      "Epoch 34/99, Train Loss: 1.1447, Test Loss: 1.1816\n",
      "Epoch 35/99, Train Loss: 1.1097, Test Loss: 1.1779\n",
      "Epoch 36/99, Train Loss: 1.1138, Test Loss: 1.1787\n",
      "Epoch 37/99, Train Loss: 1.1166, Test Loss: 1.1717\n",
      "Epoch 38/99, Train Loss: 1.1180, Test Loss: 1.1710\n",
      "Epoch 39/99, Train Loss: 1.1325, Test Loss: 1.1702\n",
      "Epoch 40/99, Train Loss: 1.1397, Test Loss: 1.1689\n",
      "Epoch 41/99, Train Loss: 1.1082, Test Loss: 1.1659\n",
      "Epoch 42/99, Train Loss: 1.1164, Test Loss: 1.1659\n",
      "Epoch 43/99, Train Loss: 1.0866, Test Loss: 1.1683\n",
      "Epoch 44/99, Train Loss: 1.1086, Test Loss: 1.1730\n",
      "Epoch 45/99, Train Loss: 1.0507, Test Loss: 1.1763\n",
      "Epoch 46/99, Train Loss: 1.1570, Test Loss: 1.1756\n",
      "Epoch 47/99, Train Loss: 1.0817, Test Loss: 1.1754\n",
      "Epoch 48/99, Train Loss: 1.0783, Test Loss: 1.1671\n",
      "Epoch 49/99, Train Loss: 1.1068, Test Loss: 1.1735\n",
      "Epoch 50/99, Train Loss: 1.0828, Test Loss: 1.1751\n",
      "Epoch 51/99, Train Loss: 1.0761, Test Loss: 1.1661\n",
      "Epoch 52/99, Train Loss: 1.0833, Test Loss: 1.1834\n",
      "Epoch 53/99, Train Loss: 1.0935, Test Loss: 1.1835\n",
      "Epoch 54/99, Train Loss: 1.0604, Test Loss: 1.1733\n",
      "Epoch 55/99, Train Loss: 1.0758, Test Loss: 1.1708\n",
      "Epoch 56/99, Train Loss: 1.0947, Test Loss: 1.1682\n",
      "Epoch 57/99, Train Loss: 1.0732, Test Loss: 1.1681\n",
      "Epoch 58/99, Train Loss: 1.0707, Test Loss: 1.1739\n",
      "Epoch 59/99, Train Loss: 1.0852, Test Loss: 1.1761\n",
      "Epoch 60/99, Train Loss: 1.0732, Test Loss: 1.1697\n",
      "Epoch 61/99, Train Loss: 1.0534, Test Loss: 1.1665\n",
      "Epoch 62/99, Train Loss: 1.0654, Test Loss: 1.1687\n",
      "Epoch 63/99, Train Loss: 1.0637, Test Loss: 1.1705\n",
      "Epoch 64/99, Train Loss: 1.0790, Test Loss: 1.1666\n",
      "Epoch 65/99, Train Loss: 1.0839, Test Loss: 1.1671\n",
      "Epoch 66/99, Train Loss: 1.0851, Test Loss: 1.1727\n",
      "Epoch 67/99, Train Loss: 1.0567, Test Loss: 1.1691\n",
      "Epoch 68/99, Train Loss: 1.0811, Test Loss: 1.1677\n",
      "Epoch 69/99, Train Loss: 1.0684, Test Loss: 1.1652\n",
      "Epoch 70/99, Train Loss: 1.0799, Test Loss: 1.1718\n",
      "Epoch 71/99, Train Loss: 1.0724, Test Loss: 1.1675\n",
      "Epoch 72/99, Train Loss: 1.0631, Test Loss: 1.1631\n",
      "Epoch 73/99, Train Loss: 1.0690, Test Loss: 1.1633\n",
      "Epoch 74/99, Train Loss: 1.0530, Test Loss: 1.1685\n",
      "Epoch 75/99, Train Loss: 1.0720, Test Loss: 1.1707\n",
      "Epoch 76/99, Train Loss: 1.0688, Test Loss: 1.1707\n",
      "Epoch 77/99, Train Loss: 1.0685, Test Loss: 1.1693\n",
      "Epoch 78/99, Train Loss: 1.0510, Test Loss: 1.1697\n",
      "Epoch 79/99, Train Loss: 1.0575, Test Loss: 1.1768\n",
      "Epoch 80/99, Train Loss: 1.0485, Test Loss: 1.1802\n",
      "Epoch 81/99, Train Loss: 1.0700, Test Loss: 1.1734\n",
      "Epoch 82/99, Train Loss: 1.0825, Test Loss: 1.1731\n",
      "Epoch 83/99, Train Loss: 1.0707, Test Loss: 1.1727\n",
      "Epoch 84/99, Train Loss: 1.0494, Test Loss: 1.1733\n",
      "Epoch 85/99, Train Loss: 1.0672, Test Loss: 1.1758\n",
      "Epoch 86/99, Train Loss: 1.0542, Test Loss: 1.1796\n",
      "Epoch 87/99, Train Loss: 1.0856, Test Loss: 1.1833\n",
      "Epoch 88/99, Train Loss: 1.0639, Test Loss: 1.1809\n",
      "Epoch 89/99, Train Loss: 1.0524, Test Loss: 1.1810\n",
      "Epoch 90/99, Train Loss: 1.0650, Test Loss: 1.1770\n",
      "Epoch 91/99, Train Loss: 1.0648, Test Loss: 1.1741\n",
      "Epoch 92/99, Train Loss: 1.0602, Test Loss: 1.1721\n",
      "Epoch 93/99, Train Loss: 1.0634, Test Loss: 1.1728\n",
      "Epoch 94/99, Train Loss: 1.0463, Test Loss: 1.1686\n",
      "Epoch 95/99, Train Loss: 1.0507, Test Loss: 1.1679\n",
      "Epoch 96/99, Train Loss: 1.0574, Test Loss: 1.1721\n",
      "Epoch 97/99, Train Loss: 1.0567, Test Loss: 1.1693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:32:16,198] Trial 178 finished with value: 1.1712744235992432 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 194, 'layer_1_size': 54, 'layer_2_size': 239, 'layer_3_size': 86, 'layer_4_size': 184, 'layer_5_size': 124, 'layer_6_size': 183, 'dropout_rate': 0.4691831752336029, 'learning_rate': 0.00015282187056850369, 'batch_size': 64, 'epochs': 99}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/99, Train Loss: 1.0509, Test Loss: 1.1691\n",
      "Epoch 99/99, Train Loss: 1.0522, Test Loss: 1.1713\n",
      "Epoch 1/19, Train Loss: 1.5263, Test Loss: 0.9501\n",
      "Epoch 2/19, Train Loss: 1.4144, Test Loss: 0.9504\n",
      "Epoch 3/19, Train Loss: 1.4042, Test Loss: 0.9507\n",
      "Epoch 4/19, Train Loss: 1.3465, Test Loss: 0.9519\n",
      "Epoch 5/19, Train Loss: 1.3163, Test Loss: 0.9529\n",
      "Epoch 6/19, Train Loss: 1.3188, Test Loss: 0.9551\n",
      "Epoch 7/19, Train Loss: 1.3147, Test Loss: 0.9580\n",
      "Epoch 8/19, Train Loss: 1.2828, Test Loss: 0.9586\n",
      "Epoch 9/19, Train Loss: 1.2067, Test Loss: 0.9613\n",
      "Epoch 10/19, Train Loss: 1.1871, Test Loss: 0.9629\n",
      "Epoch 11/19, Train Loss: 1.1808, Test Loss: 0.9627\n",
      "Epoch 12/19, Train Loss: 1.2177, Test Loss: 0.9614\n",
      "Epoch 13/19, Train Loss: 1.1845, Test Loss: 0.9601\n",
      "Epoch 14/19, Train Loss: 1.1570, Test Loss: 0.9585\n",
      "Epoch 15/19, Train Loss: 1.1516, Test Loss: 0.9573\n",
      "Epoch 16/19, Train Loss: 1.1876, Test Loss: 0.9558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:32:16,859] Trial 179 finished with value: 0.9526060223579407 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 121, 'layer_1_size': 155, 'layer_2_size': 180, 'layer_3_size': 111, 'layer_4_size': 88, 'layer_5_size': 217, 'layer_6_size': 174, 'dropout_rate': 0.38082477293564065, 'learning_rate': 0.00012833487335034625, 'batch_size': 256, 'epochs': 19}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/19, Train Loss: 1.1797, Test Loss: 0.9551\n",
      "Epoch 18/19, Train Loss: 1.1491, Test Loss: 0.9535\n",
      "Epoch 19/19, Train Loss: 1.1643, Test Loss: 0.9526\n",
      "Epoch 1/12, Train Loss: 1.2125, Test Loss: 1.1358\n",
      "Epoch 2/12, Train Loss: 1.2076, Test Loss: 1.1444\n",
      "Epoch 3/12, Train Loss: 1.1757, Test Loss: 1.1430\n",
      "Epoch 4/12, Train Loss: 1.1506, Test Loss: 1.1399\n",
      "Epoch 5/12, Train Loss: 1.1319, Test Loss: 1.1360\n",
      "Epoch 6/12, Train Loss: 1.1450, Test Loss: 1.1369\n",
      "Epoch 7/12, Train Loss: 1.1164, Test Loss: 1.1378\n",
      "Epoch 8/12, Train Loss: 1.0921, Test Loss: 1.1388\n",
      "Epoch 9/12, Train Loss: 1.1000, Test Loss: 1.1373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:32:17,909] Trial 180 finished with value: 1.1371003687381744 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 126, 'layer_1_size': 167, 'layer_2_size': 255, 'layer_3_size': 157, 'layer_4_size': 93, 'layer_5_size': 226, 'dropout_rate': 0.4801262150448441, 'learning_rate': 0.00023791631939789872, 'batch_size': 64, 'epochs': 12}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/12, Train Loss: 1.0779, Test Loss: 1.1386\n",
      "Epoch 11/12, Train Loss: 1.1041, Test Loss: 1.1375\n",
      "Epoch 12/12, Train Loss: 1.0793, Test Loss: 1.1371\n",
      "Epoch 1/37, Train Loss: 1.1973, Test Loss: 0.9433\n",
      "Epoch 2/37, Train Loss: 1.2040, Test Loss: 0.9503\n",
      "Epoch 3/37, Train Loss: 1.1608, Test Loss: 0.9493\n",
      "Epoch 4/37, Train Loss: 1.1207, Test Loss: 0.9378\n",
      "Epoch 5/37, Train Loss: 1.1229, Test Loss: 0.9359\n",
      "Epoch 6/37, Train Loss: 1.1169, Test Loss: 0.9341\n",
      "Epoch 7/37, Train Loss: 1.1328, Test Loss: 0.9331\n",
      "Epoch 8/37, Train Loss: 1.0980, Test Loss: 0.9393\n",
      "Epoch 9/37, Train Loss: 1.1567, Test Loss: 0.9385\n",
      "Epoch 10/37, Train Loss: 1.0830, Test Loss: 0.9389\n",
      "Epoch 11/37, Train Loss: 1.0507, Test Loss: 0.9354\n",
      "Epoch 12/37, Train Loss: 1.0763, Test Loss: 0.9349\n",
      "Epoch 13/37, Train Loss: 1.0982, Test Loss: 0.9299\n",
      "Epoch 14/37, Train Loss: 1.1251, Test Loss: 0.9298\n",
      "Epoch 15/37, Train Loss: 1.0885, Test Loss: 0.9351\n",
      "Epoch 16/37, Train Loss: 1.1054, Test Loss: 0.9331\n",
      "Epoch 17/37, Train Loss: 1.0593, Test Loss: 0.9396\n",
      "Epoch 18/37, Train Loss: 1.0728, Test Loss: 0.9388\n",
      "Epoch 19/37, Train Loss: 1.0520, Test Loss: 0.9363\n",
      "Epoch 20/37, Train Loss: 1.0465, Test Loss: 0.9382\n",
      "Epoch 21/37, Train Loss: 1.0394, Test Loss: 0.9429\n",
      "Epoch 22/37, Train Loss: 1.0585, Test Loss: 0.9389\n",
      "Epoch 23/37, Train Loss: 1.0276, Test Loss: 0.9404\n",
      "Epoch 24/37, Train Loss: 1.0348, Test Loss: 0.9435\n",
      "Epoch 25/37, Train Loss: 1.0477, Test Loss: 0.9450\n",
      "Epoch 26/37, Train Loss: 1.0512, Test Loss: 0.9477\n",
      "Epoch 27/37, Train Loss: 1.0473, Test Loss: 0.9419\n",
      "Epoch 28/37, Train Loss: 1.0612, Test Loss: 0.9434\n",
      "Epoch 29/37, Train Loss: 1.0297, Test Loss: 0.9460\n",
      "Epoch 30/37, Train Loss: 1.0286, Test Loss: 0.9464\n",
      "Epoch 31/37, Train Loss: 1.0656, Test Loss: 0.9499\n",
      "Epoch 32/37, Train Loss: 1.0229, Test Loss: 0.9471\n",
      "Epoch 33/37, Train Loss: 1.0250, Test Loss: 0.9443\n",
      "Epoch 34/37, Train Loss: 1.0291, Test Loss: 0.9452\n",
      "Epoch 35/37, Train Loss: 1.0414, Test Loss: 0.9448\n",
      "Epoch 36/37, Train Loss: 0.9992, Test Loss: 0.9446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:32:21,574] Trial 181 finished with value: 0.945728562772274 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 142, 'layer_1_size': 193, 'layer_2_size': 218, 'layer_3_size': 36, 'layer_4_size': 160, 'layer_5_size': 242, 'layer_6_size': 147, 'layer_7_size': 101, 'dropout_rate': 0.37297303176815966, 'learning_rate': 0.0001929434324034591, 'batch_size': 64, 'epochs': 37}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/37, Train Loss: 1.0305, Test Loss: 0.9457\n",
      "Epoch 1/31, Train Loss: 1.1765, Test Loss: 0.9359\n",
      "Epoch 2/31, Train Loss: 1.1044, Test Loss: 0.9399\n",
      "Epoch 3/31, Train Loss: 1.1839, Test Loss: 0.9275\n",
      "Epoch 4/31, Train Loss: 1.1044, Test Loss: 0.9268\n",
      "Epoch 5/31, Train Loss: 1.0327, Test Loss: 0.9321\n",
      "Epoch 6/31, Train Loss: 1.0841, Test Loss: 0.9294\n",
      "Epoch 7/31, Train Loss: 1.0554, Test Loss: 0.9263\n",
      "Epoch 8/31, Train Loss: 1.0375, Test Loss: 0.9270\n",
      "Epoch 9/31, Train Loss: 1.0347, Test Loss: 0.9328\n",
      "Epoch 10/31, Train Loss: 1.0312, Test Loss: 0.9400\n",
      "Epoch 11/31, Train Loss: 1.0450, Test Loss: 0.9546\n",
      "Epoch 12/31, Train Loss: 1.0740, Test Loss: 0.9565\n",
      "Epoch 13/31, Train Loss: 1.0507, Test Loss: 0.9420\n",
      "Epoch 14/31, Train Loss: 1.0730, Test Loss: 0.9358\n",
      "Epoch 15/31, Train Loss: 1.0127, Test Loss: 0.9388\n",
      "Epoch 16/31, Train Loss: 1.1082, Test Loss: 0.9396\n",
      "Epoch 17/31, Train Loss: 1.0159, Test Loss: 0.9436\n",
      "Epoch 18/31, Train Loss: 1.0110, Test Loss: 0.9411\n",
      "Epoch 19/31, Train Loss: 1.0466, Test Loss: 0.9497\n",
      "Epoch 20/31, Train Loss: 1.0199, Test Loss: 0.9457\n",
      "Epoch 21/31, Train Loss: 0.9996, Test Loss: 0.9447\n",
      "Epoch 22/31, Train Loss: 1.0023, Test Loss: 0.9461\n",
      "Epoch 23/31, Train Loss: 0.9786, Test Loss: 0.9473\n",
      "Epoch 24/31, Train Loss: 1.0137, Test Loss: 0.9545\n",
      "Epoch 25/31, Train Loss: 1.0483, Test Loss: 0.9525\n",
      "Epoch 26/31, Train Loss: 1.0282, Test Loss: 0.9465\n",
      "Epoch 27/31, Train Loss: 0.9762, Test Loss: 0.9435\n",
      "Epoch 28/31, Train Loss: 0.9997, Test Loss: 0.9415\n",
      "Epoch 29/31, Train Loss: 0.9728, Test Loss: 0.9444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:32:24,386] Trial 182 finished with value: 0.9412305653095245 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 133, 'layer_1_size': 189, 'layer_2_size': 211, 'layer_3_size': 40, 'layer_4_size': 101, 'layer_5_size': 246, 'layer_6_size': 133, 'dropout_rate': 0.37786626574173776, 'learning_rate': 0.00016711560587588659, 'batch_size': 64, 'epochs': 31}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/31, Train Loss: 0.9990, Test Loss: 0.9456\n",
      "Epoch 31/31, Train Loss: 1.0014, Test Loss: 0.9412\n",
      "Epoch 1/41, Train Loss: 1.1656, Test Loss: 1.0553\n",
      "Epoch 2/41, Train Loss: 1.1089, Test Loss: 1.0538\n",
      "Epoch 3/41, Train Loss: 1.1433, Test Loss: 1.0505\n",
      "Epoch 4/41, Train Loss: 1.1127, Test Loss: 1.0599\n",
      "Epoch 5/41, Train Loss: 1.0815, Test Loss: 1.0634\n",
      "Epoch 6/41, Train Loss: 1.0986, Test Loss: 1.0623\n",
      "Epoch 7/41, Train Loss: 1.0655, Test Loss: 1.0707\n",
      "Epoch 8/41, Train Loss: 1.1374, Test Loss: 1.0687\n",
      "Epoch 9/41, Train Loss: 1.0899, Test Loss: 1.0688\n",
      "Epoch 10/41, Train Loss: 1.0549, Test Loss: 1.0648\n",
      "Epoch 11/41, Train Loss: 1.0215, Test Loss: 1.0688\n",
      "Epoch 12/41, Train Loss: 1.1167, Test Loss: 1.0667\n",
      "Epoch 13/41, Train Loss: 1.0926, Test Loss: 1.0651\n",
      "Epoch 14/41, Train Loss: 1.0672, Test Loss: 1.0680\n",
      "Epoch 15/41, Train Loss: 1.0110, Test Loss: 1.0692\n",
      "Epoch 16/41, Train Loss: 1.0815, Test Loss: 1.0656\n",
      "Epoch 17/41, Train Loss: 1.0595, Test Loss: 1.0621\n",
      "Epoch 18/41, Train Loss: 1.0655, Test Loss: 1.0604\n",
      "Epoch 19/41, Train Loss: 1.0666, Test Loss: 1.0536\n",
      "Epoch 20/41, Train Loss: 1.0509, Test Loss: 1.0529\n",
      "Epoch 21/41, Train Loss: 1.0417, Test Loss: 1.0538\n",
      "Epoch 22/41, Train Loss: 1.1014, Test Loss: 1.0494\n",
      "Epoch 23/41, Train Loss: 1.0494, Test Loss: 1.0436\n",
      "Epoch 24/41, Train Loss: 1.0686, Test Loss: 1.0482\n",
      "Epoch 25/41, Train Loss: 1.0270, Test Loss: 1.0420\n",
      "Epoch 26/41, Train Loss: 1.0519, Test Loss: 1.0387\n",
      "Epoch 27/41, Train Loss: 1.0333, Test Loss: 1.0368\n",
      "Epoch 28/41, Train Loss: 1.0699, Test Loss: 1.0411\n",
      "Epoch 29/41, Train Loss: 1.0786, Test Loss: 1.0449\n",
      "Epoch 30/41, Train Loss: 1.0231, Test Loss: 1.0429\n",
      "Epoch 31/41, Train Loss: 1.0454, Test Loss: 1.0427\n",
      "Epoch 32/41, Train Loss: 1.0329, Test Loss: 1.0441\n",
      "Epoch 33/41, Train Loss: 1.0283, Test Loss: 1.0417\n",
      "Epoch 34/41, Train Loss: 1.0469, Test Loss: 1.0400\n",
      "Epoch 35/41, Train Loss: 0.9958, Test Loss: 1.0399\n",
      "Epoch 36/41, Train Loss: 0.9976, Test Loss: 1.0367\n",
      "Epoch 37/41, Train Loss: 1.0136, Test Loss: 1.0358\n",
      "Epoch 38/41, Train Loss: 1.0484, Test Loss: 1.0389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:32:28,468] Trial 183 finished with value: 1.0413742065429688 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 149, 'layer_1_size': 195, 'layer_2_size': 234, 'layer_3_size': 44, 'layer_4_size': 55, 'layer_5_size': 237, 'layer_6_size': 169, 'layer_7_size': 36, 'dropout_rate': 0.3898356863479682, 'learning_rate': 9.383463178180581e-05, 'batch_size': 64, 'epochs': 41}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/41, Train Loss: 1.0336, Test Loss: 1.0383\n",
      "Epoch 40/41, Train Loss: 0.9936, Test Loss: 1.0447\n",
      "Epoch 41/41, Train Loss: 1.0193, Test Loss: 1.0414\n",
      "Epoch 1/98, Train Loss: 1.2443, Test Loss: 1.0918\n",
      "Epoch 2/98, Train Loss: 1.2402, Test Loss: 1.0739\n",
      "Epoch 3/98, Train Loss: 1.2642, Test Loss: 1.0657\n",
      "Epoch 4/98, Train Loss: 1.2094, Test Loss: 1.0681\n",
      "Epoch 5/98, Train Loss: 1.2544, Test Loss: 1.0631\n",
      "Epoch 6/98, Train Loss: 1.1920, Test Loss: 1.0605\n",
      "Epoch 7/98, Train Loss: 1.2149, Test Loss: 1.0638\n",
      "Epoch 8/98, Train Loss: 1.2240, Test Loss: 1.0675\n",
      "Epoch 9/98, Train Loss: 1.2098, Test Loss: 1.0688\n",
      "Epoch 10/98, Train Loss: 1.1653, Test Loss: 1.0671\n",
      "Epoch 11/98, Train Loss: 1.1788, Test Loss: 1.0685\n",
      "Epoch 12/98, Train Loss: 1.1570, Test Loss: 1.0682\n",
      "Epoch 13/98, Train Loss: 1.1908, Test Loss: 1.0692\n",
      "Epoch 14/98, Train Loss: 1.1583, Test Loss: 1.0691\n",
      "Epoch 15/98, Train Loss: 1.2019, Test Loss: 1.0677\n",
      "Epoch 16/98, Train Loss: 1.1874, Test Loss: 1.0713\n",
      "Epoch 17/98, Train Loss: 1.1728, Test Loss: 1.0719\n",
      "Epoch 18/98, Train Loss: 1.1362, Test Loss: 1.0693\n",
      "Epoch 19/98, Train Loss: 1.1660, Test Loss: 1.0757\n",
      "Epoch 20/98, Train Loss: 1.1683, Test Loss: 1.0743\n",
      "Epoch 21/98, Train Loss: 1.1186, Test Loss: 1.0685\n",
      "Epoch 22/98, Train Loss: 1.1999, Test Loss: 1.0674\n",
      "Epoch 23/98, Train Loss: 1.1927, Test Loss: 1.0679\n",
      "Epoch 24/98, Train Loss: 1.1364, Test Loss: 1.0719\n",
      "Epoch 25/98, Train Loss: 1.1794, Test Loss: 1.0713\n",
      "Epoch 26/98, Train Loss: 1.1310, Test Loss: 1.0730\n",
      "Epoch 27/98, Train Loss: 1.1496, Test Loss: 1.0725\n",
      "Epoch 28/98, Train Loss: 1.1425, Test Loss: 1.0729\n",
      "Epoch 29/98, Train Loss: 1.1175, Test Loss: 1.0743\n",
      "Epoch 30/98, Train Loss: 1.1641, Test Loss: 1.0767\n",
      "Epoch 31/98, Train Loss: 1.1950, Test Loss: 1.0792\n",
      "Epoch 32/98, Train Loss: 1.1498, Test Loss: 1.0769\n",
      "Epoch 33/98, Train Loss: 1.1557, Test Loss: 1.0779\n",
      "Epoch 34/98, Train Loss: 1.1435, Test Loss: 1.0803\n",
      "Epoch 35/98, Train Loss: 1.1517, Test Loss: 1.0828\n",
      "Epoch 36/98, Train Loss: 1.1362, Test Loss: 1.0805\n",
      "Epoch 37/98, Train Loss: 1.1536, Test Loss: 1.0823\n",
      "Epoch 38/98, Train Loss: 1.1621, Test Loss: 1.0811\n",
      "Epoch 39/98, Train Loss: 1.1391, Test Loss: 1.0818\n",
      "Epoch 40/98, Train Loss: 1.1674, Test Loss: 1.0789\n",
      "Epoch 41/98, Train Loss: 1.1301, Test Loss: 1.0767\n",
      "Epoch 42/98, Train Loss: 1.1377, Test Loss: 1.0779\n",
      "Epoch 43/98, Train Loss: 1.1368, Test Loss: 1.0771\n",
      "Epoch 44/98, Train Loss: 1.1661, Test Loss: 1.0762\n",
      "Epoch 45/98, Train Loss: 1.1293, Test Loss: 1.0736\n",
      "Epoch 46/98, Train Loss: 1.0678, Test Loss: 1.0773\n",
      "Epoch 47/98, Train Loss: 1.1125, Test Loss: 1.0760\n",
      "Epoch 48/98, Train Loss: 1.1443, Test Loss: 1.0795\n",
      "Epoch 49/98, Train Loss: 1.1033, Test Loss: 1.0785\n",
      "Epoch 50/98, Train Loss: 1.1241, Test Loss: 1.0786\n",
      "Epoch 51/98, Train Loss: 1.1046, Test Loss: 1.0786\n",
      "Epoch 52/98, Train Loss: 1.1082, Test Loss: 1.0789\n",
      "Epoch 53/98, Train Loss: 1.1089, Test Loss: 1.0771\n",
      "Epoch 54/98, Train Loss: 1.1354, Test Loss: 1.0797\n",
      "Epoch 55/98, Train Loss: 1.1320, Test Loss: 1.0819\n",
      "Epoch 56/98, Train Loss: 1.1016, Test Loss: 1.0822\n",
      "Epoch 57/98, Train Loss: 1.0790, Test Loss: 1.0847\n",
      "Epoch 58/98, Train Loss: 1.1265, Test Loss: 1.0811\n",
      "Epoch 59/98, Train Loss: 1.1396, Test Loss: 1.0828\n",
      "Epoch 60/98, Train Loss: 1.1329, Test Loss: 1.0836\n",
      "Epoch 61/98, Train Loss: 1.0959, Test Loss: 1.0839\n",
      "Epoch 62/98, Train Loss: 1.1078, Test Loss: 1.0825\n",
      "Epoch 63/98, Train Loss: 1.0976, Test Loss: 1.0814\n",
      "Epoch 64/98, Train Loss: 1.1334, Test Loss: 1.0823\n",
      "Epoch 65/98, Train Loss: 1.1102, Test Loss: 1.0813\n",
      "Epoch 66/98, Train Loss: 1.1300, Test Loss: 1.0786\n",
      "Epoch 67/98, Train Loss: 1.1349, Test Loss: 1.0776\n",
      "Epoch 68/98, Train Loss: 1.0818, Test Loss: 1.0776\n",
      "Epoch 69/98, Train Loss: 1.1131, Test Loss: 1.0791\n",
      "Epoch 70/98, Train Loss: 1.0843, Test Loss: 1.0795\n",
      "Epoch 71/98, Train Loss: 1.0890, Test Loss: 1.0795\n",
      "Epoch 72/98, Train Loss: 1.1055, Test Loss: 1.0805\n",
      "Epoch 73/98, Train Loss: 1.0952, Test Loss: 1.0785\n",
      "Epoch 74/98, Train Loss: 1.0857, Test Loss: 1.0814\n",
      "Epoch 75/98, Train Loss: 1.1171, Test Loss: 1.0829\n",
      "Epoch 76/98, Train Loss: 1.1069, Test Loss: 1.0792\n",
      "Epoch 77/98, Train Loss: 1.0971, Test Loss: 1.0815\n",
      "Epoch 78/98, Train Loss: 1.1040, Test Loss: 1.0805\n",
      "Epoch 79/98, Train Loss: 1.1137, Test Loss: 1.0803\n",
      "Epoch 80/98, Train Loss: 1.0995, Test Loss: 1.0787\n",
      "Epoch 81/98, Train Loss: 1.0838, Test Loss: 1.0798\n",
      "Epoch 82/98, Train Loss: 1.1019, Test Loss: 1.0808\n",
      "Epoch 83/98, Train Loss: 1.0834, Test Loss: 1.0828\n",
      "Epoch 84/98, Train Loss: 1.0946, Test Loss: 1.0800\n",
      "Epoch 85/98, Train Loss: 1.0904, Test Loss: 1.0790\n",
      "Epoch 86/98, Train Loss: 1.1037, Test Loss: 1.0787\n",
      "Epoch 87/98, Train Loss: 1.0872, Test Loss: 1.0800\n",
      "Epoch 88/98, Train Loss: 1.0873, Test Loss: 1.0815\n",
      "Epoch 89/98, Train Loss: 1.1019, Test Loss: 1.0815\n",
      "Epoch 90/98, Train Loss: 1.0790, Test Loss: 1.0809\n",
      "Epoch 91/98, Train Loss: 1.1042, Test Loss: 1.0816\n",
      "Epoch 92/98, Train Loss: 1.0857, Test Loss: 1.0778\n",
      "Epoch 93/98, Train Loss: 1.1016, Test Loss: 1.0797\n",
      "Epoch 94/98, Train Loss: 1.1017, Test Loss: 1.0794\n",
      "Epoch 95/98, Train Loss: 1.1043, Test Loss: 1.0825\n",
      "Epoch 96/98, Train Loss: 1.1049, Test Loss: 1.0805\n",
      "Epoch 97/98, Train Loss: 1.0737, Test Loss: 1.0808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:32:37,178] Trial 184 finished with value: 1.0819811671972275 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 99, 'layer_1_size': 172, 'layer_2_size': 223, 'layer_3_size': 132, 'layer_4_size': 210, 'layer_5_size': 233, 'layer_6_size': 195, 'dropout_rate': 0.3591370759500534, 'learning_rate': 0.00011896827908680268, 'batch_size': 64, 'epochs': 98}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/98, Train Loss: 1.1007, Test Loss: 1.0820\n",
      "Epoch 1/62, Train Loss: 1.4573, Test Loss: 1.1815\n",
      "Epoch 2/62, Train Loss: 1.4412, Test Loss: 1.1948\n",
      "Epoch 3/62, Train Loss: 1.3271, Test Loss: 1.1936\n",
      "Epoch 4/62, Train Loss: 1.2604, Test Loss: 1.1870\n",
      "Epoch 5/62, Train Loss: 1.2941, Test Loss: 1.1908\n",
      "Epoch 6/62, Train Loss: 1.3399, Test Loss: 1.1867\n",
      "Epoch 7/62, Train Loss: 1.2892, Test Loss: 1.1848\n",
      "Epoch 8/62, Train Loss: 1.1985, Test Loss: 1.1916\n",
      "Epoch 9/62, Train Loss: 1.2693, Test Loss: 1.1885\n",
      "Epoch 10/62, Train Loss: 1.2420, Test Loss: 1.1811\n",
      "Epoch 11/62, Train Loss: 1.2825, Test Loss: 1.1638\n",
      "Epoch 12/62, Train Loss: 1.2412, Test Loss: 1.1671\n",
      "Epoch 13/62, Train Loss: 1.1895, Test Loss: 1.1611\n",
      "Epoch 14/62, Train Loss: 1.2248, Test Loss: 1.1499\n",
      "Epoch 15/62, Train Loss: 1.1993, Test Loss: 1.1528\n",
      "Epoch 16/62, Train Loss: 1.2012, Test Loss: 1.1512\n",
      "Epoch 17/62, Train Loss: 1.2505, Test Loss: 1.1572\n",
      "Epoch 18/62, Train Loss: 1.2349, Test Loss: 1.1530\n",
      "Epoch 19/62, Train Loss: 1.2313, Test Loss: 1.1479\n",
      "Epoch 20/62, Train Loss: 1.1883, Test Loss: 1.1493\n",
      "Epoch 21/62, Train Loss: 1.2123, Test Loss: 1.1509\n",
      "Epoch 22/62, Train Loss: 1.2283, Test Loss: 1.1521\n",
      "Epoch 23/62, Train Loss: 1.2151, Test Loss: 1.1537\n",
      "Epoch 24/62, Train Loss: 1.1679, Test Loss: 1.1515\n",
      "Epoch 25/62, Train Loss: 1.1977, Test Loss: 1.1502\n",
      "Epoch 26/62, Train Loss: 1.1861, Test Loss: 1.1516\n",
      "Epoch 27/62, Train Loss: 1.2538, Test Loss: 1.1550\n",
      "Epoch 28/62, Train Loss: 1.2122, Test Loss: 1.1577\n",
      "Epoch 29/62, Train Loss: 1.2015, Test Loss: 1.1560\n",
      "Epoch 30/62, Train Loss: 1.1986, Test Loss: 1.1534\n",
      "Epoch 31/62, Train Loss: 1.2144, Test Loss: 1.1473\n",
      "Epoch 32/62, Train Loss: 1.1690, Test Loss: 1.1561\n",
      "Epoch 33/62, Train Loss: 1.1656, Test Loss: 1.1574\n",
      "Epoch 34/62, Train Loss: 1.2106, Test Loss: 1.1514\n",
      "Epoch 35/62, Train Loss: 1.1995, Test Loss: 1.1476\n",
      "Epoch 36/62, Train Loss: 1.1859, Test Loss: 1.1477\n",
      "Epoch 37/62, Train Loss: 1.1901, Test Loss: 1.1514\n",
      "Epoch 38/62, Train Loss: 1.1990, Test Loss: 1.1519\n",
      "Epoch 39/62, Train Loss: 1.1428, Test Loss: 1.1504\n",
      "Epoch 40/62, Train Loss: 1.2021, Test Loss: 1.1533\n",
      "Epoch 41/62, Train Loss: 1.1995, Test Loss: 1.1525\n",
      "Epoch 42/62, Train Loss: 1.1995, Test Loss: 1.1498\n",
      "Epoch 43/62, Train Loss: 1.1792, Test Loss: 1.1480\n",
      "Epoch 44/62, Train Loss: 1.1664, Test Loss: 1.1459\n",
      "Epoch 45/62, Train Loss: 1.1820, Test Loss: 1.1407\n",
      "Epoch 46/62, Train Loss: 1.1639, Test Loss: 1.1393\n",
      "Epoch 47/62, Train Loss: 1.1938, Test Loss: 1.1367\n",
      "Epoch 48/62, Train Loss: 1.2172, Test Loss: 1.1342\n",
      "Epoch 49/62, Train Loss: 1.1941, Test Loss: 1.1375\n",
      "Epoch 50/62, Train Loss: 1.1822, Test Loss: 1.1394\n",
      "Epoch 51/62, Train Loss: 1.1577, Test Loss: 1.1386\n",
      "Epoch 52/62, Train Loss: 1.1596, Test Loss: 1.1357\n",
      "Epoch 53/62, Train Loss: 1.1731, Test Loss: 1.1339\n",
      "Epoch 54/62, Train Loss: 1.1731, Test Loss: 1.1385\n",
      "Epoch 55/62, Train Loss: 1.1503, Test Loss: 1.1381\n",
      "Epoch 56/62, Train Loss: 1.1671, Test Loss: 1.1418\n",
      "Epoch 57/62, Train Loss: 1.1732, Test Loss: 1.1381\n",
      "Epoch 58/62, Train Loss: 1.1506, Test Loss: 1.1393\n",
      "Epoch 59/62, Train Loss: 1.1960, Test Loss: 1.1353\n",
      "Epoch 60/62, Train Loss: 1.1486, Test Loss: 1.1418\n",
      "Epoch 61/62, Train Loss: 1.1346, Test Loss: 1.1384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:32:41,489] Trial 185 finished with value: 1.1421547830104828 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 105, 'layer_1_size': 186, 'layer_2_size': 166, 'layer_3_size': 119, 'layer_4_size': 140, 'layer_5_size': 198, 'dropout_rate': 0.3928810109355913, 'learning_rate': 7.861153358350851e-05, 'batch_size': 64, 'epochs': 62}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/62, Train Loss: 1.1673, Test Loss: 1.1422\n",
      "Epoch 1/29, Train Loss: 1.5576, Test Loss: 0.9866\n",
      "Epoch 2/29, Train Loss: 1.3617, Test Loss: 0.9890\n",
      "Epoch 3/29, Train Loss: 1.4546, Test Loss: 0.9944\n",
      "Epoch 4/29, Train Loss: 1.3754, Test Loss: 1.0001\n",
      "Epoch 5/29, Train Loss: 1.4093, Test Loss: 1.0032\n",
      "Epoch 6/29, Train Loss: 1.3452, Test Loss: 1.0029\n",
      "Epoch 7/29, Train Loss: 1.3287, Test Loss: 1.0059\n",
      "Epoch 8/29, Train Loss: 1.3621, Test Loss: 1.0098\n",
      "Epoch 9/29, Train Loss: 1.2507, Test Loss: 1.0127\n",
      "Epoch 10/29, Train Loss: 1.2962, Test Loss: 1.0195\n",
      "Epoch 11/29, Train Loss: 1.2385, Test Loss: 1.0257\n",
      "Epoch 12/29, Train Loss: 1.2228, Test Loss: 1.0289\n",
      "Epoch 13/29, Train Loss: 1.2554, Test Loss: 1.0301\n",
      "Epoch 14/29, Train Loss: 1.2006, Test Loss: 1.0290\n",
      "Epoch 15/29, Train Loss: 1.2703, Test Loss: 1.0270\n",
      "Epoch 16/29, Train Loss: 1.2198, Test Loss: 1.0279\n",
      "Epoch 17/29, Train Loss: 1.2149, Test Loss: 1.0304\n",
      "Epoch 18/29, Train Loss: 1.2152, Test Loss: 1.0344\n",
      "Epoch 19/29, Train Loss: 1.2344, Test Loss: 1.0362\n",
      "Epoch 20/29, Train Loss: 1.2460, Test Loss: 1.0376\n",
      "Epoch 21/29, Train Loss: 1.2606, Test Loss: 1.0335\n",
      "Epoch 22/29, Train Loss: 1.2235, Test Loss: 1.0309\n",
      "Epoch 23/29, Train Loss: 1.2084, Test Loss: 1.0280\n",
      "Epoch 24/29, Train Loss: 1.1956, Test Loss: 1.0253\n",
      "Epoch 25/29, Train Loss: 1.1759, Test Loss: 1.0289\n",
      "Epoch 26/29, Train Loss: 1.1292, Test Loss: 1.0262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:32:42,915] Trial 186 finished with value: 1.0227431654930115 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 138, 'layer_1_size': 207, 'layer_2_size': 206, 'layer_3_size': 240, 'layer_4_size': 119, 'layer_5_size': 222, 'layer_6_size': 141, 'dropout_rate': 0.4072003467134299, 'learning_rate': 4.479829074934682e-05, 'batch_size': 128, 'epochs': 29}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/29, Train Loss: 1.1983, Test Loss: 1.0225\n",
      "Epoch 28/29, Train Loss: 1.1527, Test Loss: 1.0216\n",
      "Epoch 29/29, Train Loss: 1.2036, Test Loss: 1.0227\n",
      "Epoch 1/92, Train Loss: 1.1248, Test Loss: 0.7799\n",
      "Epoch 2/92, Train Loss: 1.1728, Test Loss: 0.7719\n",
      "Epoch 3/92, Train Loss: 1.1310, Test Loss: 0.7670\n",
      "Epoch 4/92, Train Loss: 1.1209, Test Loss: 0.7786\n",
      "Epoch 5/92, Train Loss: 1.1726, Test Loss: 0.7740\n",
      "Epoch 6/92, Train Loss: 1.1204, Test Loss: 0.7735\n",
      "Epoch 7/92, Train Loss: 1.0580, Test Loss: 0.7772\n",
      "Epoch 8/92, Train Loss: 1.0806, Test Loss: 0.7761\n",
      "Epoch 9/92, Train Loss: 1.0693, Test Loss: 0.7767\n",
      "Epoch 10/92, Train Loss: 1.0773, Test Loss: 0.7767\n",
      "Epoch 11/92, Train Loss: 1.1226, Test Loss: 0.7765\n",
      "Epoch 12/92, Train Loss: 1.0553, Test Loss: 0.7772\n",
      "Epoch 13/92, Train Loss: 1.0451, Test Loss: 0.7779\n",
      "Epoch 14/92, Train Loss: 1.0770, Test Loss: 0.7793\n",
      "Epoch 15/92, Train Loss: 1.0457, Test Loss: 0.7737\n",
      "Epoch 16/92, Train Loss: 1.0588, Test Loss: 0.7746\n",
      "Epoch 17/92, Train Loss: 1.0463, Test Loss: 0.7714\n",
      "Epoch 18/92, Train Loss: 1.0184, Test Loss: 0.7765\n",
      "Epoch 19/92, Train Loss: 1.0302, Test Loss: 0.7789\n",
      "Epoch 20/92, Train Loss: 1.0077, Test Loss: 0.7810\n",
      "Epoch 21/92, Train Loss: 1.0045, Test Loss: 0.7785\n",
      "Epoch 22/92, Train Loss: 1.0104, Test Loss: 0.7793\n",
      "Epoch 23/92, Train Loss: 0.9986, Test Loss: 0.7794\n",
      "Epoch 24/92, Train Loss: 0.9994, Test Loss: 0.7796\n",
      "Epoch 25/92, Train Loss: 0.9897, Test Loss: 0.7764\n",
      "Epoch 26/92, Train Loss: 1.0049, Test Loss: 0.7752\n",
      "Epoch 27/92, Train Loss: 1.0055, Test Loss: 0.7731\n",
      "Epoch 28/92, Train Loss: 0.9862, Test Loss: 0.7749\n",
      "Epoch 29/92, Train Loss: 0.9893, Test Loss: 0.7742\n",
      "Epoch 30/92, Train Loss: 0.9646, Test Loss: 0.7694\n",
      "Epoch 31/92, Train Loss: 0.9898, Test Loss: 0.7715\n",
      "Epoch 32/92, Train Loss: 0.9903, Test Loss: 0.7693\n",
      "Epoch 33/92, Train Loss: 0.9772, Test Loss: 0.7682\n",
      "Epoch 34/92, Train Loss: 1.0107, Test Loss: 0.7672\n",
      "Epoch 35/92, Train Loss: 0.9706, Test Loss: 0.7670\n",
      "Epoch 36/92, Train Loss: 0.9997, Test Loss: 0.7662\n",
      "Epoch 37/92, Train Loss: 0.9748, Test Loss: 0.7636\n",
      "Epoch 38/92, Train Loss: 1.0009, Test Loss: 0.7639\n",
      "Epoch 39/92, Train Loss: 0.9701, Test Loss: 0.7654\n",
      "Epoch 40/92, Train Loss: 0.9755, Test Loss: 0.7648\n",
      "Epoch 41/92, Train Loss: 0.9839, Test Loss: 0.7689\n",
      "Epoch 42/92, Train Loss: 0.9745, Test Loss: 0.7676\n",
      "Epoch 43/92, Train Loss: 0.9930, Test Loss: 0.7702\n",
      "Epoch 44/92, Train Loss: 0.9810, Test Loss: 0.7658\n",
      "Epoch 45/92, Train Loss: 0.9737, Test Loss: 0.7641\n",
      "Epoch 46/92, Train Loss: 0.9746, Test Loss: 0.7611\n",
      "Epoch 47/92, Train Loss: 0.9672, Test Loss: 0.7634\n",
      "Epoch 48/92, Train Loss: 0.9811, Test Loss: 0.7632\n",
      "Epoch 49/92, Train Loss: 0.9797, Test Loss: 0.7604\n",
      "Epoch 50/92, Train Loss: 0.9786, Test Loss: 0.7592\n",
      "Epoch 51/92, Train Loss: 0.9859, Test Loss: 0.7586\n",
      "Epoch 52/92, Train Loss: 0.9792, Test Loss: 0.7596\n",
      "Epoch 53/92, Train Loss: 0.9885, Test Loss: 0.7585\n",
      "Epoch 54/92, Train Loss: 0.9603, Test Loss: 0.7581\n",
      "Epoch 55/92, Train Loss: 0.9589, Test Loss: 0.7586\n",
      "Epoch 56/92, Train Loss: 0.9563, Test Loss: 0.7595\n",
      "Epoch 57/92, Train Loss: 0.9596, Test Loss: 0.7590\n",
      "Epoch 58/92, Train Loss: 0.9650, Test Loss: 0.7587\n",
      "Epoch 59/92, Train Loss: 0.9649, Test Loss: 0.7573\n",
      "Epoch 60/92, Train Loss: 0.9656, Test Loss: 0.7568\n",
      "Epoch 61/92, Train Loss: 0.9626, Test Loss: 0.7566\n",
      "Epoch 62/92, Train Loss: 0.9693, Test Loss: 0.7581\n",
      "Epoch 63/92, Train Loss: 0.9618, Test Loss: 0.7581\n",
      "Epoch 64/92, Train Loss: 0.9623, Test Loss: 0.7578\n",
      "Epoch 65/92, Train Loss: 0.9664, Test Loss: 0.7576\n",
      "Epoch 66/92, Train Loss: 0.9693, Test Loss: 0.7579\n",
      "Epoch 67/92, Train Loss: 0.9524, Test Loss: 0.7584\n",
      "Epoch 68/92, Train Loss: 0.9627, Test Loss: 0.7587\n",
      "Epoch 69/92, Train Loss: 0.9655, Test Loss: 0.7588\n",
      "Epoch 70/92, Train Loss: 0.9537, Test Loss: 0.7584\n",
      "Epoch 71/92, Train Loss: 0.9573, Test Loss: 0.7569\n",
      "Epoch 72/92, Train Loss: 0.9671, Test Loss: 0.7585\n",
      "Epoch 73/92, Train Loss: 0.9704, Test Loss: 0.7583\n",
      "Epoch 74/92, Train Loss: 0.9479, Test Loss: 0.7592\n",
      "Epoch 75/92, Train Loss: 0.9735, Test Loss: 0.7591\n",
      "Epoch 76/92, Train Loss: 0.9465, Test Loss: 0.7576\n",
      "Epoch 77/92, Train Loss: 0.9601, Test Loss: 0.7585\n",
      "Epoch 78/92, Train Loss: 0.9419, Test Loss: 0.7567\n",
      "Epoch 79/92, Train Loss: 0.9658, Test Loss: 0.7568\n",
      "Epoch 80/92, Train Loss: 0.9602, Test Loss: 0.7580\n",
      "Epoch 81/92, Train Loss: 0.9552, Test Loss: 0.7597\n",
      "Epoch 82/92, Train Loss: 0.9622, Test Loss: 0.7579\n",
      "Epoch 83/92, Train Loss: 0.9434, Test Loss: 0.7575\n",
      "Epoch 84/92, Train Loss: 0.9523, Test Loss: 0.7583\n",
      "Epoch 85/92, Train Loss: 0.9602, Test Loss: 0.7568\n",
      "Epoch 86/92, Train Loss: 0.9609, Test Loss: 0.7589\n",
      "Epoch 87/92, Train Loss: 0.9561, Test Loss: 0.7582\n",
      "Epoch 88/92, Train Loss: 0.9448, Test Loss: 0.7593\n",
      "Epoch 89/92, Train Loss: 0.9647, Test Loss: 0.7595\n",
      "Epoch 90/92, Train Loss: 0.9521, Test Loss: 0.7596\n",
      "Epoch 91/92, Train Loss: 0.9642, Test Loss: 0.7585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:32:47,833] Trial 187 finished with value: 0.7587962746620178 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 215, 'layer_1_size': 61, 'layer_2_size': 200, 'layer_3_size': 32, 'layer_4_size': 48, 'layer_5_size': 57, 'dropout_rate': 0.46055766563804196, 'learning_rate': 0.00038479465633398243, 'batch_size': 64, 'epochs': 92}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/92, Train Loss: 0.9436, Test Loss: 0.7588\n",
      "Epoch 1/93, Train Loss: 1.2906, Test Loss: 1.1083\n",
      "Epoch 2/93, Train Loss: 1.2440, Test Loss: 1.0979\n",
      "Epoch 3/93, Train Loss: 1.2035, Test Loss: 1.0950\n",
      "Epoch 4/93, Train Loss: 1.2223, Test Loss: 1.0939\n",
      "Epoch 5/93, Train Loss: 1.1911, Test Loss: 1.0951\n",
      "Epoch 6/93, Train Loss: 1.1764, Test Loss: 1.0936\n",
      "Epoch 7/93, Train Loss: 1.1461, Test Loss: 1.0972\n",
      "Epoch 8/93, Train Loss: 1.1443, Test Loss: 1.1003\n",
      "Epoch 9/93, Train Loss: 1.1680, Test Loss: 1.1029\n",
      "Epoch 10/93, Train Loss: 1.1544, Test Loss: 1.1050\n",
      "Epoch 11/93, Train Loss: 1.1044, Test Loss: 1.1079\n",
      "Epoch 12/93, Train Loss: 1.1359, Test Loss: 1.1125\n",
      "Epoch 13/93, Train Loss: 1.1075, Test Loss: 1.1088\n",
      "Epoch 14/93, Train Loss: 1.1131, Test Loss: 1.1139\n",
      "Epoch 15/93, Train Loss: 1.0922, Test Loss: 1.1114\n",
      "Epoch 16/93, Train Loss: 1.1108, Test Loss: 1.1073\n",
      "Epoch 17/93, Train Loss: 1.1432, Test Loss: 1.1071\n",
      "Epoch 18/93, Train Loss: 1.0870, Test Loss: 1.1077\n",
      "Epoch 19/93, Train Loss: 1.1054, Test Loss: 1.1093\n",
      "Epoch 20/93, Train Loss: 1.0933, Test Loss: 1.1107\n",
      "Epoch 21/93, Train Loss: 1.0880, Test Loss: 1.1080\n",
      "Epoch 22/93, Train Loss: 1.0892, Test Loss: 1.1097\n",
      "Epoch 23/93, Train Loss: 1.1005, Test Loss: 1.1113\n",
      "Epoch 24/93, Train Loss: 1.0741, Test Loss: 1.1130\n",
      "Epoch 25/93, Train Loss: 1.0872, Test Loss: 1.1139\n",
      "Epoch 26/93, Train Loss: 1.0833, Test Loss: 1.1141\n",
      "Epoch 27/93, Train Loss: 1.0742, Test Loss: 1.1150\n",
      "Epoch 28/93, Train Loss: 1.0636, Test Loss: 1.1157\n",
      "Epoch 29/93, Train Loss: 1.0730, Test Loss: 1.1148\n",
      "Epoch 30/93, Train Loss: 1.0867, Test Loss: 1.1158\n",
      "Epoch 31/93, Train Loss: 1.0942, Test Loss: 1.1149\n",
      "Epoch 32/93, Train Loss: 1.0607, Test Loss: 1.1143\n",
      "Epoch 33/93, Train Loss: 1.0517, Test Loss: 1.1152\n",
      "Epoch 34/93, Train Loss: 1.0785, Test Loss: 1.1174\n",
      "Epoch 35/93, Train Loss: 1.0357, Test Loss: 1.1137\n",
      "Epoch 36/93, Train Loss: 1.0484, Test Loss: 1.1138\n",
      "Epoch 37/93, Train Loss: 1.0366, Test Loss: 1.1204\n",
      "Epoch 38/93, Train Loss: 1.0635, Test Loss: 1.1193\n",
      "Epoch 39/93, Train Loss: 1.0654, Test Loss: 1.1194\n",
      "Epoch 40/93, Train Loss: 1.0578, Test Loss: 1.1183\n",
      "Epoch 41/93, Train Loss: 1.0483, Test Loss: 1.1179\n",
      "Epoch 42/93, Train Loss: 1.0363, Test Loss: 1.1177\n",
      "Epoch 43/93, Train Loss: 1.0521, Test Loss: 1.1181\n",
      "Epoch 44/93, Train Loss: 1.0672, Test Loss: 1.1198\n",
      "Epoch 45/93, Train Loss: 1.0602, Test Loss: 1.1201\n",
      "Epoch 46/93, Train Loss: 1.0420, Test Loss: 1.1189\n",
      "Epoch 47/93, Train Loss: 1.0527, Test Loss: 1.1208\n",
      "Epoch 48/93, Train Loss: 1.0469, Test Loss: 1.1185\n",
      "Epoch 49/93, Train Loss: 1.0359, Test Loss: 1.1175\n",
      "Epoch 50/93, Train Loss: 1.0232, Test Loss: 1.1187\n",
      "Epoch 51/93, Train Loss: 1.0418, Test Loss: 1.1195\n",
      "Epoch 52/93, Train Loss: 1.0520, Test Loss: 1.1199\n",
      "Epoch 53/93, Train Loss: 1.0523, Test Loss: 1.1184\n",
      "Epoch 54/93, Train Loss: 1.0325, Test Loss: 1.1191\n",
      "Epoch 55/93, Train Loss: 1.0428, Test Loss: 1.1196\n",
      "Epoch 56/93, Train Loss: 1.0395, Test Loss: 1.1151\n",
      "Epoch 57/93, Train Loss: 1.0689, Test Loss: 1.1166\n",
      "Epoch 58/93, Train Loss: 1.0437, Test Loss: 1.1176\n",
      "Epoch 59/93, Train Loss: 1.0383, Test Loss: 1.1193\n",
      "Epoch 60/93, Train Loss: 1.0214, Test Loss: 1.1205\n",
      "Epoch 61/93, Train Loss: 1.0401, Test Loss: 1.1204\n",
      "Epoch 62/93, Train Loss: 1.0221, Test Loss: 1.1207\n",
      "Epoch 63/93, Train Loss: 1.0391, Test Loss: 1.1198\n",
      "Epoch 64/93, Train Loss: 1.0271, Test Loss: 1.1210\n",
      "Epoch 65/93, Train Loss: 1.0341, Test Loss: 1.1211\n",
      "Epoch 66/93, Train Loss: 1.0390, Test Loss: 1.1202\n",
      "Epoch 67/93, Train Loss: 1.0423, Test Loss: 1.1200\n",
      "Epoch 68/93, Train Loss: 1.0284, Test Loss: 1.1195\n",
      "Epoch 69/93, Train Loss: 1.0438, Test Loss: 1.1204\n",
      "Epoch 70/93, Train Loss: 1.0251, Test Loss: 1.1205\n",
      "Epoch 71/93, Train Loss: 1.0318, Test Loss: 1.1209\n",
      "Epoch 72/93, Train Loss: 1.0207, Test Loss: 1.1225\n",
      "Epoch 73/93, Train Loss: 1.0320, Test Loss: 1.1219\n",
      "Epoch 74/93, Train Loss: 1.0334, Test Loss: 1.1207\n",
      "Epoch 75/93, Train Loss: 1.0261, Test Loss: 1.1210\n",
      "Epoch 76/93, Train Loss: 1.0381, Test Loss: 1.1210\n",
      "Epoch 77/93, Train Loss: 1.0160, Test Loss: 1.1198\n",
      "Epoch 78/93, Train Loss: 1.0189, Test Loss: 1.1229\n",
      "Epoch 79/93, Train Loss: 1.0424, Test Loss: 1.1230\n",
      "Epoch 80/93, Train Loss: 1.0361, Test Loss: 1.1227\n",
      "Epoch 81/93, Train Loss: 1.0169, Test Loss: 1.1218\n",
      "Epoch 82/93, Train Loss: 1.0203, Test Loss: 1.1220\n",
      "Epoch 83/93, Train Loss: 1.0275, Test Loss: 1.1237\n",
      "Epoch 84/93, Train Loss: 1.0224, Test Loss: 1.1231\n",
      "Epoch 85/93, Train Loss: 1.0231, Test Loss: 1.1248\n",
      "Epoch 86/93, Train Loss: 1.0158, Test Loss: 1.1250\n",
      "Epoch 87/93, Train Loss: 1.0237, Test Loss: 1.1237\n",
      "Epoch 88/93, Train Loss: 1.0199, Test Loss: 1.1233\n",
      "Epoch 89/93, Train Loss: 1.0217, Test Loss: 1.1237\n",
      "Epoch 90/93, Train Loss: 1.0156, Test Loss: 1.1212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:32:53,303] Trial 188 finished with value: 1.1214314997196198 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 214, 'layer_1_size': 60, 'layer_2_size': 199, 'layer_3_size': 250, 'layer_4_size': 38, 'layer_5_size': 68, 'dropout_rate': 0.45397957557587143, 'learning_rate': 0.0003268298272667278, 'batch_size': 64, 'epochs': 93}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/93, Train Loss: 1.0274, Test Loss: 1.1213\n",
      "Epoch 92/93, Train Loss: 1.0296, Test Loss: 1.1210\n",
      "Epoch 93/93, Train Loss: 1.0101, Test Loss: 1.1214\n",
      "Epoch 1/97, Train Loss: 1.1919, Test Loss: 1.1105\n",
      "Epoch 2/97, Train Loss: 1.1586, Test Loss: 1.1060\n",
      "Epoch 3/97, Train Loss: 1.1682, Test Loss: 1.1092\n",
      "Epoch 4/97, Train Loss: 1.1327, Test Loss: 1.1049\n",
      "Epoch 5/97, Train Loss: 1.0521, Test Loss: 1.1039\n",
      "Epoch 6/97, Train Loss: 1.0621, Test Loss: 1.1041\n",
      "Epoch 7/97, Train Loss: 1.0525, Test Loss: 1.1052\n",
      "Epoch 8/97, Train Loss: 1.0656, Test Loss: 1.1101\n",
      "Epoch 9/97, Train Loss: 1.0375, Test Loss: 1.1069\n",
      "Epoch 10/97, Train Loss: 1.0280, Test Loss: 1.1091\n",
      "Epoch 11/97, Train Loss: 1.0438, Test Loss: 1.1059\n",
      "Epoch 12/97, Train Loss: 1.0136, Test Loss: 1.1073\n",
      "Epoch 13/97, Train Loss: 1.0219, Test Loss: 1.1096\n",
      "Epoch 14/97, Train Loss: 1.0374, Test Loss: 1.1154\n",
      "Epoch 15/97, Train Loss: 1.0011, Test Loss: 1.1149\n",
      "Epoch 16/97, Train Loss: 0.9716, Test Loss: 1.1098\n",
      "Epoch 17/97, Train Loss: 1.0039, Test Loss: 1.1100\n",
      "Epoch 18/97, Train Loss: 0.9928, Test Loss: 1.1087\n",
      "Epoch 19/97, Train Loss: 1.0500, Test Loss: 1.1097\n",
      "Epoch 20/97, Train Loss: 0.9961, Test Loss: 1.1108\n",
      "Epoch 21/97, Train Loss: 0.9860, Test Loss: 1.1107\n",
      "Epoch 22/97, Train Loss: 0.9984, Test Loss: 1.1090\n",
      "Epoch 23/97, Train Loss: 1.0014, Test Loss: 1.1118\n",
      "Epoch 24/97, Train Loss: 0.9823, Test Loss: 1.1107\n",
      "Epoch 25/97, Train Loss: 0.9859, Test Loss: 1.1115\n",
      "Epoch 26/97, Train Loss: 0.9705, Test Loss: 1.1114\n",
      "Epoch 27/97, Train Loss: 0.9687, Test Loss: 1.1120\n",
      "Epoch 28/97, Train Loss: 0.9895, Test Loss: 1.1128\n",
      "Epoch 29/97, Train Loss: 0.9783, Test Loss: 1.1125\n",
      "Epoch 30/97, Train Loss: 0.9503, Test Loss: 1.1120\n",
      "Epoch 31/97, Train Loss: 0.9813, Test Loss: 1.1136\n",
      "Epoch 32/97, Train Loss: 0.9632, Test Loss: 1.1146\n",
      "Epoch 33/97, Train Loss: 0.9571, Test Loss: 1.1146\n",
      "Epoch 34/97, Train Loss: 0.9579, Test Loss: 1.1120\n",
      "Epoch 35/97, Train Loss: 0.9548, Test Loss: 1.1103\n",
      "Epoch 36/97, Train Loss: 0.9534, Test Loss: 1.1126\n",
      "Epoch 37/97, Train Loss: 0.9498, Test Loss: 1.1143\n",
      "Epoch 38/97, Train Loss: 0.9306, Test Loss: 1.1148\n",
      "Epoch 39/97, Train Loss: 0.9480, Test Loss: 1.1153\n",
      "Epoch 40/97, Train Loss: 0.9711, Test Loss: 1.1160\n",
      "Epoch 41/97, Train Loss: 0.9429, Test Loss: 1.1143\n",
      "Epoch 42/97, Train Loss: 0.9562, Test Loss: 1.1149\n",
      "Epoch 43/97, Train Loss: 0.9603, Test Loss: 1.1163\n",
      "Epoch 44/97, Train Loss: 0.9563, Test Loss: 1.1173\n",
      "Epoch 45/97, Train Loss: 0.9428, Test Loss: 1.1166\n",
      "Epoch 46/97, Train Loss: 0.9513, Test Loss: 1.1153\n",
      "Epoch 47/97, Train Loss: 0.9382, Test Loss: 1.1144\n",
      "Epoch 48/97, Train Loss: 0.9339, Test Loss: 1.1137\n",
      "Epoch 49/97, Train Loss: 0.9495, Test Loss: 1.1137\n",
      "Epoch 50/97, Train Loss: 0.9588, Test Loss: 1.1140\n",
      "Epoch 51/97, Train Loss: 0.9429, Test Loss: 1.1143\n",
      "Epoch 52/97, Train Loss: 0.9301, Test Loss: 1.1143\n",
      "Epoch 53/97, Train Loss: 0.9453, Test Loss: 1.1131\n",
      "Epoch 54/97, Train Loss: 0.9326, Test Loss: 1.1127\n",
      "Epoch 55/97, Train Loss: 0.9401, Test Loss: 1.1129\n",
      "Epoch 56/97, Train Loss: 0.9625, Test Loss: 1.1139\n",
      "Epoch 57/97, Train Loss: 0.9262, Test Loss: 1.1133\n",
      "Epoch 58/97, Train Loss: 0.9470, Test Loss: 1.1142\n",
      "Epoch 59/97, Train Loss: 0.9180, Test Loss: 1.1126\n",
      "Epoch 60/97, Train Loss: 0.9345, Test Loss: 1.1120\n",
      "Epoch 61/97, Train Loss: 0.9337, Test Loss: 1.1117\n",
      "Epoch 62/97, Train Loss: 0.9381, Test Loss: 1.1116\n",
      "Epoch 63/97, Train Loss: 0.9566, Test Loss: 1.1114\n",
      "Epoch 64/97, Train Loss: 0.9407, Test Loss: 1.1109\n",
      "Epoch 65/97, Train Loss: 0.9414, Test Loss: 1.1117\n",
      "Epoch 66/97, Train Loss: 0.9279, Test Loss: 1.1120\n",
      "Epoch 67/97, Train Loss: 0.9346, Test Loss: 1.1120\n",
      "Epoch 68/97, Train Loss: 0.9266, Test Loss: 1.1119\n",
      "Epoch 69/97, Train Loss: 0.9298, Test Loss: 1.1124\n",
      "Epoch 70/97, Train Loss: 0.9358, Test Loss: 1.1128\n",
      "Epoch 71/97, Train Loss: 0.9331, Test Loss: 1.1114\n",
      "Epoch 72/97, Train Loss: 0.9200, Test Loss: 1.1103\n",
      "Epoch 73/97, Train Loss: 0.9366, Test Loss: 1.1103\n",
      "Epoch 74/97, Train Loss: 0.9313, Test Loss: 1.1113\n",
      "Epoch 75/97, Train Loss: 0.9307, Test Loss: 1.1105\n",
      "Epoch 76/97, Train Loss: 0.9295, Test Loss: 1.1122\n",
      "Epoch 77/97, Train Loss: 0.9136, Test Loss: 1.1124\n",
      "Epoch 78/97, Train Loss: 0.9291, Test Loss: 1.1110\n",
      "Epoch 79/97, Train Loss: 0.9291, Test Loss: 1.1106\n",
      "Epoch 80/97, Train Loss: 0.9318, Test Loss: 1.1109\n",
      "Epoch 81/97, Train Loss: 0.9247, Test Loss: 1.1113\n",
      "Epoch 82/97, Train Loss: 0.9273, Test Loss: 1.1107\n",
      "Epoch 83/97, Train Loss: 0.9444, Test Loss: 1.1100\n",
      "Epoch 84/97, Train Loss: 0.9278, Test Loss: 1.1089\n",
      "Epoch 85/97, Train Loss: 0.9271, Test Loss: 1.1084\n",
      "Epoch 86/97, Train Loss: 0.9313, Test Loss: 1.1095\n",
      "Epoch 87/97, Train Loss: 0.9432, Test Loss: 1.1097\n",
      "Epoch 88/97, Train Loss: 0.9234, Test Loss: 1.1098\n",
      "Epoch 89/97, Train Loss: 0.9352, Test Loss: 1.1098\n",
      "Epoch 90/97, Train Loss: 0.9362, Test Loss: 1.1108\n",
      "Epoch 91/97, Train Loss: 0.9248, Test Loss: 1.1102\n",
      "Epoch 92/97, Train Loss: 0.9227, Test Loss: 1.1104\n",
      "Epoch 93/97, Train Loss: 0.9207, Test Loss: 1.1108\n",
      "Epoch 94/97, Train Loss: 0.9315, Test Loss: 1.1105\n",
      "Epoch 95/97, Train Loss: 0.9277, Test Loss: 1.1100\n",
      "Epoch 96/97, Train Loss: 0.9330, Test Loss: 1.1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:32:57,794] Trial 189 finished with value: 1.1105388402938843 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 112, 'layer_1_size': 58, 'layer_2_size': 194, 'layer_3_size': 48, 'layer_4_size': 62, 'layer_5_size': 73, 'dropout_rate': 0.44300713461584834, 'learning_rate': 0.00039853646595093366, 'batch_size': 64, 'epochs': 97}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/97, Train Loss: 0.9242, Test Loss: 1.1105\n",
      "Epoch 1/96, Train Loss: 1.1368, Test Loss: 1.0530\n",
      "Epoch 2/96, Train Loss: 1.0676, Test Loss: 1.0497\n",
      "Epoch 3/96, Train Loss: 1.0881, Test Loss: 1.0474\n",
      "Epoch 4/96, Train Loss: 1.0471, Test Loss: 1.0468\n",
      "Epoch 5/96, Train Loss: 1.0565, Test Loss: 1.0476\n",
      "Epoch 6/96, Train Loss: 1.0493, Test Loss: 1.0456\n",
      "Epoch 7/96, Train Loss: 1.0570, Test Loss: 1.0542\n",
      "Epoch 8/96, Train Loss: 1.0353, Test Loss: 1.0621\n",
      "Epoch 9/96, Train Loss: 1.0566, Test Loss: 1.0699\n",
      "Epoch 10/96, Train Loss: 1.0720, Test Loss: 1.0801\n",
      "Epoch 11/96, Train Loss: 1.0274, Test Loss: 1.0789\n",
      "Epoch 12/96, Train Loss: 1.0013, Test Loss: 1.0813\n",
      "Epoch 13/96, Train Loss: 1.0236, Test Loss: 1.0878\n",
      "Epoch 14/96, Train Loss: 1.0143, Test Loss: 1.0939\n",
      "Epoch 15/96, Train Loss: 1.0059, Test Loss: 1.0996\n",
      "Epoch 16/96, Train Loss: 1.0297, Test Loss: 1.1058\n",
      "Epoch 17/96, Train Loss: 0.9988, Test Loss: 1.1001\n",
      "Epoch 18/96, Train Loss: 1.0313, Test Loss: 1.1003\n",
      "Epoch 19/96, Train Loss: 1.0402, Test Loss: 1.0948\n",
      "Epoch 20/96, Train Loss: 0.9916, Test Loss: 1.0892\n",
      "Epoch 21/96, Train Loss: 1.0154, Test Loss: 1.0830\n",
      "Epoch 22/96, Train Loss: 1.0064, Test Loss: 1.0752\n",
      "Epoch 23/96, Train Loss: 0.9972, Test Loss: 1.0753\n",
      "Epoch 24/96, Train Loss: 1.0017, Test Loss: 1.0719\n",
      "Epoch 25/96, Train Loss: 1.0133, Test Loss: 1.0773\n",
      "Epoch 26/96, Train Loss: 0.9813, Test Loss: 1.0790\n",
      "Epoch 27/96, Train Loss: 1.0200, Test Loss: 1.0861\n",
      "Epoch 28/96, Train Loss: 1.0025, Test Loss: 1.0824\n",
      "Epoch 29/96, Train Loss: 0.9627, Test Loss: 1.0827\n",
      "Epoch 30/96, Train Loss: 0.9906, Test Loss: 1.0878\n",
      "Epoch 31/96, Train Loss: 0.9806, Test Loss: 1.0826\n",
      "Epoch 32/96, Train Loss: 0.9719, Test Loss: 1.0789\n",
      "Epoch 33/96, Train Loss: 0.9725, Test Loss: 1.0839\n",
      "Epoch 34/96, Train Loss: 1.0016, Test Loss: 1.0830\n",
      "Epoch 35/96, Train Loss: 0.9864, Test Loss: 1.0882\n",
      "Epoch 36/96, Train Loss: 0.9689, Test Loss: 1.1012\n",
      "Epoch 37/96, Train Loss: 0.9747, Test Loss: 1.1014\n",
      "Epoch 38/96, Train Loss: 0.9916, Test Loss: 1.0975\n",
      "Epoch 39/96, Train Loss: 0.9569, Test Loss: 1.0956\n",
      "Epoch 40/96, Train Loss: 0.9576, Test Loss: 1.0920\n",
      "Epoch 41/96, Train Loss: 0.9715, Test Loss: 1.0946\n",
      "Epoch 42/96, Train Loss: 0.9703, Test Loss: 1.0971\n",
      "Epoch 43/96, Train Loss: 0.9819, Test Loss: 1.0987\n",
      "Epoch 44/96, Train Loss: 0.9548, Test Loss: 1.0949\n",
      "Epoch 45/96, Train Loss: 0.9664, Test Loss: 1.1095\n",
      "Epoch 46/96, Train Loss: 0.9503, Test Loss: 1.1058\n",
      "Epoch 47/96, Train Loss: 0.9337, Test Loss: 1.0989\n",
      "Epoch 48/96, Train Loss: 0.9452, Test Loss: 1.0939\n",
      "Epoch 49/96, Train Loss: 0.9292, Test Loss: 1.0982\n",
      "Epoch 50/96, Train Loss: 0.9565, Test Loss: 1.0917\n",
      "Epoch 51/96, Train Loss: 0.9699, Test Loss: 1.0921\n",
      "Epoch 52/96, Train Loss: 0.9614, Test Loss: 1.0976\n",
      "Epoch 53/96, Train Loss: 0.9243, Test Loss: 1.0907\n",
      "Epoch 54/96, Train Loss: 0.9406, Test Loss: 1.0918\n",
      "Epoch 55/96, Train Loss: 0.9391, Test Loss: 1.0937\n",
      "Epoch 56/96, Train Loss: 0.9086, Test Loss: 1.1110\n",
      "Epoch 57/96, Train Loss: 0.9295, Test Loss: 1.1096\n",
      "Epoch 58/96, Train Loss: 0.9293, Test Loss: 1.1045\n",
      "Epoch 59/96, Train Loss: 0.9398, Test Loss: 1.1093\n",
      "Epoch 60/96, Train Loss: 0.9190, Test Loss: 1.1079\n",
      "Epoch 61/96, Train Loss: 0.9108, Test Loss: 1.1115\n",
      "Epoch 62/96, Train Loss: 0.9104, Test Loss: 1.1118\n",
      "Epoch 63/96, Train Loss: 0.9171, Test Loss: 1.1289\n",
      "Epoch 64/96, Train Loss: 0.8994, Test Loss: 1.1188\n",
      "Epoch 65/96, Train Loss: 0.9117, Test Loss: 1.1028\n",
      "Epoch 66/96, Train Loss: 0.9049, Test Loss: 1.1036\n",
      "Epoch 67/96, Train Loss: 0.9080, Test Loss: 1.1018\n",
      "Epoch 68/96, Train Loss: 0.9191, Test Loss: 1.0914\n",
      "Epoch 69/96, Train Loss: 0.9106, Test Loss: 1.0921\n",
      "Epoch 70/96, Train Loss: 0.9199, Test Loss: 1.0951\n",
      "Epoch 71/96, Train Loss: 0.9056, Test Loss: 1.0887\n",
      "Epoch 72/96, Train Loss: 0.9251, Test Loss: 1.1003\n",
      "Epoch 73/96, Train Loss: 0.8941, Test Loss: 1.1167\n",
      "Epoch 74/96, Train Loss: 0.9005, Test Loss: 1.1144\n",
      "Epoch 75/96, Train Loss: 0.8913, Test Loss: 1.1064\n",
      "Epoch 76/96, Train Loss: 0.8611, Test Loss: 1.1083\n",
      "Epoch 77/96, Train Loss: 0.9238, Test Loss: 1.1076\n",
      "Epoch 78/96, Train Loss: 0.8837, Test Loss: 1.1068\n",
      "Epoch 79/96, Train Loss: 0.8813, Test Loss: 1.0926\n",
      "Epoch 80/96, Train Loss: 0.8809, Test Loss: 1.0941\n",
      "Epoch 81/96, Train Loss: 0.8758, Test Loss: 1.1019\n",
      "Epoch 82/96, Train Loss: 0.8899, Test Loss: 1.0961\n",
      "Epoch 83/96, Train Loss: 0.8432, Test Loss: 1.0942\n",
      "Epoch 84/96, Train Loss: 0.8671, Test Loss: 1.1112\n",
      "Epoch 85/96, Train Loss: 0.8487, Test Loss: 1.1376\n",
      "Epoch 86/96, Train Loss: 0.8742, Test Loss: 1.1293\n",
      "Epoch 87/96, Train Loss: 0.8517, Test Loss: 1.1144\n",
      "Epoch 88/96, Train Loss: 0.8629, Test Loss: 1.1103\n",
      "Epoch 89/96, Train Loss: 0.8545, Test Loss: 1.1101\n",
      "Epoch 90/96, Train Loss: 0.8853, Test Loss: 1.1139\n",
      "Epoch 91/96, Train Loss: 0.8736, Test Loss: 1.1153\n",
      "Epoch 92/96, Train Loss: 0.8479, Test Loss: 1.1251\n",
      "Epoch 93/96, Train Loss: 0.8671, Test Loss: 1.1123\n",
      "Epoch 94/96, Train Loss: 0.8603, Test Loss: 1.0851\n",
      "Epoch 95/96, Train Loss: 0.8570, Test Loss: 1.0751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:33:05,371] Trial 190 finished with value: 1.0824491530656815 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 217, 'layer_1_size': 77, 'layer_2_size': 188, 'layer_3_size': 233, 'layer_4_size': 47, 'layer_5_size': 153, 'dropout_rate': 0.23756623357136064, 'learning_rate': 0.0002784565018497067, 'batch_size': 64, 'epochs': 96}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/96, Train Loss: 0.8298, Test Loss: 1.0824\n",
      "Epoch 1/90, Train Loss: 1.1655, Test Loss: 1.0654\n",
      "Epoch 2/90, Train Loss: 1.0958, Test Loss: 1.0689\n",
      "Epoch 3/90, Train Loss: 1.0779, Test Loss: 1.0772\n",
      "Epoch 4/90, Train Loss: 1.0473, Test Loss: 1.0733\n",
      "Epoch 5/90, Train Loss: 1.0383, Test Loss: 1.0689\n",
      "Epoch 6/90, Train Loss: 1.0314, Test Loss: 1.0667\n",
      "Epoch 7/90, Train Loss: 1.0464, Test Loss: 1.0597\n",
      "Epoch 8/90, Train Loss: 1.0549, Test Loss: 1.0587\n",
      "Epoch 9/90, Train Loss: 1.0120, Test Loss: 1.0586\n",
      "Epoch 10/90, Train Loss: 0.9989, Test Loss: 1.0627\n",
      "Epoch 11/90, Train Loss: 0.9759, Test Loss: 1.0699\n",
      "Epoch 12/90, Train Loss: 1.0216, Test Loss: 1.0703\n",
      "Epoch 13/90, Train Loss: 0.9865, Test Loss: 1.0701\n",
      "Epoch 14/90, Train Loss: 1.0027, Test Loss: 1.0693\n",
      "Epoch 15/90, Train Loss: 1.0046, Test Loss: 1.0684\n",
      "Epoch 16/90, Train Loss: 0.9639, Test Loss: 1.0653\n",
      "Epoch 17/90, Train Loss: 0.9650, Test Loss: 1.0660\n",
      "Epoch 18/90, Train Loss: 0.9696, Test Loss: 1.0654\n",
      "Epoch 19/90, Train Loss: 0.9891, Test Loss: 1.0671\n",
      "Epoch 20/90, Train Loss: 0.9689, Test Loss: 1.0744\n",
      "Epoch 21/90, Train Loss: 0.9587, Test Loss: 1.0713\n",
      "Epoch 22/90, Train Loss: 0.9516, Test Loss: 1.0661\n",
      "Epoch 23/90, Train Loss: 0.9764, Test Loss: 1.0673\n",
      "Epoch 24/90, Train Loss: 0.9732, Test Loss: 1.0727\n",
      "Epoch 25/90, Train Loss: 0.9440, Test Loss: 1.0712\n",
      "Epoch 26/90, Train Loss: 0.9631, Test Loss: 1.0706\n",
      "Epoch 27/90, Train Loss: 0.9549, Test Loss: 1.0661\n",
      "Epoch 28/90, Train Loss: 0.9474, Test Loss: 1.0653\n",
      "Epoch 29/90, Train Loss: 0.9491, Test Loss: 1.0673\n",
      "Epoch 30/90, Train Loss: 0.9546, Test Loss: 1.0672\n",
      "Epoch 31/90, Train Loss: 0.9353, Test Loss: 1.0659\n",
      "Epoch 32/90, Train Loss: 0.9376, Test Loss: 1.0633\n",
      "Epoch 33/90, Train Loss: 0.9474, Test Loss: 1.0647\n",
      "Epoch 34/90, Train Loss: 0.9400, Test Loss: 1.0667\n",
      "Epoch 35/90, Train Loss: 0.9577, Test Loss: 1.0609\n",
      "Epoch 36/90, Train Loss: 0.9404, Test Loss: 1.0627\n",
      "Epoch 37/90, Train Loss: 0.9556, Test Loss: 1.0627\n",
      "Epoch 38/90, Train Loss: 0.9431, Test Loss: 1.0679\n",
      "Epoch 39/90, Train Loss: 0.9451, Test Loss: 1.0679\n",
      "Epoch 40/90, Train Loss: 0.9435, Test Loss: 1.0695\n",
      "Epoch 41/90, Train Loss: 0.9507, Test Loss: 1.0697\n",
      "Epoch 42/90, Train Loss: 0.9372, Test Loss: 1.0673\n",
      "Epoch 43/90, Train Loss: 0.9432, Test Loss: 1.0657\n",
      "Epoch 44/90, Train Loss: 0.9404, Test Loss: 1.0665\n",
      "Epoch 45/90, Train Loss: 0.9435, Test Loss: 1.0713\n",
      "Epoch 46/90, Train Loss: 0.9497, Test Loss: 1.0721\n",
      "Epoch 47/90, Train Loss: 0.9360, Test Loss: 1.0706\n",
      "Epoch 48/90, Train Loss: 0.9375, Test Loss: 1.0705\n",
      "Epoch 49/90, Train Loss: 0.9447, Test Loss: 1.0685\n",
      "Epoch 50/90, Train Loss: 0.9446, Test Loss: 1.0682\n",
      "Epoch 51/90, Train Loss: 0.9411, Test Loss: 1.0693\n",
      "Epoch 52/90, Train Loss: 0.9380, Test Loss: 1.0701\n",
      "Epoch 53/90, Train Loss: 0.9367, Test Loss: 1.0660\n",
      "Epoch 54/90, Train Loss: 0.9446, Test Loss: 1.0673\n",
      "Epoch 55/90, Train Loss: 0.9365, Test Loss: 1.0661\n",
      "Epoch 56/90, Train Loss: 0.9415, Test Loss: 1.0672\n",
      "Epoch 57/90, Train Loss: 0.9386, Test Loss: 1.0695\n",
      "Epoch 58/90, Train Loss: 0.9344, Test Loss: 1.0684\n",
      "Epoch 59/90, Train Loss: 0.9483, Test Loss: 1.0703\n",
      "Epoch 60/90, Train Loss: 0.9376, Test Loss: 1.0732\n",
      "Epoch 61/90, Train Loss: 0.9382, Test Loss: 1.0735\n",
      "Epoch 62/90, Train Loss: 0.9434, Test Loss: 1.0683\n",
      "Epoch 63/90, Train Loss: 0.9455, Test Loss: 1.0679\n",
      "Epoch 64/90, Train Loss: 0.9358, Test Loss: 1.0677\n",
      "Epoch 65/90, Train Loss: 0.9375, Test Loss: 1.0690\n",
      "Epoch 66/90, Train Loss: 0.9469, Test Loss: 1.0689\n",
      "Epoch 67/90, Train Loss: 0.9345, Test Loss: 1.0667\n",
      "Epoch 68/90, Train Loss: 0.9463, Test Loss: 1.0674\n",
      "Epoch 69/90, Train Loss: 0.9326, Test Loss: 1.0675\n",
      "Epoch 70/90, Train Loss: 0.9334, Test Loss: 1.0679\n",
      "Epoch 71/90, Train Loss: 0.9407, Test Loss: 1.0687\n",
      "Epoch 72/90, Train Loss: 0.9450, Test Loss: 1.0666\n",
      "Epoch 73/90, Train Loss: 0.9394, Test Loss: 1.0656\n",
      "Epoch 74/90, Train Loss: 0.9400, Test Loss: 1.0642\n",
      "Epoch 75/90, Train Loss: 0.9372, Test Loss: 1.0630\n",
      "Epoch 76/90, Train Loss: 0.9355, Test Loss: 1.0622\n",
      "Epoch 77/90, Train Loss: 0.9440, Test Loss: 1.0610\n",
      "Epoch 78/90, Train Loss: 0.9461, Test Loss: 1.0614\n",
      "Epoch 79/90, Train Loss: 0.9364, Test Loss: 1.0624\n",
      "Epoch 80/90, Train Loss: 0.9502, Test Loss: 1.0663\n",
      "Epoch 81/90, Train Loss: 0.9369, Test Loss: 1.0647\n",
      "Epoch 82/90, Train Loss: 0.9424, Test Loss: 1.0657\n",
      "Epoch 83/90, Train Loss: 0.9391, Test Loss: 1.0629\n",
      "Epoch 84/90, Train Loss: 0.9432, Test Loss: 1.0618\n",
      "Epoch 85/90, Train Loss: 0.9467, Test Loss: 1.0612\n",
      "Epoch 86/90, Train Loss: 0.9327, Test Loss: 1.0617\n",
      "Epoch 87/90, Train Loss: 0.9475, Test Loss: 1.0630\n",
      "Epoch 88/90, Train Loss: 0.9421, Test Loss: 1.0648\n",
      "Epoch 89/90, Train Loss: 0.9343, Test Loss: 1.0613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:33:12,587] Trial 191 finished with value: 1.0611881613731384 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 233, 'layer_1_size': 178, 'layer_2_size': 203, 'layer_3_size': 33, 'layer_4_size': 106, 'layer_5_size': 40, 'layer_6_size': 73, 'dropout_rate': 0.4593071713275504, 'learning_rate': 0.0007425319100193173, 'batch_size': 64, 'epochs': 90}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/90, Train Loss: 0.9385, Test Loss: 1.0612\n",
      "Epoch 1/92, Train Loss: 1.3127, Test Loss: 1.6364\n",
      "Epoch 2/92, Train Loss: 1.2029, Test Loss: 1.6691\n",
      "Epoch 3/92, Train Loss: 1.2142, Test Loss: 1.6552\n",
      "Epoch 4/92, Train Loss: 1.2787, Test Loss: 1.6363\n",
      "Epoch 5/92, Train Loss: 1.2335, Test Loss: 1.6226\n",
      "Epoch 6/92, Train Loss: 1.1694, Test Loss: 1.6071\n",
      "Epoch 7/92, Train Loss: 1.1603, Test Loss: 1.6085\n",
      "Epoch 8/92, Train Loss: 1.1789, Test Loss: 1.6146\n",
      "Epoch 9/92, Train Loss: 1.2073, Test Loss: 1.6199\n",
      "Epoch 10/92, Train Loss: 1.1788, Test Loss: 1.6126\n",
      "Epoch 11/92, Train Loss: 1.1717, Test Loss: 1.5878\n",
      "Epoch 12/92, Train Loss: 1.1502, Test Loss: 1.5903\n",
      "Epoch 13/92, Train Loss: 1.1221, Test Loss: 1.5832\n",
      "Epoch 14/92, Train Loss: 1.1688, Test Loss: 1.5870\n",
      "Epoch 15/92, Train Loss: 1.1441, Test Loss: 1.5913\n",
      "Epoch 16/92, Train Loss: 1.1145, Test Loss: 1.5842\n",
      "Epoch 17/92, Train Loss: 1.1311, Test Loss: 1.5835\n",
      "Epoch 18/92, Train Loss: 1.1352, Test Loss: 1.5805\n",
      "Epoch 19/92, Train Loss: 1.0940, Test Loss: 1.5788\n",
      "Epoch 20/92, Train Loss: 1.1554, Test Loss: 1.5785\n",
      "Epoch 21/92, Train Loss: 1.1020, Test Loss: 1.5734\n",
      "Epoch 22/92, Train Loss: 1.1123, Test Loss: 1.5756\n",
      "Epoch 23/92, Train Loss: 1.1017, Test Loss: 1.5733\n",
      "Epoch 24/92, Train Loss: 1.1082, Test Loss: 1.5771\n",
      "Epoch 25/92, Train Loss: 1.1039, Test Loss: 1.5753\n",
      "Epoch 26/92, Train Loss: 1.0974, Test Loss: 1.5715\n",
      "Epoch 27/92, Train Loss: 1.0564, Test Loss: 1.5757\n",
      "Epoch 28/92, Train Loss: 1.0882, Test Loss: 1.5703\n",
      "Epoch 29/92, Train Loss: 1.0688, Test Loss: 1.5714\n",
      "Epoch 30/92, Train Loss: 1.1168, Test Loss: 1.5717\n",
      "Epoch 31/92, Train Loss: 1.0998, Test Loss: 1.5673\n",
      "Epoch 32/92, Train Loss: 1.1158, Test Loss: 1.5679\n",
      "Epoch 33/92, Train Loss: 1.0803, Test Loss: 1.5653\n",
      "Epoch 34/92, Train Loss: 1.0807, Test Loss: 1.5660\n",
      "Epoch 35/92, Train Loss: 1.0962, Test Loss: 1.5685\n",
      "Epoch 36/92, Train Loss: 1.0980, Test Loss: 1.5703\n",
      "Epoch 37/92, Train Loss: 1.0984, Test Loss: 1.5677\n",
      "Epoch 38/92, Train Loss: 1.1214, Test Loss: 1.5679\n",
      "Epoch 39/92, Train Loss: 1.0778, Test Loss: 1.5662\n",
      "Epoch 40/92, Train Loss: 1.1125, Test Loss: 1.5623\n",
      "Epoch 41/92, Train Loss: 1.1112, Test Loss: 1.5587\n",
      "Epoch 42/92, Train Loss: 1.0866, Test Loss: 1.5592\n",
      "Epoch 43/92, Train Loss: 1.0829, Test Loss: 1.5575\n",
      "Epoch 44/92, Train Loss: 1.0795, Test Loss: 1.5623\n",
      "Epoch 45/92, Train Loss: 1.0757, Test Loss: 1.5608\n",
      "Epoch 46/92, Train Loss: 1.0893, Test Loss: 1.5627\n",
      "Epoch 47/92, Train Loss: 1.0936, Test Loss: 1.5569\n",
      "Epoch 48/92, Train Loss: 1.0827, Test Loss: 1.5638\n",
      "Epoch 49/92, Train Loss: 1.0779, Test Loss: 1.5640\n",
      "Epoch 50/92, Train Loss: 1.0504, Test Loss: 1.5640\n",
      "Epoch 51/92, Train Loss: 1.0963, Test Loss: 1.5616\n",
      "Epoch 52/92, Train Loss: 1.0651, Test Loss: 1.5605\n",
      "Epoch 53/92, Train Loss: 1.0638, Test Loss: 1.5631\n",
      "Epoch 54/92, Train Loss: 1.0709, Test Loss: 1.5645\n",
      "Epoch 55/92, Train Loss: 1.0774, Test Loss: 1.5682\n",
      "Epoch 56/92, Train Loss: 1.0772, Test Loss: 1.5701\n",
      "Epoch 57/92, Train Loss: 1.0689, Test Loss: 1.5704\n",
      "Epoch 58/92, Train Loss: 1.0597, Test Loss: 1.5672\n",
      "Epoch 59/92, Train Loss: 1.0362, Test Loss: 1.5658\n",
      "Epoch 60/92, Train Loss: 1.0359, Test Loss: 1.5639\n",
      "Epoch 61/92, Train Loss: 1.0529, Test Loss: 1.5646\n",
      "Epoch 62/92, Train Loss: 1.0727, Test Loss: 1.5625\n",
      "Epoch 63/92, Train Loss: 1.0602, Test Loss: 1.5639\n",
      "Epoch 64/92, Train Loss: 1.0678, Test Loss: 1.5661\n",
      "Epoch 65/92, Train Loss: 1.0731, Test Loss: 1.5648\n",
      "Epoch 66/92, Train Loss: 1.0678, Test Loss: 1.5664\n",
      "Epoch 67/92, Train Loss: 1.0483, Test Loss: 1.5618\n",
      "Epoch 68/92, Train Loss: 1.0532, Test Loss: 1.5624\n",
      "Epoch 69/92, Train Loss: 1.0506, Test Loss: 1.5612\n",
      "Epoch 70/92, Train Loss: 1.0677, Test Loss: 1.5597\n",
      "Epoch 71/92, Train Loss: 1.0847, Test Loss: 1.5625\n",
      "Epoch 72/92, Train Loss: 1.0417, Test Loss: 1.5596\n",
      "Epoch 73/92, Train Loss: 1.0733, Test Loss: 1.5557\n",
      "Epoch 74/92, Train Loss: 1.0401, Test Loss: 1.5600\n",
      "Epoch 75/92, Train Loss: 1.0586, Test Loss: 1.5610\n",
      "Epoch 76/92, Train Loss: 1.0511, Test Loss: 1.5578\n",
      "Epoch 77/92, Train Loss: 1.0398, Test Loss: 1.5569\n",
      "Epoch 78/92, Train Loss: 1.0480, Test Loss: 1.5608\n",
      "Epoch 79/92, Train Loss: 1.0386, Test Loss: 1.5630\n",
      "Epoch 80/92, Train Loss: 1.0421, Test Loss: 1.5583\n",
      "Epoch 81/92, Train Loss: 1.0284, Test Loss: 1.5611\n",
      "Epoch 82/92, Train Loss: 1.0605, Test Loss: 1.5601\n",
      "Epoch 83/92, Train Loss: 1.0482, Test Loss: 1.5592\n",
      "Epoch 84/92, Train Loss: 1.0387, Test Loss: 1.5589\n",
      "Epoch 85/92, Train Loss: 1.0470, Test Loss: 1.5613\n",
      "Epoch 86/92, Train Loss: 1.0420, Test Loss: 1.5636\n",
      "Epoch 87/92, Train Loss: 1.0354, Test Loss: 1.5599\n",
      "Epoch 88/92, Train Loss: 1.0562, Test Loss: 1.5614\n",
      "Epoch 89/92, Train Loss: 1.0441, Test Loss: 1.5635\n",
      "Epoch 90/92, Train Loss: 1.0484, Test Loss: 1.5598\n",
      "Epoch 91/92, Train Loss: 1.0451, Test Loss: 1.5570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:33:18,617] Trial 192 finished with value: 1.5596877932548523 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 203, 'layer_1_size': 149, 'layer_2_size': 212, 'layer_3_size': 39, 'layer_4_size': 51, 'layer_5_size': 229, 'dropout_rate': 0.417675867260412, 'learning_rate': 0.00014525515256389804, 'batch_size': 64, 'epochs': 92}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/92, Train Loss: 1.0257, Test Loss: 1.5597\n",
      "Epoch 1/100, Train Loss: 1.3798, Test Loss: 1.0631\n",
      "Epoch 2/100, Train Loss: 1.2829, Test Loss: 1.0462\n",
      "Epoch 3/100, Train Loss: 1.2874, Test Loss: 1.0476\n",
      "Epoch 4/100, Train Loss: 1.2996, Test Loss: 1.0361\n",
      "Epoch 5/100, Train Loss: 1.2164, Test Loss: 1.0280\n",
      "Epoch 6/100, Train Loss: 1.2041, Test Loss: 1.0074\n",
      "Epoch 7/100, Train Loss: 1.2289, Test Loss: 1.0056\n",
      "Epoch 8/100, Train Loss: 1.2403, Test Loss: 1.0018\n",
      "Epoch 9/100, Train Loss: 1.2191, Test Loss: 0.9970\n",
      "Epoch 10/100, Train Loss: 1.2547, Test Loss: 0.9963\n",
      "Epoch 11/100, Train Loss: 1.1916, Test Loss: 0.9984\n",
      "Epoch 12/100, Train Loss: 1.2376, Test Loss: 1.0005\n",
      "Epoch 13/100, Train Loss: 1.1858, Test Loss: 1.0027\n",
      "Epoch 14/100, Train Loss: 1.2573, Test Loss: 0.9994\n",
      "Epoch 15/100, Train Loss: 1.2080, Test Loss: 1.0053\n",
      "Epoch 16/100, Train Loss: 1.2106, Test Loss: 1.0054\n",
      "Epoch 17/100, Train Loss: 1.1834, Test Loss: 1.0026\n",
      "Epoch 18/100, Train Loss: 1.1349, Test Loss: 1.0037\n",
      "Epoch 19/100, Train Loss: 1.2133, Test Loss: 0.9986\n",
      "Epoch 20/100, Train Loss: 1.1923, Test Loss: 1.0005\n",
      "Epoch 21/100, Train Loss: 1.1680, Test Loss: 0.9993\n",
      "Epoch 22/100, Train Loss: 1.2114, Test Loss: 0.9956\n",
      "Epoch 23/100, Train Loss: 1.1612, Test Loss: 0.9997\n",
      "Epoch 24/100, Train Loss: 1.1371, Test Loss: 0.9968\n",
      "Epoch 25/100, Train Loss: 1.1397, Test Loss: 0.9990\n",
      "Epoch 26/100, Train Loss: 1.1707, Test Loss: 0.9999\n",
      "Epoch 27/100, Train Loss: 1.1441, Test Loss: 1.0018\n",
      "Epoch 28/100, Train Loss: 1.1695, Test Loss: 1.0035\n",
      "Epoch 29/100, Train Loss: 1.1577, Test Loss: 1.0013\n",
      "Epoch 30/100, Train Loss: 1.1439, Test Loss: 0.9975\n",
      "Epoch 31/100, Train Loss: 1.1279, Test Loss: 0.9966\n",
      "Epoch 32/100, Train Loss: 1.1270, Test Loss: 0.9973\n",
      "Epoch 33/100, Train Loss: 1.1506, Test Loss: 0.9960\n",
      "Epoch 34/100, Train Loss: 1.1497, Test Loss: 0.9954\n",
      "Epoch 35/100, Train Loss: 1.1052, Test Loss: 0.9974\n",
      "Epoch 36/100, Train Loss: 1.1468, Test Loss: 0.9966\n",
      "Epoch 37/100, Train Loss: 1.0999, Test Loss: 0.9977\n",
      "Epoch 38/100, Train Loss: 1.1835, Test Loss: 0.9972\n",
      "Epoch 39/100, Train Loss: 1.1346, Test Loss: 0.9974\n",
      "Epoch 40/100, Train Loss: 1.1125, Test Loss: 0.9960\n",
      "Epoch 41/100, Train Loss: 1.1607, Test Loss: 0.9935\n",
      "Epoch 42/100, Train Loss: 1.1365, Test Loss: 0.9929\n",
      "Epoch 43/100, Train Loss: 1.1254, Test Loss: 0.9945\n",
      "Epoch 44/100, Train Loss: 1.1021, Test Loss: 0.9937\n",
      "Epoch 45/100, Train Loss: 1.1014, Test Loss: 0.9935\n",
      "Epoch 46/100, Train Loss: 1.1226, Test Loss: 0.9970\n",
      "Epoch 47/100, Train Loss: 1.0996, Test Loss: 0.9957\n",
      "Epoch 48/100, Train Loss: 1.1026, Test Loss: 0.9935\n",
      "Epoch 49/100, Train Loss: 1.1197, Test Loss: 0.9929\n",
      "Epoch 50/100, Train Loss: 1.0865, Test Loss: 0.9941\n",
      "Epoch 51/100, Train Loss: 1.1031, Test Loss: 0.9914\n",
      "Epoch 52/100, Train Loss: 1.1085, Test Loss: 0.9921\n",
      "Epoch 53/100, Train Loss: 1.0681, Test Loss: 0.9914\n",
      "Epoch 54/100, Train Loss: 1.1194, Test Loss: 0.9917\n",
      "Epoch 55/100, Train Loss: 1.0927, Test Loss: 0.9927\n",
      "Epoch 56/100, Train Loss: 1.0853, Test Loss: 0.9921\n",
      "Epoch 57/100, Train Loss: 1.1103, Test Loss: 0.9934\n",
      "Epoch 58/100, Train Loss: 1.1177, Test Loss: 0.9919\n",
      "Epoch 59/100, Train Loss: 1.1082, Test Loss: 0.9914\n",
      "Epoch 60/100, Train Loss: 1.1005, Test Loss: 0.9929\n",
      "Epoch 61/100, Train Loss: 1.0982, Test Loss: 0.9924\n",
      "Epoch 62/100, Train Loss: 1.0995, Test Loss: 0.9909\n",
      "Epoch 63/100, Train Loss: 1.0776, Test Loss: 0.9904\n",
      "Epoch 64/100, Train Loss: 1.1037, Test Loss: 0.9912\n",
      "Epoch 65/100, Train Loss: 1.0689, Test Loss: 0.9904\n",
      "Epoch 66/100, Train Loss: 1.0834, Test Loss: 0.9923\n",
      "Epoch 67/100, Train Loss: 1.1051, Test Loss: 0.9924\n",
      "Epoch 68/100, Train Loss: 1.0928, Test Loss: 0.9913\n",
      "Epoch 69/100, Train Loss: 1.0794, Test Loss: 0.9925\n",
      "Epoch 70/100, Train Loss: 1.0834, Test Loss: 0.9909\n",
      "Epoch 71/100, Train Loss: 1.0968, Test Loss: 0.9912\n",
      "Epoch 72/100, Train Loss: 1.0890, Test Loss: 0.9911\n",
      "Epoch 73/100, Train Loss: 1.0955, Test Loss: 0.9918\n",
      "Epoch 74/100, Train Loss: 1.1020, Test Loss: 0.9929\n",
      "Epoch 75/100, Train Loss: 1.0789, Test Loss: 0.9910\n",
      "Epoch 76/100, Train Loss: 1.0942, Test Loss: 0.9915\n",
      "Epoch 77/100, Train Loss: 1.0961, Test Loss: 0.9919\n",
      "Epoch 78/100, Train Loss: 1.0823, Test Loss: 0.9919\n",
      "Epoch 79/100, Train Loss: 1.0812, Test Loss: 0.9926\n",
      "Epoch 80/100, Train Loss: 1.0912, Test Loss: 0.9941\n",
      "Epoch 81/100, Train Loss: 1.0997, Test Loss: 0.9914\n",
      "Epoch 82/100, Train Loss: 1.0727, Test Loss: 0.9920\n",
      "Epoch 83/100, Train Loss: 1.0725, Test Loss: 0.9912\n",
      "Epoch 84/100, Train Loss: 1.0786, Test Loss: 0.9903\n",
      "Epoch 85/100, Train Loss: 1.0590, Test Loss: 0.9913\n",
      "Epoch 86/100, Train Loss: 1.0610, Test Loss: 0.9896\n",
      "Epoch 87/100, Train Loss: 1.0740, Test Loss: 0.9902\n",
      "Epoch 88/100, Train Loss: 1.0738, Test Loss: 0.9899\n",
      "Epoch 89/100, Train Loss: 1.0757, Test Loss: 0.9904\n",
      "Epoch 90/100, Train Loss: 1.0769, Test Loss: 0.9908\n",
      "Epoch 91/100, Train Loss: 1.0894, Test Loss: 0.9911\n",
      "Epoch 92/100, Train Loss: 1.0863, Test Loss: 0.9917\n",
      "Epoch 93/100, Train Loss: 1.0921, Test Loss: 0.9906\n",
      "Epoch 94/100, Train Loss: 1.0784, Test Loss: 0.9914\n",
      "Epoch 95/100, Train Loss: 1.0873, Test Loss: 0.9906\n",
      "Epoch 96/100, Train Loss: 1.0630, Test Loss: 0.9907\n",
      "Epoch 97/100, Train Loss: 1.0915, Test Loss: 0.9905\n",
      "Epoch 98/100, Train Loss: 1.0817, Test Loss: 0.9897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:33:25,739] Trial 193 finished with value: 0.9890568554401398 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 157, 'layer_1_size': 99, 'layer_2_size': 174, 'layer_3_size': 36, 'layer_4_size': 201, 'layer_5_size': 48, 'dropout_rate': 0.46265252771305154, 'learning_rate': 0.00022392219105499386, 'batch_size': 64, 'epochs': 100}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100, Train Loss: 1.0677, Test Loss: 0.9894\n",
      "Epoch 100/100, Train Loss: 1.0702, Test Loss: 0.9891\n",
      "Epoch 1/17, Train Loss: 1.1471, Test Loss: 0.7941\n",
      "Epoch 2/17, Train Loss: 1.0918, Test Loss: 0.8058\n",
      "Epoch 3/17, Train Loss: 1.1441, Test Loss: 0.8100\n",
      "Epoch 4/17, Train Loss: 1.1119, Test Loss: 0.8057\n",
      "Epoch 5/17, Train Loss: 1.1253, Test Loss: 0.8019\n",
      "Epoch 6/17, Train Loss: 1.1387, Test Loss: 0.8123\n",
      "Epoch 7/17, Train Loss: 1.1056, Test Loss: 0.8077\n",
      "Epoch 8/17, Train Loss: 1.1578, Test Loss: 0.8030\n",
      "Epoch 9/17, Train Loss: 1.0667, Test Loss: 0.7998\n",
      "Epoch 10/17, Train Loss: 1.0934, Test Loss: 0.8051\n",
      "Epoch 11/17, Train Loss: 1.1262, Test Loss: 0.8039\n",
      "Epoch 12/17, Train Loss: 1.1168, Test Loss: 0.8044\n",
      "Epoch 13/17, Train Loss: 1.0579, Test Loss: 0.8087\n",
      "Epoch 14/17, Train Loss: 1.0932, Test Loss: 0.8012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:33:27,360] Trial 194 finished with value: 0.8030170053243637 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 129, 'layer_1_size': 70, 'layer_2_size': 243, 'layer_3_size': 41, 'layer_4_size': 187, 'layer_5_size': 32, 'layer_6_size': 155, 'dropout_rate': 0.42709010819714277, 'learning_rate': 5.814488659181848e-05, 'batch_size': 64, 'epochs': 17}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/17, Train Loss: 1.0900, Test Loss: 0.8001\n",
      "Epoch 16/17, Train Loss: 1.1209, Test Loss: 0.7989\n",
      "Epoch 17/17, Train Loss: 1.1179, Test Loss: 0.8030\n",
      "Epoch 1/20, Train Loss: 1.2450, Test Loss: 1.0648\n",
      "Epoch 2/20, Train Loss: 1.2617, Test Loss: 1.0736\n",
      "Epoch 3/20, Train Loss: 1.2508, Test Loss: 1.0793\n",
      "Epoch 4/20, Train Loss: 1.1929, Test Loss: 1.0679\n",
      "Epoch 5/20, Train Loss: 1.1981, Test Loss: 1.0650\n",
      "Epoch 6/20, Train Loss: 1.1746, Test Loss: 1.0631\n",
      "Epoch 7/20, Train Loss: 1.2144, Test Loss: 1.0579\n",
      "Epoch 8/20, Train Loss: 1.2440, Test Loss: 1.0578\n",
      "Epoch 9/20, Train Loss: 1.1563, Test Loss: 1.0456\n",
      "Epoch 10/20, Train Loss: 1.2459, Test Loss: 1.0402\n",
      "Epoch 11/20, Train Loss: 1.1966, Test Loss: 1.0434\n",
      "Epoch 12/20, Train Loss: 1.1956, Test Loss: 1.0462\n",
      "Epoch 13/20, Train Loss: 1.2640, Test Loss: 1.0391\n",
      "Epoch 14/20, Train Loss: 1.2110, Test Loss: 1.0393\n",
      "Epoch 15/20, Train Loss: 1.1330, Test Loss: 1.0329\n",
      "Epoch 16/20, Train Loss: 1.1561, Test Loss: 1.0404\n",
      "Epoch 17/20, Train Loss: 1.1775, Test Loss: 1.0414\n",
      "Epoch 18/20, Train Loss: 1.2104, Test Loss: 1.0319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:33:29,066] Trial 195 finished with value: 1.0346943736076355 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 127, 'layer_1_size': 68, 'layer_2_size': 243, 'layer_3_size': 43, 'layer_4_size': 175, 'layer_5_size': 56, 'layer_6_size': 185, 'dropout_rate': 0.4386340468188823, 'learning_rate': 5.9416458530373035e-05, 'batch_size': 64, 'epochs': 20}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Train Loss: 1.1425, Test Loss: 1.0347\n",
      "Epoch 20/20, Train Loss: 1.1688, Test Loss: 1.0347\n",
      "Epoch 1/18, Train Loss: 1.2528, Test Loss: 1.2663\n",
      "Epoch 2/18, Train Loss: 1.0906, Test Loss: 1.2823\n",
      "Epoch 3/18, Train Loss: 1.0472, Test Loss: 1.2582\n",
      "Epoch 4/18, Train Loss: 1.0358, Test Loss: 1.2461\n",
      "Epoch 5/18, Train Loss: 1.0541, Test Loss: 1.2399\n",
      "Epoch 6/18, Train Loss: 1.0159, Test Loss: 1.2411\n",
      "Epoch 7/18, Train Loss: 1.0312, Test Loss: 1.2407\n",
      "Epoch 8/18, Train Loss: 0.9999, Test Loss: 1.2409\n",
      "Epoch 9/18, Train Loss: 1.0117, Test Loss: 1.2439\n",
      "Epoch 10/18, Train Loss: 0.9944, Test Loss: 1.2520\n",
      "Epoch 11/18, Train Loss: 0.9828, Test Loss: 1.2628\n",
      "Epoch 12/18, Train Loss: 0.9933, Test Loss: 1.2527\n",
      "Epoch 13/18, Train Loss: 0.9701, Test Loss: 1.2515\n",
      "Epoch 14/18, Train Loss: 0.9687, Test Loss: 1.2597\n",
      "Epoch 15/18, Train Loss: 0.9644, Test Loss: 1.2590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:33:31,141] Trial 196 finished with value: 1.268694818019867 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 131, 'layer_1_size': 69, 'layer_2_size': 243, 'layer_3_size': 32, 'layer_4_size': 189, 'layer_5_size': 90, 'layer_6_size': 165, 'dropout_rate': 0.42449871272539297, 'learning_rate': 0.00047074719396042523, 'batch_size': 64, 'epochs': 18}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/18, Train Loss: 0.9732, Test Loss: 1.2630\n",
      "Epoch 17/18, Train Loss: 0.9609, Test Loss: 1.2577\n",
      "Epoch 18/18, Train Loss: 0.9717, Test Loss: 1.2687\n",
      "Epoch 1/23, Train Loss: 1.2243, Test Loss: 0.9265\n",
      "Epoch 2/23, Train Loss: 1.2101, Test Loss: 0.9341\n",
      "Epoch 3/23, Train Loss: 1.0922, Test Loss: 0.9346\n",
      "Epoch 4/23, Train Loss: 1.0668, Test Loss: 0.9412\n",
      "Epoch 5/23, Train Loss: 1.0868, Test Loss: 0.9460\n",
      "Epoch 6/23, Train Loss: 1.0849, Test Loss: 0.9448\n",
      "Epoch 7/23, Train Loss: 1.0684, Test Loss: 0.9568\n",
      "Epoch 8/23, Train Loss: 1.0987, Test Loss: 0.9490\n",
      "Epoch 9/23, Train Loss: 1.0602, Test Loss: 0.9442\n",
      "Epoch 10/23, Train Loss: 1.0486, Test Loss: 0.9607\n",
      "Epoch 11/23, Train Loss: 1.0868, Test Loss: 0.9707\n",
      "Epoch 12/23, Train Loss: 1.0922, Test Loss: 0.9703\n",
      "Epoch 13/23, Train Loss: 1.0481, Test Loss: 0.9598\n",
      "Epoch 14/23, Train Loss: 1.0457, Test Loss: 0.9665\n",
      "Epoch 15/23, Train Loss: 1.0170, Test Loss: 0.9568\n",
      "Epoch 16/23, Train Loss: 1.0212, Test Loss: 0.9470\n",
      "Epoch 17/23, Train Loss: 1.0116, Test Loss: 0.9505\n",
      "Epoch 18/23, Train Loss: 1.0074, Test Loss: 0.9489\n",
      "Epoch 19/23, Train Loss: 1.0102, Test Loss: 0.9508\n",
      "Epoch 20/23, Train Loss: 1.0017, Test Loss: 0.9474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:33:32,437] Trial 197 finished with value: 0.9609218388795853 and parameters: {'num_hidden_layers': 5, 'layer_0_size': 118, 'layer_1_size': 45, 'layer_2_size': 201, 'layer_3_size': 63, 'layer_4_size': 193, 'dropout_rate': 0.47426847845671605, 'learning_rate': 0.0005424082914598093, 'batch_size': 64, 'epochs': 23}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/23, Train Loss: 1.0229, Test Loss: 0.9453\n",
      "Epoch 22/23, Train Loss: 0.9824, Test Loss: 0.9524\n",
      "Epoch 23/23, Train Loss: 1.0113, Test Loss: 0.9609\n",
      "Epoch 1/14, Train Loss: 1.6808, Test Loss: 1.0711\n",
      "Epoch 2/14, Train Loss: 1.5076, Test Loss: 1.0826\n",
      "Epoch 3/14, Train Loss: 1.4484, Test Loss: 1.0898\n",
      "Epoch 4/14, Train Loss: 1.3604, Test Loss: 1.0866\n",
      "Epoch 5/14, Train Loss: 1.2839, Test Loss: 1.0827\n",
      "Epoch 6/14, Train Loss: 1.2736, Test Loss: 1.0769\n",
      "Epoch 7/14, Train Loss: 1.2845, Test Loss: 1.0749\n",
      "Epoch 8/14, Train Loss: 1.2513, Test Loss: 1.0722\n",
      "Epoch 9/14, Train Loss: 1.2072, Test Loss: 1.0699\n",
      "Epoch 10/14, Train Loss: 1.2150, Test Loss: 1.0693\n",
      "Epoch 11/14, Train Loss: 1.1859, Test Loss: 1.0679\n",
      "Epoch 12/14, Train Loss: 1.1727, Test Loss: 1.0673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:33:33,079] Trial 198 finished with value: 1.063033252954483 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 102, 'layer_1_size': 76, 'layer_2_size': 249, 'layer_3_size': 244, 'layer_4_size': 185, 'layer_5_size': 49, 'layer_6_size': 159, 'dropout_rate': 0.4284531903248976, 'learning_rate': 0.0001795848715054481, 'batch_size': 128, 'epochs': 14}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/14, Train Loss: 1.2490, Test Loss: 1.0629\n",
      "Epoch 14/14, Train Loss: 1.2032, Test Loss: 1.0630\n",
      "Epoch 1/10, Train Loss: 1.2082, Test Loss: 1.1137\n",
      "Epoch 2/10, Train Loss: 1.2718, Test Loss: 1.1087\n",
      "Epoch 3/10, Train Loss: 1.1695, Test Loss: 1.0891\n",
      "Epoch 4/10, Train Loss: 1.2002, Test Loss: 1.0898\n",
      "Epoch 5/10, Train Loss: 1.1488, Test Loss: 1.0967\n",
      "Epoch 6/10, Train Loss: 1.0738, Test Loss: 1.0982\n",
      "Epoch 7/10, Train Loss: 1.1223, Test Loss: 1.1009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:33:34,062] Trial 199 finished with value: 1.0990824273654394 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 241, 'layer_1_size': 64, 'layer_2_size': 207, 'layer_3_size': 55, 'layer_4_size': 197, 'layer_5_size': 83, 'dropout_rate': 0.44809005844596245, 'learning_rate': 0.0001067761433991927, 'batch_size': 32, 'epochs': 10}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Loss: 1.0953, Test Loss: 1.0836\n",
      "Epoch 9/10, Train Loss: 1.1155, Test Loss: 1.0985\n",
      "Epoch 10/10, Train Loss: 1.0894, Test Loss: 1.0991\n",
      "Epoch 1/16, Train Loss: 1.3167, Test Loss: 0.8253\n",
      "Epoch 2/16, Train Loss: 1.2705, Test Loss: 0.8211\n",
      "Epoch 3/16, Train Loss: 1.1611, Test Loss: 0.8185\n",
      "Epoch 4/16, Train Loss: 1.1511, Test Loss: 0.8167\n",
      "Epoch 5/16, Train Loss: 1.2188, Test Loss: 0.8168\n",
      "Epoch 6/16, Train Loss: 1.1613, Test Loss: 0.8161\n",
      "Epoch 7/16, Train Loss: 1.1468, Test Loss: 0.8164\n",
      "Epoch 8/16, Train Loss: 1.1784, Test Loss: 0.8161\n",
      "Epoch 9/16, Train Loss: 1.1797, Test Loss: 0.8168\n",
      "Epoch 10/16, Train Loss: 1.1131, Test Loss: 0.8174\n",
      "Epoch 11/16, Train Loss: 1.1311, Test Loss: 0.8178\n",
      "Epoch 12/16, Train Loss: 1.1230, Test Loss: 0.8163\n",
      "Epoch 13/16, Train Loss: 1.0703, Test Loss: 0.8162\n",
      "Epoch 14/16, Train Loss: 1.1209, Test Loss: 0.8151\n",
      "Epoch 15/16, Train Loss: 1.1300, Test Loss: 0.8142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:33:35,427] Trial 200 finished with value: 0.8135131821036339 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 108, 'layer_1_size': 60, 'layer_2_size': 238, 'layer_3_size': 82, 'layer_4_size': 215, 'layer_5_size': 218, 'layer_6_size': 155, 'dropout_rate': 0.4894476943114174, 'learning_rate': 0.00037812846950692177, 'batch_size': 64, 'epochs': 16}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/16, Train Loss: 1.1132, Test Loss: 0.8135\n",
      "Epoch 1/17, Train Loss: 1.2244, Test Loss: 0.9336\n",
      "Epoch 2/17, Train Loss: 1.1595, Test Loss: 0.9430\n",
      "Epoch 3/17, Train Loss: 1.1785, Test Loss: 0.9406\n",
      "Epoch 4/17, Train Loss: 1.1564, Test Loss: 0.9362\n",
      "Epoch 5/17, Train Loss: 1.1755, Test Loss: 0.9348\n",
      "Epoch 6/17, Train Loss: 1.1322, Test Loss: 0.9315\n",
      "Epoch 7/17, Train Loss: 1.0842, Test Loss: 0.9310\n",
      "Epoch 8/17, Train Loss: 1.0748, Test Loss: 0.9324\n",
      "Epoch 9/17, Train Loss: 1.1003, Test Loss: 0.9310\n",
      "Epoch 10/17, Train Loss: 1.0852, Test Loss: 0.9305\n",
      "Epoch 11/17, Train Loss: 1.0752, Test Loss: 0.9322\n",
      "Epoch 12/17, Train Loss: 1.0928, Test Loss: 0.9305\n",
      "Epoch 13/17, Train Loss: 1.0745, Test Loss: 0.9336\n",
      "Epoch 14/17, Train Loss: 1.0698, Test Loss: 0.9360\n",
      "Epoch 15/17, Train Loss: 1.0751, Test Loss: 0.9377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:33:36,799] Trial 201 finished with value: 0.931090384721756 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 113, 'layer_1_size': 55, 'layer_2_size': 237, 'layer_3_size': 81, 'layer_4_size': 214, 'layer_5_size': 206, 'layer_6_size': 146, 'dropout_rate': 0.4716886974345353, 'learning_rate': 0.0003591451030511287, 'batch_size': 64, 'epochs': 17}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/17, Train Loss: 1.0642, Test Loss: 0.9340\n",
      "Epoch 17/17, Train Loss: 1.0406, Test Loss: 0.9311\n",
      "Epoch 1/16, Train Loss: 1.2059, Test Loss: 0.9820\n",
      "Epoch 2/16, Train Loss: 1.1704, Test Loss: 0.9867\n",
      "Epoch 3/16, Train Loss: 1.1134, Test Loss: 0.9932\n",
      "Epoch 4/16, Train Loss: 1.1532, Test Loss: 0.9904\n",
      "Epoch 5/16, Train Loss: 1.1171, Test Loss: 0.9877\n",
      "Epoch 6/16, Train Loss: 1.0939, Test Loss: 0.9806\n",
      "Epoch 7/16, Train Loss: 1.0816, Test Loss: 0.9840\n",
      "Epoch 8/16, Train Loss: 1.1279, Test Loss: 0.9807\n",
      "Epoch 9/16, Train Loss: 1.0905, Test Loss: 0.9791\n",
      "Epoch 10/16, Train Loss: 1.0803, Test Loss: 0.9798\n",
      "Epoch 11/16, Train Loss: 1.1022, Test Loss: 0.9794\n",
      "Epoch 12/16, Train Loss: 1.0470, Test Loss: 0.9776\n",
      "Epoch 13/16, Train Loss: 1.0441, Test Loss: 0.9782\n",
      "Epoch 14/16, Train Loss: 1.0453, Test Loss: 0.9784\n",
      "Epoch 15/16, Train Loss: 1.0525, Test Loss: 0.9775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:33:37,908] Trial 202 finished with value: 0.9788874983787537 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 107, 'layer_1_size': 62, 'layer_2_size': 240, 'layer_3_size': 86, 'layer_4_size': 206, 'layer_5_size': 38, 'layer_6_size': 154, 'dropout_rate': 0.4863708429701194, 'learning_rate': 0.0005572965092732052, 'batch_size': 64, 'epochs': 16}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/16, Train Loss: 1.0352, Test Loss: 0.9789\n",
      "Epoch 1/6, Train Loss: 1.3054, Test Loss: 1.1381\n",
      "Epoch 2/6, Train Loss: 1.2271, Test Loss: 1.1397\n",
      "Epoch 3/6, Train Loss: 1.1856, Test Loss: 1.1357\n",
      "Epoch 4/6, Train Loss: 1.1775, Test Loss: 1.1380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:33:38,483] Trial 203 finished with value: 1.1375257819890976 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 96, 'layer_1_size': 53, 'layer_2_size': 246, 'layer_3_size': 90, 'layer_4_size': 212, 'layer_5_size': 213, 'layer_6_size': 150, 'dropout_rate': 0.4809237046950243, 'learning_rate': 0.000245376173084536, 'batch_size': 64, 'epochs': 6}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/6, Train Loss: 1.1422, Test Loss: 1.1408\n",
      "Epoch 6/6, Train Loss: 1.1485, Test Loss: 1.1375\n",
      "Epoch 1/20, Train Loss: 1.2764, Test Loss: 1.1299\n",
      "Epoch 2/20, Train Loss: 1.2055, Test Loss: 1.1241\n",
      "Epoch 3/20, Train Loss: 1.2313, Test Loss: 1.1253\n",
      "Epoch 4/20, Train Loss: 1.2003, Test Loss: 1.1255\n",
      "Epoch 5/20, Train Loss: 1.1907, Test Loss: 1.1243\n",
      "Epoch 6/20, Train Loss: 1.2132, Test Loss: 1.1222\n",
      "Epoch 7/20, Train Loss: 1.1621, Test Loss: 1.1240\n",
      "Epoch 8/20, Train Loss: 1.1751, Test Loss: 1.1217\n",
      "Epoch 9/20, Train Loss: 1.1632, Test Loss: 1.1250\n",
      "Epoch 10/20, Train Loss: 1.1307, Test Loss: 1.1276\n",
      "Epoch 11/20, Train Loss: 1.0997, Test Loss: 1.1294\n",
      "Epoch 12/20, Train Loss: 1.1517, Test Loss: 1.1258\n",
      "Epoch 13/20, Train Loss: 1.0944, Test Loss: 1.1249\n",
      "Epoch 14/20, Train Loss: 1.1453, Test Loss: 1.1245\n",
      "Epoch 15/20, Train Loss: 1.1336, Test Loss: 1.1277\n",
      "Epoch 16/20, Train Loss: 1.0855, Test Loss: 1.1278\n",
      "Epoch 17/20, Train Loss: 1.0890, Test Loss: 1.1261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:33:39,971] Trial 204 finished with value: 1.126914143562317 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 33, 'layer_1_size': 87, 'layer_2_size': 251, 'layer_3_size': 76, 'layer_4_size': 45, 'layer_5_size': 60, 'layer_6_size': 178, 'dropout_rate': 0.498083914131622, 'learning_rate': 0.00041854405226021923, 'batch_size': 64, 'epochs': 20}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20, Train Loss: 1.1063, Test Loss: 1.1260\n",
      "Epoch 19/20, Train Loss: 1.1137, Test Loss: 1.1254\n",
      "Epoch 20/20, Train Loss: 1.0725, Test Loss: 1.1269\n",
      "Epoch 1/14, Train Loss: 1.3777, Test Loss: 0.9379\n",
      "Epoch 2/14, Train Loss: 1.2336, Test Loss: 0.9305\n",
      "Epoch 3/14, Train Loss: 1.2099, Test Loss: 0.9310\n",
      "Epoch 4/14, Train Loss: 1.2560, Test Loss: 0.9332\n",
      "Epoch 5/14, Train Loss: 1.2521, Test Loss: 0.9306\n",
      "Epoch 6/14, Train Loss: 1.1781, Test Loss: 0.9264\n",
      "Epoch 7/14, Train Loss: 1.1811, Test Loss: 0.9253\n",
      "Epoch 8/14, Train Loss: 1.1540, Test Loss: 0.9213\n",
      "Epoch 9/14, Train Loss: 1.2001, Test Loss: 0.9237\n",
      "Epoch 10/14, Train Loss: 1.1613, Test Loss: 0.9174\n",
      "Epoch 11/14, Train Loss: 1.1781, Test Loss: 0.9111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:33:41,289] Trial 205 finished with value: 0.9173095673322678 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 181, 'layer_1_size': 60, 'layer_2_size': 232, 'layer_3_size': 70, 'layer_4_size': 219, 'layer_5_size': 218, 'layer_6_size': 139, 'dropout_rate': 0.49216651060229677, 'learning_rate': 0.00030793177524441413, 'batch_size': 64, 'epochs': 14}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/14, Train Loss: 1.1191, Test Loss: 0.9128\n",
      "Epoch 13/14, Train Loss: 1.1903, Test Loss: 0.9132\n",
      "Epoch 14/14, Train Loss: 1.1411, Test Loss: 0.9173\n",
      "Epoch 1/95, Train Loss: 1.2721, Test Loss: 1.0168\n",
      "Epoch 2/95, Train Loss: 1.2455, Test Loss: 1.0229\n",
      "Epoch 3/95, Train Loss: 1.1843, Test Loss: 1.0180\n",
      "Epoch 4/95, Train Loss: 1.1727, Test Loss: 1.0229\n",
      "Epoch 5/95, Train Loss: 1.1349, Test Loss: 1.0247\n",
      "Epoch 6/95, Train Loss: 1.1185, Test Loss: 1.0309\n",
      "Epoch 7/95, Train Loss: 1.1350, Test Loss: 1.0386\n",
      "Epoch 8/95, Train Loss: 1.1367, Test Loss: 1.0341\n",
      "Epoch 9/95, Train Loss: 1.1158, Test Loss: 1.0254\n",
      "Epoch 10/95, Train Loss: 1.1561, Test Loss: 1.0311\n",
      "Epoch 11/95, Train Loss: 1.1140, Test Loss: 1.0336\n",
      "Epoch 12/95, Train Loss: 1.1398, Test Loss: 1.0388\n",
      "Epoch 13/95, Train Loss: 1.0957, Test Loss: 1.0373\n",
      "Epoch 14/95, Train Loss: 1.1028, Test Loss: 1.0362\n",
      "Epoch 15/95, Train Loss: 1.0878, Test Loss: 1.0416\n",
      "Epoch 16/95, Train Loss: 1.1058, Test Loss: 1.0439\n",
      "Epoch 17/95, Train Loss: 1.1053, Test Loss: 1.0429\n",
      "Epoch 18/95, Train Loss: 1.0702, Test Loss: 1.0338\n",
      "Epoch 19/95, Train Loss: 1.1295, Test Loss: 1.0315\n",
      "Epoch 20/95, Train Loss: 1.1097, Test Loss: 1.0320\n",
      "Epoch 21/95, Train Loss: 1.0903, Test Loss: 1.0379\n",
      "Epoch 22/95, Train Loss: 1.0618, Test Loss: 1.0453\n",
      "Epoch 23/95, Train Loss: 1.0650, Test Loss: 1.0463\n",
      "Epoch 24/95, Train Loss: 1.0781, Test Loss: 1.0472\n",
      "Epoch 25/95, Train Loss: 1.0781, Test Loss: 1.0403\n",
      "Epoch 26/95, Train Loss: 1.0924, Test Loss: 1.0395\n",
      "Epoch 27/95, Train Loss: 1.0840, Test Loss: 1.0381\n",
      "Epoch 28/95, Train Loss: 1.0743, Test Loss: 1.0352\n",
      "Epoch 29/95, Train Loss: 1.0743, Test Loss: 1.0328\n",
      "Epoch 30/95, Train Loss: 1.0894, Test Loss: 1.0326\n",
      "Epoch 31/95, Train Loss: 1.1019, Test Loss: 1.0292\n",
      "Epoch 32/95, Train Loss: 1.0172, Test Loss: 1.0265\n",
      "Epoch 33/95, Train Loss: 1.0706, Test Loss: 1.0350\n",
      "Epoch 34/95, Train Loss: 1.0407, Test Loss: 1.0373\n",
      "Epoch 35/95, Train Loss: 1.0920, Test Loss: 1.0353\n",
      "Epoch 36/95, Train Loss: 1.0527, Test Loss: 1.0360\n",
      "Epoch 37/95, Train Loss: 1.0469, Test Loss: 1.0370\n",
      "Epoch 38/95, Train Loss: 1.0746, Test Loss: 1.0372\n",
      "Epoch 39/95, Train Loss: 1.0708, Test Loss: 1.0349\n",
      "Epoch 40/95, Train Loss: 1.0833, Test Loss: 1.0387\n",
      "Epoch 41/95, Train Loss: 1.0598, Test Loss: 1.0396\n",
      "Epoch 42/95, Train Loss: 1.0499, Test Loss: 1.0436\n",
      "Epoch 43/95, Train Loss: 1.0498, Test Loss: 1.0431\n",
      "Epoch 44/95, Train Loss: 1.0853, Test Loss: 1.0476\n",
      "Epoch 45/95, Train Loss: 1.0606, Test Loss: 1.0513\n",
      "Epoch 46/95, Train Loss: 1.0709, Test Loss: 1.0532\n",
      "Epoch 47/95, Train Loss: 1.0563, Test Loss: 1.0519\n",
      "Epoch 48/95, Train Loss: 1.0451, Test Loss: 1.0603\n",
      "Epoch 49/95, Train Loss: 1.0704, Test Loss: 1.0583\n",
      "Epoch 50/95, Train Loss: 1.0691, Test Loss: 1.0560\n",
      "Epoch 51/95, Train Loss: 1.0464, Test Loss: 1.0559\n",
      "Epoch 52/95, Train Loss: 1.0685, Test Loss: 1.0522\n",
      "Epoch 53/95, Train Loss: 1.0327, Test Loss: 1.0513\n",
      "Epoch 54/95, Train Loss: 1.0561, Test Loss: 1.0523\n",
      "Epoch 55/95, Train Loss: 1.0590, Test Loss: 1.0574\n",
      "Epoch 56/95, Train Loss: 1.0618, Test Loss: 1.0563\n",
      "Epoch 57/95, Train Loss: 1.0342, Test Loss: 1.0526\n",
      "Epoch 58/95, Train Loss: 1.0443, Test Loss: 1.0531\n",
      "Epoch 59/95, Train Loss: 1.0302, Test Loss: 1.0536\n",
      "Epoch 60/95, Train Loss: 1.0492, Test Loss: 1.0555\n",
      "Epoch 61/95, Train Loss: 1.0431, Test Loss: 1.0554\n",
      "Epoch 62/95, Train Loss: 1.0434, Test Loss: 1.0544\n",
      "Epoch 63/95, Train Loss: 1.0226, Test Loss: 1.0503\n",
      "Epoch 64/95, Train Loss: 1.0306, Test Loss: 1.0500\n",
      "Epoch 65/95, Train Loss: 1.0311, Test Loss: 1.0515\n",
      "Epoch 66/95, Train Loss: 1.0313, Test Loss: 1.0512\n",
      "Epoch 67/95, Train Loss: 1.0399, Test Loss: 1.0493\n",
      "Epoch 68/95, Train Loss: 1.0400, Test Loss: 1.0496\n",
      "Epoch 69/95, Train Loss: 1.0341, Test Loss: 1.0527\n",
      "Epoch 70/95, Train Loss: 1.0321, Test Loss: 1.0545\n",
      "Epoch 71/95, Train Loss: 1.0339, Test Loss: 1.0486\n",
      "Epoch 72/95, Train Loss: 1.0448, Test Loss: 1.0468\n",
      "Epoch 73/95, Train Loss: 1.0615, Test Loss: 1.0431\n",
      "Epoch 74/95, Train Loss: 1.0279, Test Loss: 1.0424\n",
      "Epoch 75/95, Train Loss: 1.0068, Test Loss: 1.0424\n",
      "Epoch 76/95, Train Loss: 1.0347, Test Loss: 1.0457\n",
      "Epoch 77/95, Train Loss: 1.0058, Test Loss: 1.0469\n",
      "Epoch 78/95, Train Loss: 1.0287, Test Loss: 1.0458\n",
      "Epoch 79/95, Train Loss: 1.0118, Test Loss: 1.0458\n",
      "Epoch 80/95, Train Loss: 1.0265, Test Loss: 1.0424\n",
      "Epoch 81/95, Train Loss: 1.0171, Test Loss: 1.0384\n",
      "Epoch 82/95, Train Loss: 1.0220, Test Loss: 1.0367\n",
      "Epoch 83/95, Train Loss: 1.0272, Test Loss: 1.0371\n",
      "Epoch 84/95, Train Loss: 1.0196, Test Loss: 1.0387\n",
      "Epoch 85/95, Train Loss: 1.0331, Test Loss: 1.0459\n",
      "Epoch 86/95, Train Loss: 1.0279, Test Loss: 1.0479\n",
      "Epoch 87/95, Train Loss: 1.0109, Test Loss: 1.0458\n",
      "Epoch 88/95, Train Loss: 1.0291, Test Loss: 1.0443\n",
      "Epoch 89/95, Train Loss: 1.0282, Test Loss: 1.0478\n",
      "Epoch 90/95, Train Loss: 1.0426, Test Loss: 1.0489\n",
      "Epoch 91/95, Train Loss: 1.0376, Test Loss: 1.0489\n",
      "Epoch 92/95, Train Loss: 1.0113, Test Loss: 1.0472\n",
      "Epoch 93/95, Train Loss: 1.0191, Test Loss: 1.0481\n",
      "Epoch 94/95, Train Loss: 1.0138, Test Loss: 1.0434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:33:46,771] Trial 206 finished with value: 1.042882040143013 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 122, 'layer_1_size': 73, 'layer_2_size': 196, 'layer_3_size': 50, 'layer_4_size': 98, 'layer_5_size': 167, 'dropout_rate': 0.40287725996784784, 'learning_rate': 0.0001313573462076233, 'batch_size': 64, 'epochs': 95}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/95, Train Loss: 1.0484, Test Loss: 1.0429\n",
      "Epoch 1/18, Train Loss: 1.3812, Test Loss: 0.9770\n",
      "Epoch 2/18, Train Loss: 1.3394, Test Loss: 0.9848\n",
      "Epoch 3/18, Train Loss: 1.3291, Test Loss: 0.9788\n",
      "Epoch 4/18, Train Loss: 1.2925, Test Loss: 0.9790\n",
      "Epoch 5/18, Train Loss: 1.2923, Test Loss: 0.9783\n",
      "Epoch 6/18, Train Loss: 1.2062, Test Loss: 0.9761\n",
      "Epoch 7/18, Train Loss: 1.2603, Test Loss: 0.9783\n",
      "Epoch 8/18, Train Loss: 1.2706, Test Loss: 0.9820\n",
      "Epoch 9/18, Train Loss: 1.1750, Test Loss: 0.9824\n",
      "Epoch 10/18, Train Loss: 1.2494, Test Loss: 0.9937\n",
      "Epoch 11/18, Train Loss: 1.2080, Test Loss: 0.9916\n",
      "Epoch 12/18, Train Loss: 1.1596, Test Loss: 0.9923\n",
      "Epoch 13/18, Train Loss: 1.2245, Test Loss: 1.0011\n",
      "Epoch 14/18, Train Loss: 1.1805, Test Loss: 1.0058\n",
      "Epoch 15/18, Train Loss: 1.1826, Test Loss: 1.0029\n",
      "Epoch 16/18, Train Loss: 1.1698, Test Loss: 1.0072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:33:48,334] Trial 207 finished with value: 1.0150758922100067 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 116, 'layer_1_size': 136, 'layer_2_size': 214, 'layer_3_size': 94, 'layer_4_size': 206, 'layer_5_size': 222, 'layer_6_size': 162, 'dropout_rate': 0.4330274173697485, 'learning_rate': 7.301375584637074e-05, 'batch_size': 64, 'epochs': 18}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/18, Train Loss: 1.1922, Test Loss: 1.0052\n",
      "Epoch 18/18, Train Loss: 1.1063, Test Loss: 1.0151\n",
      "Epoch 1/11, Train Loss: 1.1787, Test Loss: 1.0534\n",
      "Epoch 2/11, Train Loss: 1.1434, Test Loss: 1.0588\n",
      "Epoch 3/11, Train Loss: 1.1299, Test Loss: 1.0298\n",
      "Epoch 4/11, Train Loss: 1.0911, Test Loss: 1.0104\n",
      "Epoch 5/11, Train Loss: 1.1215, Test Loss: 0.9983\n",
      "Epoch 6/11, Train Loss: 1.1477, Test Loss: 1.0075\n",
      "Epoch 7/11, Train Loss: 1.0792, Test Loss: 1.0125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:33:49,231] Trial 208 finished with value: 0.9864104837179184 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 221, 'layer_1_size': 66, 'layer_2_size': 256, 'layer_3_size': 255, 'layer_4_size': 182, 'layer_5_size': 32, 'dropout_rate': 0.29135262117303934, 'learning_rate': 0.0001973593816955804, 'batch_size': 64, 'epochs': 11}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/11, Train Loss: 1.1093, Test Loss: 1.0112\n",
      "Epoch 9/11, Train Loss: 1.0622, Test Loss: 1.0037\n",
      "Epoch 10/11, Train Loss: 1.0782, Test Loss: 0.9942\n",
      "Epoch 11/11, Train Loss: 1.0949, Test Loss: 0.9864\n",
      "Epoch 1/22, Train Loss: 1.2541, Test Loss: 1.2671\n",
      "Epoch 2/22, Train Loss: 1.2216, Test Loss: 1.2609\n",
      "Epoch 3/22, Train Loss: 1.2325, Test Loss: 1.2584\n",
      "Epoch 4/22, Train Loss: 1.1887, Test Loss: 1.2526\n",
      "Epoch 5/22, Train Loss: 1.1904, Test Loss: 1.2510\n",
      "Epoch 6/22, Train Loss: 1.1728, Test Loss: 1.2608\n",
      "Epoch 7/22, Train Loss: 1.1719, Test Loss: 1.2556\n",
      "Epoch 8/22, Train Loss: 1.1880, Test Loss: 1.2531\n",
      "Epoch 9/22, Train Loss: 1.1602, Test Loss: 1.2551\n",
      "Epoch 10/22, Train Loss: 1.1572, Test Loss: 1.2538\n",
      "Epoch 11/22, Train Loss: 1.1231, Test Loss: 1.2583\n",
      "Epoch 12/22, Train Loss: 1.1188, Test Loss: 1.2644\n",
      "Epoch 13/22, Train Loss: 1.1378, Test Loss: 1.2583\n",
      "Epoch 14/22, Train Loss: 1.1461, Test Loss: 1.2590\n",
      "Epoch 15/22, Train Loss: 1.1404, Test Loss: 1.2561\n",
      "Epoch 16/22, Train Loss: 1.1337, Test Loss: 1.2550\n",
      "Epoch 17/22, Train Loss: 1.0936, Test Loss: 1.2573\n",
      "Epoch 18/22, Train Loss: 1.1334, Test Loss: 1.2554\n",
      "Epoch 19/22, Train Loss: 1.1074, Test Loss: 1.2504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:33:50,650] Trial 209 finished with value: 1.259230226278305 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 108, 'layer_1_size': 144, 'layer_2_size': 191, 'layer_3_size': 79, 'layer_4_size': 56, 'layer_5_size': 141, 'layer_6_size': 169, 'dropout_rate': 0.4140513934965072, 'learning_rate': 0.00016116846586299377, 'batch_size': 64, 'epochs': 22}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/22, Train Loss: 1.1059, Test Loss: 1.2514\n",
      "Epoch 21/22, Train Loss: 1.0918, Test Loss: 1.2523\n",
      "Epoch 22/22, Train Loss: 1.1181, Test Loss: 1.2592\n",
      "Epoch 1/58, Train Loss: 1.2727, Test Loss: 0.9463\n",
      "Epoch 2/58, Train Loss: 1.2460, Test Loss: 0.9578\n",
      "Epoch 3/58, Train Loss: 1.2771, Test Loss: 0.9653\n",
      "Epoch 4/58, Train Loss: 1.2754, Test Loss: 0.9706\n",
      "Epoch 5/58, Train Loss: 1.2853, Test Loss: 0.9655\n",
      "Epoch 6/58, Train Loss: 1.2507, Test Loss: 0.9633\n",
      "Epoch 7/58, Train Loss: 1.2604, Test Loss: 0.9596\n",
      "Epoch 8/58, Train Loss: 1.2461, Test Loss: 0.9561\n",
      "Epoch 9/58, Train Loss: 1.2204, Test Loss: 0.9534\n",
      "Epoch 10/58, Train Loss: 1.2503, Test Loss: 0.9525\n",
      "Epoch 11/58, Train Loss: 1.1781, Test Loss: 0.9513\n",
      "Epoch 12/58, Train Loss: 1.2067, Test Loss: 0.9506\n",
      "Epoch 13/58, Train Loss: 1.2097, Test Loss: 0.9483\n",
      "Epoch 14/58, Train Loss: 1.1863, Test Loss: 0.9468\n",
      "Epoch 15/58, Train Loss: 1.2822, Test Loss: 0.9446\n",
      "Epoch 16/58, Train Loss: 1.2051, Test Loss: 0.9431\n",
      "Epoch 17/58, Train Loss: 1.1701, Test Loss: 0.9436\n",
      "Epoch 18/58, Train Loss: 1.1343, Test Loss: 0.9438\n",
      "Epoch 19/58, Train Loss: 1.2224, Test Loss: 0.9426\n",
      "Epoch 20/58, Train Loss: 1.2017, Test Loss: 0.9443\n",
      "Epoch 21/58, Train Loss: 1.2359, Test Loss: 0.9429\n",
      "Epoch 22/58, Train Loss: 1.1745, Test Loss: 0.9432\n",
      "Epoch 23/58, Train Loss: 1.2129, Test Loss: 0.9422\n",
      "Epoch 24/58, Train Loss: 1.1913, Test Loss: 0.9431\n",
      "Epoch 25/58, Train Loss: 1.1448, Test Loss: 0.9428\n",
      "Epoch 26/58, Train Loss: 1.2142, Test Loss: 0.9423\n",
      "Epoch 27/58, Train Loss: 1.1734, Test Loss: 0.9417\n",
      "Epoch 28/58, Train Loss: 1.1934, Test Loss: 0.9431\n",
      "Epoch 29/58, Train Loss: 1.1963, Test Loss: 0.9437\n",
      "Epoch 30/58, Train Loss: 1.1518, Test Loss: 0.9423\n",
      "Epoch 31/58, Train Loss: 1.2086, Test Loss: 0.9422\n",
      "Epoch 32/58, Train Loss: 1.1681, Test Loss: 0.9414\n",
      "Epoch 33/58, Train Loss: 1.2019, Test Loss: 0.9415\n",
      "Epoch 34/58, Train Loss: 1.1455, Test Loss: 0.9418\n",
      "Epoch 35/58, Train Loss: 1.1652, Test Loss: 0.9415\n",
      "Epoch 36/58, Train Loss: 1.2003, Test Loss: 0.9407\n",
      "Epoch 37/58, Train Loss: 1.1435, Test Loss: 0.9412\n",
      "Epoch 38/58, Train Loss: 1.1870, Test Loss: 0.9394\n",
      "Epoch 39/58, Train Loss: 1.1490, Test Loss: 0.9406\n",
      "Epoch 40/58, Train Loss: 1.2295, Test Loss: 0.9398\n",
      "Epoch 41/58, Train Loss: 1.1896, Test Loss: 0.9408\n",
      "Epoch 42/58, Train Loss: 1.1640, Test Loss: 0.9417\n",
      "Epoch 43/58, Train Loss: 1.1180, Test Loss: 0.9406\n",
      "Epoch 44/58, Train Loss: 1.1568, Test Loss: 0.9391\n",
      "Epoch 45/58, Train Loss: 1.0903, Test Loss: 0.9387\n",
      "Epoch 46/58, Train Loss: 1.1155, Test Loss: 0.9392\n",
      "Epoch 47/58, Train Loss: 1.1616, Test Loss: 0.9397\n",
      "Epoch 48/58, Train Loss: 1.1824, Test Loss: 0.9408\n",
      "Epoch 49/58, Train Loss: 1.1291, Test Loss: 0.9406\n",
      "Epoch 50/58, Train Loss: 1.1560, Test Loss: 0.9393\n",
      "Epoch 51/58, Train Loss: 1.1466, Test Loss: 0.9386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:33:52,786] Trial 210 finished with value: 0.9397853910923004 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 189, 'layer_1_size': 173, 'layer_2_size': 218, 'layer_3_size': 39, 'layer_4_size': 192, 'layer_5_size': 131, 'dropout_rate': 0.4654822315388545, 'learning_rate': 8.572501097962664e-05, 'batch_size': 128, 'epochs': 58}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/58, Train Loss: 1.1609, Test Loss: 0.9385\n",
      "Epoch 53/58, Train Loss: 1.1488, Test Loss: 0.9390\n",
      "Epoch 54/58, Train Loss: 1.1347, Test Loss: 0.9398\n",
      "Epoch 55/58, Train Loss: 1.1612, Test Loss: 0.9402\n",
      "Epoch 56/58, Train Loss: 1.1769, Test Loss: 0.9390\n",
      "Epoch 57/58, Train Loss: 1.1066, Test Loss: 0.9389\n",
      "Epoch 58/58, Train Loss: 1.1469, Test Loss: 0.9398\n",
      "Epoch 1/8, Train Loss: 1.2691, Test Loss: 0.8995\n",
      "Epoch 2/8, Train Loss: 1.2934, Test Loss: 0.9364\n",
      "Epoch 3/8, Train Loss: 1.2225, Test Loss: 0.9186\n",
      "Epoch 4/8, Train Loss: 1.2086, Test Loss: 0.9014\n",
      "Epoch 5/8, Train Loss: 1.2239, Test Loss: 0.9033\n",
      "Epoch 6/8, Train Loss: 1.2407, Test Loss: 0.8932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:33:53,652] Trial 211 finished with value: 0.879990067332983 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 136, 'layer_1_size': 191, 'layer_2_size': 226, 'layer_3_size': 44, 'layer_4_size': 202, 'layer_5_size': 251, 'layer_6_size': 129, 'layer_7_size': 180, 'dropout_rate': 0.3803630309064504, 'learning_rate': 5.845541867351755e-05, 'batch_size': 64, 'epochs': 8}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/8, Train Loss: 1.1603, Test Loss: 0.8792\n",
      "Epoch 8/8, Train Loss: 1.2518, Test Loss: 0.8800\n",
      "Epoch 1/34, Train Loss: 1.1958, Test Loss: 1.0364\n",
      "Epoch 2/34, Train Loss: 1.2685, Test Loss: 1.0361\n",
      "Epoch 3/34, Train Loss: 1.2423, Test Loss: 1.0209\n",
      "Epoch 4/34, Train Loss: 1.2373, Test Loss: 1.0197\n",
      "Epoch 5/34, Train Loss: 1.2731, Test Loss: 1.0170\n",
      "Epoch 6/34, Train Loss: 1.2842, Test Loss: 1.0145\n",
      "Epoch 7/34, Train Loss: 1.2614, Test Loss: 1.0128\n",
      "Epoch 8/34, Train Loss: 1.2856, Test Loss: 1.0110\n",
      "Epoch 9/34, Train Loss: 1.2107, Test Loss: 1.0088\n",
      "Epoch 10/34, Train Loss: 1.2735, Test Loss: 1.0095\n",
      "Epoch 11/34, Train Loss: 1.2024, Test Loss: 1.0101\n",
      "Epoch 12/34, Train Loss: 1.1600, Test Loss: 1.0049\n",
      "Epoch 13/34, Train Loss: 1.2025, Test Loss: 1.0093\n",
      "Epoch 14/34, Train Loss: 1.1545, Test Loss: 1.0116\n",
      "Epoch 15/34, Train Loss: 1.1573, Test Loss: 1.0088\n",
      "Epoch 16/34, Train Loss: 1.2331, Test Loss: 1.0111\n",
      "Epoch 17/34, Train Loss: 1.1590, Test Loss: 1.0086\n",
      "Epoch 18/34, Train Loss: 1.1707, Test Loss: 1.0103\n",
      "Epoch 19/34, Train Loss: 1.1774, Test Loss: 1.0095\n",
      "Epoch 20/34, Train Loss: 1.1498, Test Loss: 1.0097\n",
      "Epoch 21/34, Train Loss: 1.1914, Test Loss: 1.0091\n",
      "Epoch 22/34, Train Loss: 1.1993, Test Loss: 1.0106\n",
      "Epoch 23/34, Train Loss: 1.1665, Test Loss: 1.0113\n",
      "Epoch 24/34, Train Loss: 1.1805, Test Loss: 1.0197\n",
      "Epoch 25/34, Train Loss: 1.1706, Test Loss: 1.0152\n",
      "Epoch 26/34, Train Loss: 1.1773, Test Loss: 1.0137\n",
      "Epoch 27/34, Train Loss: 1.1263, Test Loss: 1.0149\n",
      "Epoch 28/34, Train Loss: 1.1077, Test Loss: 1.0160\n",
      "Epoch 29/34, Train Loss: 1.1488, Test Loss: 1.0148\n",
      "Epoch 30/34, Train Loss: 1.1622, Test Loss: 1.0161\n",
      "Epoch 31/34, Train Loss: 1.1429, Test Loss: 1.0150\n",
      "Epoch 32/34, Train Loss: 1.1655, Test Loss: 1.0151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:33:56,715] Trial 212 finished with value: 1.0152132734656334 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 104, 'layer_1_size': 182, 'layer_2_size': 210, 'layer_3_size': 39, 'layer_4_size': 154, 'layer_5_size': 227, 'layer_6_size': 156, 'layer_7_size': 159, 'dropout_rate': 0.367217511117002, 'learning_rate': 4.883567902829305e-05, 'batch_size': 64, 'epochs': 34}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/34, Train Loss: 1.1488, Test Loss: 1.0141\n",
      "Epoch 34/34, Train Loss: 1.1452, Test Loss: 1.0152\n",
      "Epoch 1/27, Train Loss: 1.4383, Test Loss: 0.8720\n",
      "Epoch 2/27, Train Loss: 1.4285, Test Loss: 0.8710\n",
      "Epoch 3/27, Train Loss: 1.3742, Test Loss: 0.8658\n",
      "Epoch 4/27, Train Loss: 1.3312, Test Loss: 0.8674\n",
      "Epoch 5/27, Train Loss: 1.3258, Test Loss: 0.8685\n",
      "Epoch 6/27, Train Loss: 1.3998, Test Loss: 0.8687\n",
      "Epoch 7/27, Train Loss: 1.3057, Test Loss: 0.8717\n",
      "Epoch 8/27, Train Loss: 1.2731, Test Loss: 0.8753\n",
      "Epoch 9/27, Train Loss: 1.2630, Test Loss: 0.8643\n",
      "Epoch 10/27, Train Loss: 1.2610, Test Loss: 0.8651\n",
      "Epoch 11/27, Train Loss: 1.2109, Test Loss: 0.8723\n",
      "Epoch 12/27, Train Loss: 1.1962, Test Loss: 0.8715\n",
      "Epoch 13/27, Train Loss: 1.2116, Test Loss: 0.8743\n",
      "Epoch 14/27, Train Loss: 1.1683, Test Loss: 0.8710\n",
      "Epoch 15/27, Train Loss: 1.1851, Test Loss: 0.8710\n",
      "Epoch 16/27, Train Loss: 1.2139, Test Loss: 0.8695\n",
      "Epoch 17/27, Train Loss: 1.1734, Test Loss: 0.8726\n",
      "Epoch 18/27, Train Loss: 1.1697, Test Loss: 0.8712\n",
      "Epoch 19/27, Train Loss: 1.2145, Test Loss: 0.8686\n",
      "Epoch 20/27, Train Loss: 1.1532, Test Loss: 0.8647\n",
      "Epoch 21/27, Train Loss: 1.1835, Test Loss: 0.8677\n",
      "Epoch 22/27, Train Loss: 1.1282, Test Loss: 0.8655\n",
      "Epoch 23/27, Train Loss: 1.1726, Test Loss: 0.8652\n",
      "Epoch 24/27, Train Loss: 1.1831, Test Loss: 0.8671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:33:58,669] Trial 213 finished with value: 0.8675086349248886 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 144, 'layer_1_size': 185, 'layer_2_size': 220, 'layer_3_size': 47, 'layer_4_size': 35, 'layer_5_size': 240, 'layer_6_size': 139, 'dropout_rate': 0.39420848834917965, 'learning_rate': 3.3639906042378894e-05, 'batch_size': 64, 'epochs': 27}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/27, Train Loss: 1.1901, Test Loss: 0.8685\n",
      "Epoch 26/27, Train Loss: 1.1687, Test Loss: 0.8656\n",
      "Epoch 27/27, Train Loss: 1.1882, Test Loss: 0.8675\n",
      "Epoch 1/39, Train Loss: 1.2029, Test Loss: 0.8922\n",
      "Epoch 2/39, Train Loss: 1.1821, Test Loss: 0.8823\n",
      "Epoch 3/39, Train Loss: 1.1988, Test Loss: 0.8858\n",
      "Epoch 4/39, Train Loss: 1.2709, Test Loss: 0.8839\n",
      "Epoch 5/39, Train Loss: 1.2384, Test Loss: 0.8808\n",
      "Epoch 6/39, Train Loss: 1.2169, Test Loss: 0.8795\n",
      "Epoch 7/39, Train Loss: 1.2077, Test Loss: 0.8779\n",
      "Epoch 8/39, Train Loss: 1.2190, Test Loss: 0.8763\n",
      "Epoch 9/39, Train Loss: 1.2392, Test Loss: 0.8834\n",
      "Epoch 10/39, Train Loss: 1.1512, Test Loss: 0.8850\n",
      "Epoch 11/39, Train Loss: 1.1371, Test Loss: 0.8831\n",
      "Epoch 12/39, Train Loss: 1.1597, Test Loss: 0.8885\n",
      "Epoch 13/39, Train Loss: 1.2213, Test Loss: 0.8862\n",
      "Epoch 14/39, Train Loss: 1.1611, Test Loss: 0.8831\n",
      "Epoch 15/39, Train Loss: 1.1863, Test Loss: 0.8921\n",
      "Epoch 16/39, Train Loss: 1.1744, Test Loss: 0.8864\n",
      "Epoch 17/39, Train Loss: 1.1886, Test Loss: 0.8862\n",
      "Epoch 18/39, Train Loss: 1.1609, Test Loss: 0.8864\n",
      "Epoch 19/39, Train Loss: 1.1329, Test Loss: 0.8816\n",
      "Epoch 20/39, Train Loss: 1.1478, Test Loss: 0.8816\n",
      "Epoch 21/39, Train Loss: 1.1680, Test Loss: 0.8804\n",
      "Epoch 22/39, Train Loss: 1.1960, Test Loss: 0.8821\n",
      "Epoch 23/39, Train Loss: 1.1637, Test Loss: 0.8788\n",
      "Epoch 24/39, Train Loss: 1.1807, Test Loss: 0.8762\n",
      "Epoch 25/39, Train Loss: 1.1161, Test Loss: 0.8774\n",
      "Epoch 26/39, Train Loss: 1.1269, Test Loss: 0.8775\n",
      "Epoch 27/39, Train Loss: 1.1384, Test Loss: 0.8806\n",
      "Epoch 28/39, Train Loss: 1.1122, Test Loss: 0.8820\n",
      "Epoch 29/39, Train Loss: 1.1099, Test Loss: 0.8791\n",
      "Epoch 30/39, Train Loss: 1.1303, Test Loss: 0.8809\n",
      "Epoch 31/39, Train Loss: 1.0789, Test Loss: 0.8826\n",
      "Epoch 32/39, Train Loss: 1.1032, Test Loss: 0.8839\n",
      "Epoch 33/39, Train Loss: 1.1267, Test Loss: 0.8849\n",
      "Epoch 34/39, Train Loss: 1.1151, Test Loss: 0.8774\n",
      "Epoch 35/39, Train Loss: 1.1041, Test Loss: 0.8840\n",
      "Epoch 36/39, Train Loss: 1.1027, Test Loss: 0.8819\n",
      "Epoch 37/39, Train Loss: 1.0736, Test Loss: 0.8866\n",
      "Epoch 38/39, Train Loss: 1.1306, Test Loss: 0.8865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:34:03,291] Trial 214 finished with value: 0.8815685734152794 and parameters: {'num_hidden_layers': 9, 'layer_0_size': 208, 'layer_1_size': 201, 'layer_2_size': 201, 'layer_3_size': 59, 'layer_4_size': 222, 'layer_5_size': 234, 'layer_6_size': 145, 'layer_7_size': 195, 'layer_8_size': 170, 'dropout_rate': 0.4797742304282859, 'learning_rate': 5.395050391976308e-05, 'batch_size': 64, 'epochs': 39}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/39, Train Loss: 1.0854, Test Loss: 0.8816\n",
      "Epoch 1/16, Train Loss: 1.2775, Test Loss: 1.5005\n",
      "Epoch 2/16, Train Loss: 1.1867, Test Loss: 1.4927\n",
      "Epoch 3/16, Train Loss: 1.2024, Test Loss: 1.4898\n",
      "Epoch 4/16, Train Loss: 1.1942, Test Loss: 1.4881\n",
      "Epoch 5/16, Train Loss: 1.1917, Test Loss: 1.4848\n",
      "Epoch 6/16, Train Loss: 1.1813, Test Loss: 1.4857\n",
      "Epoch 7/16, Train Loss: 1.1597, Test Loss: 1.4876\n",
      "Epoch 8/16, Train Loss: 1.1500, Test Loss: 1.4855\n",
      "Epoch 9/16, Train Loss: 1.1772, Test Loss: 1.4885\n",
      "Epoch 10/16, Train Loss: 1.1853, Test Loss: 1.4974\n",
      "Epoch 11/16, Train Loss: 1.1168, Test Loss: 1.4947\n",
      "Epoch 12/16, Train Loss: 1.1366, Test Loss: 1.4864\n",
      "Epoch 13/16, Train Loss: 1.1605, Test Loss: 1.4901\n",
      "Epoch 14/16, Train Loss: 1.1341, Test Loss: 1.4867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:34:04,294] Trial 215 finished with value: 1.4904685616493225 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 112, 'layer_1_size': 60, 'layer_2_size': 235, 'layer_3_size': 32, 'layer_4_size': 51, 'layer_5_size': 210, 'layer_6_size': 118, 'dropout_rate': 0.44083885283765784, 'learning_rate': 7.064664323996562e-05, 'batch_size': 64, 'epochs': 16}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/16, Train Loss: 1.1693, Test Loss: 1.4898\n",
      "Epoch 16/16, Train Loss: 1.1430, Test Loss: 1.4905\n",
      "Epoch 1/13, Train Loss: 1.5354, Test Loss: 0.9352\n",
      "Epoch 2/13, Train Loss: 1.5331, Test Loss: 0.9398\n",
      "Epoch 3/13, Train Loss: 1.4507, Test Loss: 0.9413\n",
      "Epoch 4/13, Train Loss: 1.4480, Test Loss: 0.9366\n",
      "Epoch 5/13, Train Loss: 1.3350, Test Loss: 0.9375\n",
      "Epoch 6/13, Train Loss: 1.3288, Test Loss: 0.9347\n",
      "Epoch 7/13, Train Loss: 1.3052, Test Loss: 0.9308\n",
      "Epoch 8/13, Train Loss: 1.3021, Test Loss: 0.9276\n",
      "Epoch 9/13, Train Loss: 1.2777, Test Loss: 0.9301\n",
      "Epoch 10/13, Train Loss: 1.2970, Test Loss: 0.9321\n",
      "Epoch 11/13, Train Loss: 1.2745, Test Loss: 0.9277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:34:05,178] Trial 216 finished with value: 0.9293332695960999 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 147, 'layer_1_size': 163, 'layer_2_size': 243, 'layer_3_size': 188, 'layer_4_size': 93, 'layer_5_size': 217, 'dropout_rate': 0.4877503269022952, 'learning_rate': 4.311001400378899e-05, 'batch_size': 64, 'epochs': 13}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/13, Train Loss: 1.2125, Test Loss: 0.9299\n",
      "Epoch 13/13, Train Loss: 1.1733, Test Loss: 0.9293\n",
      "Epoch 1/20, Train Loss: 1.2849, Test Loss: 1.0115\n",
      "Epoch 2/20, Train Loss: 1.2632, Test Loss: 1.0113\n",
      "Epoch 3/20, Train Loss: 1.2331, Test Loss: 1.0129\n",
      "Epoch 4/20, Train Loss: 1.1887, Test Loss: 1.0164\n",
      "Epoch 5/20, Train Loss: 1.2056, Test Loss: 1.0189\n",
      "Epoch 6/20, Train Loss: 1.1804, Test Loss: 1.0226\n",
      "Epoch 7/20, Train Loss: 1.1557, Test Loss: 1.0254\n",
      "Epoch 8/20, Train Loss: 1.1637, Test Loss: 1.0271\n",
      "Epoch 9/20, Train Loss: 1.1897, Test Loss: 1.0288\n",
      "Epoch 10/20, Train Loss: 1.1292, Test Loss: 1.0280\n",
      "Epoch 11/20, Train Loss: 1.1549, Test Loss: 1.0286\n",
      "Epoch 12/20, Train Loss: 1.1433, Test Loss: 1.0271\n",
      "Epoch 13/20, Train Loss: 1.1562, Test Loss: 1.0274\n",
      "Epoch 14/20, Train Loss: 1.1090, Test Loss: 1.0281\n",
      "Epoch 15/20, Train Loss: 1.1673, Test Loss: 1.0280\n",
      "Epoch 16/20, Train Loss: 1.0814, Test Loss: 1.0274\n",
      "Epoch 17/20, Train Loss: 1.1439, Test Loss: 1.0287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:34:06,096] Trial 217 finished with value: 1.0263534784317017 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 129, 'layer_1_size': 242, 'layer_2_size': 215, 'layer_3_size': 74, 'layer_4_size': 198, 'layer_5_size': 202, 'dropout_rate': 0.4223279703957729, 'learning_rate': 0.0002087369445276962, 'batch_size': 256, 'epochs': 20}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20, Train Loss: 1.1103, Test Loss: 1.0283\n",
      "Epoch 19/20, Train Loss: 1.1427, Test Loss: 1.0280\n",
      "Epoch 20/20, Train Loss: 1.1133, Test Loss: 1.0264\n",
      "Epoch 1/24, Train Loss: 1.1719, Test Loss: 1.1263\n",
      "Epoch 2/24, Train Loss: 1.1969, Test Loss: 1.1172\n",
      "Epoch 3/24, Train Loss: 1.1512, Test Loss: 1.1030\n",
      "Epoch 4/24, Train Loss: 1.1576, Test Loss: 1.1071\n",
      "Epoch 5/24, Train Loss: 1.1199, Test Loss: 1.1023\n",
      "Epoch 6/24, Train Loss: 1.1077, Test Loss: 1.1183\n",
      "Epoch 7/24, Train Loss: 1.1510, Test Loss: 1.1200\n",
      "Epoch 8/24, Train Loss: 1.1435, Test Loss: 1.1201\n",
      "Epoch 9/24, Train Loss: 1.1049, Test Loss: 1.1181\n",
      "Epoch 10/24, Train Loss: 1.0947, Test Loss: 1.1128\n",
      "Epoch 11/24, Train Loss: 1.1045, Test Loss: 1.1072\n",
      "Epoch 12/24, Train Loss: 1.0954, Test Loss: 1.1122\n",
      "Epoch 13/24, Train Loss: 1.0704, Test Loss: 1.1206\n",
      "Epoch 14/24, Train Loss: 1.1006, Test Loss: 1.1228\n",
      "Epoch 15/24, Train Loss: 1.0863, Test Loss: 1.1076\n",
      "Epoch 16/24, Train Loss: 1.0441, Test Loss: 1.1023\n",
      "Epoch 17/24, Train Loss: 1.1030, Test Loss: 1.1033\n",
      "Epoch 18/24, Train Loss: 1.0814, Test Loss: 1.1018\n",
      "Epoch 19/24, Train Loss: 1.0717, Test Loss: 1.1038\n",
      "Epoch 20/24, Train Loss: 1.0507, Test Loss: 1.1080\n",
      "Epoch 21/24, Train Loss: 1.0707, Test Loss: 1.1113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:34:07,973] Trial 218 finished with value: 1.1050083935260773 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 121, 'layer_1_size': 180, 'layer_2_size': 229, 'layer_3_size': 100, 'layer_4_size': 43, 'layer_5_size': 224, 'layer_6_size': 149, 'dropout_rate': 0.33842810180595045, 'learning_rate': 0.0001486704073353566, 'batch_size': 64, 'epochs': 24}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/24, Train Loss: 1.1184, Test Loss: 1.1031\n",
      "Epoch 23/24, Train Loss: 1.0513, Test Loss: 1.0993\n",
      "Epoch 24/24, Train Loss: 1.0547, Test Loss: 1.1050\n",
      "Epoch 1/18, Train Loss: 1.1740, Test Loss: 1.0860\n",
      "Epoch 2/18, Train Loss: 1.1533, Test Loss: 1.0916\n",
      "Epoch 3/18, Train Loss: 1.1425, Test Loss: 1.0647\n",
      "Epoch 4/18, Train Loss: 1.1240, Test Loss: 1.0534\n",
      "Epoch 5/18, Train Loss: 1.1279, Test Loss: 1.0540\n",
      "Epoch 6/18, Train Loss: 1.1507, Test Loss: 1.0602\n",
      "Epoch 7/18, Train Loss: 1.1288, Test Loss: 1.0580\n",
      "Epoch 8/18, Train Loss: 1.1250, Test Loss: 1.0661\n",
      "Epoch 9/18, Train Loss: 1.1052, Test Loss: 1.0541\n",
      "Epoch 10/18, Train Loss: 1.1061, Test Loss: 1.0611\n",
      "Epoch 11/18, Train Loss: 1.0885, Test Loss: 1.0804\n",
      "Epoch 12/18, Train Loss: 1.1168, Test Loss: 1.0814\n",
      "Epoch 13/18, Train Loss: 1.0889, Test Loss: 1.0742\n",
      "Epoch 14/18, Train Loss: 1.1039, Test Loss: 1.0740\n",
      "Epoch 15/18, Train Loss: 1.0727, Test Loss: 1.0672\n",
      "Epoch 16/18, Train Loss: 1.1109, Test Loss: 1.0740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:34:09,377] Trial 219 finished with value: 1.0730373561382294 and parameters: {'num_hidden_layers': 8, 'layer_0_size': 140, 'layer_1_size': 108, 'layer_2_size': 206, 'layer_3_size': 250, 'layer_4_size': 131, 'layer_5_size': 158, 'layer_6_size': 206, 'layer_7_size': 113, 'dropout_rate': 0.2678832081560451, 'learning_rate': 0.0002456352022670592, 'batch_size': 64, 'epochs': 18}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/18, Train Loss: 1.1004, Test Loss: 1.0692\n",
      "Epoch 18/18, Train Loss: 1.0957, Test Loss: 1.0730\n",
      "Epoch 1/95, Train Loss: 1.2932, Test Loss: 1.0755\n",
      "Epoch 2/95, Train Loss: 1.2506, Test Loss: 1.0750\n",
      "Epoch 3/95, Train Loss: 1.3108, Test Loss: 1.0789\n",
      "Epoch 4/95, Train Loss: 1.2436, Test Loss: 1.0792\n",
      "Epoch 5/95, Train Loss: 1.2673, Test Loss: 1.0773\n",
      "Epoch 6/95, Train Loss: 1.2374, Test Loss: 1.0816\n",
      "Epoch 7/95, Train Loss: 1.3011, Test Loss: 1.0814\n",
      "Epoch 8/95, Train Loss: 1.2972, Test Loss: 1.0807\n",
      "Epoch 9/95, Train Loss: 1.3153, Test Loss: 1.0789\n",
      "Epoch 10/95, Train Loss: 1.3076, Test Loss: 1.0820\n",
      "Epoch 11/95, Train Loss: 1.2494, Test Loss: 1.0806\n",
      "Epoch 12/95, Train Loss: 1.2666, Test Loss: 1.0829\n",
      "Epoch 13/95, Train Loss: 1.2134, Test Loss: 1.0883\n",
      "Epoch 14/95, Train Loss: 1.2630, Test Loss: 1.0846\n",
      "Epoch 15/95, Train Loss: 1.1871, Test Loss: 1.0870\n",
      "Epoch 16/95, Train Loss: 1.2614, Test Loss: 1.0823\n",
      "Epoch 17/95, Train Loss: 1.2512, Test Loss: 1.0844\n",
      "Epoch 18/95, Train Loss: 1.3162, Test Loss: 1.0878\n",
      "Epoch 19/95, Train Loss: 1.2820, Test Loss: 1.0837\n",
      "Epoch 20/95, Train Loss: 1.2273, Test Loss: 1.0957\n",
      "Epoch 21/95, Train Loss: 1.1984, Test Loss: 1.0950\n",
      "Epoch 22/95, Train Loss: 1.2099, Test Loss: 1.0977\n",
      "Epoch 23/95, Train Loss: 1.2490, Test Loss: 1.0928\n",
      "Epoch 24/95, Train Loss: 1.2781, Test Loss: 1.0916\n",
      "Epoch 25/95, Train Loss: 1.1898, Test Loss: 1.0897\n",
      "Epoch 26/95, Train Loss: 1.2384, Test Loss: 1.0959\n",
      "Epoch 27/95, Train Loss: 1.2320, Test Loss: 1.0913\n",
      "Epoch 28/95, Train Loss: 1.2537, Test Loss: 1.0964\n",
      "Epoch 29/95, Train Loss: 1.2167, Test Loss: 1.0949\n",
      "Epoch 30/95, Train Loss: 1.2435, Test Loss: 1.0931\n",
      "Epoch 31/95, Train Loss: 1.2413, Test Loss: 1.0931\n",
      "Epoch 32/95, Train Loss: 1.2232, Test Loss: 1.0960\n",
      "Epoch 33/95, Train Loss: 1.1964, Test Loss: 1.0969\n",
      "Epoch 34/95, Train Loss: 1.2154, Test Loss: 1.1024\n",
      "Epoch 35/95, Train Loss: 1.2550, Test Loss: 1.0983\n",
      "Epoch 36/95, Train Loss: 1.2774, Test Loss: 1.1010\n",
      "Epoch 37/95, Train Loss: 1.2076, Test Loss: 1.0960\n",
      "Epoch 38/95, Train Loss: 1.2051, Test Loss: 1.0917\n",
      "Epoch 39/95, Train Loss: 1.2430, Test Loss: 1.0900\n",
      "Epoch 40/95, Train Loss: 1.2064, Test Loss: 1.0898\n",
      "Epoch 41/95, Train Loss: 1.2313, Test Loss: 1.1016\n",
      "Epoch 42/95, Train Loss: 1.2127, Test Loss: 1.1000\n",
      "Epoch 43/95, Train Loss: 1.2064, Test Loss: 1.0955\n",
      "Epoch 44/95, Train Loss: 1.2021, Test Loss: 1.0895\n",
      "Epoch 45/95, Train Loss: 1.1921, Test Loss: 1.0938\n",
      "Epoch 46/95, Train Loss: 1.2366, Test Loss: 1.0923\n",
      "Epoch 47/95, Train Loss: 1.2712, Test Loss: 1.0933\n",
      "Epoch 48/95, Train Loss: 1.1955, Test Loss: 1.1032\n",
      "Epoch 49/95, Train Loss: 1.2176, Test Loss: 1.0991\n",
      "Epoch 50/95, Train Loss: 1.2322, Test Loss: 1.0944\n",
      "Epoch 51/95, Train Loss: 1.2715, Test Loss: 1.0965\n",
      "Epoch 52/95, Train Loss: 1.1977, Test Loss: 1.0960\n",
      "Epoch 53/95, Train Loss: 1.2159, Test Loss: 1.0941\n",
      "Epoch 54/95, Train Loss: 1.2190, Test Loss: 1.0902\n",
      "Epoch 55/95, Train Loss: 1.1500, Test Loss: 1.0893\n",
      "Epoch 56/95, Train Loss: 1.2078, Test Loss: 1.1049\n",
      "Epoch 57/95, Train Loss: 1.2662, Test Loss: 1.0987\n",
      "Epoch 58/95, Train Loss: 1.2121, Test Loss: 1.0987\n",
      "Epoch 59/95, Train Loss: 1.2172, Test Loss: 1.1049\n",
      "Epoch 60/95, Train Loss: 1.1926, Test Loss: 1.1082\n",
      "Epoch 61/95, Train Loss: 1.1245, Test Loss: 1.1031\n",
      "Epoch 62/95, Train Loss: 1.1722, Test Loss: 1.0970\n",
      "Epoch 63/95, Train Loss: 1.2145, Test Loss: 1.0993\n",
      "Epoch 64/95, Train Loss: 1.1819, Test Loss: 1.0948\n",
      "Epoch 65/95, Train Loss: 1.1987, Test Loss: 1.1001\n",
      "Epoch 66/95, Train Loss: 1.1921, Test Loss: 1.1008\n",
      "Epoch 67/95, Train Loss: 1.2139, Test Loss: 1.0945\n",
      "Epoch 68/95, Train Loss: 1.1897, Test Loss: 1.0991\n",
      "Epoch 69/95, Train Loss: 1.2151, Test Loss: 1.1012\n",
      "Epoch 70/95, Train Loss: 1.1877, Test Loss: 1.0969\n",
      "Epoch 71/95, Train Loss: 1.1697, Test Loss: 1.0946\n",
      "Epoch 72/95, Train Loss: 1.1559, Test Loss: 1.1017\n",
      "Epoch 73/95, Train Loss: 1.2418, Test Loss: 1.1014\n",
      "Epoch 74/95, Train Loss: 1.2040, Test Loss: 1.0959\n",
      "Epoch 75/95, Train Loss: 1.2009, Test Loss: 1.0935\n",
      "Epoch 76/95, Train Loss: 1.2215, Test Loss: 1.1006\n",
      "Epoch 77/95, Train Loss: 1.1833, Test Loss: 1.0953\n",
      "Epoch 78/95, Train Loss: 1.1616, Test Loss: 1.0958\n",
      "Epoch 79/95, Train Loss: 1.1690, Test Loss: 1.1003\n",
      "Epoch 80/95, Train Loss: 1.1563, Test Loss: 1.0938\n",
      "Epoch 81/95, Train Loss: 1.1503, Test Loss: 1.0969\n",
      "Epoch 82/95, Train Loss: 1.1769, Test Loss: 1.0936\n",
      "Epoch 83/95, Train Loss: 1.2205, Test Loss: 1.1001\n",
      "Epoch 84/95, Train Loss: 1.1306, Test Loss: 1.0986\n",
      "Epoch 85/95, Train Loss: 1.1057, Test Loss: 1.0952\n",
      "Epoch 86/95, Train Loss: 1.2133, Test Loss: 1.0928\n",
      "Epoch 87/95, Train Loss: 1.1516, Test Loss: 1.0934\n",
      "Epoch 88/95, Train Loss: 1.1669, Test Loss: 1.0930\n",
      "Epoch 89/95, Train Loss: 1.1553, Test Loss: 1.0993\n",
      "Epoch 90/95, Train Loss: 1.1727, Test Loss: 1.1002\n",
      "Epoch 91/95, Train Loss: 1.2000, Test Loss: 1.0939\n",
      "Epoch 92/95, Train Loss: 1.1994, Test Loss: 1.1002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:34:16,710] Trial 220 finished with value: 1.1004440784454346 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 100, 'layer_1_size': 195, 'layer_2_size': 247, 'layer_3_size': 37, 'layer_4_size': 178, 'layer_5_size': 176, 'layer_6_size': 189, 'dropout_rate': 0.4559917504878886, 'learning_rate': 2.395108712799093e-05, 'batch_size': 64, 'epochs': 95}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/95, Train Loss: 1.1610, Test Loss: 1.0971\n",
      "Epoch 94/95, Train Loss: 1.1402, Test Loss: 1.1061\n",
      "Epoch 95/95, Train Loss: 1.1904, Test Loss: 1.1004\n",
      "Epoch 1/75, Train Loss: 1.3158, Test Loss: 1.0764\n",
      "Epoch 2/75, Train Loss: 1.2907, Test Loss: 1.0790\n",
      "Epoch 3/75, Train Loss: 1.2525, Test Loss: 1.0869\n",
      "Epoch 4/75, Train Loss: 1.2510, Test Loss: 1.0852\n",
      "Epoch 5/75, Train Loss: 1.1989, Test Loss: 1.0834\n",
      "Epoch 6/75, Train Loss: 1.2492, Test Loss: 1.0905\n",
      "Epoch 7/75, Train Loss: 1.2574, Test Loss: 1.0879\n",
      "Epoch 8/75, Train Loss: 1.2697, Test Loss: 1.0848\n",
      "Epoch 9/75, Train Loss: 1.1716, Test Loss: 1.0865\n",
      "Epoch 10/75, Train Loss: 1.1869, Test Loss: 1.0860\n",
      "Epoch 11/75, Train Loss: 1.2293, Test Loss: 1.0810\n",
      "Epoch 12/75, Train Loss: 1.2405, Test Loss: 1.0745\n",
      "Epoch 13/75, Train Loss: 1.1599, Test Loss: 1.0826\n",
      "Epoch 14/75, Train Loss: 1.2331, Test Loss: 1.0798\n",
      "Epoch 15/75, Train Loss: 1.2014, Test Loss: 1.0854\n",
      "Epoch 16/75, Train Loss: 1.1953, Test Loss: 1.0824\n",
      "Epoch 17/75, Train Loss: 1.1855, Test Loss: 1.0836\n",
      "Epoch 18/75, Train Loss: 1.1830, Test Loss: 1.0806\n",
      "Epoch 19/75, Train Loss: 1.1971, Test Loss: 1.0797\n",
      "Epoch 20/75, Train Loss: 1.1942, Test Loss: 1.0788\n",
      "Epoch 21/75, Train Loss: 1.1633, Test Loss: 1.0871\n",
      "Epoch 22/75, Train Loss: 1.1534, Test Loss: 1.0828\n",
      "Epoch 23/75, Train Loss: 1.2177, Test Loss: 1.0786\n",
      "Epoch 24/75, Train Loss: 1.1882, Test Loss: 1.0822\n",
      "Epoch 25/75, Train Loss: 1.1766, Test Loss: 1.0833\n",
      "Epoch 26/75, Train Loss: 1.1761, Test Loss: 1.0835\n",
      "Epoch 27/75, Train Loss: 1.1409, Test Loss: 1.0841\n",
      "Epoch 28/75, Train Loss: 1.1181, Test Loss: 1.0781\n",
      "Epoch 29/75, Train Loss: 1.1494, Test Loss: 1.0780\n",
      "Epoch 30/75, Train Loss: 1.1428, Test Loss: 1.0731\n",
      "Epoch 31/75, Train Loss: 1.1665, Test Loss: 1.0732\n",
      "Epoch 32/75, Train Loss: 1.1505, Test Loss: 1.0707\n",
      "Epoch 33/75, Train Loss: 1.1415, Test Loss: 1.0699\n",
      "Epoch 34/75, Train Loss: 1.1073, Test Loss: 1.0704\n",
      "Epoch 35/75, Train Loss: 1.1358, Test Loss: 1.0768\n",
      "Epoch 36/75, Train Loss: 1.0959, Test Loss: 1.0744\n",
      "Epoch 37/75, Train Loss: 1.0742, Test Loss: 1.0716\n",
      "Epoch 38/75, Train Loss: 1.1395, Test Loss: 1.0674\n",
      "Epoch 39/75, Train Loss: 1.1236, Test Loss: 1.0678\n",
      "Epoch 40/75, Train Loss: 1.1817, Test Loss: 1.0694\n",
      "Epoch 41/75, Train Loss: 1.1229, Test Loss: 1.0682\n",
      "Epoch 42/75, Train Loss: 1.1114, Test Loss: 1.0677\n",
      "Epoch 43/75, Train Loss: 1.0893, Test Loss: 1.0676\n",
      "Epoch 44/75, Train Loss: 1.0945, Test Loss: 1.0701\n",
      "Epoch 45/75, Train Loss: 1.1091, Test Loss: 1.0675\n",
      "Epoch 46/75, Train Loss: 1.0867, Test Loss: 1.0688\n",
      "Epoch 47/75, Train Loss: 1.1358, Test Loss: 1.0708\n",
      "Epoch 48/75, Train Loss: 1.1195, Test Loss: 1.0718\n",
      "Epoch 49/75, Train Loss: 1.1115, Test Loss: 1.0730\n",
      "Epoch 50/75, Train Loss: 1.0955, Test Loss: 1.0694\n",
      "Epoch 51/75, Train Loss: 1.1232, Test Loss: 1.0670\n",
      "Epoch 52/75, Train Loss: 1.1132, Test Loss: 1.0670\n",
      "Epoch 53/75, Train Loss: 1.0419, Test Loss: 1.0666\n",
      "Epoch 54/75, Train Loss: 1.0994, Test Loss: 1.0697\n",
      "Epoch 55/75, Train Loss: 1.1147, Test Loss: 1.0704\n",
      "Epoch 56/75, Train Loss: 1.1071, Test Loss: 1.0723\n",
      "Epoch 57/75, Train Loss: 1.0801, Test Loss: 1.0720\n",
      "Epoch 58/75, Train Loss: 1.0821, Test Loss: 1.0707\n",
      "Epoch 59/75, Train Loss: 1.0733, Test Loss: 1.0704\n",
      "Epoch 60/75, Train Loss: 1.0885, Test Loss: 1.0722\n",
      "Epoch 61/75, Train Loss: 1.1014, Test Loss: 1.0731\n",
      "Epoch 62/75, Train Loss: 1.0807, Test Loss: 1.0688\n",
      "Epoch 63/75, Train Loss: 1.1152, Test Loss: 1.0711\n",
      "Epoch 64/75, Train Loss: 1.0721, Test Loss: 1.0703\n",
      "Epoch 65/75, Train Loss: 1.0638, Test Loss: 1.0704\n",
      "Epoch 66/75, Train Loss: 1.0760, Test Loss: 1.0740\n",
      "Epoch 67/75, Train Loss: 1.0824, Test Loss: 1.0718\n",
      "Epoch 68/75, Train Loss: 1.0895, Test Loss: 1.0744\n",
      "Epoch 69/75, Train Loss: 1.0599, Test Loss: 1.0729\n",
      "Epoch 70/75, Train Loss: 1.0726, Test Loss: 1.0752\n",
      "Epoch 71/75, Train Loss: 1.0787, Test Loss: 1.0742\n",
      "Epoch 72/75, Train Loss: 1.0587, Test Loss: 1.0716\n",
      "Epoch 73/75, Train Loss: 1.0630, Test Loss: 1.0717\n",
      "Epoch 74/75, Train Loss: 1.1090, Test Loss: 1.0700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:34:22,467] Trial 221 finished with value: 1.073871374130249 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 108, 'layer_1_size': 192, 'layer_2_size': 198, 'layer_3_size': 65, 'layer_4_size': 229, 'layer_5_size': 229, 'layer_6_size': 155, 'dropout_rate': 0.4741245307811697, 'learning_rate': 9.788539852418074e-05, 'batch_size': 64, 'epochs': 75}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/75, Train Loss: 1.0565, Test Loss: 1.0739\n",
      "Epoch 1/73, Train Loss: 1.3370, Test Loss: 0.9320\n",
      "Epoch 2/73, Train Loss: 1.3120, Test Loss: 0.9388\n",
      "Epoch 3/73, Train Loss: 1.2634, Test Loss: 0.9433\n",
      "Epoch 4/73, Train Loss: 1.2972, Test Loss: 0.9426\n",
      "Epoch 5/73, Train Loss: 1.2654, Test Loss: 0.9424\n",
      "Epoch 6/73, Train Loss: 1.2537, Test Loss: 0.9373\n",
      "Epoch 7/73, Train Loss: 1.2487, Test Loss: 0.9380\n",
      "Epoch 8/73, Train Loss: 1.2655, Test Loss: 0.9363\n",
      "Epoch 9/73, Train Loss: 1.3132, Test Loss: 0.9345\n",
      "Epoch 10/73, Train Loss: 1.2661, Test Loss: 0.9339\n",
      "Epoch 11/73, Train Loss: 1.2322, Test Loss: 0.9351\n",
      "Epoch 12/73, Train Loss: 1.2860, Test Loss: 0.9351\n",
      "Epoch 13/73, Train Loss: 1.2518, Test Loss: 0.9354\n",
      "Epoch 14/73, Train Loss: 1.2246, Test Loss: 0.9366\n",
      "Epoch 15/73, Train Loss: 1.2210, Test Loss: 0.9362\n",
      "Epoch 16/73, Train Loss: 1.2266, Test Loss: 0.9376\n",
      "Epoch 17/73, Train Loss: 1.2454, Test Loss: 0.9359\n",
      "Epoch 18/73, Train Loss: 1.2432, Test Loss: 0.9349\n",
      "Epoch 19/73, Train Loss: 1.2143, Test Loss: 0.9320\n",
      "Epoch 20/73, Train Loss: 1.2164, Test Loss: 0.9328\n",
      "Epoch 21/73, Train Loss: 1.2236, Test Loss: 0.9318\n",
      "Epoch 22/73, Train Loss: 1.1740, Test Loss: 0.9347\n",
      "Epoch 23/73, Train Loss: 1.1984, Test Loss: 0.9306\n",
      "Epoch 24/73, Train Loss: 1.2189, Test Loss: 0.9313\n",
      "Epoch 25/73, Train Loss: 1.1832, Test Loss: 0.9309\n",
      "Epoch 26/73, Train Loss: 1.2007, Test Loss: 0.9291\n",
      "Epoch 27/73, Train Loss: 1.2141, Test Loss: 0.9306\n",
      "Epoch 28/73, Train Loss: 1.1685, Test Loss: 0.9313\n",
      "Epoch 29/73, Train Loss: 1.1710, Test Loss: 0.9305\n",
      "Epoch 30/73, Train Loss: 1.2346, Test Loss: 0.9283\n",
      "Epoch 31/73, Train Loss: 1.1710, Test Loss: 0.9295\n",
      "Epoch 32/73, Train Loss: 1.1501, Test Loss: 0.9286\n",
      "Epoch 33/73, Train Loss: 1.2075, Test Loss: 0.9279\n",
      "Epoch 34/73, Train Loss: 1.1524, Test Loss: 0.9295\n",
      "Epoch 35/73, Train Loss: 1.1823, Test Loss: 0.9297\n",
      "Epoch 36/73, Train Loss: 1.1992, Test Loss: 0.9299\n",
      "Epoch 37/73, Train Loss: 1.1617, Test Loss: 0.9270\n",
      "Epoch 38/73, Train Loss: 1.2013, Test Loss: 0.9278\n",
      "Epoch 39/73, Train Loss: 1.1332, Test Loss: 0.9286\n",
      "Epoch 40/73, Train Loss: 1.1910, Test Loss: 0.9285\n",
      "Epoch 41/73, Train Loss: 1.1891, Test Loss: 0.9274\n",
      "Epoch 42/73, Train Loss: 1.1682, Test Loss: 0.9294\n",
      "Epoch 43/73, Train Loss: 1.1661, Test Loss: 0.9287\n",
      "Epoch 44/73, Train Loss: 1.1859, Test Loss: 0.9284\n",
      "Epoch 45/73, Train Loss: 1.1618, Test Loss: 0.9273\n",
      "Epoch 46/73, Train Loss: 1.1698, Test Loss: 0.9293\n",
      "Epoch 47/73, Train Loss: 1.1329, Test Loss: 0.9280\n",
      "Epoch 48/73, Train Loss: 1.1950, Test Loss: 0.9261\n",
      "Epoch 49/73, Train Loss: 1.1499, Test Loss: 0.9267\n",
      "Epoch 50/73, Train Loss: 1.1694, Test Loss: 0.9285\n",
      "Epoch 51/73, Train Loss: 1.1528, Test Loss: 0.9276\n",
      "Epoch 52/73, Train Loss: 1.1438, Test Loss: 0.9278\n",
      "Epoch 53/73, Train Loss: 1.1658, Test Loss: 0.9267\n",
      "Epoch 54/73, Train Loss: 1.1447, Test Loss: 0.9268\n",
      "Epoch 55/73, Train Loss: 1.1551, Test Loss: 0.9278\n",
      "Epoch 56/73, Train Loss: 1.1231, Test Loss: 0.9287\n",
      "Epoch 57/73, Train Loss: 1.1432, Test Loss: 0.9275\n",
      "Epoch 58/73, Train Loss: 1.1409, Test Loss: 0.9271\n",
      "Epoch 59/73, Train Loss: 1.1392, Test Loss: 0.9264\n",
      "Epoch 60/73, Train Loss: 1.1259, Test Loss: 0.9274\n",
      "Epoch 61/73, Train Loss: 1.1316, Test Loss: 0.9270\n",
      "Epoch 62/73, Train Loss: 1.1749, Test Loss: 0.9279\n",
      "Epoch 63/73, Train Loss: 1.1544, Test Loss: 0.9275\n",
      "Epoch 64/73, Train Loss: 1.1668, Test Loss: 0.9272\n",
      "Epoch 65/73, Train Loss: 1.1675, Test Loss: 0.9280\n",
      "Epoch 66/73, Train Loss: 1.1183, Test Loss: 0.9271\n",
      "Epoch 67/73, Train Loss: 1.1519, Test Loss: 0.9266\n",
      "Epoch 68/73, Train Loss: 1.1436, Test Loss: 0.9280\n",
      "Epoch 69/73, Train Loss: 1.1564, Test Loss: 0.9288\n",
      "Epoch 70/73, Train Loss: 1.1263, Test Loss: 0.9270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:34:28,861] Trial 222 finished with value: 0.9272587820887566 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 113, 'layer_1_size': 176, 'layer_2_size': 210, 'layer_3_size': 55, 'layer_4_size': 227, 'layer_5_size': 224, 'layer_6_size': 159, 'dropout_rate': 0.4994164302553089, 'learning_rate': 8.33270035702393e-05, 'batch_size': 64, 'epochs': 73}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/73, Train Loss: 1.1196, Test Loss: 0.9272\n",
      "Epoch 72/73, Train Loss: 1.1167, Test Loss: 0.9277\n",
      "Epoch 73/73, Train Loss: 1.1432, Test Loss: 0.9273\n",
      "Epoch 1/45, Train Loss: 1.4308, Test Loss: 0.8710\n",
      "Epoch 2/45, Train Loss: 1.3646, Test Loss: 0.8799\n",
      "Epoch 3/45, Train Loss: 1.3263, Test Loss: 0.8773\n",
      "Epoch 4/45, Train Loss: 1.3745, Test Loss: 0.8782\n",
      "Epoch 5/45, Train Loss: 1.3789, Test Loss: 0.8735\n",
      "Epoch 6/45, Train Loss: 1.3019, Test Loss: 0.8693\n",
      "Epoch 7/45, Train Loss: 1.3117, Test Loss: 0.8710\n",
      "Epoch 8/45, Train Loss: 1.2180, Test Loss: 0.8654\n",
      "Epoch 9/45, Train Loss: 1.3128, Test Loss: 0.8635\n",
      "Epoch 10/45, Train Loss: 1.2839, Test Loss: 0.8633\n",
      "Epoch 11/45, Train Loss: 1.3084, Test Loss: 0.8588\n",
      "Epoch 12/45, Train Loss: 1.3170, Test Loss: 0.8607\n",
      "Epoch 13/45, Train Loss: 1.2336, Test Loss: 0.8594\n",
      "Epoch 14/45, Train Loss: 1.2299, Test Loss: 0.8612\n",
      "Epoch 15/45, Train Loss: 1.2395, Test Loss: 0.8604\n",
      "Epoch 16/45, Train Loss: 1.2373, Test Loss: 0.8565\n",
      "Epoch 17/45, Train Loss: 1.1669, Test Loss: 0.8564\n",
      "Epoch 18/45, Train Loss: 1.2355, Test Loss: 0.8538\n",
      "Epoch 19/45, Train Loss: 1.2396, Test Loss: 0.8557\n",
      "Epoch 20/45, Train Loss: 1.2231, Test Loss: 0.8567\n",
      "Epoch 21/45, Train Loss: 1.2213, Test Loss: 0.8560\n",
      "Epoch 22/45, Train Loss: 1.1813, Test Loss: 0.8562\n",
      "Epoch 23/45, Train Loss: 1.1939, Test Loss: 0.8570\n",
      "Epoch 24/45, Train Loss: 1.2581, Test Loss: 0.8551\n",
      "Epoch 25/45, Train Loss: 1.2309, Test Loss: 0.8559\n",
      "Epoch 26/45, Train Loss: 1.2590, Test Loss: 0.8558\n",
      "Epoch 27/45, Train Loss: 1.1688, Test Loss: 0.8555\n",
      "Epoch 28/45, Train Loss: 1.2052, Test Loss: 0.8569\n",
      "Epoch 29/45, Train Loss: 1.1338, Test Loss: 0.8573\n",
      "Epoch 30/45, Train Loss: 1.1934, Test Loss: 0.8570\n",
      "Epoch 31/45, Train Loss: 1.2359, Test Loss: 0.8588\n",
      "Epoch 32/45, Train Loss: 1.1943, Test Loss: 0.8581\n",
      "Epoch 33/45, Train Loss: 1.1779, Test Loss: 0.8564\n",
      "Epoch 34/45, Train Loss: 1.2255, Test Loss: 0.8551\n",
      "Epoch 35/45, Train Loss: 1.2487, Test Loss: 0.8588\n",
      "Epoch 36/45, Train Loss: 1.1887, Test Loss: 0.8566\n",
      "Epoch 37/45, Train Loss: 1.1601, Test Loss: 0.8565\n",
      "Epoch 38/45, Train Loss: 1.1715, Test Loss: 0.8581\n",
      "Epoch 39/45, Train Loss: 1.1551, Test Loss: 0.8623\n",
      "Epoch 40/45, Train Loss: 1.1935, Test Loss: 0.8605\n",
      "Epoch 41/45, Train Loss: 1.1962, Test Loss: 0.8617\n",
      "Epoch 42/45, Train Loss: 1.1915, Test Loss: 0.8611\n",
      "Epoch 43/45, Train Loss: 1.1278, Test Loss: 0.8610\n",
      "Epoch 44/45, Train Loss: 1.1700, Test Loss: 0.8592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:34:34,194] Trial 223 finished with value: 0.8572713285684586 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 117, 'layer_1_size': 189, 'layer_2_size': 203, 'layer_3_size': 70, 'layer_4_size': 241, 'layer_5_size': 237, 'layer_6_size': 151, 'dropout_rate': 0.48136259372972107, 'learning_rate': 6.281216404327772e-05, 'batch_size': 64, 'epochs': 45}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/45, Train Loss: 1.1440, Test Loss: 0.8573\n",
      "Epoch 1/78, Train Loss: 1.2154, Test Loss: 1.0781\n",
      "Epoch 2/78, Train Loss: 1.2477, Test Loss: 1.0729\n",
      "Epoch 3/78, Train Loss: 1.2170, Test Loss: 1.0690\n",
      "Epoch 4/78, Train Loss: 1.2774, Test Loss: 1.0629\n",
      "Epoch 5/78, Train Loss: 1.2519, Test Loss: 1.0650\n",
      "Epoch 6/78, Train Loss: 1.1923, Test Loss: 1.0528\n",
      "Epoch 7/78, Train Loss: 1.2575, Test Loss: 1.0545\n",
      "Epoch 8/78, Train Loss: 1.1844, Test Loss: 1.0584\n",
      "Epoch 9/78, Train Loss: 1.1778, Test Loss: 1.0540\n",
      "Epoch 10/78, Train Loss: 1.2255, Test Loss: 1.0534\n",
      "Epoch 11/78, Train Loss: 1.1551, Test Loss: 1.0509\n",
      "Epoch 12/78, Train Loss: 1.2045, Test Loss: 1.0514\n",
      "Epoch 13/78, Train Loss: 1.1755, Test Loss: 1.0399\n",
      "Epoch 14/78, Train Loss: 1.1531, Test Loss: 1.0437\n",
      "Epoch 15/78, Train Loss: 1.2416, Test Loss: 1.0403\n",
      "Epoch 16/78, Train Loss: 1.1571, Test Loss: 1.0396\n",
      "Epoch 17/78, Train Loss: 1.1327, Test Loss: 1.0479\n",
      "Epoch 18/78, Train Loss: 1.1148, Test Loss: 1.0515\n",
      "Epoch 19/78, Train Loss: 1.2430, Test Loss: 1.0472\n",
      "Epoch 20/78, Train Loss: 1.1671, Test Loss: 1.0411\n",
      "Epoch 21/78, Train Loss: 1.1778, Test Loss: 1.0464\n",
      "Epoch 22/78, Train Loss: 1.1487, Test Loss: 1.0466\n",
      "Epoch 23/78, Train Loss: 1.1673, Test Loss: 1.0462\n",
      "Epoch 24/78, Train Loss: 1.1336, Test Loss: 1.0355\n",
      "Epoch 25/78, Train Loss: 1.1604, Test Loss: 1.0387\n",
      "Epoch 26/78, Train Loss: 1.1538, Test Loss: 1.0491\n",
      "Epoch 27/78, Train Loss: 1.1357, Test Loss: 1.0398\n",
      "Epoch 28/78, Train Loss: 1.1325, Test Loss: 1.0407\n",
      "Epoch 29/78, Train Loss: 1.1203, Test Loss: 1.0415\n",
      "Epoch 30/78, Train Loss: 1.1352, Test Loss: 1.0358\n",
      "Epoch 31/78, Train Loss: 1.1041, Test Loss: 1.0387\n",
      "Epoch 32/78, Train Loss: 1.1288, Test Loss: 1.0370\n",
      "Epoch 33/78, Train Loss: 1.0726, Test Loss: 1.0315\n",
      "Epoch 34/78, Train Loss: 1.1227, Test Loss: 1.0330\n",
      "Epoch 35/78, Train Loss: 1.0768, Test Loss: 1.0370\n",
      "Epoch 36/78, Train Loss: 1.0900, Test Loss: 1.0377\n",
      "Epoch 37/78, Train Loss: 1.1665, Test Loss: 1.0390\n",
      "Epoch 38/78, Train Loss: 1.1024, Test Loss: 1.0368\n",
      "Epoch 39/78, Train Loss: 1.1126, Test Loss: 1.0318\n",
      "Epoch 40/78, Train Loss: 1.1423, Test Loss: 1.0361\n",
      "Epoch 41/78, Train Loss: 1.0733, Test Loss: 1.0346\n",
      "Epoch 42/78, Train Loss: 1.0996, Test Loss: 1.0437\n",
      "Epoch 43/78, Train Loss: 1.1105, Test Loss: 1.0430\n",
      "Epoch 44/78, Train Loss: 1.1679, Test Loss: 1.0439\n",
      "Epoch 45/78, Train Loss: 1.1072, Test Loss: 1.0466\n",
      "Epoch 46/78, Train Loss: 1.1403, Test Loss: 1.0463\n",
      "Epoch 47/78, Train Loss: 1.1182, Test Loss: 1.0437\n",
      "Epoch 48/78, Train Loss: 1.1055, Test Loss: 1.0453\n",
      "Epoch 49/78, Train Loss: 1.0929, Test Loss: 1.0424\n",
      "Epoch 50/78, Train Loss: 1.1027, Test Loss: 1.0509\n",
      "Epoch 51/78, Train Loss: 1.0797, Test Loss: 1.0359\n",
      "Epoch 52/78, Train Loss: 1.0972, Test Loss: 1.0334\n",
      "Epoch 53/78, Train Loss: 1.0717, Test Loss: 1.0369\n",
      "Epoch 54/78, Train Loss: 1.0778, Test Loss: 1.0408\n",
      "Epoch 55/78, Train Loss: 1.0906, Test Loss: 1.0360\n",
      "Epoch 56/78, Train Loss: 1.1014, Test Loss: 1.0305\n",
      "Epoch 57/78, Train Loss: 1.1281, Test Loss: 1.0400\n",
      "Epoch 58/78, Train Loss: 1.0748, Test Loss: 1.0341\n",
      "Epoch 59/78, Train Loss: 1.0976, Test Loss: 1.0362\n",
      "Epoch 60/78, Train Loss: 1.0934, Test Loss: 1.0410\n",
      "Epoch 61/78, Train Loss: 1.0930, Test Loss: 1.0418\n",
      "Epoch 62/78, Train Loss: 1.0735, Test Loss: 1.0392\n",
      "Epoch 63/78, Train Loss: 1.0641, Test Loss: 1.0415\n",
      "Epoch 64/78, Train Loss: 1.0694, Test Loss: 1.0446\n",
      "Epoch 65/78, Train Loss: 1.0963, Test Loss: 1.0390\n",
      "Epoch 66/78, Train Loss: 1.1113, Test Loss: 1.0395\n",
      "Epoch 67/78, Train Loss: 1.0810, Test Loss: 1.0408\n",
      "Epoch 68/78, Train Loss: 1.1172, Test Loss: 1.0365\n",
      "Epoch 69/78, Train Loss: 1.0962, Test Loss: 1.0443\n",
      "Epoch 70/78, Train Loss: 1.0741, Test Loss: 1.0434\n",
      "Epoch 71/78, Train Loss: 1.0782, Test Loss: 1.0424\n",
      "Epoch 72/78, Train Loss: 1.0742, Test Loss: 1.0442\n",
      "Epoch 73/78, Train Loss: 1.0528, Test Loss: 1.0390\n",
      "Epoch 74/78, Train Loss: 1.0873, Test Loss: 1.0466\n",
      "Epoch 75/78, Train Loss: 1.0687, Test Loss: 1.0386\n",
      "Epoch 76/78, Train Loss: 1.0650, Test Loss: 1.0410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:34:48,015] Trial 224 finished with value: 1.0420143178531103 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 108, 'layer_1_size': 198, 'layer_2_size': 115, 'layer_3_size': 218, 'layer_4_size': 217, 'layer_5_size': 245, 'layer_6_size': 137, 'dropout_rate': 0.4683125169673494, 'learning_rate': 0.00011863360580197263, 'batch_size': 32, 'epochs': 78}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/78, Train Loss: 1.0467, Test Loss: 1.0406\n",
      "Epoch 78/78, Train Loss: 1.0638, Test Loss: 1.0420\n",
      "Epoch 1/98, Train Loss: 1.1806, Test Loss: 0.9974\n",
      "Epoch 2/98, Train Loss: 1.1839, Test Loss: 1.0015\n",
      "Epoch 3/98, Train Loss: 1.1300, Test Loss: 1.0008\n",
      "Epoch 4/98, Train Loss: 1.1893, Test Loss: 0.9973\n",
      "Epoch 5/98, Train Loss: 1.1120, Test Loss: 0.9880\n",
      "Epoch 6/98, Train Loss: 1.1076, Test Loss: 0.9798\n",
      "Epoch 7/98, Train Loss: 1.0688, Test Loss: 0.9827\n",
      "Epoch 8/98, Train Loss: 1.0879, Test Loss: 0.9801\n",
      "Epoch 9/98, Train Loss: 1.0866, Test Loss: 0.9835\n",
      "Epoch 10/98, Train Loss: 1.0065, Test Loss: 0.9775\n",
      "Epoch 11/98, Train Loss: 1.0652, Test Loss: 0.9850\n",
      "Epoch 12/98, Train Loss: 1.1244, Test Loss: 0.9817\n",
      "Epoch 13/98, Train Loss: 1.0482, Test Loss: 0.9783\n",
      "Epoch 14/98, Train Loss: 1.0506, Test Loss: 0.9801\n",
      "Epoch 15/98, Train Loss: 1.0677, Test Loss: 0.9754\n",
      "Epoch 16/98, Train Loss: 1.0974, Test Loss: 0.9745\n",
      "Epoch 17/98, Train Loss: 1.0598, Test Loss: 0.9712\n",
      "Epoch 18/98, Train Loss: 1.0386, Test Loss: 0.9740\n",
      "Epoch 19/98, Train Loss: 1.0471, Test Loss: 0.9655\n",
      "Epoch 20/98, Train Loss: 1.0532, Test Loss: 0.9637\n",
      "Epoch 21/98, Train Loss: 1.0451, Test Loss: 0.9643\n",
      "Epoch 22/98, Train Loss: 1.0013, Test Loss: 0.9706\n",
      "Epoch 23/98, Train Loss: 1.0526, Test Loss: 0.9684\n",
      "Epoch 24/98, Train Loss: 1.0526, Test Loss: 0.9689\n",
      "Epoch 25/98, Train Loss: 1.0585, Test Loss: 0.9711\n",
      "Epoch 26/98, Train Loss: 1.0530, Test Loss: 0.9692\n",
      "Epoch 27/98, Train Loss: 1.0415, Test Loss: 0.9687\n",
      "Epoch 28/98, Train Loss: 1.0055, Test Loss: 0.9700\n",
      "Epoch 29/98, Train Loss: 1.0481, Test Loss: 0.9691\n",
      "Epoch 30/98, Train Loss: 1.0548, Test Loss: 0.9716\n",
      "Epoch 31/98, Train Loss: 1.0421, Test Loss: 0.9706\n",
      "Epoch 32/98, Train Loss: 1.0289, Test Loss: 0.9690\n",
      "Epoch 33/98, Train Loss: 1.0533, Test Loss: 0.9710\n",
      "Epoch 34/98, Train Loss: 1.0530, Test Loss: 0.9719\n",
      "Epoch 35/98, Train Loss: 1.0350, Test Loss: 0.9728\n",
      "Epoch 36/98, Train Loss: 1.0141, Test Loss: 0.9720\n",
      "Epoch 37/98, Train Loss: 1.0311, Test Loss: 0.9739\n",
      "Epoch 38/98, Train Loss: 1.0017, Test Loss: 0.9700\n",
      "Epoch 39/98, Train Loss: 1.0347, Test Loss: 0.9678\n",
      "Epoch 40/98, Train Loss: 1.0065, Test Loss: 0.9653\n",
      "Epoch 41/98, Train Loss: 1.0257, Test Loss: 0.9650\n",
      "Epoch 42/98, Train Loss: 1.0025, Test Loss: 0.9643\n",
      "Epoch 43/98, Train Loss: 1.0212, Test Loss: 0.9647\n",
      "Epoch 44/98, Train Loss: 1.0251, Test Loss: 0.9653\n",
      "Epoch 45/98, Train Loss: 1.0198, Test Loss: 0.9641\n",
      "Epoch 46/98, Train Loss: 1.0094, Test Loss: 0.9671\n",
      "Epoch 47/98, Train Loss: 1.0186, Test Loss: 0.9620\n",
      "Epoch 48/98, Train Loss: 1.0053, Test Loss: 0.9615\n",
      "Epoch 49/98, Train Loss: 0.9880, Test Loss: 0.9608\n",
      "Epoch 50/98, Train Loss: 1.0044, Test Loss: 0.9625\n",
      "Epoch 51/98, Train Loss: 1.0268, Test Loss: 0.9624\n",
      "Epoch 52/98, Train Loss: 1.0204, Test Loss: 0.9643\n",
      "Epoch 53/98, Train Loss: 1.0151, Test Loss: 0.9647\n",
      "Epoch 54/98, Train Loss: 1.0243, Test Loss: 0.9656\n",
      "Epoch 55/98, Train Loss: 1.0047, Test Loss: 0.9625\n",
      "Epoch 56/98, Train Loss: 0.9782, Test Loss: 0.9629\n",
      "Epoch 57/98, Train Loss: 0.9825, Test Loss: 0.9613\n",
      "Epoch 58/98, Train Loss: 1.0124, Test Loss: 0.9614\n",
      "Epoch 59/98, Train Loss: 1.0090, Test Loss: 0.9615\n",
      "Epoch 60/98, Train Loss: 0.9810, Test Loss: 0.9613\n",
      "Epoch 61/98, Train Loss: 0.9917, Test Loss: 0.9600\n",
      "Epoch 62/98, Train Loss: 0.9845, Test Loss: 0.9616\n",
      "Epoch 63/98, Train Loss: 1.0090, Test Loss: 0.9645\n",
      "Epoch 64/98, Train Loss: 1.0079, Test Loss: 0.9633\n",
      "Epoch 65/98, Train Loss: 1.0033, Test Loss: 0.9628\n",
      "Epoch 66/98, Train Loss: 0.9824, Test Loss: 0.9628\n",
      "Epoch 67/98, Train Loss: 0.9683, Test Loss: 0.9653\n",
      "Epoch 68/98, Train Loss: 0.9670, Test Loss: 0.9631\n",
      "Epoch 69/98, Train Loss: 1.0027, Test Loss: 0.9610\n",
      "Epoch 70/98, Train Loss: 0.9764, Test Loss: 0.9617\n",
      "Epoch 71/98, Train Loss: 0.9921, Test Loss: 0.9645\n",
      "Epoch 72/98, Train Loss: 1.0038, Test Loss: 0.9636\n",
      "Epoch 73/98, Train Loss: 0.9998, Test Loss: 0.9614\n",
      "Epoch 74/98, Train Loss: 0.9799, Test Loss: 0.9594\n",
      "Epoch 75/98, Train Loss: 0.9917, Test Loss: 0.9583\n",
      "Epoch 76/98, Train Loss: 0.9837, Test Loss: 0.9613\n",
      "Epoch 77/98, Train Loss: 0.9849, Test Loss: 0.9620\n",
      "Epoch 78/98, Train Loss: 0.9963, Test Loss: 0.9645\n",
      "Epoch 79/98, Train Loss: 0.9728, Test Loss: 0.9652\n",
      "Epoch 80/98, Train Loss: 1.0043, Test Loss: 0.9700\n",
      "Epoch 81/98, Train Loss: 0.9663, Test Loss: 0.9702\n",
      "Epoch 82/98, Train Loss: 0.9982, Test Loss: 0.9677\n",
      "Epoch 83/98, Train Loss: 0.9571, Test Loss: 0.9656\n",
      "Epoch 84/98, Train Loss: 0.9670, Test Loss: 0.9660\n",
      "Epoch 85/98, Train Loss: 1.0011, Test Loss: 0.9656\n",
      "Epoch 86/98, Train Loss: 0.9734, Test Loss: 0.9650\n",
      "Epoch 87/98, Train Loss: 0.9911, Test Loss: 0.9675\n",
      "Epoch 88/98, Train Loss: 0.9662, Test Loss: 0.9670\n",
      "Epoch 89/98, Train Loss: 0.9726, Test Loss: 0.9642\n",
      "Epoch 90/98, Train Loss: 0.9927, Test Loss: 0.9645\n",
      "Epoch 91/98, Train Loss: 0.9813, Test Loss: 0.9615\n",
      "Epoch 92/98, Train Loss: 0.9666, Test Loss: 0.9626\n",
      "Epoch 93/98, Train Loss: 0.9926, Test Loss: 0.9607\n",
      "Epoch 94/98, Train Loss: 0.9964, Test Loss: 0.9631\n",
      "Epoch 95/98, Train Loss: 0.9709, Test Loss: 0.9618\n",
      "Epoch 96/98, Train Loss: 0.9679, Test Loss: 0.9630\n",
      "Epoch 97/98, Train Loss: 0.9616, Test Loss: 0.9661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:34:55,775] Trial 225 finished with value: 0.96293044090271 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 102, 'layer_1_size': 186, 'layer_2_size': 205, 'layer_3_size': 246, 'layer_4_size': 85, 'layer_5_size': 233, 'dropout_rate': 0.39988289360030754, 'learning_rate': 0.0001753539114097043, 'batch_size': 64, 'epochs': 98}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/98, Train Loss: 0.9561, Test Loss: 0.9629\n",
      "Epoch 1/5, Train Loss: 1.2470, Test Loss: 1.1522\n",
      "Epoch 2/5, Train Loss: 1.2360, Test Loss: 1.1497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:34:56,154] Trial 226 finished with value: 1.1481693983078003 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 123, 'layer_1_size': 49, 'layer_2_size': 199, 'layer_3_size': 61, 'layer_4_size': 172, 'layer_5_size': 227, 'dropout_rate': 0.4916294589926192, 'learning_rate': 5.0625984621572044e-05, 'batch_size': 64, 'epochs': 5}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Train Loss: 1.2416, Test Loss: 1.1461\n",
      "Epoch 4/5, Train Loss: 1.1874, Test Loss: 1.1461\n",
      "Epoch 5/5, Train Loss: 1.2058, Test Loss: 1.1482\n",
      "Epoch 1/72, Train Loss: 1.2407, Test Loss: 1.1098\n",
      "Epoch 2/72, Train Loss: 1.2083, Test Loss: 1.1023\n",
      "Epoch 3/72, Train Loss: 1.2222, Test Loss: 1.1043\n",
      "Epoch 4/72, Train Loss: 1.2091, Test Loss: 1.1021\n",
      "Epoch 5/72, Train Loss: 1.1929, Test Loss: 1.1061\n",
      "Epoch 6/72, Train Loss: 1.1880, Test Loss: 1.1034\n",
      "Epoch 7/72, Train Loss: 1.1821, Test Loss: 1.0979\n",
      "Epoch 8/72, Train Loss: 1.1786, Test Loss: 1.1027\n",
      "Epoch 9/72, Train Loss: 1.1250, Test Loss: 1.1003\n",
      "Epoch 10/72, Train Loss: 1.1433, Test Loss: 1.0987\n",
      "Epoch 11/72, Train Loss: 1.1650, Test Loss: 1.1019\n",
      "Epoch 12/72, Train Loss: 1.1755, Test Loss: 1.1002\n",
      "Epoch 13/72, Train Loss: 1.1348, Test Loss: 1.1008\n",
      "Epoch 14/72, Train Loss: 1.1486, Test Loss: 1.1003\n",
      "Epoch 15/72, Train Loss: 1.1765, Test Loss: 1.1019\n",
      "Epoch 16/72, Train Loss: 1.1381, Test Loss: 1.1040\n",
      "Epoch 17/72, Train Loss: 1.1780, Test Loss: 1.1005\n",
      "Epoch 18/72, Train Loss: 1.1186, Test Loss: 1.0932\n",
      "Epoch 19/72, Train Loss: 1.1086, Test Loss: 1.1000\n",
      "Epoch 20/72, Train Loss: 1.1524, Test Loss: 1.1003\n",
      "Epoch 21/72, Train Loss: 1.1394, Test Loss: 1.0982\n",
      "Epoch 22/72, Train Loss: 1.1452, Test Loss: 1.0970\n",
      "Epoch 23/72, Train Loss: 1.1651, Test Loss: 1.1007\n",
      "Epoch 24/72, Train Loss: 1.1452, Test Loss: 1.0986\n",
      "Epoch 25/72, Train Loss: 1.1756, Test Loss: 1.1025\n",
      "Epoch 26/72, Train Loss: 1.1727, Test Loss: 1.0929\n",
      "Epoch 27/72, Train Loss: 1.1207, Test Loss: 1.0961\n",
      "Epoch 28/72, Train Loss: 1.1134, Test Loss: 1.0968\n",
      "Epoch 29/72, Train Loss: 1.1187, Test Loss: 1.0970\n",
      "Epoch 30/72, Train Loss: 1.1653, Test Loss: 1.0936\n",
      "Epoch 31/72, Train Loss: 1.1683, Test Loss: 1.0949\n",
      "Epoch 32/72, Train Loss: 1.1405, Test Loss: 1.0920\n",
      "Epoch 33/72, Train Loss: 1.1741, Test Loss: 1.0994\n",
      "Epoch 34/72, Train Loss: 1.1408, Test Loss: 1.0922\n",
      "Epoch 35/72, Train Loss: 1.1395, Test Loss: 1.0920\n",
      "Epoch 36/72, Train Loss: 1.1591, Test Loss: 1.0903\n",
      "Epoch 37/72, Train Loss: 1.1796, Test Loss: 1.0882\n",
      "Epoch 38/72, Train Loss: 1.1471, Test Loss: 1.0920\n",
      "Epoch 39/72, Train Loss: 1.1850, Test Loss: 1.0863\n",
      "Epoch 40/72, Train Loss: 1.1099, Test Loss: 1.0865\n",
      "Epoch 41/72, Train Loss: 1.1296, Test Loss: 1.0866\n",
      "Epoch 42/72, Train Loss: 1.1187, Test Loss: 1.0954\n",
      "Epoch 43/72, Train Loss: 1.1570, Test Loss: 1.0919\n",
      "Epoch 44/72, Train Loss: 1.1594, Test Loss: 1.0954\n",
      "Epoch 45/72, Train Loss: 1.1403, Test Loss: 1.0872\n",
      "Epoch 46/72, Train Loss: 1.1437, Test Loss: 1.0881\n",
      "Epoch 47/72, Train Loss: 1.1114, Test Loss: 1.0913\n",
      "Epoch 48/72, Train Loss: 1.1457, Test Loss: 1.0890\n",
      "Epoch 49/72, Train Loss: 1.0810, Test Loss: 1.0906\n",
      "Epoch 50/72, Train Loss: 1.1337, Test Loss: 1.0925\n",
      "Epoch 51/72, Train Loss: 1.1477, Test Loss: 1.0913\n",
      "Epoch 52/72, Train Loss: 1.1105, Test Loss: 1.0920\n",
      "Epoch 53/72, Train Loss: 1.1290, Test Loss: 1.0937\n",
      "Epoch 54/72, Train Loss: 1.1756, Test Loss: 1.0937\n",
      "Epoch 55/72, Train Loss: 1.1413, Test Loss: 1.0912\n",
      "Epoch 56/72, Train Loss: 1.2040, Test Loss: 1.0879\n",
      "Epoch 57/72, Train Loss: 1.1364, Test Loss: 1.0855\n",
      "Epoch 58/72, Train Loss: 1.1244, Test Loss: 1.0881\n",
      "Epoch 59/72, Train Loss: 1.1138, Test Loss: 1.0922\n",
      "Epoch 60/72, Train Loss: 1.1427, Test Loss: 1.0925\n",
      "Epoch 61/72, Train Loss: 1.1147, Test Loss: 1.0868\n",
      "Epoch 62/72, Train Loss: 1.1253, Test Loss: 1.0940\n",
      "Epoch 63/72, Train Loss: 1.1527, Test Loss: 1.0929\n",
      "Epoch 64/72, Train Loss: 1.1435, Test Loss: 1.0890\n",
      "Epoch 65/72, Train Loss: 1.1440, Test Loss: 1.0914\n",
      "Epoch 66/72, Train Loss: 1.1157, Test Loss: 1.0882\n",
      "Epoch 67/72, Train Loss: 1.1358, Test Loss: 1.0937\n",
      "Epoch 68/72, Train Loss: 1.0905, Test Loss: 1.0894\n",
      "Epoch 69/72, Train Loss: 1.1142, Test Loss: 1.0905\n",
      "Epoch 70/72, Train Loss: 1.1272, Test Loss: 1.0891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:35:03,414] Trial 227 finished with value: 1.0876337587833405 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 153, 'layer_1_size': 57, 'layer_2_size': 215, 'layer_3_size': 53, 'layer_4_size': 233, 'layer_5_size': 219, 'layer_6_size': 166, 'dropout_rate': 0.37747576806895466, 'learning_rate': 1.7491939327544636e-05, 'batch_size': 64, 'epochs': 72}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/72, Train Loss: 1.0680, Test Loss: 1.0912\n",
      "Epoch 72/72, Train Loss: 1.1375, Test Loss: 1.0876\n",
      "Epoch 1/16, Train Loss: 1.2129, Test Loss: 0.9642\n",
      "Epoch 2/16, Train Loss: 1.2120, Test Loss: 0.9601\n",
      "Epoch 3/16, Train Loss: 1.1841, Test Loss: 0.9561\n",
      "Epoch 4/16, Train Loss: 1.1621, Test Loss: 0.9536\n",
      "Epoch 5/16, Train Loss: 1.1422, Test Loss: 0.9526\n",
      "Epoch 6/16, Train Loss: 1.1480, Test Loss: 0.9515\n",
      "Epoch 7/16, Train Loss: 1.1333, Test Loss: 0.9540\n",
      "Epoch 8/16, Train Loss: 1.1418, Test Loss: 0.9527\n",
      "Epoch 9/16, Train Loss: 1.1320, Test Loss: 0.9531\n",
      "Epoch 10/16, Train Loss: 1.1596, Test Loss: 0.9523\n",
      "Epoch 11/16, Train Loss: 1.1385, Test Loss: 0.9538\n",
      "Epoch 12/16, Train Loss: 1.1330, Test Loss: 0.9540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:35:04,283] Trial 228 finished with value: 0.9584275782108307 and parameters: {'num_hidden_layers': 6, 'layer_0_size': 113, 'layer_1_size': 168, 'layer_2_size': 194, 'layer_3_size': 83, 'layer_4_size': 223, 'layer_5_size': 112, 'dropout_rate': 0.38624942706439397, 'learning_rate': 9.183171210035325e-05, 'batch_size': 128, 'epochs': 16}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/16, Train Loss: 1.1296, Test Loss: 0.9548\n",
      "Epoch 14/16, Train Loss: 1.1186, Test Loss: 0.9568\n",
      "Epoch 15/16, Train Loss: 1.1072, Test Loss: 0.9580\n",
      "Epoch 16/16, Train Loss: 1.1204, Test Loss: 0.9584\n",
      "Epoch 1/49, Train Loss: 1.2154, Test Loss: 1.0713\n",
      "Epoch 2/49, Train Loss: 1.2604, Test Loss: 1.0632\n",
      "Epoch 3/49, Train Loss: 1.2156, Test Loss: 1.0469\n",
      "Epoch 4/49, Train Loss: 1.1784, Test Loss: 1.0476\n",
      "Epoch 5/49, Train Loss: 1.2519, Test Loss: 1.0497\n",
      "Epoch 6/49, Train Loss: 1.1826, Test Loss: 1.0478\n",
      "Epoch 7/49, Train Loss: 1.1788, Test Loss: 1.0393\n",
      "Epoch 8/49, Train Loss: 1.1298, Test Loss: 1.0391\n",
      "Epoch 9/49, Train Loss: 1.1532, Test Loss: 1.0410\n",
      "Epoch 10/49, Train Loss: 1.1873, Test Loss: 1.0315\n",
      "Epoch 11/49, Train Loss: 1.1631, Test Loss: 1.0337\n",
      "Epoch 12/49, Train Loss: 1.1104, Test Loss: 1.0328\n",
      "Epoch 13/49, Train Loss: 1.1464, Test Loss: 1.0322\n",
      "Epoch 14/49, Train Loss: 1.1175, Test Loss: 1.0318\n",
      "Epoch 15/49, Train Loss: 1.1331, Test Loss: 1.0325\n",
      "Epoch 16/49, Train Loss: 1.1175, Test Loss: 1.0324\n",
      "Epoch 17/49, Train Loss: 1.1002, Test Loss: 1.0312\n",
      "Epoch 18/49, Train Loss: 1.1250, Test Loss: 1.0325\n",
      "Epoch 19/49, Train Loss: 1.1384, Test Loss: 1.0314\n",
      "Epoch 20/49, Train Loss: 1.1452, Test Loss: 1.0342\n",
      "Epoch 21/49, Train Loss: 1.1097, Test Loss: 1.0327\n",
      "Epoch 22/49, Train Loss: 1.1375, Test Loss: 1.0377\n",
      "Epoch 23/49, Train Loss: 1.1227, Test Loss: 1.0366\n",
      "Epoch 24/49, Train Loss: 1.1123, Test Loss: 1.0362\n",
      "Epoch 25/49, Train Loss: 1.1153, Test Loss: 1.0423\n",
      "Epoch 26/49, Train Loss: 1.0337, Test Loss: 1.0369\n",
      "Epoch 27/49, Train Loss: 1.0903, Test Loss: 1.0328\n",
      "Epoch 28/49, Train Loss: 1.0749, Test Loss: 1.0348\n",
      "Epoch 29/49, Train Loss: 1.1067, Test Loss: 1.0344\n",
      "Epoch 30/49, Train Loss: 1.0594, Test Loss: 1.0324\n",
      "Epoch 31/49, Train Loss: 1.1137, Test Loss: 1.0307\n",
      "Epoch 32/49, Train Loss: 1.0760, Test Loss: 1.0334\n",
      "Epoch 33/49, Train Loss: 1.0732, Test Loss: 1.0333\n",
      "Epoch 34/49, Train Loss: 1.0836, Test Loss: 1.0343\n",
      "Epoch 35/49, Train Loss: 1.0919, Test Loss: 1.0328\n",
      "Epoch 36/49, Train Loss: 1.0738, Test Loss: 1.0329\n",
      "Epoch 37/49, Train Loss: 1.0401, Test Loss: 1.0332\n",
      "Epoch 38/49, Train Loss: 1.0587, Test Loss: 1.0326\n",
      "Epoch 39/49, Train Loss: 1.0492, Test Loss: 1.0325\n",
      "Epoch 40/49, Train Loss: 1.0645, Test Loss: 1.0309\n",
      "Epoch 41/49, Train Loss: 1.0724, Test Loss: 1.0319\n",
      "Epoch 42/49, Train Loss: 1.0682, Test Loss: 1.0326\n",
      "Epoch 43/49, Train Loss: 1.0507, Test Loss: 1.0326\n",
      "Epoch 44/49, Train Loss: 1.0630, Test Loss: 1.0322\n",
      "Epoch 45/49, Train Loss: 1.0543, Test Loss: 1.0312\n",
      "Epoch 46/49, Train Loss: 1.0512, Test Loss: 1.0326\n",
      "Epoch 47/49, Train Loss: 1.0590, Test Loss: 1.0309\n",
      "Epoch 48/49, Train Loss: 1.0485, Test Loss: 1.0310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-12 21:35:08,212] Trial 229 finished with value: 1.0299981236457825 and parameters: {'num_hidden_layers': 7, 'layer_0_size': 133, 'layer_1_size': 178, 'layer_2_size': 221, 'layer_3_size': 42, 'layer_4_size': 188, 'layer_5_size': 215, 'layer_6_size': 174, 'dropout_rate': 0.4867337985879555, 'learning_rate': 0.00013808463208838165, 'batch_size': 64, 'epochs': 49}. Best is trial 44 with value: 0.7144440561532974.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/49, Train Loss: 1.0735, Test Loss: 1.0300\n",
      "Best trial:\n",
      "  Value: 0.7144440561532974\n",
      "  Params:\n",
      "    num_hidden_layers: 6\n",
      "    layer_0_size: 133\n",
      "    layer_1_size: 148\n",
      "    layer_2_size: 182\n",
      "    layer_3_size: 121\n",
      "    layer_4_size: 235\n",
      "    layer_5_size: 131\n",
      "    dropout_rate: 0.3744673729438832\n",
      "    learning_rate: 2.9805716870829137e-05\n",
      "    batch_size: 64\n",
      "    epochs: 99\n"
     ]
    }
   ],
   "source": [
    "# Creating function for training and evaluating the model\n",
    "def train_and_evaluate_model(model, train_loader, test_loader, criterion, optimizer, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            test_loss = 0.0\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item()\n",
    "            test_loss /= len(test_loader)\n",
    "        \n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
    "    \n",
    "    return test_loss\n",
    "\n",
    "# Creating function for selection of best hyperparameters \n",
    "def objective(trial):\n",
    "    # Defining hyperparameters\n",
    "    num_hidden_layers = trial.suggest_int('num_hidden_layers', 1, 10)\n",
    "    hidden_layer_sizes = [\n",
    "        trial.suggest_int(f'layer_{i}_size', 32, 256)\n",
    "        for i in range(num_hidden_layers)\n",
    "    ]\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    epochs = trial.suggest_int('epochs', 5, 100)\n",
    "    input_size = 14\n",
    "\n",
    "    # Creating a model and defining of optimizer\n",
    "    model = FlexibleRegressionNN(input_size, hidden_layer_sizes, dropout_rate)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Dataloader\n",
    "    train_data = torch.randn(1000, input_size), torch.randn(1000, 1)\n",
    "    test_data = torch.randn(200, input_size), torch.randn(200, 1)\n",
    "    train_loader = DataLoader(TensorDataset(*train_data), batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(TensorDataset(*test_data), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Training launch and return the test loss\n",
    "    return train_and_evaluate_model(model, train_loader, test_loader, criterion, optimizer, epochs)\n",
    "\n",
    "# Settings of searching by Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=230)\n",
    "\n",
    "# Getting the best hyperparameters\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Value: {trial.value}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
